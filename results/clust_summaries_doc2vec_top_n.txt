paper_1	Tweets and other updates have become so important in the world of information and communication because they have a great potential of passing information very fast. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. Its objective is to get groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups  [1] . It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . Some of them include summarization, compression and k-nearest neighbor which localizes search to one or a small number of clusters. Good clustering methods produce high quality clusters with either a high intra-class similarity within clusters or a low inter-class similarity between clusters. In incremental prototyping, the whole requirements are broken down into building blocks which are incremented each time a new component is integrated based on an overall design solution. The first task was to retrieve details of each of the students from their twitter accounts using an extension script which is part of the twitter Application Programming Interface. The second task involved identifying the right kind of data to use for training the expected prototype as well as testing it. This involved picking the details of a new student from twitter and trying to predict the class hence group that he should join. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . They also confirmed that the system simplified the process of group formation and made inclusivity of distant students in the groups possible. This is summarized in the chart below. Through the study, it can be underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. They therefore come with a lot of challenges including time wastage. Through social media a better and more efficient way can be used to enable online learning generally and group formation specifically. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. It was able to extract tweets from various social media accounts based on a given hash tag (task) and then pass them to a Na√Øve Bayes classifier as input. The classifier was also able to assign other or new users groups also according to their tweets and the learning that the system had undergone. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Through the study, it was underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. They therefore come with a lot of challenges including time wastage. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_2	This creates unwanted congestion during peak hours, loss of man-hours and eventually decline in productivity. The developed system uses a microcontroller of PIC89C51 microcontroller duly interfaced with sensors. Junction timings allotted are fixed. The junction is a link to three roads which include; Effurun-Sapele road, Jakpa road, and PTI road. As a result of this a lot of time is wasted in the process. In order to overcome the aforementioned problem, this research adopted a density based approach in controlling vehicular traffic. The signal timing changes automatically on sensing the traffic density at the junction. The sensors used in this project are infra-red (IR) and photodiodes. However, this system lacked inbuiltmechanism for controlling vehicular traffic based on density. Junction timings allotted are fixed. Raspberry pi is used as a microcontroller which provides the signal timing based on the traffic. An intelligent traffic lights control system using a Fuzzy Logic approach was developed by  [5] . Data was analyzed and presented using descriptive statistics; tables and graphs by using excel 2003. The top down design approach was adopted here. The complete circuitry is operated with TTL logic level of 0-5V. It comprises of a 0V to 12V transformer which is required to step down the 220V AC supply to 12V AC. This is converted by a bridge rectifier to a dc voltage. Below are the ratings of the transformer 0.7 volts for silicon diodes). For this circuit, V peak rectified = 16.67-2(0.7) =15.27v dc This voltage is the input voltage of the capacitor. A capacitor is needed to effectively carry out the filtering of the rectified AC signal to eliminate ripples  [9]  and can thus be calculated using the equation below. For convenience, a capacitor of 1000uF is used. Figure 4  shows the circuit diagram of the system. The Transformer steps down the 220 v AC supply to 12 v AC. The time delay in the traffic signal is set based on the density of vehicles on the roads. The IR sensors are used to sense the number of vehicles on the road. The Vero board is also called a strip board. And after soldering each unit, continuity test was carried out to ensure that proper soldering was done. The PIC Microcontroller was programmed using Embedded C language. The codes are as shown in the Appendix. Simulation was done via Proteus software. In order to address this problem, an advanced traffic congestion control system is required. One of such systems is the automatic signaling using IR sensors and Microcontroller.
paper_3	In the beginning, a related work on the architecture trends of Web-based AEHSs is presented. (b) Adaptive Presentation: It adapts the content presented in each hypermedia node according to specific characteristics of the learner. AEHS MATHEMA (Meta-Adaptation Technology Hypermedia for Electro-Magnetism Approach) combines the constructivist, socio-cultural and meta-cognitive teaching model and supports personalized and collaborative learning. The teaching strategies are based on  [9]  learning cycle and learning style model. The aim of this article is to highlight the main contributions of the architecture of the MATHEMA in the improvement of the functionality of AEHSs. The key idea is the decentralization of their functions. The rest of the paper is organized as follows: In the section 2 related works for various models of architecture of AEHSs is presented. In the section 3 the architecture of the AEHS MATHEMA is presented in detail. For the AEHSs various models of architecture have been developed so far on which the designers of these systems are based on. In this section some models that are most suited to the MATHEMA system are presented. The AHAM architecture model proposed by  [6]  is shown in the  Figure 1 . The AEHSs fit so nicely in this model. When designing a course it is important to first list the concepts. The next thing to do is to determine dependencies between the concepts. This leads to a structure of prerequisite relationships. The prerequisites are most common in educational material. The rules together form the adaptation model in AHAM. This is done by means of a teaching model which consists of pedagogical rules. AEHSs applications need to maintain a permanent user model. [8] presented a similar architecture in the Proper system based on the AHAM model. Proper's architecture is a combined architecture of SCORM LMS and AEHS. Educational content can be either SCO or Asset. Thus its architecture is a typical of a SCORM compliant LMS. They use the Apache Tomcat 5.5 as Web and application server and the MySQL 5 as database server. The adaptive navigation techniques have been applied to it via Java servlets and Java Server Pages (JSP). In the  Figure 3  the architecture of the WELSA  [14]  system is presented. [12] proposed a framework for AEHS (  Fig. 4 ) based on JSP, Java servlets, and Javabeans. The Interaction Analyzer is responsible for acquiring information on learner's behavior. The Presentation Generator requests a composition of the presentation to the JavaServer Pages. ULUL-ILM  [1]  is an AEHS that focuses on student's learning styles. It enables to recognize the student's learning style automatically in real time by means of Multi Layer Feed-Forward Neural network (MLFF). The result of that analysis is called domain model. For supporting features, it uses the (JSP) pages, Java servlets, JavaBeans, JavaScripts, and Apache Struts technologies. The content loaded to the MySQL database is accessed via JDBC API. A servlet is a Java class that implements the Java servlet API, a protocol to respond to HTTP requests. These servlets are complete programs that are capable of creating JSPs. This allows for much more flexibility in creating the page than XML. Oddly enough, servlets do not face any of the problems faced by classical CGI programming because a servlet has a lifecycle. Servlets dynamically create JSP pages according to student requirements. JavaBeans technology comes into play. (3) Protects your intellectual property by keeping source code secure. The Apache Struts is a formalized framework for the architecture style design pattern Model View Controller (MVC). Figure 5  shows the architecture of the AEHS MATHEMA. Bellow the operation of the units will be presented and analyzed. A component of an adaptive educational system is the representation of knowledge. The domain knowledge is structured in a way that supports the ability of the system to choose the educational material, depending on the learner's requirements and current status. The domain knowledge of the AEHS MATHEMA is the basis of the system's adaptation. Content is on separate pages, enabling it to be re-used. Student model contains information about student characteristics that allows the system to make adjustments using these characteristics. The model that supports the AEHS MATHEMA is the overlay model. It also takes into account the decisions of the data collection and monitoring unit of the learner's actions. This is an innovation on the architecture of AEHSs. It also offers additional information about the navigation techniques that it supports. knowledge of the cognitive goal. Figure 7  shows a snapshot of the meta-adaptation result. A snapshot of a meta-adaptation result. The pages displayed to the learner are dynamically generated. This module is responsible for obtaining information about the data the learner entries in the system and for monitoring his or her actions (interaction with the system). Figure 9  shows a snapshot of the page responsible for selecting preferences of the learner. MATHEMA supports the learner to find the most suitable peer for the formation of a collaboration team. Figure 10  shows a snapshot of adaptive group formation in the MATHEMA. This module is responsible for monitoring and supporting synchronous communication between learners via a chattool. Assessment of the pedagogical and technological aspects of the AEHS MATHEMA has been carried out. The evaluation of the system was carried out by students of the Department of Informatics and Telecommunications of the University of Athens, Greece. The implemented AEHSs so far use various techniques to implement their functions. The architecture of AEHSs becomes more complex as more and more functions are implemented. The key idea is the decentralization of their functions.
paper_21	The proposed scheme ZSISMP is validated on all BCH codes of known minimum distance. An efficient solution of this problem is the use of error correcting codes. The error-correcting capability of these codes is directly related to their minimum distance. For these codes, only a lower bound is known and the minimum distance is known only for some lengths and special cases  [2-3-4-5-6-7-8] . The remainder of this paper is organized as follows: The next section presents the main related works. The section 3 presents the proposed scheme ZSISMP. The section 4 presents the main results. Determining the minimum distance of BCH codes is an important, but difficult, problem. This section summarizes the most important ones. The use of this method has finished the table of BCH codes of length 255. In  [10] , Augot and Sendrier found idempotent codewords of minimum weight for several primitive narrow-sense BCH codes. In  [22] , Aylaj and Belkasmi improve the classical Simulated Annealing presented in  [16] . This section presents the proposed scheme for finding the lowest weight in BCH codes. For finding the minimum distance of BCH codes. Output: -d as estimated minimum distance of BCH (n, k, Œ¥) The table 2 summarizes the obtained results. These results demonstrate that the proposed scheme outperform greatly the famous Zimmermann algorithm. The table 3 presents a comparison between the proposed scheme and MIM-RSC method  [24] . In this paper, we have proposed a new efficient scheme to find the minimum distance for large BCH codes. The experimental results show that the proposed scheme outperforms several known powerful techniques.
paper_31	From 1990 to 2017, the morbidity of chronic diseases with high incidence in China had been increasing continuously  [1] . According to China National Health and Nutrition Big Data Report 2018, 20% of Chinese suffer from chronic diseases, and chronic disease mortality was 86%. Therefore, it is crucial to effectively prevent and control chronic diseases. With the development of Internet, people's life has been profoundly changed. Since the end of last century, some Chinese hospitals and universities have carried out some researches in this field. For example, the General Hospital of the People's Liberation Army (PLAGH) communicated and discussed some diseases with foreign experts through satellites. For example, some domestic hospitals use Bluetooth technology to monitor the ward room. In the same year, American scholar Jutra had founded teleradiology. Therefore, the word "Telemedicine" arose. At the end of the last century, the rapid development of semiconductor technology drove the revolution of information technology. This is the second stage of the development of WITMED. During this stage, information technology was connected with medical health care more closely. With the great improvement of the data transmission capability, rapid development in telemedicine consultation and distance transmission of medical images occurred  [4] . At this stage, telemedicine had made considerable progress. This is the third stage of the development of WITMED. Kumar establishes an enhanced shortcut tree routing-based geographic location (ESTRBGL) protocol  [7] . Haji Bagheri Fard present methods based on deep learning algorithms to achieve high classification accuracy  [8] . Zigbee has been implemented on the Health Care Profile. What's more, the coverage of 4G network and the transmission distance of WIFI are often easy to cause the loss of transmission data. (2) The problem of power consumption. (3) The problems of unitary monitoring data. Chronic diseases patients are always required to be monitored for multiple vital signs simultaneously. (4) The problem of data processing. Finally, the processed data will be sent back to patients and their family doctors. The traditional prevention and treatment greatly increase the economic cost and time cost of patients. The system architecture diagram is as follow: Fast and correct data acquisition is the basis of the platform's efficient operation. The process of reading and transmitting data is a loop. The data acquisition structure charts The related knowledge base in  Figure 3  can be regarded as an expert system. Data analysis roadmap is shown as follows: Therefore, it is obvious that the matching degree is a fraction smaller than 100%. Finally, the request is inserted into the message queuing sequence by the matching degree. Definition 2: let {X1, Y1}, {X2, Y2} is concept form of {X, Y}, {X2, Y2b {X1, Y1}. Definition 3: The concept similarity is calculated as:  formula_1 Patients' health information involves their privacy. Therefore, it is crucial to guarantee the security of data transmission in the process of information transmission. This system adopts AES (Advanced Encryption Standard) encryption algorithm which is widely used at present. The calculation is shown as  Figure 7 : Simulation and test show that the method adopted in this paper is correct. Compared with the data presentation at present, it is more intuitive. The intelligent system realizes data acquisition, data encryption and processing, and big data analysis.
paper_38	The study adopted the descriptive research design. Pearson Product Moment Correlation (PPMC) Coefficient and Multinomial Logistics Regression (MLR) were the statistics used to answer the four research questions used. Education is an essential issue regarding the development of any country in the world. It is a progressive development of knowledge and skills of examinees through stages of teaching and learning at various levels  [1] . The number of undergraduate population in Nigerian Universities has increased from 103 in 1948 to an estimated population of 600,000 in 2018  [4] . There have been various perspectives presented by researchers and psychologists about what is academic performance and its importance. The CGPA score takes into consideration students' tests, assignments, practicals, examinations and sometimes lecture attendance. Formula 1 is used for calculating the CGPA. As such, the CGPA is considered to be a good predictor indicator of a student's academic performance. The rest of this paper is organised as follows: section 2 is the review of related literature, section 3 presents the methodology, section 4 discusses the results obtained, and the last section presents the conclusion and recommendation. Over the years in Nigerian tertiary institutions, there has been rife with complaints about students' poor academic performance. The objectives of this study are to: i. Investigate the relationship between the students' performance of their entry qualification and the academic performance in the 100 level CGPA examinations. The following research questions directed the study: 1. It is a yardstick that is used to ascertain the competences of a student from which his abilities could be determined. There are a lot of definitions of students' performance based on previous works of literature. Reference  [9]  stated that students' performance could be obtained by measuring the learning assessment and co-curriculum. However, most of the studies mention graduation being the measure of students' success. Several studies have criticised the use of UTME and PUTME as an imperfect instrument for predicting academic performance of students. Reference  [11] , in his study, monitored the performance of science education students admitted through Post UME screening in 2005/2006 academic session. A sample of 214 students records was used for data collection. The records of students who graduated in the 2009/2010 and 2010/2011 academic sessions from seven faculties were obtained using the stratified random sampling technique. Their results revealed that the correlations coefficient between PUTME and CGPA for the four departments were negative/low, positive/low and positive/moderate coefficients. The author further recommended the need for the PUTME exercise to be strengthened to have a fruitful admission of candidates. He explained that there are two types of ex-post facto research designs namely the correlational and the casual comparative. The Faculty of Science consists of nine undergraduate B. Sc. The sample distribution is as shown in  Table 1 . The session with the largest sample size was 2014/2015 with 710 (21.8%) students. The semester examinations were mostly essay type questions. However, the OL results obtained from the database was for students admitted during 2010/2011 to 2014/2015 academic sessions. The total score for five relevant subjects in OL is then computed and coded together with the UTME and PUTME scores which are as shown in  Table 3 . The coding for the CGPA is also shown in  Table 4 . It was used in this research study. The data were regrouped and analysed by academic session and programme of study. PPMC is used to determine the degree of relationship between two sets of variables and compute the strength of association between the variables  [19] . There are three types of linear relationship that may exist between these two variables namely positive linear correlation, negative linear correlation and no correlation. In other words, MLR is used to predict a nominal dependent variable given one or more independent variables. MLR can have interactions between nominal and continuous independent variables to predict the dependent variable. The Pearson Correlation analysis was carried out to find out if there exists a strong positive correlation between OL and CGPA, UTME and CGPA and PUTME and CGPA. For the Computer Science programme, as shown in  Table 5 , the correlation coefficient indicated a low negative correlation in UTME-CGPA (-0.092) and PUTME-CGPA (-0.001) and a low positive correlation for OL-CGPA (0.072). Similarly, there exists a low negative correlation in UTME-CGPA (-0.082) and PUTME-CGPA (-0.038) and a low positive relationship in OL-CGPA (0.089) for the Mathematics programme. Table 6  is the likelihood ratio test results for the Computer Science, Mathematics and Physics degree programmes. Likelihood Ratio Tests is a statistical test of the goodness-offit between two models. Table 7  shows the results of the Parameter estimates, which is also called coefficients, for the Multinomial Logistic Regression (MLR) for each degree programme. For the Computer Science programme, the slopes (B) of OL in all the CGPA categories are positive. Besides, the slopes (B) of UTME in the CGPA categories of 'Pass' and '1 st Class' are positive while the rest are negative. Table 8  shows the summary of correlations coefficient between OL-CGPA, UTME-CGPA and PUTME-CGPA aimed at all the degree programmes for the academic sessions ranging from 2010/2011 to 2014/2015. PPMC was used to analyse the data for this research question. The summary of  Table 8  shows the correlations of OL and CGPA scores, UTME and CGPA scores and PUTME and CGPA scores for all the programmes used for the analysis. In the 2010/2011 academic session, the correlation coefficient indicated a low positive correlation in OL-CGPA (0.198), UTME-CGPA (0.189), and PUTME-CGPA (0.232). In 2011/2012, OL-CGPA (-0.114) and PUTME-CGPA (-0.043) display low negative correlation while UTME-CGPA (0.114) has low positive correlation. In 2013/2014, there exist a low positive association for OL-CGPA (0.061), UTME-CGPA (0.056), and PUTME-CGPA (0.038). The OL-CGPA (-0.071), UTME-CGPA (-0.090), and PUTME-CGPA (-0.040) in the 2014/2015 session signifies a low negative correlation. Table 9  is the likelihood ratio test results for all academic sessions ranging from 2010/2011 to 2014/2015. Table 10 , on the other hand, shows the results of the Parameter estimates for the MLR for each academic session. A Multinomial Logistic Regression (MLR) was performed to model the relationship between the predictors and membership in the CGPA categories for each academic session. Table 10 , the traditional 0.05 criterion of statistical significance was also used. For the students admitted during the 2010/2011 academic session, the slopes (B) of OL in all the CGPA categories are positive. Conclusively, the slope (B) of PUTME in the CGPA category of '3 rd Class' is positive and the rest negative. This indicates that the relative strength of UTME is higher than those with a CGPA category of 'Fail' and the rest of the CGPA categories are negative which indicates otherwise. This predictor holds true especially for students who are in the CGPA category of '2 nd Class Lower' and '1 st Class' respectively.
paper_57	This paper attempts to overcome stagnation problem of Ant Colony Optimization (ACO) algorithms. This problem has been addressed by Genetic operations (GO) incorporated into ACO framework. [2]  as Ant system (AS) algorithm. The aim is to find the shortest path. where œÅ ‚àà (0,1) is the pheromone persistence (1 -œÅ is evaporation rate) and m is the number of ants. An ant in each node has to make a decision which arc to take. Let's mention ant colony system (ACS) with pseudo-random proportional rule  [3]  in which random uniformly distributed variable q ‚àà (0,1) is compared with a tunable parameter formula_5 else the probability selection rule (3) is applied; random selection applied to AS rank  [4]  where random selection rate r is the probability of random selection and represents a user parameter which adjusts the balance between exploration and exploitation; prevention of quick convergence (i) and stagnation avoidance (ii) mechanisms applied to AS  [5] . If q ‚â• p k ij (t), then choose the next node randomly. Genetic algorithms (GA) were proposed by  Holland (1975) . The original GA is known as simple genetic algorithm (SGA). Each component is adapted in order to provide feasible solution for ACO algorithm. The population size is given by the number of genomes, i.e. Mutation (ii) mimics random gene changes. If more such nodes occur, random selection is applied. If more of such nodes exist, random selection is applied. At first mutation is applied. If mutation is not feasible, another node is chosen. After all mutation operations are performed, crossover operations are applied. Parent strings are random selected. If crossover operation is not feasible, another second string is selected. Genetic operations do not have to be necessarily feasible. Feasibility of genetic operations depends on the graph and generated tours. The above described ACO GO algorithm has been tested on a random generated graph. Test graph is a symmetrical multi-graph with 80 nodes and 300 arcs  (Fig. The task is to find the shortest path between start node n s = 1 and end node n e = 80. For each setting 500 trials were performed. For better results representation three graphs are provided. To prevent interference, no mutation operation was allowed. The results vary  (Fig. 8) ; the highest output was gained for 60% of crossover rate. Beyond 60% threshold ants foraging behavior is suppressed by crossover overload. This is caused by the search space dimension. It is too large for ten ants to meet. GO does not affect the length of the search process. The mean value of the cycle when the best value was found is 109.081 with standard deviation 2.617. It has been proved that genetic operations increase ACO algorithm performance. Limit of crossover is 60% of crossover rate. Mutation operation causes better results than crossover operation. The higher amount of mutation operations the higher the performance gain is. No limit for amount of mutation operation was found during the simulation. The results are promising; GO improves ACO algorithm performance more than twice.
paper_78	Each formation has its own composition, structure, distribution range and environment, and all of them should be protected effectively. It is applicable in ecology because the description of ecological systems is not always possible in terms of a binary approach  [4] . Like licorice, most medicinal plants are harvested in the wild and the extended use has led to some medicinal species endangered and tending to be extinct, and the protection of such medicinal species has been urgent  [12, 13] . Classification is the process of dividing plots into clusters (communities) so that plots in the same cluster are as similar as possible, and plots in different clusters are as dissimilar as possible. The Fuzzy C-means clustering attempts to divide a finite collection of n elements (plots) formula_0 into a collection of C fuzzy clusters with respect to some given criterion. Given a finite set of data matrix (species √ó plots), the method calculates a list of C cluster centres  formula_1 formula_2 Where i = 1, 2, ‚Ä¶, N = the number of plots; j = 1, 2, ‚Ä¶, C = the number of clusters; U = {U ij } = the matrix of membership values, U ij is the membership of plot i in cluster j; V is a matrix of cluster centers; m is fuzzifier which determines the level of cluster fuzziness (1 ‚â§ m < ‚àû). (dA ij ) 2 is the distance index: formula_3 X i is the vector of attribute measurements in plots, usually a vector of ordination scores; V j is the centre of cluster j, if A is a unit matrix, then formula_4 Based on the algorithm above, the procedure of fuzzy C-means clustering is as follows: (1) Selecting a common ordination method, we use Detrended Correspondence Analysis (DCA), and do ordination analysis for species data. For a plot, any value can be given as its membership value in cluster j, but the sum of memberships for a plot must be equal to 1:  2  is calculated using equations (2) and (3). formula_5 formula_7 Based on the new membership values U, we go back to the fourth step and calculated the next turn V j , (dA ij )  2  and U ij iteratively, and until the membership values become approximately stable. Based on a general survey of Glycyrrhiza uralensis and its community distribution, five study regions, Chifeng (in inner Mongolia), Hengjinqi (in inner Mongolia), Minqin (in Ganshu), Aletai (in Xinjing) and Kashi (in Xinjing), were selected as sampling sites  [7] . Twenty plots of 5 m √ó 5 m were established randomly at each site which is over 40 ha in area. The coverage, mean height, individual number for shrub and herb species was measured in each plot. The community has a total cover of 75%, a shrub layer cover of 20% and an herb layer cover of 65%. The average cover of Glycyrrhiza uralensis in this community is 25% with a density of 3600 ha -1 . Its disturbance intensity is medium and heavy. The community has a total cover of 70%, a shrub layer cover of 5% and an herb layer cover of 70%. The average cover of Glycyrrhiza uralensis in this community is 35% with a density of 5700 ha -1 . The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 60%. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 59500 ha -1 . Its disturbance intensity is medium and heavy. The community has a total cover of 65%, a shrub layer cover of 10% and an herb layer cover of 60%. The average cover of Glycyrrhiza uralensis in this community is 37% with a density of 4900 ha -1 . It is distributed from 350 to 650 m in hills with slope 15 -35¬∞ in sunny and semi-sunny slope and sandy soil. Its disturbance intensity is heavy. The community has a total cover of 60%, a shrub layer cover of 5% and an herb layer cover of 58%. The average cover of Glycyrrhiza uralensis in this community is 33% with a density of 5500 ha -1 . It is distributed from 400 to 700 m in hills with slope 10 -30¬∞ in sunny and semi-sunny slope and sandy soil. The community has a total cover of 75%, a shrub layer cover of 10% and an herb layer cover of 70%. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 6000 ha -1 . It is distributed from 400 to 750 m in hills with slope 15 -35¬∞ in sunny and semi-sunny slope and sandy soil. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 65%. The average cover of Glycyrrhiza uralensis in this community is 29% with a density of 4100 ha -1 . It is distributed from 400 to 800 m in hills with slope 20 -35¬∞ in sunny and semi-sunny slope and sandy soil. The community has a total cover of 80%, a shrub layer cover of 10% and an herb layer cover of 75%. The average cover of Glycyrrhiza uralensis in this community is 30% with a density of 4000 ha -1 . This study proved that fuzzy C-means clustering is fully usable in classification analysis of plant communities  [1, 19] . These further confirm that fuzzy C-means clustering is an effective technique in vegetation analysis  [28, 29] .
paper_96	With the rapid development of China's transportation, the frequency of traffic accidents is also high. This not only restricted the development of China's transportation greatly, but also threatened to people's safety seriously. In fact, this "warning system of embedded electronic guide dog avoiding obstacles" is similar to a car anti-collision avoidance system. In this way, the design of obstacle-avoiding early warning system of embedded electronic guide dog has the following advantages. It has a simple design, and small volume, so that it is convenient to carry the "dog". Its operating system is thought from the perspectives of practical application, development tool, instantaneity, technical services, and price, etc. [3]  The alarm system of guide dog adopts the combination of light and sound. Sound will produce auditory stimulus for the blind. The hardware of the obstacle-avoiding early warning system of embedded electronic guide dog can reference the Figure 1:  Figure 1 : Schematic diagram of hardware system structure  [4] The techniques of information collection are ultrasonic wave, laser, infrared ray, machine vision and interactive method. Information processing is mainly to analyze the collected information, usually using ARM, MCU and DSP microprocessor, etc. The full name of ARM is Advanced RISC Machines. It has a variety of merits, like small size, high performance, low power consumption, and cheap cost. It extensively uses the registers with a fast speed of instruction execution. And its way to addressing is much easier and more flexible, and its operating efficiency is high. Also, it could complete most of the data manipulation in the register. At present, the embedded RTOS (Real Time Operate System) mainly includes two categories--commercial and free. These are the most remarkable features that uCLinux owned. In addition, the design of obstacle-avoiding early warning system of embedded electronic guide dog also makes a full use of USB. The reference model is shown in  Figure 2 . of USB reference model  [4] The traffic safety problem of the blind is an urgent issue to solve. This humanized technology innovation is the embodiment of environmental science and technology aesthetics theory in the field of science and technology innovation practice.
paper_134	Olive oil is one of the most important agricultural crops due to its digestive properties and economic status. However, olive oil production is a costly process which causes an expensive price of the final product. Data mining takes advantage of the progress made in artificial intelligence and statistics. Detecting the purity of different materials can be done in a variety of ways  [3] . These methods are very damaging, costly and time-consuming. In recent years, nondestructive methods have been considered in purification. Dielectric properties are one of the most important physical properties of agricultural and food products. One of the main reasons for this low consumption is the high price of this oil. Also, processing of data and results were presented in fourth section  [11] . (2010) conducted a research on olive oil for detecting adulteration using a dielectric spectrum. They also used the partial least squares model (PLS) to detect oil falsification. In addition, PCA has been used to classify virgin oil samples, separately from fake oils. The results of the PLS calibration model showed good results for the adulterated oils with soybean oil as a counterfeit oil. Soltani et al (2010) used a non-destructive capacitive method to detect the amount of banana fruit juice. The relationship between the rates of fruit dielectric constant in a frequency of 1 kHz to 10 MHz was investigated. The results showed a significant difference between DC / DV ratio during storage period. The correlation coefficients and the mean square error of the statistical indexes were 0.98 and 0.0006, respectively. 70% of the data was for network training, 15% for validation and 15% for the network testing. In this study, a recent study has been carried out to identify the authentication of olive oil. In this article the experiment was done by olive, sunflower, Canola and corn oil. Samples of olive oil provided from Khorramshahr Oil Company and produced at Rudbar oil plant located in Manjil. The samples of sunflower oil, canola oil and corn oil which are known as adulterated oils were also obtained from national markets. The device used consists of the Arduino board, ICL8083 and AD8302. The USB port is used to communicate or send and receive information between the device and the computer. The Arduino board is used on the Uno device, which uses the Atmega2560 microcontroller. When the microcomputer's components are placed in a chip and put together, a microcontroller is created. One of the new ideas in identifying and categorizing patterns is a backup vector machine or SVM. In this study, coarse function was used to regression test data. Figure 4  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn. Figure 5  shows the results of adulterated oil boosted tree regression. The results were predicted and modeled using regression methods. In this study, Quadratic function was used to regression test data. In the current research, three different techniques were applied to predict olive oil adulterated.
paper_139	At the present time web contains many indistinguishable documents. Web mining is categorized into three group Web Content mining  [6] , Web structure mining, Web usage mining. Traditional web mining algorithms handle with structured document  [7] [8] [9] [10] [11] [12] [13] [14] [15]  than the advanced methodology of mining algorithm can dealthe entire heterogeneous document comprises of images  [9] , graphs, videos  [16] , etc. A query is searched in a web search tool to recover some significant and required data for the client, either the search query is known or unintelligible to the client, it generally to reply with relevant data rather than redundant, however we can't guarantee that the reply for the query about the significance and redundancy. The Document retrieved must follow some constraints which have less time & space requirements, based upon the criteria the extracted web document must be preprocessed, for preprocessing & information selection, need to apply some techniques such as stop word removal, Stemming of word, phrasing, normalization of tokens. Step2: Pick the minimum degree vertex 'v' in the set-up and include in the set U. Step3:While U doesn't include all vertices Step3A: Include the entire isolated vertex which is adjacent to the vertex 'v' to U. Step3B: Find the adjacent vertex 'u' to 'v' which is not in U and has maximumdegree. Iterate throughall adjacent vertices if possible. Case I:Regular set-up Connected Regular Setup Case I: A Consider the following connected set-up G 1 in figure 2, having 12 nodes having 3 degree in all vertices along with redundant links. Now the set U consists of the nodes A, B. 4.2 Search the node B, the unvisited adjacent node is from B as 2 and 7. Now the set U consists of the nodes A, B, 2. After inclusion of the node 1 the set U consists of the nodes A, B, 2, 1. 4.4 Discover the node 1, the unvisited adjacent node is from 1 as only 3. Now the set U consists of the nodes A, B, 2, 1, 3. Also we observed derived linked graph need not be unique but this approach will provide the optimized cost analysis report in future in data science field.
paper_145	The present study was based on data collected from 900 respondents of both urban and rural areas of Bangladesh. Among them 44.3 percent were overweight and obese. Majority (70.6%) were diabetic patients. With the increase in age significant increase in prevalence rate of obesity was observed. Higher prevalence rate was also observed among housewives. Higher prevalence of obesity was noted among females. The prevalence of overweight and obesity has increased rapidly over the last decades especially in developed countries  [1] [2] [3] . In 2016 WHO estimated that globally approximately 1.9 billion adults (age > 20 years) were overweight and more than 650 million adults were obese  [4] . The major contribution of obesity is to lead the increase in the prevalence of chronic diseases and cancers  [7] [8] [9] [10] . Behavioral factors have significant effects on metabolic risk. The study was based on data collected from both urban and rural people of Bangladesh. The investigated diabetic patients were 544. The latter information were provided by the diabetic patients. The analysis was done by using SPSS [version 20.0]. The respondents were classified as underweight  [BMI < 20] , normal [BMI,  20 -25] , overweight [BMI < 30] and obese [BMI 30+]. Factor analysis was done to identify the factors for obesity. Obesity was observed among 15.3 percent people and overweight respondents were 34.0 percent. There were 58.9 percent males among the respondents and 47.2 percent of them were normal. However, compared to males more females were obese. Significant differences in proportions of obesity among the two religious groups were noted [P (œá2 ‚â• 10.82)= 0.012]. However, there was significant differences in proportions of different levels of obesity among the two marital groups of respondents [P (œá2 ‚â• 22.933) = 0.028]. Majority (52.9%) of the respondents were of age 50 years and above and 48.5 percent of them were normal [  Table 5 ]. The percentages of normal groups among the respondents of ages 25 -40 and 40 -50 were 35.5 and 36.6, respectively. Levels of obesity was significantly associated with levels of ages [P (œá2 ‚â• 18.34) = 0.008]. Maximum normal group of respondents (53.8%) was observed among agriculturists. Maximum (25.5%) respondents of obesity was noted among housewives. The data indicated that 54.4% respondents had income less than 30,000.00 taka. Significant association was noted between the level of obesity and the level of income [P(œá2 ‚â• 64.994) = 0.00]. Now, let us observe the association of level of obesity and prevalence of diabetes. Table 10  showed that 67.6 percent underweight respondents were affected by diabetes. With the increase in levels of body mass index [BMI] the rates of prevalence of diabetes were also increased. There was no significant association between level obesity and prevalence of diabetes [P(œá2 ‚â• 0.851) = 0.837]. The present study also indicated similar result [  Table 11 ]. Among the smokers 47.2 percent were normal and 37.2 percent were overweight. The association between smoking habit and level of obesity was significant [P(œá2 ‚â• 20.189) = 0.0.002]. This was done by factor analysis. These three variables were more important for the variation in the level of obesity. The coefficients of the components were presented in  Table 13 . These coefficients were observed from the first component. This component explained 25.733 percent variation in the data of obesity. This component explained 10.86 percent variation of the obesity. The respondents were investigated mostly by the doctors and nurses from their working places. Around 50 percent respondents were overweight and obese. Similar finding was also noted in another study  [21] . The prevalence of overweight and obesity were significantly associated with age, religion, education, occupation, marital status, income and smoking habit. Around 50.6 percent people of urban area were overweight and obese. They were classified as underweight (BMI <20), normal (BMI, 20 -<25), overweight (BMI, 25 -< 30) and obese (BMI = 30+). The percentages of these four groups of respondents were 7.6, 43.1.34.0 and 15.3, respectively. Most the respondents were in normal and overweight groups. In this study also higher prevalence of diabetes was observed among them. Among the respondents 84.1 percent were of the age 40 years and above and among them 42.8 percent were overweight and obese. c) Counseling is needed for the obese children and adolescents. The public health authority can play a decisive role for the above steps.
paper_212	Despite the spread of HIV/AIDS having been explored widely, not much literature is available on the Gillespie Algorithm based SIR model. We conclude that the simulated model reflects reality. On June 5, 1981 a mysterious disease was recognized among MSM in the USA. It was characterized by its etiological agent HIV in 1983,  [1] . A serological test, was then made available. In 1984, Robert Gallo discovered that HIV was responsible for AIDS. In 1984, several HIV and AIDS cases were documented in Kenya. This accounts for 4% of new pediatric infections worldwide. Among the expectant women there are 13,000 new HIV infections among children. There are various questions still left unanswered to date on the HIV epidemic. In 2010 several authors came up with a model to predict HIV transmission in China in 2002  [3] . In this model, there were no forms of intervention. The formulated model was used to forecast the number of PLWHA. The number of HIV infections in 2010 was predicted to approximately 1000000. The group comprising of the largest individuals predicted to be living with HIV/AIDS was approximately 650000. A stochastic differential equation SI model with demographic stochasticity has already been developed  [8] . They used the Milstein method to simulate for analysis. Other authors have made contributions to mathematical epidemiology by performing simulations that explain the process of disease spread. This study seeks to incorporate a stochastic aspect in the deterministic SIR epidemiological model. By contrast, in a deterministic process, there is no randomness  [12] . This will allow us to derive new insight from the analysis of the simulation of this SIR model. The transmission and infection rates were considered to be variant. All these aspects determine the quality of the inference drawn. The graphical representation of the developed stochastic model is shown;  Figure 1 . The stochastic SIR model. The assumption is that the population is finite and is sub-divided into categories of finite discrete compartments. Assuming that the resulting state is . A propensity function is the probability of one event occurring in the time interval ! Continuous-time Markov chains are the basic tool for building discrete population epidemic models. This implies the probabilities are individual therefore discrete. Discrete evolution is modelled in discrete time. Continuoustime Markov chains are the basic tool for building discrete population epidemic models. The data was obtained from NACC for HIV/AIDS cases. The means and variances of the simulated and natural data were computed. A stochastic SIR model was simulated with a mean step size of 0.006336446. 537 tau steps were made in the model. Curves produced are illustrated below. where C represents the variance of D The calculated value found was 64.958. The critical value was 47.4. Since the calculated value is greater than the critical value, the decision rule is to reject the null hypothesis. Therefore, the conclusion is that the simulated data model fits the natural data model. After simulating, values were produced by the algorithm for each time step. The simulated curves were compared to HIV/AIDS data. The simulated curves were found to resemble the data available in reality. Mathematical modeling is an area that requires more research.
paper_214	To create simulation envolves changing strategic economical indicators and keeping restrains on others so they reflect reality and the business environment. Objectives of decision-making process aimed at adopting the best solution from many possible alternatives. Optimizing the use of a certain class of models for finding the best solutions. Among the methods most commonly used are decision analysis and mathematical programming  [1, 2] . Decision analysis applies to situations which have a relatively small number of alternative solutions. Each alternative are attached estimates and probabilities of achievement. Solving the problem is to build decision tables or decision trees, from which it selects the best alternative. Apply mathematical programming problems which lead to the formalization of a mathematical relationship between decision variables and purpose. The simulation, conducted testing process is carried out using computers on a defined pattern  [2, 6] . It is the only method that can be applied to unstructured problems. data model can be used in the construction of real observations (numerical values) or knowledge. These are translated into algorithms which are executed by a computer system. This led to consideration of simulation as one of the most powerful tools in decision making. Simulation becomes a technical coordination of procedures using the computer. The solution offered is one spot that has no counterpart in the real system. In the first case, the user is provided customized views of data stored by performing a diverse set of operations on transactional data. The decident system uses a dialog interface with the key users of the company, enabling connectivity and communication between networks with different topologies and areas. Based on patterns defined by an efficient simulation can generate alternatives. Intuition, creativity and experience allow decision-makers compare alternatives; predict outcomes of each alternative separately. After the final resolution of the model, select the best alternative is chosen implementation plan. The assessment in turn depends on the search method. Structural problems use mathematical formulas and analytical method to achieve an optimum solution. In order to improve efficiency the best solution search algorithms are used. Methods called heuristics, based on a thorough analysis of the issue. Basically successive tests are performed, the search progressing from a solution to another. Implementation is the phase that involves the integration model chosen solution in context and simulating the real system. Personal data is data that relates to the behavioral aspects of decisionmakers in making decisions. The data source, internal or external, data is extracted and managed by a management database. The management of the database depends on the organization of data. The data dictionary is a catalog of all data from the database. It contains data definitions, data sources and their intrinsic significance. The data dictionaries are permitted operations to add new data, deletion or retrieval of existing information according to certain criteria. The SQL language is used, which accepts requests for data from other systems. It is the component that differentiates interactive decision support systems to other systems. The models are domain-specific and models can be classified into strategic, tactical and operational models models. The models are used currently in operational and transactional system that aims of the organization. In building a data warehouse is based on the analysis of data. Extract information in order to obtain information for decision making. Basically there are two steps: designing and populating data. Conceptual models are multidimensional and designed to organize data necessary decision-making process on issues.
paper_216	The Black-Scholes model is a well-known model for hedging and pricing derivative securities. Amongst the non-parametric approaches used to improve the accuracy of the model in this study is the Wavelet-based pricing model. The study was carried out using simulated stock prices of 1024 observations. Derivatives are instruments whose value depend on an underlying asset. Derivatives includes; Forwards, futures, options and swaps. The ones traded on the exchange are standardized and regulated. An option is a type of derivative that gives the option holder the right but not obligation to buy or sell an underlying security at a specified price, at any time on or before a given date in the future as agreed on. According to the right to sell or to buy, we distinguish the two types of options. The holder of a call option has the right but not obligation to buy the underlying security at a specified price, at a specified time in the future. The holder of a put option has the right but not the obligation to sell underlying security under some predefined terms of an agreement which includes; strike or exercise price, maturity date and option volume. The Monte Carlo method, which is based on repeated computation and random sampling can be used for pricing American options  [1] . In the Kenyan market, derivatives are yet to be developed. This study therefore prices a European option using two nonparametric methods and a parametric method. The Wavelet based pricing model is another nonparametric method alternative used to price derivatives  [4] . The implied risk neutral moment-generating functions (MGF) is estimated by the wavelet method. This method only requires a reasonable amount of data (different strikes) and is very efficient, unlike other non-parametric methods which requires large amount of data. This research therefore compared the performance of Wavelet based pricing model and Black-Scholes model in the valuation of a European option. The remainder of the paper is structured as follows: Section 2 discusses the existing literature; Section 3 presents an overview of the Black-Scholes model and Wavelet based pricing model. Lastly, section 5 concludes the study. The simple closed form solution of European options was derived during the financial crisis  [2] . The de-noising ability of wavelets was also recognized in  [7] . European options can also be priced using the Shannon wavelet  [14] . is the cumulative distribution function. As it has been seen from the reviewed literature, this model is a new method in the field of finance. Based on some general assumptions like independent and identically distribution (iid) for asset returns and that the moment generating function is defined well, the wavelet based pricing model can be expressed as follows; formula_1 Where, t is the current time, is the underlying asset price at time t, # $ is the time t price for a European call option written on asset, K is the strike price and T is the future maturity date. ( is the bilateral inverse Laplace transform. Interest rate r is assumed to be constant. This needs to be approximated by wavelets. This function also emulates the probability density function of asset returns. To estimate the risk neutral MGF of the return per unit time, we use the Franklin hat function f (t) expressed as; In this study Monte Carlo simulation was used to generate 1024 stock prices. This is evident from the values of the RMSE and MSE, whereby MSE of wavelet model is lower than that of Black-Scholes model. Moreover, hedging performance may be evaluated with respect to each of the two models.
paper_219	This paper introduces a framework to determine data quality on enterprise networks for net-centric and net-ready initiatives as introduced by the US Department of Defense (DoD). Traditionally quality of data delivered to an enterprise user focuses on network performance, i.e. quality of service (QoS). The QDS attribute brings distinction to the resultant data quality of the network's quality of service. The DR attribute is introduced; it is important in enabling enterprise data consumers to sort, filter and prioritize data. There is also a need to assess the quality of data sharing across the enterprise network. The resultant minimax value correlates to the lowest performing attributes of the framework. The presented framework offers decision support tools to enable agencies to allocate limited resources towards improving the performance of their net-centric service offerings to the enterprise network. That same data abundance challenges the network capacity and overloads the capacity of the human operator. All of this is in addition to the historical problems of network management and quality of service. When discussing the theoretical, it is easy to dismiss the challenges of limited resources to implement new policies such as schedule, budget, manpower, etc. Traditional QoS expects prioritization has occurred prior to data entering the network. The value of data quality can be used to indicate the priority of data delivery to the consumer. Section 2 of this paper provides a brief overview of the origins of net-centric and net-ready policies. Section 3 defines a new model for valuing the quality of data by measuring user satisfaction. The attributes in the new model represents data quality of the enterprise system within three contexts: net-centric measures, traditional network quality of service, and cyber-security. To satisfy the attribute of entered and managed on the network, the IT connections to external networks required to perform net-centric operational tasks must be identified  [3] . The identification of the connections must be specific (i.e. Evaluation of the data sharing enterprise requires a holistic view that considers the net-centric attributes of the data simultaneously with the quality of service for the data network. The subjective measure of overall user-satisfaction of a service or application is referred to as quality of experience (QoE)  [9] . Sampling users is the preferred and direct method for measuring QoE. The question arises how to get the users' opinions. Usually the MOS is formed from arithmetic mean of a population of user opinions. develop or enhance the prediction model of QoE for each attribute. For a sample of size n consider v strata with sample s k in each stratum, i.e. ‚àë i=1 to v n i = n and s = U k=1 to v s k . where n ‚â§ N and s = {1, 2,‚Ä¶, n}. Where it is not easy to design a stratified sampling plan a stratification post collection of n samples without replacement can be constructed. The QDS takes into account the effects of environmental conditions on sensor performance for given design parameters. QDS is a subjective rating from the perspective of the end user  [11] . But cost and time to use panels of experts to assess MOS has resulted in seeking alternative approaches. Full reference PESQ is formed by taking the speech codec output and comparing it with the original signal inputted into the codec  [13] . A standard to address the rating of motion imagery (i.e. Over the last several decades there have been many papers on the topic of QoS. But just for completeness a brief explanation of QoS is given here. User satisfaction is traditionally associated with network metrics: delay, jitter, throughput, packet loss, order preservation. And the service level agreement (SLA) is the users' agreement with network provider(s) on acceptable ranges for the metrics. User-satisfaction provides a true gauge of a network QoS  [11]  and the subjective assessment of that satisfaction is provided by QoE. However there are a number of challenges to QoE discussed in  [19]  and  [20] . To understand cause and effect it is ideal to have the full reference i.e. As stated in  [20]  QoE is likely to be biased by negative responses, tendency to higher responses from unhappy consumers. These models of QoE based on QoS objective measures are often referred to as "QoE\QoS" correlators  [21]  and  [12] . Models have been developed to correlate the QoS with QoE for multimedia applications  [12] . A number of authors  [12, 15] The measure of data relevance expresses the utility provided by the data towards the consumer's objective(s). A scalar measure is required to support evaluating the degree of relevance on the enterprise performance. Intrinsic data relevance represents the relative value (i.e. QoE) that the data would provide to the consumer assuming perfect discovery and delivery. Enterprise data systems can offer multiple forms of data to the consumer (e.g. The data's spatial properties relate to the location of the data collection sensor relative to the target of interest (e.g. metadata) with those of the producer. We first discuss the design philosophy for the overall assessment of the data quality for the enterprise: (1) The enterprise data quality is determined by the attribute with the lowest user satisfaction (QoE). Alternatively the data relevance user satisfaction to match the other attributes can be improved. In game theory, the minimax value of a player is the smallest value that other players can force without knowing the player's actions. Consider a simple example in  Table 1  to illustrate the minimax in a game theory context. DR subspace could consist of parameters to measure objectively various properties and elements of data relevance, e.g. f d (s) = u HT . The x-axis of the chart in  Fig. 3  is organized to reflect the connection between QDS and QoS. The final enterprise data quality, as shown in  Fig. 3 , as formula_6 In  Fig. This paper introduced a framework to determine data quality on enterprise networks for DoD net-centric and net-ready initiatives. The final value of data quality for the enterprise network was demonstrated as the attribute with the lowest score. The paper notes the definition of the relationships between attribute's objective measures and the final quality of experience. In particular QoS objective measures for audio can be used to predict the QoE as an alternative to conducting surveys.
paper_241	Transformers are the key equipment in electrical power transmission. C transmission, power transformer is one of the most important equipment. It is expensive uninterrupted and desired to be kept in good condition always to have supply. The system is capable of sustaining a variety of environmental and operating impacts that resemble normal operating conditions which may include lightning striking the transmission lines, excessive loading, deterioration or breakdown of the equipment insulation resulting to power systems experiencing occasional faults such as phase to phase or phase to ground faults, over current, over voltage or even temperature based faults. differential current) and can be prevented using differential protection and microcontroller based relay protection. The Differential current protection scheme is based on the principle that the input power to the transformer under normal condition is equal to the output power and is concerned with having a differential or balanced current between the primary and secondary side which would be digitally displayed  [2] . The protection techniques employed differential relay mechanism with Arduino. At all times, the Arduino senses the condition of the transformer. A power transformer functions as a node to connect two different voltage levels  [3] . The output power in a transformer is equal to that of the input power hence, for differential current protection of the current transformers reduces the currents at the primary and secondary sides to a measureable value and in such a way that they are equal  [5] . It is transient in nature so it lasts for just a few seconds and does not cause any permanent damage to the transformer. The power transformer to be protected is connected to the main AC supply and delivered power (P=IV) to the output terminal. The power transformer primary and secondary currents are measured by the current transformers, which are to be protected by the Arduino and the circuit breaker. The step-down transformer is used to step down the primary or source voltage from 220 to 12V. The Arduino microcontroller acts as differential current comparator connected to both current sensors ACS712 and as control unit in this design. The LCD display is used to display the differential current values of the transformer and to display the voltage and current level of the system as well. The following table shows the result from the conditions of the system From the test carried out, it was observed that the current values gotten from the primary and secondary side of the system in Faulty conditions were larger than that gotten in Normal conditions and also the current difference were also larger for Faulty than Normal condition.
paper_251	Cloud computing is associated with a new paradigm for the provision of computing infrastructure and services. However different users have different requirements of computational power and application and systems software. Advancement of electronics and telecommunication field has done the job. Various requirements require numerous specialized devices (CPU, storage etc.) Secondly most of the resources are idle i.e. According to the Lewis Chunningham  [20]  "Cloud computing is the internet to access someone else's software running on someone else's hardware in someone else's data center". Cloud offers instant service (software, platform or infrastructure) to requisite dynamically. In the cloud computing. They are able to communicate i.e. social in nature, mobile i.e. autonomous and distributive in nature. Codenvy -To develop an / are application i.e. SaaS (Software as a service). For the proposed system an java web application using JSP (Java Server pages) application has been chosen to develop on to the codevnySaaS cloud service provider. Platform as a Service (PaaS). For this Cloudbees service provider has been integrated onto the developed SaaS application. New Relic -To develop the core functionality of the proposed system. monitoring and scheduling using software agent New Relic service has been subscribed. In this the java agent has been customized to meet the monitoring and scheduling of the SaaS services. These are following 1. Service scheduling delay 2. Better Provisioning of the SaaS 4. Detail Objective of the proposed agent based SaaS Service. For SaaS development Codenvy has been subscribed. In which jsp based application has been develop and deployed on cloudbees PaaS. Cloud bees service integrating a SaaS deployment on it and delivering through agent. Measuring the performance of the proposed analytical approach (influenced from Aneka) in cloud services such as public Cloud bees. Model for Proposed Work Our proposed work is to schedule and monitor cloud SaaS application onto the cloud and evaluate the performance of the same using proposed agent based. Number of tasks submitted at instant i (Ni) 2. Time to execute the task 3. Response time is same as to average utilization ratio in addition to CPU usages. Hence proposed system performs better than  [2]  and any other methods as shown in figure 3. The developed java web application (SaaS) has been developed with the help of codenvy SaaS developed platform.
paper_272	Parameters of a vacuum interrupter are essential. A more exact name would be metal vapor arc inside vacuum electroplate. J=2*10 6 (A/m 2 ) Joule heating as thermionic emission Where √ò is the thermionic work function. K is Boltzmann's constant (1.37x10-23 Joules/K) The constant A is approximately 6x10 5 for most metals The total above processing time was measured by Harris approximately (20-250¬µs)  [15] . A high frequency current governed by the circuit parameters flows. Restrike overvoltage due to the multiple re-ignition of circuit breaker when a switching interrupter process is initiated before current zero. Although the current in the vacuum interrupter can chop to zero almost instantaneously (fraction of a microsecond), the current in the load inductance-3ph coils in the power transformer cannot attain zero value instantaneously. These two high frequency transients and the voltage loop are associated with the current chop and the immediate re ignition of current. We note that the only parameter involved is Œ∑. So that a family of generalized curves can be drawn from equation  4 for different values of Œ∑ with dimensionless quantity -t`, as abscissa. This has been done in Figure. Where Œ∑ =0.5, the sine function changes from a circular to a hyperbolic function. We might have developed the curves for this condition, following the same argument. Suppose Vo =13.8‚àö2 KV So. Zo = ‚àöL/C = 5X104 ohms Œ∑. The curve labelled Œ∑=2 in  figure 5  gives the shape of the current we are looking for. The ionization electroplates zone will be evaluated between (20us -250 us) respectively. Chopping current times (400us-800us). Transient over-voltages (450us -480us-500us).
paper_294	The cultural heritage of any people can be redressed through the preserved cultural values tangibly stored and preserved in media and forms retrievable and usable. Audiovisual heritage are quite essential that the public library system must not afford to elude its storage not only for users but also for posterity. Cassava has played an important role as a staple crop in the feeding of the Tiv people. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. The study sampled 377 out of the population of 20,000. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. The cultural heritage of any people can be redressed through the preserved cultural values tangibly stored and preserved in media and forms retrievable and usable. The photographs people take, videos they shoot, speeches and music they record, capture in bits, every moment of their life, culture, event and times that inevitably speak volumes of their history. It is a legacy that people would want to impart to their grandchildren, so that the next generation would have the opportunity to understand their heritage. Audiovisual heritage are quite essential that the public library system must not afford to elude its storage not only for users but also for posterity. Cassava has played an important role as a staple crop in the feeding of the Tiv people. Moreover, the public library should also ensure that the host communities and users are met with their particular information needs; but if this is not done, then the aim of establishing such libraries will be defeated. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Specifically, the study sought to: 1. storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? They are the venue through which the overall information resources are made freely available to all (Edoka, 2000; Assoh, 2011; Shidi, Aju & Ashaver, 2014). Information processing and retrieval are the core aims and objectives of librarianship, which warrants adequate coverage at all levels of education and above all service to all users both learned and non-learned. Apparently, in order to achieve this mission, public libraries preserve a variety of information materials including print materials such as books, monographs, serials and periodicals; and non-print materials such as audio or audiovisual materials such as cassettes, microfiche and films among others. The Tiv people used to make a ridge round their houses and plant cassava on it; and when the cassava grew up, it became a cassava fence surrounding the house. Agber (2007)  reported that a variety of cassava species are found in the Benue Basin including Aiv-kpenga, Pavnya (pronounced Panya) and Imande; and that Panya was discovered by a Tiv hunter called Adaga from Gaav Megaclan of the Tiv in about 1794 according to oral history. Postharvest priorities across the globe have evolved considerably over the past four decades, from being exclusively technical in their outlook, to being more responsive to consumer demand. Consumer-driven trends which have contributed to this shift include rising incomes in urban areas, changing dietary habits, more women in the work-place, reduced time for meal preparation and growing demand for safety, quality and convenience  (Rolle, 2006) . The food need not be removed any great distance from the harvest site, but it must be separated from the medium that produced it by a deliberate human act with the intention of starting it on its way to the table. Essentially, cassava postharvest losses can be defined as both the physical losses such as weight and quality suffered during postharvest handling operations of cassava and the loss of opportunities as a result of inability of producers to access markets or only lower value markets. Cassava has a short lifespan after harvest and as a result, the Tiv people process it into various forms for easy storage as a stratagem for postharvest loss management. Slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base) and 5. The study adopted a survey design, which is the type of design that enables the researcher to collect data from a group of people through questionnaire, interview or observation techniques for the purpose of analysis and subsequent interpretation. These are the Benue state Library Board Headquarters in Makurdi and its branches in Gboko, Vandeikya, Katsina-Ala, Gungur, Aliade and Adikpo with about 33 workers  (Mnguty√¥ & Amaakaven, 2013) . They used the Fish Bowl Technique by writing Yes and No for the respondents to choose and those who chose Yes were finally given questionnaire to respond to it. Section C also consisted of 7 variables of ways public library has impacted on Tiv management strategies for postharvest losses of cassava and Section D consisted of 5 variables of challenges faced by public libraries in acquisition and storage of audiovisual materials on Tiv management strategies of postharvest losses of cassava in Benue state. The 19 item questionnaire adapted a 4 point rating scale and respondents were asked to respond by ticking the correct or applicable responses (SA) strongly agree, (A) agree, (D) disagree and (SD) strongly disagree. Consequently, for the cassava farmers who had no western education, the research assistants read the questionnaire to their hearing and gave interpretation in Tiv language, and the options they selected were ticked for them. Data were analyzed using mean and standard deviations. The collected data were analyzed using mean and standard deviation. Demographically, data were collected from 680 Library Staffs, Cassava Farmers and Library Users, out of which 348 were male representing 51.2% while 332 representing 48.8% were female. Apparently, 28 representing 4.1% were library staffs, 376 representing 55.3% were cassava farmers and 276 representing 40.6% were library users. The collected data were analyzed and presented in  Table 1 . This means that peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips), peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour), peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom, slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base), peeling the cassava, fermenting, sieving and squeezing it into lumps known as Akpu, peeling the cassava, grating, squeezing (demoisturizing) it, mixing with salt and other ingredients and frying with palm or groundnuts oil known as Kweesi or Akweesa Akom and peeling the cassava, grating, squeezing and sieving the starch and preparing it for food as Kamu Akom are the management strategies for postharvest losses of cassava. What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? The collected data were analyzed and presented in  Table 3 . This implies that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. Findings from the study based on the result in  Table 3  showed that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. Management of public libraries should also ensure that initiatives on going from one community to another to record and shot films on indigenous knowledge are in place. Importantly, if public library has acquired and stored audiovisual materials on Tiv management strategies for postharvest losses of cassava, cassava farmers would have accessed these information materials to improve on various ways of managing postharvest losses of cassava in Benue State of Nigeria. The study concludes that public library must make it a point of duty to start acquiring, storing and preserving information materials on indigenous knowledge, particularly in audiovisual format, since this is the only way the library will be useful to the host communities.
paper_298	The study investigated the awareness and use of library catalogue for accessing information resources by users of Federal University of Kashere (FUK) Library. Four research questions guided the study. The instrument used for data collection was questionnaire. Data generated was analyzed using simple percentage and descriptive statistics. In effect, the respondents use card catalogue regularly compared to the OPAC. Majority of the respondents used the library for research work rather than for leisure and recreational purposes. Poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. The library ensures that the resources acquired are well organized to allow easy access by the library users  [1] . Overall majority of respondents 80% satisfied with OPAC functionality  [4] . Students' skills in search catalogue can be created through library tours, orientations, seminars, lectures, library staff, handbooks and use of library studies. Kumar revealed that 26.99% respondents use the OPAC fortnightly and 22.84% weekly. The author attributed the reason for the poor usage to lack of user education programme  [9] . In essence, students use the catalogue to enable them conduct research in the library. It is on this note that this study investigates the use of library catalogue by undergraduate student in the university library. Therefore, the study investigates the access and use of library catalogue by students of Federal University of Kashere, Gombe State. The study is designed to achieve the following objectives: a. To find out the students awareness about library catalogue as access point to library collection. To find out methods employ by students to consult library catalogue to search for information resources. c. To ascertain the extent of use of the library catalogue by students to access information resources. d. To identify the constraints associated with the use of library catalogue. The study is designed to answer the following research questions: a The survey research method was adopted for the study, and questionnaire was the instruments use for collecting data. Thus, 272 respondents were selected for the study based on 95% confidence level and 5% confidence interval of Sample Size Calculator. The completed and returned questionnaires were analyzed using descriptive statistics, percentage and frequency distribution. The gender breakdown is presented on  Table 1 . The question on the demographic information of the respondents was on gender presented in  Table 1 . It shows that majority of the respondents 188 (69%) were male, followed by females 84 (31%). This shows that majority of library users in the Federal University of Kashere Library were male. Table 2  revealed that majority of the respondents 262 (96.3%) were aware of the existence of the library catalogue in the university library. 26 (10%) of the respondents were aware of the existence of OPAC as a retrieval tools in the library. Their responses were presented in the  Table 4  below. Table 5  indicated that majority of the respondents 158 (60%) used the library catalogue regularly when searching for materials in the library. 24 (9%) of the respondents never used the catalogue but they were aware of it existence in the library as indicated on table 2 above. Total  F  %  F  %  F  %  F  %  F  %  F  %  100  10  20  0  0  6  12  28  56  6  12  0  0  50  200  10  23  6  14  10  23  16  36  2  5  0  0  44  300  10  19  6  11  14  26  12  22  6  11  6  11  54  400  14  25  0  0  36  64  6  11  0  0  0  0  56  500  2  3  0  0  54  93  2  3  0  0  0  0  58  Total  46  18  12  5  120  46  64  24  14  5  6  2  262  The finding indicated poor use of OPAC 12 (5%) at the university library. The study found that majority of the respondents were aware of library catalogue in the library including OPAC  [12] . Lack of skills could discourage users from using the catalogue. It could be concluded that majority of the respondents used the library for research work rather than for leisure and recreational purposes. The results are presented in  Table 8 . Table 8  focused on the challenges associated with catalogue use for information retrieval. Higher number of the respondents indicated poor computer skills hindering navigation when searching OPAC 96 (37%) and inability to locate materials on the shelves, indicated in the catalogue as being available 58 (22%) were the challenges affecting the use of catalogue as an information retrieval tools. ICT skill is needed by the respondents to be able to browse the library OPAC. This finding corroborates with that of Ogbole & Morayo which revealed that (41.1%) of the respondents rarely use OPAC. The study revealed that majority of the university library users were male. It could be concluded that majority of the respondents used the library for research work rather than for leisure and recreational purposes. Another source of frustration was reasonable number of the respondents indicated poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools.
paper_305	Mitigation of credit risk is a key aspect of portfolio management in any financial institution. In this paper, we report on the results of a MSc. Loans constitute the cornerstone of the banking industry's financial portfolios. The performance of loan contracts in good standing guarantees profitability and stability of a bank. Generally, bank loan officers rely on traditional methods to guide them in evaluating the worthiness of loan applications. Furthermore, a loan officer's credit decision or recommendation for loan worthiness is subjective. Generally, loan application evaluations are based on a loan officers' subjective assessment. Therefore, a knowledge discovery tool is needed to assist in decision making regarding the application. Further, the study champions the use of open source software tools in business intelligence applications. The development of machine learning models and tools has been welcomed as one of the most exciting in business settings. This calls for the use of more efficient and effective loan screening tools and procedures. Automated techniques have progressively become popular in contemporary loan appraisal processes. These techniques have been found to outperform earlier approaches leading to increased competitiveness. Boosting is one of the most important recent developments in classification methodology. Further, they suggested an improvement to the model by introducing a graphical interface for the loans officer. A decision stump is a decision tree with only a single root node. It works as follows: 1. Looks at all possible thresholds for each attribute 2. In this study, 'majority voting' was adopted for combining hypothesis from different learners. Figure 1  illustrates the combination criterion. The implementation detailed lay in the use of a logistic regression that models the posterior class probabilities Pr (G = k|X = x) for the K classes. The model was specified in terms of K ‚àí1 log-odds that separate each class from the base class K. Train all K decision stumps iii. Select the single best classifier at this stage iv. Combine it with the other previously selected v. classifiers vi. Reweight the data vii. Learn all K classifiers again, select the best, combine, viii. Repeat until you have T classifiers selected The development platform used for this project mainly included the following open source software products: The model was built using the training dataset and tested using three strategies. We report on cross validation as under. Separate data into fixed number of partitions (or folds) ii. Classify and obtain performance metrics. v. Classify until each partition has been used as the test set. Calculate an average performance. This strategy relies on two separate files, one for training and the other for testing. This gives an accuracy of 19/20=95% Class =Accept: The number of correctly classified instances is 12 and that of instances classified as belong to the class is 13. This gives a precision value of 7/7=1 Class =Accept: The number of correctly classified instances is 12 and the number of instances belonging to the class is 12. This gives a recall value of 7/8=0.88  Figure 2 . Test Split ROC graph P(true | false) calculated as 1-d/c+d The ROC area has the following indicators: . Indicates a perfect prediction .
paper_310	We are now living in the 21 st century. Now, smart phone has become the most essential thing in our daily life. Here we are using Bluetooth communication, interface microcontroller and android application. We are using Arduino software to interface the Bluetooth module with microcontroller. Here we use mainly Arduino UNO (ATMEGA 328P), Bluetooth module (HC-05). The controlling device of the whole system is a Microcontroller. Bluetooth module, DC motors are interfaced to the Microcontroller. Still there exists a requirement of a cost-effective automation system, which will be easy to implement. b) Develop a robot which will be helpful for travelling. f) Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. g) The Bluetooth module is connected with the Arduino UNO board for the connection with the user. Microcontroller will act as the brain of the robot. The robot movement will be decided by the microcontroller. In this system we will be using microcontroller named Arduino UNO which contains ATMEGA 328P microcontroller chip (  Figure 1 ). The microcontroller is programmed with the help of the Embedded C programming. Cprogram is very easy to implement for programming the Arduino UNO. Bluetooth module will give the commands given by smart-phone to the microcontroller. The smart phone is the transmitter of this circuit. It sends the data to microcontroller through Bluetooth module. Motor driver IC is used to control the dc motors. Here we use programming language 'C' for coding. The program for executing this project has been written in C language. The program is burnt in the microcontroller using burner software. There are two steps of the programming. First set up section where we define all the variables. The working principle is kept as simple as possible. The working principle of the circuit has been elaborated with the help of a block diagram, of the system interconnection as shown in  Figure 6 . As seen from the  Figure 6 . A DC power supply is required to run the system. The DC power supple feeds the Microcontroller and the Bluetooth module. Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. The Bluetooth module is connected with the Arduino UNO board for the connection with the user. The functions of the given instructions are operated by the microcontroller. The instructions are sent by the smart phone. We can easily control the movements of the dc motor. Here we are using four 12 V, 200 R.P.M DC motors and a 9 V DC battery as main power supply of this system. The robot can be used for surveillance.
paper_333	There are several issues and diseases which try to decline the yield with quality. This study presented a general process model to classify a given Enset leaf image as normal or infected. This diagnosis apply K-means clustering, color distribution, shape measurements, Gabor texture extraction and wavelet transform as key approaches for image processing techniques. The government committed 15% to 17% of expenditures to the sector; it covers 41% of gross domestic product (GDP), over 90% of export value and directly supports 80% of the populations' livelihoods  [1] . There are several issues and diseases which tries to decline the yield with quality. As a matter of facts, visual or manual detections may have defects in terms of accuracy in detection along with lower precision. There is a potential need for technology supported alternative systems to support the manual identification of Enset crop diseases so as to optimize the accuracy for remedial action. If the Enset crop diseases can be minimized then definitely the production will significantly be increased to contribute to the country's economy towards a new boost. Therefore, the implementation of IT based solution in the sector will have a paramount importance to facilitate the activities such as economic, social and ecologic development in the country by increasing efficiency in both quality and quantity of Enset crops production, to sustain dependability of customer preferences and to preserve the ecology  [2] . In our country few researchers found the promising solutions to different plant diseases diagnosis such as maize, rose flower, coffee and others using computer vision and machine learning techniques  [3, 4, 5] . Experimental results are reported in Section IV. Section V presents the conclusion and discusses of future works. The occurrence, distribution and the incidence level also indicated to vary from one Enset-growing locality to the other. Figure 2  shows the architecture of the proposed system A. Dataset Partitioning The images of the dataset are collected from different parts of southern Ethiopia. Those images are collected by using a digital camera and some of them are collected from secondary sources like research centers which collects pictures for visual inspection of Enset disease. Test Result For the experimentation a total of fifteen experimental setups which is color, texture morphological and a combination of each and a multiclass Kernel support vector machine model: Linear Kernel, polynomial Kernel, RBF kernel and Quadratic Kernel was used. From the figure it is shown that the RBF kernel classifier has classified the given enset image in its correct class category by 94.04% and 92.44% respectively. We designed a system which can detect Enset disease automatically and this will help the farmers to detect the diseases in its early stage and to take relevant action. If there is a shortage of experts in the area of those disease identification the system is going to be a replacement for the experts.
paper_389	In this paper, a large number of experiments have been carried out using conditional random fields. The experimental corpus has been tested by Changjiang Daily for many years. Peng F establishes a Chinese character segmentation model based on CRF. Common Chinese character tagging method is based on the Chinese characters appear in the words of different locations marked different labels. Therefore, Chinese word segmentation method has become a frequently used method to study word segmentation. Enter the sentence as "This is Wuhan." 3, Mark B can only be followed by the mark I. The following  Table 1, Table 2 ,  Table 3  is used in the experiment some of the characteristics of the template. Table 2 and Table 3  are two new feature templates. Secondly, feature extraction is carried out to generate training corpus and test corpus which can be recognized by CRF model tool. Using the CRF model generated by the training, the test corpus is tested and a prediction result is obtained. Finally, the evaluation program is used to evaluate the prediction results and get the evaluation results. The main corpus used in this paper is the training corpus and test corpus of Changjiang Daily. The corpus is from 1950 to 2005, and the scale is 2564168000 sentences. The coding method is GB code. The corpus content mainly comes from newspaper news. The format consists of a sentence segment consisting of words marked with spaces. The performance of a word segmentation system mainly depends on two aspects: segmentation precision and word segmentation speed. The performance of Chinese automatic word segmentation is evaluated by the following three indexes: correct rate (P), recall rate (R) and F value. Where, P refers to the accuracy of word segmentation; R refers to the word recall rate; F value refers to the P and R integrated value. The main factors that affect the speed of word segmentation are the structure of word segmentation dictionary and word segmentation algorithm. The query speed of the word dictionary depends on the organization structure of the dictionary. In different applications, the performance requirements of the word segmentation system have different emphases. It can be seen from  Table 5  that the results of the CRF system are better than those of other models under the same conditions as the training corpus and the test corpus. The experimental results show that conditional random field is an efficient segmentation method. And then use these tools and the corpus carried out a number of experiments.
paper_391	Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. Globally, tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The WEKA 3.6.1 tool was used for classification, clustering, association and attribute selection. The maximum confidence attained in the general association is 0.98. But, in the class attribute, it is 0.72. The literacy status of the mother has high information gain with the value 0.046. Multilayer perceptron classifier has the lowest average error at 32.72% compared to others. Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. Tetanus is caused by a toxin produced during the anaerobic growth of Clostridium tetani. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The information is rich and massive. In this study, we use different data mining techniques that have been tested on TT dataset. Selection phase generates the target data set from the whole data set of EDHS 2011. The next phase is the transformation of the preprocessed data into a suitable form for performing the desired data mining task. However, not all of the patterns are useful. Those patterns that remain represent the discovered knowledge. The methodology of this study was the practical research method applied on the Tetanus Toxoid data of the Ethiopian DHS 2011. The EDHS of 2011 dataset was used as a source for this study and WEKA 3.6.1 machine learning tools are used. It has a dimension of 7033 rows and 12 columns. This data processed and arranged for the purposes of training and testing. Finally, data have been saved in ". csv" file formats and stored as an ". Classification is one of the data mining techniques and it is used to group the instances which belong to same class  [8] . How does this classification work? The approaches are; (a). Decision tree (J48) approach It is a flowchart-like a tree structure. And the topmost node in a tree is the root node. The training tuples are described by m attributes. Each of the tuples represents a point in an m-dimensional space. They can predict class membership probabilities like, the probability that a given tuple belongs to a particular class  [10, 11] . Bayesian classifiers have also exhibited high accuracy and speed when applied to large databases. This assumption is called class conditional independence. It is made to simplify the computations involved and, in this sense, is considered "Na√Øve"  [7] . Confusion matrix; is used in this study, include accuracy, sensitivity, specificity, and precision. Training and testing are performed k times. Thus, in this study, we have used five clusters. "How does the k-means ( ) algorithm work?" From the selected 7037 mothers, 3351 of mothers received TT Immunizations. (Table 1)  As can be seen in  Figure 2 , the accuracy rate of the classifiers on training data is relatively high. A multilayer perceptron is the best classifier in our data set. Briefly described as follows: Cluster 0-This group have 1534 (27%) instances of which consist of mothers who live in the rural, no access to radio, no access to TV, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 1-This group consists of 1004 (18%) instances of which mothers who live in the rural, no access to radio, access to TV yes, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, head of household male, tt-vaccinated yes. Cluster 2-This group consists of 2063 (37%) instances of which mothers who live in the rural, no access to radio, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated No. Cluster 3-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 4-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, orthodox in religion, Amhara in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education no education, women age 35-49, mother marital status married, hh_head Female, tt-vaccinated yes. csv" file and features were described using WEKA performance. Multilayer perceptron classifier has the lowest average error at 32.72% compared to others.
paper_402	This paper presents machine learning algorithms based on back-propagation neural network (BPNN) that employs sequential feature selection (SFS) for predicting the compressive strength of Ultra-High Performance Concrete (UHPC). The BPNN and SFS were used interchangeably to identify the relevant features that contributed with the response variable. The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. The need for soft computing tools and models for the prediction of behavioural properties of engineering components, systems and materials is continuously rising. ANN emerged as one of soft computing paradigms that have been successfully applied in several engineering fields  [1] . Specifically, ANN has been used to solve a wide variety of civil engineering problems  [2] [3] [4] . Mainly, ANN was utilized to model the nonlinear behaviour of fatigue and creep of Reinforced Concrete (RC) members  [5] [6] [7] [8] . The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete  [14] [15] [16] [17] . Afterwards, several authors began developing ANN models for the prediction of compressive strength of high performance concrete  [18] [19] [20] [21] . As a result, BPNN proved to be more accurate than SMD in the prediction of compressive strength and slump flow of UHPC. Despite the statistical advantages of ANN, it has been long regarded as a black box that evaluates functions using input covariates and yielding outputs. Meaning, the model does not produce any analytical model with a mathematical structure that can be studied. [29]  used the Markov Blanket with a wrapper method to select the most relevant features of human motion recognition. Moreover, Rodriguez-Galiano et al. Four types of machine learning algorithms were used as wrappers for the SFS. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. There are two types of ANN models: (1) feed forward; and (2) feed backward. The input neurons are responsible for containing the independent parameter presented by the user, the wires represent the randomly generated matrices called weights that manipulate the function's slope or steepness, the hidden neurons map the weights variables using an activation function, and the bias units control the output function's shift, either upward or downward. Equation  1 shows the linear combination of mapping weights from each input neuron, via wires, to the hidden neurons. There are two types of search algorithms: sequential forward selection and sequential backward selection. In this study, the previously verified ANN model was used as the objective function and the forward selection was used in selecting the relevant features. Figure 1  shows the algorithm SFS uses when performing forward selection. Based on the results of these trials, the most abundant combination during the SFS analysis, within a 20% threshold, was selected as the important parameters that contribute mostly in the model. The selected features, using SFS, were analyzed by the previous BPNN model. Table 3  shows the statistical measurements calculated for both cases. The LSR model is a linear function and its form is shown in  (2) . fc = Œ∏1C + Œ∏2SI + Œ∏3FA + Œ∏4W (2) This study was conducted to detect the correlation between the material constituents of UHPC and its compressive strength. The correlation coefficient (r 2 ) before and after the use of SFS improved from 81.6% to 99.1% while the NMSE improved from 0.0594 to 0.026, respectively.
paper_418	An important aspect of descriptive time series analysis is the choice of model for time series decomposition. Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model. The models most commonly used to describe time series data are additive, multiplicative and mixed models. They do not depend on the level of the trend  [3] . On the other hand, the appropriate model is multiplicative when the seasonal standard deviations show appreciable increase/decrease relative to any increase /decrease in the seasonal means. Here again, no statistical test was provided for the choice. The multiplicative model was adopted when the magnitude of the seasonal pattern in the data depends on the magnitude of the series. The higher the trend, the more intensive these variations are. An important aspect of descriptive time series analysis is the choice of model for time series decomposition. In such cases it is appropriate to use a multiplicative model. Oladugba et al  [9]  gave brief description of additive and multiplicative seasonality. This is an improvement over what is in existence. The method adopted in this study is the Buys-Ballot procedure in descriptive time series. The row and overall variances contain both trending parameters and seasonal indices for additive and mixed models. formula_5 Source: Iwueze and Nwogu (2014), Nwogu el al (2019). That is when ( 0 b = ), it is clear from  For mixed model, we obtain using the expression in  Table 1  . formula_10 When there is no trend ( 0 b = ) we obtain from  (Table 1)  . Table 3  that when there is no trend i.e. The data of 120 observations has been arranged in a Buys-Ballot table as monthly data (s = 12) and for 10 years (m = 10). The actual and transformed series are given in figures 3.1 and 3.2. The expressing of a linear trend and seasonal indices for an additive model is given as Estimates of trend parameters and seasonal indices are discussed. Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model.
paper_428	The present article examines the influence of thermal radiation on two-dimensional incompressible magnetohydrodynamic (MHD) mixed convective heat transfer flow of Williamson fluid flowing past a porous wedge. An adequate similarity transformation is adopted to reduce the fundamental boundary layer partial differential equations of Williamson fluid model in to a set of non-linear ordinary differential equations. In Non-Newtonian fluids, the most frequently encountered fluids are pseudoplastic fluids, and Navier-stokes equations alone are insufficient to describe the rheological properties of these fluids, therefore, to overcome this defect, several rheological model like Ellis model, Power law model, Carreaus model and Cross model are presented, but little attention has been compensated to the Williamson fluid model and estimated to explain the rheological properties of pseudoplastic fluids. studied the peristaltic motion of non-Newtonian fluid through asymmetric channels along porous wall by means of various phase and amplitude, and also studied the manipulation of different wave structures on the fluid flow model  [8] . developed a model for the transport of Williamson fluid in an annular region  [10] . found numerically convergent solutions of two dimensional flows of non-Newtonian fluids along chemically reactive species  [11] . MHD is very useful to analyze the interaction of electrically directing fluids. analytically discussed MHD flow of viscous fluid through Stretching sheet using DTM-pade approach to solve boundary layer equations of given flow problem  [21] . Jabar addressed the influence of viscous dissipation and joule heating on MHD flow through a stratified sheet subjected to power Law Heat Flux having heat source  [22] . Bau investigated the thermal convection in a saturated stratified medium bounded between two parallel eccentric cylinders with the help of a regular perturbation expansion along Daarcy-Rayleigh number; it was observed that the appropriate preference of eccentricity values can maximize the heat transfer inside annulus of various thermal insulators  [37] . studied unsteady combined convection flow in a cavity in presence of nanofluid  [41] . Chaudhary and Jain studied the impact of mass transfer, radiation and hall on MHD mixed convection flow of viscoelastic fluid in an infinite plate  [43] . Malleswaran & Sivasankaran carried an analysis for mixed convection flow and noticed that the average heat transfer decreases with an increase in Richardson number but in general heat transfer is better at force convection mode than free convection mode  [45] . General example of porous wedge is sand, soil, sandstone and foams. Deka   is wedge angle parameter. E 0 = E 6 , E‚Ä≤ 0 = 0, F 0 = 1 (9) E‚Ä≤ ‚àû = 1, F ‚àû = 0 Where E 6 is injection/suction parameter. Further details about the obtained numerical solutions are presented in the next section. The transformed governing equations  (11) (12)  subjected to boundary conditions  (13)  are solved numerically by employing the fifth order Runge-Kutta-Fehlberg method. [78] , Yih  [79]  and Rashidi et al. Figures (1-2)  illustrate the influence of wedge angle parameter < with on velocity and temperature profile. Furthermore an increase in Œª may cause increase in temperature of flow. It is clear from graph that an increase in thermal radiation parameter leads to increase in temperature and thermal boundary layer thickness. The important conclusions of the analysis are 1. The non-dimensional velocity profile increases by increasing the wedge angle parameter <.
paper_432	The mathematical analysis method used. The Cotangent Bundles * of manifold d provides the basic model of a symplectic manifold. If are local coordinates in neighborhood of a point ‚àà . Time -dependent smooth Hamiltonian on Œ§ * , the cotangent bundles of . Generically, the corresponding Hamiltonian system `(%) =& H '%, % ). Given any , manifold , of dimensionn, with -. We see that(E > ) ,‚Ä¶, (E ) is a basis of T 5 * (M). The Cotangent Bundle Œ§ * contains the following classes of Lagrangian submanifolds; The fibers of Œ§ * . Let ‚àà and let I; T { * Q ‚Üí Œ§ * be the natural inclusion mapping. b) Let exp ‚à∂ ‚Üí 3 > be given by exp(% =J ‚Ä¢" ‚Ä¶ ‚àà 3 > . Let 0 ‚à∂ 3 ‚Üí 0 be the projection map. Let be a lie group‚Ä¢ ‚àà o‚ãÉ ~‚àû‚Ä¢. 8 ' : K V ‚Üí R s> ( K ), satisfying R o 8 ' u 0 ' and the equivariance property. 8 ' ( K , n, ' ) = 8 ' ( K, n ). ' Let be a Lie subgroup of a Iie group , and 3 a manifold on which acts. ), the left multiplication of : n`. these are easily seen to be free and proper. The twisted product √ó ¬∞ is the quotient of √ó 3 by the twist. Now consider a G on a manifold. ≈°J% ¬± ‚àà , with isotropy subgroup = ¬≤ . Define the map, ‚àÖ : ¬¥s > (0) ‚Üí * ( / ) by, for every 0 ‚àà ¬µ * Q. Then ‚àÖisc , invariant surjective submersion and descends to a symplectic homeomorphic. Note that ‚àÖ is "injective mod '', meaning that ‚àÖ(¬± 1 ) =‚àÖ(¬± 2 ) if and only if ¬± > =n.
paper_444	The other constraint is the resource constraint in which the demand of resource shall not exceed the total supply of resource for each resource type at any time. Furthermore, the equivalent form of the above model is given and its equivalence is proved. In this case, many scholars begin to consider the uncertain resource constrained project scheduling problem. However, the cost that effected by the increased quantities of human resource was not taken into account. The construction of this paper is organized as follows. In Section 2, an uncertain resource constrained project scheduling model will be built and transformed into a crisp form. (1) This paper only considers renewable resources. (3) Assume that the finish time of each activity is a decision variable. (4) No interruption is allowed for each activity in progress. " : The set of underway activities at time #. The uncertain resource constrained time-cost trade-off problem can be described as following optimization model: formula_0 In the above model, objective ‚óã 1 is to minimize the project total completion time; Objective ‚óã 2 is to minimize the expected project total cost which consists of the activity cost and the additional resource cost. Constraint ‚óã 4 reflects that for any time # and each resource type , the demand for resources shall not exceed the total supply of resources with at least given confidence level < = . Constraint ‚óã 6 shows the range of decision variables. Then excepted value of G is formula_1 Theorem 2. Model (1) is equivalent to the following model. is an uncertain variable, and the inverse uncertainty distribution of ? is formula_6 By Theorem 1, we know that the excepted value of ? is formula_7 Since ‚Ñ≥6‚àë ‚àà8 9 ‚â§ + ; ‚â• < = , then, ‚Ñ≥6‚àë ‚àà8 9 ‚àí ‚àí ‚â§ 0; ‚â• < = . By Theorem 2, we have ‚àë ‚àà8 9 ‚àí ‚àí Œ¶ MA (1 ‚àí <) ‚â§ 0Ôºå  (12)  i.e. formula_10 Therefore, the model (1) is equivalent to the model (2). The type of resources chosen in this case is human resource. For every activity , the increased quantity of resource is assumed to be a linear uncertain  variable Y(3, 7) . The duration time, cost and resource requirement of activities are presented in  Table 1 . The manager tends to the demand of resource shall not exceed the total resource supply with at least given confidence level 0.9 for any time # and each resource type . The cost per time unit of additional resource ! The constrains are recourse constraint and precedence constraint. The decision variable is finish time of each activity. The information of the activities. By describing the increased quantities of resource as uncertain variables, an uncertain resource constrained project scheduling problem is discussed in this paper.
paper_462	The goal of postgraduate education for master's degree in clinical medicine is to cultivate high-level medical and health professionals. Since 1998, the postgraduate education for master's degree in clinical medicine has been piloted in China. There are many problems such as insufficient practical ability training. "5 + 3" new training mode of training in combination. Chongqing Medical University was founded in 1956. Since 1979, it has enrolled doctoral and master's degree students. In 1981, it was approved as the first authorized unit of doctoral and master's degree in China. In 1998, it became the first batch of doctoral and master's degree in clinical medicine in China. Pilot units for professional degrees. Since 1998, the school has vigorously developed professional degree postgraduate education  [1] . Since 2009, the school has been focusing on reforming the training mode of clinical master degree postgraduates (hereinafter referred to as clinical master). It not only guarantees the quality of professional degree postgraduate training, but also supplies a large number of high-quality talents for related industries. How to reform the training mode of clinical master, improve the quality of training, and bring up a large number of high-level applied medical talents is the main problem for graduate educators. Educational development and other key problems of clinical master training. The organic cohesion of syndromes has effectively improved the quality of clinical master training. The above policy support ensures that our clinical master can fully participate in clinical practice skills training and ensure the quality of training. After completing the dissertation defense and meeting the award requirements, the graduate certificate and master's professional degree certificate can be obtained. Effective management during the transition period. At the same time, the workload of guiding professional degree postgraduates is directly linked to the promotion of their professional titles. Our school has made corresponding reforms in the curriculum system of clinical master's degree. The reformed curriculum system not only meets the training objectives of clinical master, but also meets the requirements of theoretical knowledge for trainees. Secondly, the curriculum of Master of Clinical Science is adapted to the requirement of training students' theoretical knowledge and foreign language. It is closely related to clinical practice. The secondary clinical colleges are responsible for both the health bureau, the training of residents and the school, and the training of postgraduates with professional degrees. Students are brought into the "two levels, two stages" training after they enter school. Requirements for the first stage of training. System for Clinical Ability Clinical competence is the core of clinical master training. School research and explore more rigorous clinical training and assessment methods. The clinical master emphasizes on examining the clinical competence of postgraduates. At the same time, the cost of training clinical masters has increased substantially. Compared with academic degree postgraduates, the training objectives of professional degree postgraduates are obviously different. The school revises the standard of clinical master's degree award, which reduces the requirement of publishing articles. Postgraduates can devote all their energy and time to clinical ability training. Schools from several aspects to improve the mentors, managers and related personnel of clinical master's education awareness: First, extensive publicity. Thirdly, we should formulate an enrollment system conducive to the development of professional degrees. The number of professional degree postgraduates enrolled by tutors in clinical departments is clearly required. All tutors working in clinical departments must enroll professional degree postgraduates. Qualification certificate students are not allowed to practise medicine in other places in accordance with the Law of Licensed Physicians, so it is difficult to carry out clinical training  [14] . It provides a mature reference model for the docking of professional degree postgraduate education and industry access standards in other fields in China. The curriculum system is guided by the improvement of vocational competence and solves the contradiction between curriculum learning and clinical competence training. The clinical competence assessment system checks the training process from qualitative and quantitative aspects, and achieves the whole process management  [15] . It has overcome many obstacles and steadfastly promoted the reform. It has basically realized the seamless connection between the degree education of clinical medicine specialty and the certification of professional qualification. The main results are as follows: Dozens of brothers such as Fudan University learn from our experience. This model is in line with the reality of the development of medical and health care and higher education in China. In the past five years, a total of 2063 clinical masters enrolled in the school have been integrated into the training. Many excellent students have become an indispensable new force in clinical colleges. They have achieved the ultimate goal of training "doctors who can really see a doctor". The employment rate of graduates has been guaranteed to be 100% for a long time. In December 2013, the school sponsored the National Symposium on the Reform of the Training Model of Clinical Master's Degree Postgraduates. Our school has 10 affiliated hospitals of Grade A and 28 key national clinical specialties. Since the reform of training mode, the enrollment of clinical masters in our university has been increasing rapidly. In the past five years, a total of 2063 clinical masters have been enrolled in the standardized resident training system. Professor Jin Xianqing and Professor Xie Peng were appointed members of the National Medical Graduate Education Steering Committee in 1998 and 2010 respectively. The implementation of "three combinations", namely, the combination of enrollment and enrollment, training and training, degree award and physician access. The current management system and mechanism are not suited to the key issues of clinical medicine master training, such as the development of professional degree postgraduate education  [18] .
paper_476	This paper uses JD Logistics as an example to quantitatively analyze the relationship mentioned above. In addition, it is important for the logistics enterprises to consider the constraints such as delivery resources and consumer satisfaction to achieve the appropriate decision of delivery frequency. at various stages in the delivery system, which will more likely to result in uneconomical performance. At present, most express companies are operating different delivery frequencies in different regions. The order volume, the orders' splitting ratio, the operational efficiency of delivery resource and the delivery costs will affect the delivery frequency. There are various of applications of system dynamics in the area of transportation  [2, 3] . Fan Xuemei et al. [5]  pointed out that joint delivery can effectively improve the efficiency of urban delivery. It proposed an assessment framework for joint delivery. This paper uses Jingdong Logistics (JDL for short) as research objects. First, it analyzes the boundary and causality of its delivery system. Then, it establishes a simulation model for the operation of the delivery system on the base of system dynamics. Each delivery operation mainly includes: storage, ferry, sorting, transportation and terminal delivery. The relationship of the workflows in the delivery system is shown in  Figure 1 . The delivery cost mainly includes the equipment usage costs and labor costs in the three stages of sorting, transportation and terminal delivery. The equipment usage costs can be divided into two parts: the fixed costs and the variable costs. The number of employees is affected by factors such as order quantity, transportation efficiency, delivery frequency, and the number of transport vehicles. Performance wage is determined by the actual amount of work performed during the delivery process. Sorting staffs consist of sorters and on-site logistics personnel. The sorter is the person who operates on the sorting equipment. The delivery frequency refers to the number of times of terminal delivery by the company in unit time (in days). Based on the surveys of JDL and interviews with related professionals, this paper summarizes 55 influencing factors on delivery frequency. These factors present a complex, nonlinear, and inverse relationship between each other. As is shown in  Figure 2 , 58 causal loops are formed. The extreme condition when the order quantity equals to zero was examined as well. The main parameters involved in the model are as follows: (   Table 1 , and the explanations are shown in  Table 2 . On the other hand, the cost index was calculated based on the summed number of shipments as JDL adopts single-batch delivery. Second  Third  Fourth   1  60  30  40  20  2  30  30  20  20  3  10  30  30  20  4  -10  10  30  5  ---10  Table 4 . The delivery frequency in scenarios 2 and 3 was increased by 9.5% over that in scenario 1. When the volume of orders increased from 1496 to 5300, the increased percentage of total delivery cost showed a downward trend followed by an upward trend. The minimum value of 3.9% appeared in the order volume of 3086-3154 units. The total delivery cost in scenario 4 was increased by 25% in average than that in scenario 1. As the order volume increased, the increased percentage of total cost tended to decrease in fluctuation. In this situation, the delivery frequency should be increased accordingly. Within the orders volume ruled in this paper, the delivery frequency and the cumulative delivery costs are positively correlated. Compared to scenario 1, scenario 4 had an average increase of 315 yuan per day in total sorting costs. Regarding on the sorting efficiency equipment, the operation time of sorting equipment became longer and the sorting cost increased. Figures 12 and 13  show that the transportation cost in scenario 2 was increased by an average of 14% than that in scenario 1. The increased percentage behaved a negative growth trend followed by a positive growth trend as the order increased. The transportation cost in scenario 4 was increased by an average of 45% over scenario 1. The transportation cost in scenario 1 showed a rising trend with the increase of orders. When the order volume was 2406, 2530 and 4700, the transportation cost rose rapidly. When the volume of orders increased to a certain value, the transportation cost of scenario 1 became the highest among the three scenarios. As a result, the overall efficiency of the delivery staff was reduced. At the same time, the increase in the delivery frequency also improved the rental cost of the delivery station. Figures 16, 17 and 18  demonstrate that the delivery frequency hade different effects on different transportation vehicles. Figures 19, 20, 21  and 22show that the delivery frequency had different effects on terminal delivery operations. In some cases, the area utilization efficiencies in Scenario 1 and Scenario 3 were the same. This is because the difference of orders volume in the largest batch between two scenarios was small. As a result, the company had to increase the number of delivery personnel and the area of shipments. As shown in  Figures 21 and 22 , the overall utilization efficiency of delivery station A was approximately 5% higher than that of delivery station B. In addition, scenario 4 can effectively ease the utilization of area and personnel in the delivery site. In addition, the average split of order quantity can effectively reduce the tension of delivery resources. When transportation vehicles are in tight supply, the delivery frequency should be increased when the order volume is 2530. Therefore, JDL should combine the simulation results with the actual situation before making the delivery frequency decision.
paper_479	It has been established that there are varieties of handwritten documents ranging from forgeries, counterfeiting, identity theft, fraud, suicidal note, contested wills. A forensic document examiner is saddled with the task of document authenticity. Let us presume that there are two opposing ideas. In ' competing, ' it can be said that they are mutually exclusive, but may not necessarily include all possible alternatives. Consider, as an example, a situation where the question is whether a specific writing belongs to a particular person. One proposition is 'The suspect is the author of the document in question'. A counter argument may be' The defendant is not the author of the document in question'. Such two theories are mutually exclusive, but there may be other reasons even if they are far-fetched. For instance, someone might have fabricated the writing with some special skills. The propositions involved should be relevant and the latter case does not seem to be applicable. The purpose of this paper is to illustrate how neural network approach to comparative handwriting analysis can greatly affect the outcome of a forensic investigation and make full likelihood ratio achievable. The writer's profile is a very important factor that is considered to accurately estimate a full likelihood ratio. Strength of proof serves an integral part of this problem. Figure 1  shows the graphical representation for the handwriting modeling. Original and disguised handwriting were gotten from each writer over a period of six months and a skilled forger was asked to forge these writings. The handwritings were preprocessed using the Otsu method after which they were segmented into different words using the Sobel edge detection algorithm. Back Propagation Neural Network (BPNN) served the sake of supervised learning of the system. BPNN in the context of this paper was to model the handwriting pattern of each writer over a period of time. We target is set for each character in the handwriting. The target will help to know which handwriting is original and disguised. The strength of neural computation comes from linking neurons in a network. The weighted number of the inputs is the activation of the neuron. Transfer function can add non-linearity to the network. Each character variable has a weight W i which shows its contribution in the training process. weight-recognition factor with weight ! " The weight from input points i and two hidden unit j is ! " Weight from second hidden unit i and output unit j is ! ) After randomly selecting the weights of the network, the backpropagation algorithm is used to measure the necessary corrections. Each scanned, segmented and clustered characters and alphabets which were collated over a period of six months were trained to learn the pattern for each writer. Base on decision law i.e. If L R value greater than 1 H p is true If L R value less than 1 H p is false. A more elaborate and collated result table is presented in  Table  2 . Thus there must be agreement in sign otherwise there is disagreement. Full L R void of nuisance parameters is needed for most forensic investigators. Several factors have to be put in place to estimate a full L R for forensic handwriting investigations.
paper_492	This function is the behavior of so-called short-term memory. It is human intelligence that gave birth to the technology. But the structure of our brain has not changed since tens of thousands of years ago. In this paper, a new neural network and its behavior are presented based on the above idea. Acceptance and generation of time series data of the nervous system of long-term memory is carried out in consistent with short-term memory. The number of logical elements used may not differ much from the sum of nerve cells in insects or zooplankton. The same is true for the recognition process. The same applies to general figures. Many East Asian recognize kanji as a combination of parts. Deductive logical development is desired. Finally, it shows the hierarchically connected neural network that can process for general time series data. The divided subsequence is defined the basic subsequence. For simplicity, assume time series data consisted of 10 type elements from a 0 to a 9 . Example shown in  Figure 1  shows that the time series data (1) arranged by randomly selecting the elements is divided into five basic subsequences (2). The dividing is done by the following procedure. (1) The first element is the beginning of the first subsequence. In given example, the leading element is a1, followed by a7, a4, a6 and a6. (2) If the same element exists in already divided subsequence. In this example a6 is the concerned element. The subsequences divided by above procedure are defined as the basic subsequence. Figure 2  shows the affinity with the neural circuit. It is a neural network having an input consisting of a plurality of bits are shown. When the first data c 0 is received activate the bottom. Four portions are activated in the  Figure 2 . By receiving c 2 , c 3 and so on the activated portions become narrow and narrow. It may be seen as the output of the connected AND logic element. The elements involved in the conversion are still activated at the time of output. This operation is a generation of (learned) time series data. that can be said a conversion of parallel to serial triggered by the first data. After the first data reception, the connected elements are activated as described above. That is, the operation with no reaction within waiting time is invalid and aborted. Both state transition diagram is shown in the existing  Figure 3 . For each basic subsequence a neural network that accepts and generates the basic subsequence is considered. This neural network is called a basic unit. Received data comes from the environment that encourages some judgment. It can be said as "stance to the event" that most animals have  [2] . The movement will be mentioned in the next chapter. However, recognition is limited to the area of the vicinity that the sensory organ catches. In order to be able to exchange information, it is necessary to transmit and reproduce the act. The ability to imitate fellow's action is indispensable. Imitating is first step of learning the behavior of the fellows who is transmitting information. The nervous system which is involved in the imitating function is called mirror neurons. In addition, it spreads to the story beyond space and time. "Let's ask store A to put a chocolate plate on the cake with your name on it." And you'll think of the shape of the cake and the action of lifting the chocolate plate and mouthing it. Two types of time series data can be considered in the above situation. Like talking about your childhood while eating cake. In order to avoid confusion, the episode must be corrected by reality. Figure 5  is an illustration for showing the state change of each part in the neural network. Stimulus from the sensory organs from the bottom becomes time series data and is transmitted to the upper part. In neuroscience, the morphism corresponds to synapses and axons, and is responsible for the transmission of information between objects. Therefore, the existence of the above functor would be considered naturally. Thus, the behavior of both categories can be migrated to each other. In this paper, short-term memory and long-term memory are both regarded as time series data brought about by activity in the brain. It is a short-term memory to be involved in the recognition and manipulation of the thing in the vicinity of the animal itself. It is necessary that the contents of both memories are consistent in the animal's life. Of course, the description does not convey all the features of the hippocampus. Only units contained in the tree structure corresponding to the context of the behavior can work. The bud is also felt in the judgment and the movement in our daily life.
paper_507	In present study we used the already existing topographical maps, satellite imageries and field work. The main aim of present study is to integrate the result of our study with spatial data, soil parameters, land inventory and used the output as a user friendly application using GIS which could predict the future susceptibility of region to landslide and% contribution of each factor for the same. ArcGIS 9.3, ERDAS and Excel software have been used for zonation, and statistical analysis respectively. A three-layered ANN with an input layer, two hidden layers, and one output layer is found to be optimal. As far as Indian scenario is concerned approximately 0.49 million km  2  or 15% of land area of country is vulnerable to landslide hazard and 80% is spread over Himalayas, Nilgiris, Ranchi plateau and eastern ghats  (GSI 2006)    [3] . Uttarakhand is an appropriate choice for the study since the newly developed state has been fighting with the catastrophe and is making front paper headlines for landslide at Vishnuprayag, Baldora, Lambagharchatti, Jharkula, phatabyung, and Amiya landslides  [4] . al, 1999 and varnes 1984)    [7, 8, 9] . The result obtained i.e. Whole study of this paper is based on following objectives 1. Identification of factors which affects to the landslide. Determination of the extent to which the various factors contribute to landslide. Annual average rainfall observed is 1750.50mm and mean temperature 16¬∞C. In the present paper landslide hazard zonation map has been prepared for the Rishikesh-Uttarkashi-Gaumukh-Gangotri. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area (Mohammad Onagh, 2012). slope, aspect, lithology, rainfall, land cover etc. were generated using ERDAS and ARCGIS v. 9.3. DEM (Digital elevation model) was obtained from BHUVAN. The factors in relevance to the landslide susceptibility analysis of Uttarkashi are: The back-propagation training algorithm is trained using a set of examples of associated input and output values. A neural network consists of a number of interconnected nodes. Architecture of neural network (source:  (Lee, 2009) ). In the present study we selected 107 points and all the six factors namely soil depth, soil type, rock type, land cover, slope and elevation and a excel database is created. The dataset is categorized into 60% training and 40% validation. The data is categorized into training, testing and validation, and all the six factors namely soil depth, soil type, rock and a excel database is created. The dataset is categorized into 60% training and 40% validation. As we have seen neural network can compute the output for a given input. For this three-layered feed-forward network was implemented using the MATLAB software package. For a new dataset the weights are unknown. The number of epochs was set to 3,000. Weightage of different factors are shown in table 1. The results are compiled below. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area  [13] . slope, aspect, lithology, rainfall, land cover etc. were generated using ERDAS and ARCGIS v. 9.3. Precipitation The next objective of our study was to present the weightage of various factors causing landslide. An artificial neural network technique was used. The dataset is categorized into 60% training and 40% validation. Using ArcGIS the Landslide susceptibility for whole of the map region can be seen. The result of present study with spatial data, soil parameters, land inventory and presented by a landslide hazard zonation map and a user friendly application using GIS that could predict the future susceptibility of region to landslide and percentage contribution of each factor for the same. The reliability of ANN is high over other methods.
paper_1	This helps in solving the problem of losing vital information that is generated from the social media. Clustering is a descriptive task of data mining. It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . A number of techniques can be used to do clustering. The quality of clustering also depends on both the similarity measure used by the method and its implementation  [10] . The system design methodology used was incremental prototyping. Requirements and Architectural Design can be done up front and then each prototype developed as the project progresses. The solution is complete when all the components are in place. Several activities were performed to come up with the system. The relationship between the data items can be established using the k-nearest neighbor technique. The fourth step was testing the system. The prototype was then subjected to testing using the test data. This is a collection of data whose class labels are already known. They are part of the data that was used to train the system but its results are already known. Finally, the model was used to classify a new user into a group. The illustration of the proposed prototype is given below. This is summarized in the chart below. The Na√Øve Bayes Classifier was also tested to evaluate its accuracy, precision and recall  [9] . A total of 200 tweets were extracted and used for this test which was summarized in a confusion matrix. This matrix consists of the following parameters: TP, TN, FP and FN, which are defined below. The results below illustrate a summary of what was obtained when 200 tweets were used to test the Na√Øve Bayes Classifier. This classifier was doing the classification using the unigrams. From this analysis the classifier performed above average with an accuracy of 71.4%. Precision and recall were however average. The classifier then grouped the users into different categories based on various tweets that they posted on the task. The study has proved that it can actually be used constructively in learning in various institutions.
paper_2	The developed system uses a microcontroller of PIC89C51 microcontroller duly interfaced with sensors. Junction timings allotted are fixed. The junction is a link to three roads which include; Effurun-Sapele road, Jakpa road, and PTI road. But the system only increases the level of traffic congestion during peak hours. As a result of this a lot of time is wasted in the process. The sensors used in this project are infra-red (IR) and photodiodes. However, this system lacked inbuiltmechanism for controlling vehicular traffic based on density. Junction timings allotted are fixed. Raspberry pi is used as a microcontroller which provides the signal timing based on the traffic. An intelligent traffic lights control system using a Fuzzy Logic approach was developed by  [5] . Data was analyzed and presented using descriptive statistics; tables and graphs by using excel 2003. For testing the adaptive traffic light controllers, a simulation system using Qt, C++ software integrated with MATLAB tools was developed. The top down design approach was adopted here. The complete circuitry is operated with TTL logic level of 0-5V. It comprises of a 0V to 12V transformer which is required to step down the 220V AC supply to 12V AC. This is converted by a bridge rectifier to a dc voltage. Below are the ratings of the transformer 0.7 volts for silicon diodes). For this circuit, V peak rectified = 16.67-2(0.7) =15.27v dc This voltage is the input voltage of the capacitor. For convenience, a capacitor of 1000uF is used. The reason for using PIC16F876A microcontroller over ATMEGA microcontroller is that, the former is cheaper and more readily available. Figure 4  shows the circuit diagram of the system. The Transformer steps down the 220 v AC supply to 12 v AC. The time delay in the traffic signal is set based on the density of vehicles on the roads. The IR sensors are used to sense the number of vehicles on the road. The Vero board is also called a strip board. And after soldering each unit, continuity test was carried out to ensure that proper soldering was done. The PIC Microcontroller was programmed using Embedded C language. The codes are as shown in the Appendix. Simulation was done via Proteus software. In order to address this problem, an advanced traffic congestion control system is required. One of such systems is the automatic signaling using IR sensors and Microcontroller. Although the aims and objectives of the project were achieved satisfactorily, it could be further improved upon.
paper_3	In the beginning, a related work on the architecture trends of Web-based AEHSs is presented. The teaching strategies are based on  [9]  learning cycle and learning style model. The aim of this article is to highlight the main contributions of the architecture of the MATHEMA in the improvement of the functionality of AEHSs. The key idea is the decentralization of their functions. The rest of the paper is organized as follows: In the section 2 related works for various models of architecture of AEHSs is presented. In the section 3 the architecture of the AEHS MATHEMA is presented in detail. This model retains in details all required information by the system from which the various decisions could be made. In this section some models that are most suited to the MATHEMA system are presented. The AHAM architecture model proposed by  [6]  is shown in the  Figure 1 . The AEHSs fit so nicely in this model. When designing a course it is important to first list the concepts. The next thing to do is to determine dependencies between the concepts. This leads to a structure of prerequisite relationships. The prerequisites are most common in educational material. The rules together form the adaptation model in AHAM. This is done by means of a teaching model which consists of pedagogical rules. AEHSs applications need to maintain a permanent user model. Proper's architecture is a combined architecture of SCORM LMS and AEHS. Educational content can be either SCO or Asset. Thus its architecture is a typical of a SCORM compliant LMS. They use the Apache Tomcat 5.5 as Web and application server and the MySQL 5 as database server. The DM structure is exported by the manifest file and is stored into Java Object Files. However additional data about the course is stored into the database. In the  Figure 3  the architecture of the WELSA  [14]  system is presented. The adaptation servlet queries the learner model database, in order to find the ULSM preferences of the current student. [12] proposed a framework for AEHS (  Fig. 4 ) based on JSP, Java servlets, and Javabeans. The Interaction Analyzer is responsible for acquiring information on learner's behavior. The Presentation Generator requests a composition of the presentation to the JavaServer Pages. ULUL-ILM  [1]  is an AEHS that focuses on student's learning styles. The result of that analysis is called domain model. For supporting features, it uses the (JSP) pages, Java servlets, JavaBeans, JavaScripts, and Apache Struts technologies. The content loaded to the MySQL database is accessed via JDBC API. This is an interface for Java that standardizes database connections, queries, and results of relational databases. A servlet is a Java class that implements the Java servlet API, a protocol to respond to HTTP requests. These servlets are complete programs that are capable of creating JSPs. This allows for much more flexibility in creating the page than XML. Servlets dynamically create JSP pages according to student requirements. JavaBeans technology comes into play. (3) Protects your intellectual property by keeping source code secure. The Apache Struts is a formalized framework for the architecture style design pattern Model View Controller (MVC). Figure 5  shows the architecture of the AEHS MATHEMA. It shows how all units work together to make the system operational. Bellow the operation of the units will be presented and analyzed. It is a set of pedagogical rules that combine the learner's model with the domain knowledge for adaptive performance. A component of an adaptive educational system is the representation of knowledge. The domain knowledge of the AEHS MATHEMA is the basis of the system's adaptation. Content is on separate pages, enabling it to be re-used. Student model contains information about student characteristics that allows the system to make adjustments using these characteristics. The model that supports the AEHS MATHEMA is the overlay model. This is an innovation on the architecture of AEHSs. It also offers additional information about the navigation techniques that it supports. knowledge of the cognitive goal. Figure 7  shows a snapshot of the meta-adaptation result. The learner has the ability to change his or her navigation technique. A snapshot of a meta-adaptation result. The pages displayed to the learner are dynamically generated. Figure 9  shows a snapshot of the page responsible for selecting preferences of the learner. MATHEMA supports the learner to find the most suitable peer for the formation of a collaboration team. Figure 10  shows a snapshot of adaptive group formation in the MATHEMA. This module is responsible for monitoring and supporting synchronous communication between learners via a chattool. Assessment of the pedagogical and technological aspects of the AEHS MATHEMA has been carried out. Research has shown that high school students increase their performance by studying through the AEHS MATHEMA  [13] . The evaluation of the system was carried out by students of the Department of Informatics and Telecommunications of the University of Athens, Greece. The implemented AEHSs so far use various techniques to implement their functions. The architecture of AEHSs becomes more complex as more and more functions are implemented. In order to support all these functions the architecture of the MATHEMA is more complicated from other AEHSs. The key idea is the decentralization of their functions.
paper_21	This scheme consists in evaluating the minimum distance of the reduced dimension sub code fixed by a Self Invertible Stabilizer Multiplier Permutation by Zimmermann algorithm. An efficient solution of this problem is the use of error correcting codes. For these codes, only a lower bound is known and the minimum distance is known only for some lengths and special cases  [2-3-4-5-6-7-8] . The remainder of this paper is organized as follows: The next section presents the main related works. The section 3 presents the proposed scheme ZSISMP. The section 4 presents the main results. The conclusion and possible future directions of this research are outlined in section 5. Determining the minimum distance of BCH codes is an important, but difficult, problem. For these codes, only a lower bound is known but the true value is still unknown for large codes. This section summarizes the most important ones. The use of this method has finished the table of BCH codes of length 255. In  [22] , Aylaj and Belkasmi improve the classical Simulated Annealing presented in  [16] . From these stabilizers, we take only a Self Invertible stabilizer if it exist and by using a mathematical tool, we find the sub code fixed by this involution and then we evaluate the minimum distance by using the famous Zimmermann algorithm. This section presents the proposed scheme for finding the lowest weight in BCH codes. For finding the minimum distance of BCH codes. Output: -d as estimated minimum distance of BCH (n, k, Œ¥) The both last result have been proved in  [9] , by using the Newton's identities. A comparison between the proposed scheme with Zimmermann algorithm, on some BCH codes are made. The table 2 summarizes the obtained results. The table 3 presents a comparison between the proposed scheme and MIM-RSC method  [24] .
paper_31	Chronic diseases have long duration, difficulty in treatment and high cost. From 1990 to 2017, the morbidity of chronic diseases with high incidence in China had been increasing continuously  [1] . Therefore, it is crucial to effectively prevent and control chronic diseases. With the development of Internet, people's life has been profoundly changed. Since the end of last century, some Chinese hospitals and universities have carried out some researches in this field. Meanwhile, some also have applied various wireless sensor technology to medical monitoring system. For example, some domestic hospitals use Bluetooth technology to monitor the ward room. In the same year, American scholar Jutra had founded teleradiology. Therefore, the word "Telemedicine" arose. At the end of the last century, the rapid development of semiconductor technology drove the revolution of information technology. This is the second stage of the development of WITMED. TV links were put into use in mental health services and other medical treatment. At this stage, telemedicine had made considerable progress. This is the third stage of the development of WITMED. Haji Bagheri Fard present methods based on deep learning algorithms to achieve high classification accuracy  [8] . Zigbee has been implemented on the Health Care Profile. (2) The problem of power consumption. (3) The problems of unitary monitoring data. (4) The problem of data processing. Finally, the processed data will be sent back to patients and their family doctors. The traditional prevention and treatment greatly increase the economic cost and time cost of patients. Based on ZigBee technology, a design of intelligent medical system based on semantic matching is proposed. Then the coordinator transmits the data collected to the intelligent medical system. The system architecture diagram is as follow: Fast and correct data acquisition is the basis of the platform's efficient operation. The process of reading and transmitting data is a loop. The data acquisition structure charts The related knowledge base in  Figure 3  can be regarded as an expert system. Data analysis roadmap is shown as follows: If there are X words appearing in the taglib and N-X words not appearing in it, the matching degree of the information is (N-X) / N * 100%. Therefore, it is obvious that the matching degree is a fraction smaller than 100%. The general algorithm of this part is as follows: Define priority function: f x, y ax by Œµ, with a basic definition as follows: x: semantic matching degree, 0‚â§x‚â§1 y: a request arriving at the serial position, 0<y‚â§1 a is defined as the weight of semantic matching degree, 0<a<1 b is defined as the weight of serial position of Request arrives, 0<b<1, a+b=1 Œµ is defined as the disturbance value, 0‚â§Œµ‚â§0.1, with the default as 0. With the definitions above, f x, y can turn to be: f x, y ax 1 a y Œµ , namely f x, y y a x y Œµ . Definition 2: let {X1, Y1}, {X2, Y2} is concept form of {X, Y}, {X2, Y2b {X1, Y1}. Definition 3: The concept similarity is calculated as:  formula_1 Patients' health information involves their privacy. Therefore, it is crucial to guarantee the security of data transmission in the process of information transmission. This system adopts AES (Advanced Encryption Standard) encryption algorithm which is widely used at present. The state matrix after row shift is multiplied by the fixed matrix to obtain the confused state matrix. The calculation is shown as  Figure 7 : Compared with the data presentation at present, it is more intuitive. The intelligent system realizes data acquisition, data encryption and processing, and big data analysis.
paper_38	The study adopted the descriptive research design. Education is an essential issue regarding the development of any country in the world. It is a progressive development of knowledge and skills of examinees through stages of teaching and learning at various levels  [1] . In the 2017/2018 academic session, a total number of students admitted was 4,031, and 1,632 was admitted into the Faculty of Science. There have been various perspectives presented by researchers and psychologists about what is academic performance and its importance. The CGPA score takes into consideration students' tests, assignments, practicals, examinations and sometimes lecture attendance. Formula 1 is used for calculating the CGPA. formula_0 Most universities in Nigeria have been using 5.0 as their perfect CGPA score. As such, the CGPA is considered to be a good predictor indicator of a student's academic performance. The rest of this paper is organised as follows: section 2 is the review of related literature, section 3 presents the methodology, section 4 discusses the results obtained, and the last section presents the conclusion and recommendation. Over the years in Nigerian tertiary institutions, there has been rife with complaints about students' poor academic performance. The objectives of this study are to: i. Investigate the relationship between the students' performance of their entry qualification and the academic performance in the 100 level CGPA examinations. The following research questions directed the study: 1. What is the relationship between OL results, UTME scores and Post-UTME scores of students and their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? There are a lot of definitions of students' performance based on previous works of literature. Reference  [9]  stated that students' performance could be obtained by measuring the learning assessment and co-curriculum. Several studies have criticised the use of UTME and PUTME as an imperfect instrument for predicting academic performance of students. However, the following review of literature examined the relationships between UTME and PUTME scores as a predictor for the academic performance of students have revealed contradictions in their findings. A sample of 214 students records was used for data collection. The records of students who graduated in the 2009/2010 and 2010/2011 academic sessions from seven faculties were obtained using the stratified random sampling technique. Reference  [1]  used the Pearson Product Moment Correlation Coefficient (PPMCC) to predict the academic performance of first-year students in four departments in the University of Abuja from 2008/2009 to 2010/2011 academic sessions using UTME, PUTME and CGPA. The authors used records of students admitted via the JAMB UTME from a Nigerian private university. The author further recommended the need for the PUTME exercise to be strengthened to have a fruitful admission of candidates. This section discussed in detail the methodology employed such as research design, sample of study, instruments used, the procedure for the collection of data and data analysis. The Faculty of Science consists of nine undergraduate B. Sc. The sample distribution is as shown in  Table 1 . The session with the largest sample size was 2014/2015 with 710 (21.8%) students. The semester examinations were mostly essay type questions. However, the OL results obtained from the database was for students admitted during 2010/2011 to 2014/2015 academic sessions. The stanine grades in the OL results obtained at either NECO or WAEC were collected and coded as shown in  Table  2 . The coding for the CGPA is also shown in  Table 4 . It was used in this research study. The data were regrouped and analysed by academic session and programme of study. PPMC is used to determine the degree of relationship between two sets of variables and compute the strength of association between the variables  [19] . In other words, MLR is used to predict a nominal dependent variable given one or more independent variables. MLR can have interactions between nominal and continuous independent variables to predict the dependent variable. The results of the data analysis are presented in tables according to the research questions that guided the study in this section. The Pearson Correlation analysis was carried out to find out if there exists a strong positive correlation between OL and CGPA, UTME and CGPA and PUTME and CGPA. For the Computer Science programme, as shown in  Table 5 , the correlation coefficient indicated a low negative correlation in UTME-CGPA (-0.092) and PUTME-CGPA (-0.001) and a low positive correlation for OL-CGPA (0.072). Table 6  is the likelihood ratio test results for the Computer Science, Mathematics and Physics degree programmes. Likelihood Ratio Tests is a statistical test of the goodness-offit between two models. Table 7  shows the results of the Parameter estimates, which is also called coefficients, for the Multinomial Logistic Regression (MLR) for each degree programme. The traditional 0.05 criterion of statistical significance was employed for all tests in  Table 7 . For the Computer Science programme, the slopes (B) of OL in all the CGPA categories are positive. Besides, the slopes (B) of UTME in the CGPA categories of 'Pass' and '1 st Class' are positive while the rest are negative. However, the relative strength of OL, UTME and PUTME on CGPA performance of Computer Science students is not statistically significant. What is the relationship between OL results, UTME scores and Post-UTME scores of students and their firstyear CGPA across the academic session, 2010/2011 to 2014/2015? PPMC was used to analyse the data for this research question. The summary of  Table 8  shows the correlations of OL and CGPA scores, UTME and CGPA scores and PUTME and CGPA scores for all the programmes used for the analysis. In the 2010/2011 academic session, the correlation coefficient indicated a low positive correlation in OL-CGPA (0.198), UTME-CGPA (0.189), and PUTME-CGPA (0.232). In 2011/2012, OL-CGPA (-0.114) and PUTME-CGPA (-0.043) display low negative correlation while UTME-CGPA (0.114) has low positive correlation. In 2013/2014, there exist a low positive association for OL-CGPA (0.061), UTME-CGPA (0.056), and PUTME-CGPA (0.038). The OL-CGPA (-0.071), UTME-CGPA (-0.090), and PUTME-CGPA (-0.040) in the 2014/2015 session signifies a low negative correlation. Table 9  is the likelihood ratio test results for all academic sessions ranging from 2010/2011 to 2014/2015. Table 10 , on the other hand, shows the results of the Parameter estimates for the MLR for each academic session. A Multinomial Logistic Regression (MLR) was performed to model the relationship between the predictors and membership in the CGPA categories for each academic session. Table 10 , the traditional 0.05 criterion of statistical significance was also used. For the students admitted during the 2010/2011 academic session, the slopes (B) of OL in all the CGPA categories are positive. Conclusively, the slope (B) of PUTME in the CGPA category of '3 rd Class' is positive and the rest negative. This predictor holds true especially for students who are in the CGPA category of '2 nd Class Lower' and '1 st Class' respectively.
paper_57	This paper attempts to overcome stagnation problem of Ant Colony Optimization (ACO) algorithms. In this case some heuristic information which guide search process is useful. Ant colony optimization represents an efficient tool for optimization and design of graph oriented problems. It is a multi-agent meta-heuristic approach and was first purposed by M. Dorigo et al. [2]  as Ant system (AS) algorithm. The aim is to find the shortest path. where œÅ ‚àà (0,1) is the pheromone persistence (1 -œÅ is evaporation rate) and m is the number of ants. Evaporation rate is a user adjusted parameter and affects pheromone durability; i.e. how long the acquired information will be available. An ant in each node has to make a decision which arc to take. If q ‚â• p k ij (t), then choose the next node randomly. Genetic algorithms (GA) were proposed by  Holland (1975) . The original GA is known as simple genetic algorithm (SGA). The population size is given by the number of genomes, i.e. Mutation (ii) mimics random gene changes. The simplest form is one point mutation on  Fig. If more of such nodes exist, random selection is applied. At first mutation is applied. It is applied on random selected tour T k (t) in random selected node. If mutation is not feasible, another node is chosen. After all mutation operations are performed, crossover operations are applied. Parent strings are random selected. After all genetic operations are executed fitness evaluation and pheromone update are scheduled. Genetic operations do not have to be necessarily feasible. The above described ACO GO algorithm has been tested on a random generated graph. The task is to find the shortest path between start node n s = 1 and end node n e = 80. For each setting 500 trials were performed. Simulation results were divided into three groups according to number of crossover pairs and are listed in the  Table 2 . The outcome with different mutation distribution is asymmetric. To prevent interference, no mutation operation was allowed. The results vary  (Fig. Beyond 60% threshold ants foraging behavior is suppressed by crossover overload. This is caused by the search space dimension. It is too large for ten ants to meet. GO does not affect the length of the search process. It has been proved that genetic operations increase ACO algorithm performance. Limit of crossover is 60% of crossover rate. The higher the crossover rate, the lower the accomplished / required ratio. Mutation operation causes better results than crossover operation. The higher amount of mutation operations the higher the performance gain is. The results are promising; GO improves ACO algorithm performance more than twice.
paper_78	Each formation has its own composition, structure, distribution range and environment, and all of them should be protected effectively. The results suggest that fuzzy C-means clustering is an useful technique for classification of plant community. Quantitative methods, such as numerical classification and ordination, are significant in ecological analysis of plant communities  [1] . It is applicable in ecology because the description of ecological systems is not always possible in terms of a binary approach  [4] . Ecological communities, such as Glycyrrhiza uralensis communities, have great variations in their species composition respond more or less independently to environmental variables. Because of this, methods based on fuzzy mathematics might be more appropriate in ecological analysis  [6, 7] . Like licorice, most medicinal plants are harvested in the wild and the extended use has led to some medicinal species endangered and tending to be extinct, and the protection of such medicinal species has been urgent  [12, 13] . Classification is the process of dividing plots into clusters (communities) so that plots in the same cluster are as similar as possible, and plots in different clusters are as dissimilar as possible. Fuzzy C-means clustering is a soft classification technique  [5, 7]  in which a plot can belong to more than one cluster, and associated with each plot is a set of membership levels. The Fuzzy C-means clustering attempts to divide a finite collection of n elements (plots) formula_0 into a collection of C fuzzy clusters with respect to some given criterion. Given a finite set of data matrix (species √ó plots), the method calculates a list of C cluster centres  formula_1 formula_2 Where i = 1, 2, ‚Ä¶, N = the number of plots; j = 1, 2, ‚Ä¶, C = the number of clusters; U = {U ij } = the matrix of membership values, U ij is the membership of plot i in cluster j; V is a matrix of cluster centers; m is fuzzifier which determines the level of cluster fuzziness (1 ‚â§ m < ‚àû). (dA ij ) 2 is the distance index: formula_3 X i is the vector of attribute measurements in plots, usually a vector of ordination scores; V j is the centre of cluster j, if A is a unit matrix, then formula_4 Based on the algorithm above, the procedure of fuzzy C-means clustering is as follows: (1) Selecting a common ordination method, we use Detrended Correspondence Analysis (DCA), and do ordination analysis for species data. For a plot, any value can be given as its membership value in cluster j, but the sum of memberships for a plot must be equal to 1:  2  is calculated using equations (2) and (3). formula_5 formula_7 Based on the new membership values U, we go back to the fourth step and calculated the next turn V j , (dA ij )  2  and U ij iteratively, and until the membership values become approximately stable. However for clustering purpose, a plot should belong to the cluster in which it had the maximum membership value. Based on a general survey of Glycyrrhiza uralensis and its community distribution, five study regions, Chifeng (in inner Mongolia), Hengjinqi (in inner Mongolia), Minqin (in Ganshu), Aletai (in Xinjing) and Kashi (in Xinjing), were selected as sampling sites  [7] . Twenty plots of 5 m √ó 5 m were established randomly at each site which is over 40 ha in area. The coverage, mean height, individual number for shrub and herb species was measured in each plot. Fuzzy C-means clustering is a nonhierarchical method which provides C clusters and their plot compositions  (Table  1) . The name and characteristics in species composition, structure and environment of each community are described below. The average cover of Glycyrrhiza uralensis in this community is 35% with a density of 5700 ha -1 . Its disturbance intensity is medium and heavy. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 59500 ha -1 . Its disturbance intensity is medium and heavy. The average cover of Glycyrrhiza uralensis in this community is 37% with a density of 4900 ha -1 . Its disturbance intensity is heavy. The average cover of Glycyrrhiza uralensis in this community is 33% with a density of 5500 ha -1 . It is distributed from 400 to 700 m in hills with slope 10 -30¬∞ in sunny and semi-sunny slope and sandy soil. Its disturbance intensity is heavy. It is distributed from 400 to 800 m in hills with slope 20 -35¬∞ in sunny and semi-sunny slope and sandy soil. Its disturbance intensity is heavy. Theoretically, fuzzy C-means clustering is the only one soft clustering method and can deal with much imprecise and incomplete information and has advantages in solving non-linear problems and in studying complex system  [2, 17] . Therefore, fuzzy C-means clustering should describe ecological phenomena and rules better  [5, 6] . This study proved that fuzzy C-means clustering is fully usable in classification analysis of plant communities  [1, 19] . They distribute from temperate meadow grassland, through typical temperate steppe and desert steppe to temperate desert regions from east to west in North China  [15] . These communities are important not only for protection of medicinal plant species but also for conservation of ecosystems and their environments in semi-arid and arid regions in North China  [23, 24] . The classification results by fuzzy C-means clustering are reasonable according to vegetation classification system of China  [25] [26] [27] .
paper_96	With the rapid development of China's transportation, the frequency of traffic accidents is also high. Such warning system of obstacle avoidance of embedded electronic guide dog can effectively eliminate the impact of the traffic environment and the subjective factors of the blind, warning in advance for the travelling blind in time, effectively avoiding obstacles such as vehicles, to reduce traffic accidents caused by the their blindness. This humane technology innovation is the specific embodiment of environmental science and technology aesthetic theory in the field of scientific and technological innovation. In fact, this "warning system of embedded electronic guide dog avoiding obstacles" is similar to a car anti-collision avoidance system. In this way, the design of obstacle-avoiding early warning system of embedded electronic guide dog has the following advantages. [3]  The alarm system of guide dog adopts the combination of light and sound. Sound will produce auditory stimulus for the blind. The hardware of the obstacle-avoiding early warning system of embedded electronic guide dog can reference the Figure 1:  Figure 1 : Schematic diagram of hardware system structure  [4] We should optimize the choice of the obstacle-avoiding early warning system technique of embedded electronic guide dog, based on China's national situation. The design of obstacle-avoiding early warning system of embedded electronic guide dog also adopts the generally used embedded processor. The full name of ARM is Advanced RISC Machines. It has a variety of merits, like small size, high performance, low power consumption, and cheap cost. It extensively uses the registers with a fast speed of instruction execution. And its way to addressing is much easier and more flexible, and its operating efficiency is high. Also, it could complete most of the data manipulation in the register. At present, the embedded RTOS (Real Time Operate System) mainly includes two categories--commercial and free. These are the most remarkable features that uCLinux owned. In addition, the design of obstacle-avoiding early warning system of embedded electronic guide dog also makes a full use of USB. The reference model is shown in  Figure 2 . of USB reference model  [4] The traffic safety problem of the blind is an urgent issue to solve. The warning system of embedded electronic guide dog avoiding obstacles studied in the project can effectively eliminate the influence of traffic environment and blind subjective factors, send the pre-trip alarm for the blind in time, effectively avoid obstacles such vehicles, to reduce traffic accidents caused due to the blindness. This humanized technology innovation is the embodiment of environmental science and technology aesthetics theory in the field of science and technology innovation practice.
paper_134	So, the aim of this study was to develop a dielectric-based system to Authenticate in olive oil using cylindrical capacitive sensor. Both of these areas work in model identification and data classification issues and will, in effect, be directly used in data mining, and both groups are active in identifying and using neural networks and decision trees  [2] . These methods are very damaging, costly and time-consuming. In recent years, nondestructive methods have been considered in purification. If the dielectric coefficient is bigger, it will be a better insulating property  [6] . One of the main reasons for this low consumption is the high price of this oil. Also, processing of data and results were presented in fourth section  [11] . (2010) conducted a research on olive oil for detecting adulteration using a dielectric spectrum. They also used the partial least squares model (PLS) to detect oil falsification. The results of the PLS calibration model showed good results for the adulterated oils with soybean oil as a counterfeit oil. The relationship between the rates of fruit dielectric constant in a frequency of 1 kHz to 10 MHz was investigated. The results of the prediction were satisfactory and the correlation coefficient between the actual yield and the expected value of the fruit was 0.85  [13] . (2006) predicted egg quality parameters using its capacitive properties. The results showed a significant difference between DC / DV ratio during storage period. The correlation coefficients and the mean square error of the statistical indexes were 0.98 and 0.0006, respectively. 70% of the data was for network training, 15% for validation and 15% for the network testing. In order to select the best network, the number of hidden layer neurons was changed from 2 to 50  [15] . In this study, a recent study has been carried out to identify the authentication of olive oil. In this article the experiment was done by olive, sunflower, Canola and corn oil. The device used consists of the Arduino board, ICL8083 and AD8302. The Arduino board is used on the Uno device, which uses the Atmega2560 microcontroller. The AD8302 phase detection chip provides a simple method for measuring the input ratio of the domain and the phase difference of the two signals simultaneously (22). One of the new ideas in identifying and categorizing patterns is a backup vector machine or SVM. In particular, SVM does not have the problem of local optimization in its training, it builds the categorizer with maximum generalization, determines its structure and topology optimally, and functions nonlinear differentiation easily with low computing using It forms the concept of an inner product in the Hilbert space. In this study, coarse function was used to regression test data. Figure 4  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn. Using this technique, the amount of RMSE for the samples were equal to 3.56, 2.45, 3.05 and the amount R 2 was equal to 0.92, 96 and 94, respectively. Figure 5  shows the results of adulterated oil boosted tree regression. According to the results, Most R 2 related to Olive-Canola (0.90) and the least amount of R 2 pertain to Olive-Sunflower (0.86). The results were predicted and modeled using regression methods. In this study, Quadratic function was used to regression test data. Figure 2  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn Using this technique, the amount of RMSE for the samples were equal to 3.65, 2.46, 3.31 and the amount R 2 was equal to 0.92, 96 and 93, respectively. In the current research, three different techniques were applied to predict olive oil adulterated.
paper_139	At the present time web contains many indistinguishable documents. We introduce the correct graph theory based KTMIN-JAK-MAXAM algorithm filters out the redundant link. From the proposed system, we have relevant information with more accuracy. Traditional web mining algorithms handle with structured document  [7] [8] [9] [10] [11] [12] [13] [14] [15]  than the advanced methodology of mining algorithm can dealthe entire heterogeneous document comprises of images  [9] , graphs, videos  [16] , etc. The proposed algorithm shows the procedure to eliminate indistinguishable pages in the set-up of web pages. Iterate throughall adjacent vertices if possible. Here all nodes have an equal degree  (3) . Now the set U consists of the nodes A, B. 4.2 Search the node B, the unvisited adjacent node is from B as 2 and 7. Now the set U consists of the nodes A, B, 2. 4.3 Now travel around the node 2, the unvisited adjacent node is from 2 as 1 and 4. After inclusion of the node 1 the set U consists of the nodes A, B, 2, 1. 4.4 Discover the node 1, the unvisited adjacent node is from 1 as only 3. Now the set U consists of the nodes A, B, 2, 1, 3. We propose a graph theoretical based algorithm for detecting and eliminating redundant links.
paper_145	The present study was based on data collected from 900 respondents of both urban and rural areas of Bangladesh. Among them 44.3 percent were overweight and obese. Majority (70.6%) were diabetic patients. With the increase in age significant increase in prevalence rate of obesity was observed. Higher prevalence rate was also observed among housewives. Higher prevalence of obesity was noted among females. Behavioral factors have significant effects on metabolic risk. The study was based on data collected from both urban and rural people of Bangladesh. The investigated diabetic patients were 544. The number of this latter group of respondents was 346. The analysis was done by using SPSS [version 20.0]. The respondents were classified as underweight  [BMI < 20] , normal [BMI,  20 -25] , overweight [BMI < 30] and obese [BMI 30+]. Factor analysis was done to identify the factors for obesity. Of course major respondents (81.4%) were from urban area. The levels of obesity were significantly different among males and females [  Table 2 , P (œá2 ‚â• 27.546) = 0.000]. There were 58.9 percent males among the respondents and 47.2 percent of them were normal. The corresponding figure among females were 37.3 percent. However, compared to males more females were obese. Significant differences in proportions of obesity among the two religious groups were noted [P (œá2 ‚â• 10.82)= 0.012]. The percentages of normal groups among the respondents of ages 25 -40 and 40 -50 were 35.5 and 36.6, respectively. Levels of obesity was significantly associated with levels of ages [P (œá2 ‚â• 18.34) = 0.008]. Maximum (25.5%) respondents of obesity was noted among housewives. The data indicated that 54.4% respondents had income less than 30,000.00 taka. Obese group was also more (30.3% 0) among them. These two groups of people were 450 and 164 of them were overweight. Again, those who were not doing any physical labor (23. Table 10  showed that 67.6 percent underweight respondents were affected by diabetes. There was no significant association between level obesity and prevalence of diabetes [P(œá2 ‚â• 0.851) = 0.837]. The present study also indicated similar result [  Table 11 ]. Among the smokers 47.2 percent were normal and 37.2 percent were overweight. The association between smoking habit and level of obesity was significant [P(œá2 ‚â• 20.189) = 0.0.002]. This was done by factor analysis. The analysis helps to identify the important variables to explain the variation in the data set  [15, 21] . Moreover, no communality for a variable was less than 0.4  [22] . So, the inclusion of variables were satisfactory. These three variables were more important for the variation in the level of obesity. The coefficients of the components were presented in  Table 13 . This component explained 25.733 percent variation in the data of obesity. This component explained 10.86 percent variation of the obesity. Around 50 percent respondents were overweight and obese. Higher (71.6%) prevalence rate of diabetes was noted among the overweight and obese group of respondents. Similar finding was also noted in another study  [21] . The prevalence of overweight and obesity were significantly associated with age, religion, education, occupation, marital status, income and smoking habit. Similar results were also noted in separate studies  [5-9, 15, 20 -21] . Around 50.6 percent people of urban area were overweight and obese. This result was also similar as was observed in another study  [21] . They were classified as underweight (BMI <20), normal (BMI, 20 -<25), overweight (BMI, 25 -< 30) and obese (BMI = 30+). The percentages of these four groups of respondents were 7.6, 43.1.34.0 and 15.3, respectively. Most the respondents were in normal and overweight groups. Among the respondents 84.1 percent were of the age 40 years and above and among them 42.8 percent were overweight and obese. This finding is similar to that observed in both home and abroad  [23] [24] [25] . c) Counseling is needed for the obese children and adolescents. The public health authority can play a decisive role for the above steps.
paper_212	Epidemiological disease models have been used to explain the dynamics of HIV/AIDS in the population from the early 1900s. The models developed however faced considerable challenges ranging from inaccurate representation of natural data for deterministic models, to methods of forecasting such as statistical extrapolation which assumes that current conditions will prevail which is not always the case. Despite the spread of HIV/AIDS having been explored widely, not much literature is available on the Gillespie Algorithm based SIR model. We conclude that the simulated model reflects reality. In 1982 the CDC identified the same disease among IVDU, hemophiliacs and Haitian residents. It was characterized by its etiological agent HIV in 1983,  [1] . A serological test, was then made available. In 1984, Robert Gallo discovered that HIV was responsible for AIDS. In 1984, several HIV and AIDS cases were documented in Kenya. This accounts for 4% of new pediatric infections worldwide. Among the expectant women there are 13,000 new HIV infections among children. There are various questions still left unanswered to date on the HIV epidemic. They applied these dynamic models to forecast the transmission of HIV for the Chinese population. In this model, there were no forms of intervention. The formulated model was used to forecast the number of PLWHA. The number of HIV infections in 2010 was predicted to approximately 1000000. The group most affected would be the 31-40 years group. A stochastic differential equation SI model with demographic stochasticity has already been developed  [8] . They used the Milstein method to simulate for analysis. This makes it a prime candidate for the tau step vantage point. Other authors have made contributions to mathematical epidemiology by performing simulations that explain the process of disease spread. There is need to come up with a stochastic mathematical model that better expresses the changing number of HIV/AIDS cases. This study seeks to incorporate a stochastic aspect in the deterministic SIR epidemiological model. This will allow us to derive new insight from the analysis of the simulation of this SIR model. The transmission and infection rates were considered to be variant. The SIR model explained how the epidemic manifests in all the compartments. The graphical representation of the developed stochastic model is shown;  Figure 1 . The stochastic SIR model. Assuming that the resulting state is . A propensity function is the probability of one event occurring in the time interval ! This implies the probabilities are individual therefore discrete. Discrete evolution is modelled in discrete time. The data was obtained from NACC for HIV/AIDS cases. The means and variances of the simulated and natural data were computed. A stochastic SIR model was simulated with a mean step size of 0.006336446. 537 tau steps were made in the model. Curves produced are illustrated below. where C represents the variance of D The calculated value found was 64.958. The critical value was 47.4. Since the calculated value is greater than the critical value, the decision rule is to reject the null hypothesis. Therefore, the conclusion is that the simulated data model fits the natural data model. Mathematical modeling of disease trajectory using Gillespie based algorithms is yet to be explored extensively in literature. The simulated curves were compared to HIV/AIDS data. The simulated curves were found to resemble the data available in reality. Mathematical modeling is an area that requires more research.
paper_214	Objectives of decision-making process aimed at adopting the best solution from many possible alternatives. Optimizing the use of a certain class of models for finding the best solutions. Among the methods most commonly used are decision analysis and mathematical programming  [1, 2] . Each alternative are attached estimates and probabilities of achievement. Solving the problem is to build decision tables or decision trees, from which it selects the best alternative. Tabelelele decision highlights a possible alternative schematic characteristic information. Apply mathematical programming problems which lead to the formalization of a mathematical relationship between decision variables and purpose. The simulation, conducted testing process is carried out using computers on a defined pattern  [2, 6] . It is the only method that can be applied to unstructured problems. data model can be used in the construction of real observations (numerical values) or knowledge. These are translated into algorithms which are executed by a computer system. This led to consideration of simulation as one of the most powerful tools in decision making. Simulation becomes a technical coordination of procedures using the computer. The solution offered is one spot that has no counterpart in the real system. In complex cases, the problem breaks down into subproblems more manageable, easier structured. Based on patterns defined by an efficient simulation can generate alternatives. Intuition, creativity and experience allow decision-makers compare alternatives; predict outcomes of each alternative separately. After the final resolution of the model, select the best alternative is chosen implementation plan. The assessment in turn depends on the search method. Structural problems use mathematical formulas and analytical method to achieve an optimum solution. In order to improve efficiency the best solution search algorithms are used. Methods called heuristics, based on a thorough analysis of the issue. Basically successive tests are performed, the search progressing from a solution to another. Assisting decision states that the decision is the responsibility of the user. Personal data is data that relates to the behavioral aspects of decisionmakers in making decisions. The data source, internal or external, data is extracted and managed by a management database. The management of the database depends on the organization of data. In most cases there is SGBS transactional relational data system and a management database for multidimensional data warehouses created. The data dictionary is a catalog of all data from the database. It contains data definitions, data sources and their intrinsic significance. The data dictionaries are permitted operations to add new data, deletion or retrieval of existing information according to certain criteria. The SQL language is used, which accepts requests for data from other systems. It is the component that differentiates interactive decision support systems to other systems. The models are domain-specific and models can be classified into strategic, tactical and operational models models. The models are used currently in operational and transactional system that aims of the organization. In building a data warehouse is based on the analysis of data. Extract information in order to obtain information for decision making. Basically there are two steps: designing and populating data. Conceptual models are multidimensional and designed to organize data necessary decision-making process on issues. In such a model is highlighted: quantitative data centralized called measures of activity quantitative criteria for centralized aggregation  ,  referred sizes  relational table that stores the measures identified by   the facts dimensions is called table  Tables where aggregation criteria has explicit codes,  called type tables list.
paper_216	The Black-Scholes model is a well-known model for hedging and pricing derivative securities. A number of studies have attempted to reduce these biases in different ways. Amongst the non-parametric approaches used to improve the accuracy of the model in this study is the Wavelet-based pricing model. The study was carried out using simulated stock prices of 1024 observations. Derivatives are instruments whose value depend on an underlying asset. Derivatives includes; Forwards, futures, options and swaps. The ones traded on the exchange are standardized and regulated. According to the right to sell or to buy, we distinguish the two types of options. In the Kenyan market, derivatives are yet to be developed. This study therefore prices a European option using two nonparametric methods and a parametric method. The Wavelet based pricing model is another nonparametric method alternative used to price derivatives  [4] . The remainder of the paper is structured as follows: Section 2 discusses the existing literature; Section 3 presents an overview of the Black-Scholes model and Wavelet based pricing model. Section 4 describes the data, shows the empirical results and performance measures of the models. Lastly, section 5 concludes the study. A lot of improvements have been done to the original Black-Scholes formula since the paper of  [2] . The Black Scholes model does not correctly price in high volatility markets  [5] . Employing the wavelet method to de-seasonalize prices of electricity  [9] . The de-noising ability of wavelets was also recognized in  [7] . Evidence have been provided to support that wavelet based local linear approximation  [11] . European options can also be priced using the Shannon wavelet  [14] . Another example is the application of the wavelets in nonparametrically estimation of a diffusion function  [15] . Wavelets can also be used in calibrating parameters with long memory processes  [16] . is the cumulative distribution function. As it has been seen from the reviewed literature, this model is a new method in the field of finance. ( is the bilateral inverse Laplace transform. Interest rate r is assumed to be constant. When T -t = 1, Œò (v) is the risk-neutral MGF for the rate of return per unit of time. This needs to be approximated by wavelets. This function also emulates the probability density function of asset returns. In this study, for us to compare the pricing performance of the three models, we used the commonly used statistical criteria. In this study Monte Carlo simulation was used to generate 1024 stock prices. Other complex options include; Bermuda options and exotic options. Moreover, hedging performance may be evaluated with respect to each of the two models.
paper_219	Traditionally quality of data delivered to an enterprise user focuses on network performance, i.e. quality of service (QoS). The QDS attribute brings distinction to the resultant data quality of the network's quality of service. There is also a need to assess the quality of data sharing across the enterprise network. One recent method subjectively assess the quality of data is to measure the user satisfaction referred to as quality of experience (QoE). The resultant minimax value correlates to the lowest performing attributes of the framework. That same data abundance challenges the network capacity and overloads the capacity of the human operator. At the same time, developers are also facing a new host of challenges from increasing cyber threats. All of this is in addition to the historical problems of network management and quality of service. Traditional QoS expects prioritization has occurred prior to data entering the network. Section 2 of this paper provides a brief overview of the origins of net-centric and net-ready policies. Section 3 defines a new model for valuing the quality of data by measuring user satisfaction. Section 4 presents a method for applying the new model attributes to evaluate data quality across the enterprise. Vital to the value of NCW is "the content, quality, and timeliness of information moving between nodes on the [enterprise] network." These enterprise catalogs and registries contain discovery metadata entries for an individual data asset or group of data assets  [6] . Untrusted data can introduce error, uncertainty, and delay into the military decision process  [8] . The performance of the IT must be quantifiable with threshold and objective values that are traceable to measures of effectiveness (MOEs)  [3] . The identification of the connections must be specific (i.e. 1  illustrates the mapping relationship for each enterprise attribute to various DoD objectives of net-centric, net-ready and cyber-security. The subjective measure of overall user-satisfaction of a service or application is referred to as quality of experience (QoE)  [9] . Sampling users is the preferred and direct method for measuring QoE. The question arises how to get the users' opinions. Usually the MOS is formed from arithmetic mean of a population of user opinions. develop or enhance the prediction model of QoE for each attribute. For a sample of size n consider v strata with sample s k in each stratum, i.e. ‚àë i=1 to v n i = n and s = U k=1 to v s k . where n ‚â§ N and s = {1, 2,‚Ä¶, n}. The QDS takes into account the effects of environmental conditions on sensor performance for given design parameters. QDS is a subjective rating from the perspective of the end user  [11] . Traditionally these QoE MOS ratings were undertaken by panels of experts. The reason to use the full reference is to capture environmental conditions resulting in the most accurate predictions of ratings. A standard to address the rating of motion imagery (i.e. In  [17]  and  [15]  there are equations to estimate the interpretability for still imagery and video, respectively. Over the last several decades there have been many papers on the topic of QoS. But just for completeness a brief explanation of QoS is given here. However there are a number of challenges to QoE discussed in  [19]  and  [20] . To understand cause and effect it is ideal to have the full reference i.e. These models of QoE based on QoS objective measures are often referred to as "QoE\QoS" correlators  [21]  and  [12] . Models have been developed to correlate the QoS with QoE for multimedia applications  [12] . For both models  [21]  showed QoE had an exponential model in terms of their QoS objective metric(s). A number of authors  [12, 15] The measure of data relevance expresses the utility provided by the data towards the consumer's objective(s). A scalar measure is required to support evaluating the degree of relevance on the enterprise performance. Intrinsic data relevance represents the relative value (i.e. Intrinsic relevance reflects three properties of the data: form, spatial, and temporal. The data's form properties indicate the suitability for the type of data to convey the required information. Enterprise data systems can offer multiple forms of data to the consumer (e.g. overhead, side-view, rear-view, distant, near). metadata) with those of the producer. (2) A high user satisfaction for the enterprise data quality can only occur when all three attributes have high QoEs. Alternatively the data relevance user satisfaction to match the other attributes can be improved. Consider a simple example in  Table 1  to illustrate the minimax in a game theory context. Player one has three action options U, D, N to choose from. Thus, the minimax strategy is Player one move of N and Player two move of R with a payoff vector (4, 3). The minimax strategy in game theory inspired the decision theory approach of Abraham Wald's minimax model  [25] . DR subspace could consist of parameters to measure objectively various properties and elements of data relevance, e.g. f d (s) = u HT . The x-axis of the chart in  Fig. 3  is organized to reflect the connection between QDS and QoS. The final enterprise data quality, as shown in  Fig. 3 , as formula_6 In  Fig. This paper introduced a framework to determine data quality on enterprise networks for DoD net-centric and net-ready initiatives. The framework's data quality model consists of three attributes: quality of data at the source, data relevance, and network QoS. The final value of data quality for the enterprise network was demonstrated as the attribute with the lowest score. The paper notes the definition of the relationships between attribute's objective measures and the final quality of experience.
paper_241	Transformers are the key equipment in electrical power transmission. C transmission, power transformer is one of the most important equipment. The system is capable of sustaining a variety of environmental and operating impacts that resemble normal operating conditions which may include lightning striking the transmission lines, excessive loading, deterioration or breakdown of the equipment insulation resulting to power systems experiencing occasional faults such as phase to phase or phase to ground faults, over current, over voltage or even temperature based faults. differential current) and can be prevented using differential protection and microcontroller based relay protection. The protection techniques employed differential relay mechanism with Arduino. At all times, the Arduino senses the condition of the transformer. [4] Ochieng designed a microcontroller based power transformer protection system which uses a current sensor as the interfacing instrument between the current transformer and the pic16f690 microcontroller. The current sensor acs712x series was used in the project as the interfacing instrument between the power transformer and the pic16f690 microcontroller. A power transformer functions as a node to connect two different voltage levels  [3] . The differential protection scheme is concerned with the faults that arise from associated difficulties such as the magnetizing inrush current and saturation. The power transformer to be protected is connected to the main AC supply and delivered power (P=IV) to the output terminal. The power transformer primary and secondary currents are measured by the current transformers, which are to be protected by the Arduino and the circuit breaker. The step-down transformer is used to step down the primary or source voltage from 220 to 12V. This 12V is regulated to 5V (using L7805CV voltage regulator) which is the required voltage for the Arduino. A bridge rectifier is used to convert the alternating current from the secondary side of the step-down transformer to direct current for use by the microcontroller. The current are first connected in series and then in parallel to the secondary side of the step-up transformer to display. The LCD display is used to display the differential current values of the transformer and to display the voltage and current level of the system as well.
paper_251	Secondly, Now computing power or connected computing power (with network) has more demanding and significant role in almost all areas of epoch including market analysis, searching, map, accounting, medical, trading, shopping, rescue operations and many more, the list is endless. Advancement of electronics and telecommunication field has done the job. Secondly most of the resources are idle i.e. Cloud offers instant service (software, platform or infrastructure) to requisite dynamically. Proposed agent based approach has provides the efficient and accurate solutions for efficient scheduling and monitoring in cloud computing. In the cloud computing. They are able to communicate i.e. social in nature, mobile i.e. can roam in the network, perform the task at remote stations and send back the results to source platform (where they been originated), agents are also clone themselves and one of the core property of the agent is autonomy i.e. autonomous and distributive in nature. Hence agent based solution has been proposed to meet the requirement of the modern cloud computing with pace of dynamic provision to insure shrink in shrink out (elasticity) of the cloud service provider to achieve highest scalability and reliability in extent of the maximum availability of the service to the requisites. Codenvy -To develop an / are application i.e. SaaS (Software as a service). Platform as a Service (PaaS). New Relic -To develop the core functionality of the proposed system. monitoring and scheduling using software agent New Relic service has been subscribed. In this the java agent has been customized to meet the monitoring and scheduling of the SaaS services. These are following 1. Service scheduling delay 2. Better Provisioning of the SaaS 4. Detail Objective of the proposed agent based SaaS Service. For SaaS development Codenvy has been subscribed. In which jsp based application has been develop and deployed on cloudbees PaaS. Provisions of service and resources in cloud PaaS is an important function that provides analytical statistics about the current view of cloud (running instance for a user or group of users). Resources: Algorithm for Scheduling (influenced from Aneka) developed onto the Cloudbees Proposed Algorithm Step1: Register user login Step2: Sign in the user login Step3: Select multi monitoring agent in cloud environment (for each subc_request with QoS constraints) Step 4: Mapping of resources between cloud client, cloud agent and cloud service providers. Number of tasks submitted at instant i (Ni) 2. Time to execute the task 3. Proposed multi agent based solution has influenced from [2] but it's not the realization of cloud federation rather it has to evaluate the scheduling and monitoring of the SaaS (task) application in public cloud's (cloud federation not interoperability). Table 2  shows the response time of the deployed SaaS obtained results and has been compared with existing agent based method (in seconds) . Table 3  shows the user satisfaction of the deployed SaaS obtained results and has been compared with existing malty agent based method  Rather than performing analytical calculation, proposed system has been compared with the author's MABOCCF and NMABOCCF method. Hence proposed system performs better than  [2]  and any other methods as shown in figure 3. This paper presents the enhanced agent based solution to ensure better elasticity and monitoring solution. The developed java web application (SaaS) has been developed with the help of codenvy SaaS developed platform.
paper_272	Parameters of a vacuum interrupter are essential. Since many years up to date now, we are still constructing the vacuum circuit breaker by classical design, but the interacting among the characteristics inside each vacuum interrupter must be scientifically analysis as a high values of the general specification which must be thoroughly understood before the breaker can be applied with safety confidence. Moreover the switching process associated with power electronics offers many modern benefits as a static switching system when joints together by using a snubber circuits for damping chopping current and transient over voltages or other facilities  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] . In addition the inside electroplates some characteristics can be described as followings: Non-sustained disruptive discharge voltages In general, the above characteristics are not involving associated with only vacuum switchgear itself a lone, but when connects the switchgear with power distribution transforms or synchronous motors, there are also many negative interactive processes for instance hysteresis currents, eddy currents and high voltage capacitive inside each transformer (Parasites electrostatics charged) feedback to the interrupter process. A more exact name would be metal vapor arc inside vacuum electroplate. K is Boltzmann's constant (1.37x10-23 Joules/K) The constant A is approximately 6x10 5 for most metals The term multiple re-ignitions refers to the series of alternate re-ignitions and high frequency (typical several hundred KHz)interruptions usually resulting in an increasing train of voltage peaks; this overall phenomena is usually defined as voltage escalation  [3, 4] . It is important to appreciate that while current chopping and voltage escalation can occur in a single -phase circuit, virtual current chopping is specifically a 3-phase characteristic: the effects of normal current chopping, multiple re ignition and over voltage escalation in one phase can generate surge over voltages in the second and third phases. A high frequency current governed by the circuit parameters flows. However pre-striking transient over voltages is less severe than multiple re-ignitions occurring during load-dropping, first because the contact gap at the first prestrike is very small and second because the contact gap is rapidly decreasing rather than increasing with respect of time. EMTP which is called electromagnetic transient program or others such as SIMULINK/MATLAB are excellent numerical tools that allow for depth studies of switching transients in industrial as well as utility power systems. Since the high frequency characteristics of the power system equipment are depend on stray capacitances that mentioned above and inductances also and they in turn depend on physical dimensions, equipment layout, materials, cable lengths, all these can be represented as follows  [17] ; The figure represents TRV & Re-ignitions computed in the system when switching off power transformer load that the transient voltage approximately (350¬µs-450 ¬µs) and shows very high frequency sub transients both going in the negative direction of the voltage -Left side. The analysis shows that the effect of damping in an oscillatory circuit can be described in terms of a single parameter, designed ≈ã, or its reciprocal  é, which is the ratio of the resistance to the surge impedance of the circuit: This fact permits the construction of generalized damping curves for different values of ≈ã, from which the solutions to many practical problems can be extracted with about the same effort as one expand in using a table logarithms. We note that the only parameter involved is Œ∑. This has been done in Figure. Suppose Vo =13.8‚àö2 KV So. Zo = ‚àöL/C = 5X104 ohms Œ∑. The electrostatic charges -static charged -parasitism are almost depleted in the a mount current begin to decrease in the same time of first peak value which generates chopping currents, moreover the formula of the transition processing rate of du/dt or di/dt have to be considered. Chopping current times (400us-800us). Transient over-voltages (450us -480us-500us).
paper_294	Cassava has played an important role as a staple crop in the feeding of the Tiv people. This study therefore, looks at the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. The study sampled 377 out of the population of 20,000. The snowball sampling technique was used in selecting the subjects for the study. Mean and standard deviation statistics was used in answering the research questions. The study found that the Tiv people had various strategies they adopted in managing postharvest losses of cassava. This is the very core of storage and preservation. Cassava has played an important role as a staple crop in the feeding of the Tiv people. Apparently, if this is allowed to continue, the consequences cannot be foretold in the near future. The study intends to investigate the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. Specifically, the study sought to: 1. Identify Tiv management strategies of postharvest losses of cassava 2. Determined whether public library has impacted on Tiv management strategies of postharvest losses of cassava. storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. What are the Tiv management strategies for postharvest losses of cassava? In what ways does the public library impact on Tiv management strategies for postharvest losses of cassava? What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? Public library is a library that is established and managed with public funds. Public libraries are established wholly or partly from public funds. They are not restricted to any class of persons in the community but freely available to all. They are the venue through which the overall information resources are made freely available to all (Edoka, 2000; Assoh, 2011; Shidi, Aju & Ashaver, 2014). They are a reservoir of society's intellectual history, the custodians of people's knowledge and information. This is their most crucial function of all. Fresh cassava has a very short postharvest storage life  (Karuri, Mbugua, Karugia, Wanda & Jagwe, 2001 ). Food losses may be direct or indirect. A direct loss is disappearance of food by spillage, or consumption by insects, rodents, and birds. Peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips) 2. Peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom 4. Slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base) and 5. The sample size of 680 out of the population of 20,000 was drawn using the sample size table, (Emaikwu, 2015). The snowball sampling was adopted in selecting the subjects. The researchers did this until they arrive at the sample size of 680. The instrument used for data collection was Questionnaire constructed by the researchers. Section A of the questionnaire contained respondents' bio-data, which included sex and occupation. Data were analyzed using mean and standard deviations. Data was collected using 4 point rating scale instrument. Importantly, on each research question, data were collected on related items in the instrument. The collected data were analyzed using mean and standard deviation. What are the Tiv management strategies for postharvest losses of cassava? The collected data were analyzed and presented in  Table 1 . From  Table 1 , it can be seen that the mean rating scores of all the items are above 2.50. In order to answer the research question, data were collected relating to the research question, analyzed and presented in  Table 2 . What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? To answer the research question, data were collected relating to challenges faced by public libraries on acquisition and storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. The collected data were analyzed and presented in  Table 3 . Table 3 , it can be seen that the mean rating scores of all the items are above 2.50. The research discovered that the Tiv people had various strategies they adopted in managing postharvest losses of cassava. The study was carried out to investigate the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. The study found out that public library does not impact on Tiv management strategies of postharvest losses of cassava through storage of audiovisual materials on the subject matter.
paper_298	The study investigated the awareness and use of library catalogue for accessing information resources by users of Federal University of Kashere (FUK) Library. Four research questions guided the study. The instrument used for data collection was questionnaire. Data generated was analyzed using simple percentage and descriptive statistics. Majority of the respondents used the library for research work rather than for leisure and recreational purposes. Poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. Library catalogue exist in different form. Overall majority of respondents 80% satisfied with OPAC functionality  [4] . Kumar revealed that 26.99% respondents use the OPAC fortnightly and 22.84% weekly. In essence, students use the catalogue to enable them conduct research in the library. It is on this note that this study investigates the use of library catalogue by undergraduate student in the university library. Therefore, the study investigates the access and use of library catalogue by students of Federal University of Kashere, Gombe State. The study is designed to achieve the following objectives: a. To find out the students awareness about library catalogue as access point to library collection. To find out methods employ by students to consult library catalogue to search for information resources. c. To ascertain the extent of use of the library catalogue by students to access information resources. d. To identify the constraints associated with the use of library catalogue. The study is designed to answer the following research questions: a The survey research method was adopted for the study, and questionnaire was the instruments use for collecting data. Thus, 272 respondents were selected for the study based on 95% confidence level and 5% confidence interval of Sample Size Calculator. The completed and returned questionnaires were analyzed using descriptive statistics, percentage and frequency distribution. This section presents questionnaire collected on the basis of gender of the research respondents. The gender breakdown is presented on  Table 1 . The question on the demographic information of the respondents was on gender presented in  Table 1 . It shows that majority of the respondents 188 (69%) were male, followed by females 84 (31%). This shows that majority of library users in the Federal University of Kashere Library were male. This implies that the male ranked highest in utilizing the library catalogue more than the female. Table 2  revealed that majority of the respondents 262 (96.3%) were aware of the existence of the library catalogue in the university library. shows that majority of the respondents 178 (68%) were aware of the card catalogues as a access point / retrieval tool for searching for information resources in the library. 26 (10%) of the respondents were aware of the existence of OPAC as a retrieval tools in the library. Their responses were presented in the  Table 4  below. Never  Total  F  %  F  %  F  %  100  20  40  28  56  2  4  50  200  14  32  20  45  10  23  44  300  24  44  20  37  10  19  54  400  42  75  12  21  2  4  56  500  58  100  0  0  0  0  58  Total  158  60  80  31  24  9  262  Key: F=Frequency, %= Percentage. Table 5  indicated that majority of the respondents 158 (60%) used the library catalogue regularly when searching for materials in the library. 24 (9%) of the respondents never used the catalogue but they were aware of it existence in the library as indicated on table 2 above. Total  F  %  F  %  F  %  F  %  F  %  F  %  100  10  20  0  0  6  12  28  56  6  12  0  0  50  200  10  23  6  14  10  23  16  36  2  5  0  0  44  300  10  19  6  11  14  26  12  22  6  11  6  11  54  400  14  25  0  0  36  64  6  11  0  0  0  0  56  500  2  3  0  0  54  93  2  3  0  0  0  0  58  Total  46  18  12  5  120  46  64  24  14  5  6  2  262  The finding indicated poor use of OPAC 12 (5%) at the university library. The study found that majority of the respondents were aware of library catalogue in the library including OPAC  [12] . Lack of skills could discourage users from using the catalogue. It could be concluded that majority of the respondents used the library for research work rather than for leisure and recreational purposes. The results are presented in  Table 8 . Table 8  focused on the challenges associated with catalogue use for information retrieval. Higher number of the respondents indicated poor computer skills hindering navigation when searching OPAC 96 (37%) and inability to locate materials on the shelves, indicated in the catalogue as being available 58 (22%) were the challenges affecting the use of catalogue as an information retrieval tools. ICT skill is needed by the respondents to be able to browse the library OPAC. This finding corroborates with that of Ogbole & Morayo which revealed that (41.1%) of the respondents rarely use OPAC. The study revealed that majority of the university library users were male. Another source of frustration was reasonable number of the respondents indicated poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools.
paper_305	Mitigation of credit risk is a key aspect of portfolio management in any financial institution. In this paper, we report on the results of a MSc. Thesis 1 in the application of an ensemble learning algorithm in development of a computer program that can greatly enhance the underwriting process. Loans constitute the cornerstone of the banking industry's financial portfolios. Furthermore, a loan officer's credit decision or recommendation for loan worthiness is subjective. Therefore, a knowledge discovery tool is needed to assist in decision making regarding the application. Further, the study champions the use of open source software tools in business intelligence applications. This calls for the use of more efficient and effective loan screening tools and procedures. These techniques have been found to outperform earlier approaches leading to increased competitiveness. Boosting is one of the most important recent developments in classification methodology. Although the model was reported to perform relatively better than models developed using other approaches; as part of the limitations and recommendation, they suggested that such a model is usually a black box and more insight the model parameters was required to make it more effective. A decision stump is a decision tree with only a single root node. It works as follows: 1. Looks at all possible thresholds for each attribute 2. Selects the one with the max information gain 3. In this study, 'majority voting' was adopted for combining hypothesis from different learners. Figure 1  illustrates the combination criterion. The model was specified in terms of K ‚àí1 log-odds that separate each class from the base class K. Train all K decision stumps iii. Select the single best classifier at this stage iv. Combine it with the other previously selected v. classifiers vi. Reweight the data vii. Learn all K classifiers again, select the best, combine, viii. Repeat until you have T classifiers selected The development platform used for this project mainly included the following open source software products: The system was implemented on a Java platform comprising of the JDK compiler, netbeans IDE developer, weka API and the exe4j executable file converter. The model was built using the training dataset and tested using three strategies. We report on cross validation as under. Separate data into fixed number of partitions (or folds) ii. Classify and obtain performance metrics. Select the next partition as testing and use the rest as training data. v. Classify until each partition has been used as the test set. Calculate an average performance. The accuracy returned by the training set is 19 correctly classified instances out of 20 instances. This gives an accuracy of 19/20=95% Class =Accept: The number of correctly classified instances is 12 and that of instances classified as belong to the class is 13. This gives a precision value of 7/7=1 Class =Accept: The number of correctly classified instances is 12 and the number of instances belonging to the class is 12. This gives a recall value of 12/12=1 Class =Reject: The number of correctly classified instances is 7 and the number of instances belonging to the class is 8. This gives a recall value of 7/8=0.88  Figure 2 . Test Split ROC graph P(true | false) calculated as 1-d/c+d The ROC area has the following indicators: . Indicates a perfect prediction . The model accuracy using this procedure was 86.86% making it a fairly reliable strategy b) The use of separate training and testing data sets returned an accuracy of 95% making it a relatively better strategy c) The use of a ratio to determine the size of the training and testing files from one data set returned an accuracy of 88.64% Therefore, it implies from these findings that the use of separate files for training and testing of the model returns the best model accuracy and hence should be adopted.
paper_310	We are now living in the 21 st century. Now, smart phone has become the most essential thing in our daily life. Here we are using Bluetooth communication, interface microcontroller and android application. We are using Arduino software to interface the Bluetooth module with microcontroller. Here we use mainly Arduino UNO (ATMEGA 328P), Bluetooth module (HC-05). The controlling device of the whole system is a Microcontroller. Bluetooth module, DC motors are interfaced to the Microcontroller. The controller acts accordingly on the DC motors of the Robot. An example of such a costeffective project has been proposed here. b) Develop a robot which will be helpful for travelling. f) Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. g) The Bluetooth module is connected with the Arduino UNO board for the connection with the user. h) Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. Microcontroller will act as the brain of the robot. The microcontroller is programmed with the help of the Embedded C programming. Cprogram is very easy to implement for programming the Arduino UNO. The smart phone is the transmitter of this circuit. It sends the data to microcontroller through Bluetooth module. Motor driver IC is used to control the dc motors. Here we use programming language 'C' for coding. The program for executing this project has been written in C language. The program is burnt in the microcontroller using burner software. The program is stored in the EEPROM of the microcontroller, which is present in the Arduino board. There are two steps of the programming. First set up section where we define all the variables. The working principle is kept as simple as possible. As seen from the  Figure 6 . A DC power supply is required to run the system. The DC power supple feeds the Microcontroller and the Bluetooth module. Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. The Bluetooth module is connected with the Arduino UNO board for the connection with the user. Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. This device is connected with the Arduino board (microcontroller section) by the means wirelessly i.e. The instructions are sent by the smart phone. We can easily control the movements of the dc motor. This is indeed a cost-effective and efficient project. The robot can be used for surveillance.
paper_333	Enset is an important food crops produced in Southern parts of the Ethiopia with great role in food security. There are several issues and diseases which try to decline the yield with quality. This study presented a general process model to classify a given Enset leaf image as normal or infected. This diagnosis apply K-means clustering, color distribution, shape measurements, Gabor texture extraction and wavelet transform as key approaches for image processing techniques. Therefore, an efficient practice of IT based solution in this domain will increases productivity and quality of Enset products. It is also the sector that is given an overriding focus in the government's plan for growth of the economy as a whole. Enset crop is related to and resembles the banana plant which is an indigenous plant classified under the monocarpic genus Enset and family Musaceae. There are several issues and diseases which tries to decline the yield with quality. As a matter of facts, visual or manual detections may have defects in terms of accuracy in detection along with lower precision. There is a potential need for technology supported alternative systems to support the manual identification of Enset crop diseases so as to optimize the accuracy for remedial action. If the Enset crop diseases can be minimized then definitely the production will significantly be increased to contribute to the country's economy towards a new boost. Therefore, the implementation of IT based solution in the sector will have a paramount importance to facilitate the activities such as economic, social and ecologic development in the country by increasing efficiency in both quality and quantity of Enset crops production, to sustain dependability of customer preferences and to preserve the ecology  [2] . In our country few researchers found the promising solutions to different plant diseases diagnosis such as maize, rose flower, coffee and others using computer vision and machine learning techniques  [3, 4, 5] . So far a number of fungal, nematode, viral and bacterial diseases were reported to cause damage at different degrees of intensity that was mainly explored by Quimio research finding since 1992. Figure 2  shows the architecture of the proposed system Those images are collected by using a digital camera and some of them are collected from secondary sources like research centers which collects pictures for visual inspection of Enset disease. Test Result For the experimentation a total of fifteen experimental setups which is color, texture morphological and a combination of each and a multiclass Kernel support vector machine model: Linear Kernel, polynomial Kernel, RBF kernel and Quadratic Kernel was used. From the figure it is shown that the RBF kernel classifier has classified the given enset image in its correct class category by 94.04% and 92.44% respectively. We designed a system which can detect Enset disease automatically and this will help the farmers to detect the diseases in its early stage and to take relevant action. If there is a shortage of experts in the area of those disease identification the system is going to be a replacement for the experts.
paper_389	In this paper, a large number of experiments have been carried out using conditional random fields. The experimental corpus has been tested by Changjiang Daily for many years. Word probability, the paper explores the probability characteristic of word location. Peng F establishes a Chinese character segmentation model based on CRF. In addition to using some common features, But also used a lot of domain knowledge. Therefore, Chinese word segmentation method has become a frequently used method to study word segmentation. Figure 1  is an example of the use of Chinese characters marked word segmentation. Enter the sentence as "This is Wuhan." 2, marking O cannot appear behind the mark I, can only be O or B. 3, Mark B can only be followed by the mark I. The following  Table 1, Table 2 ,  Table 3  is used in the experiment some of the characteristics of the template. Table 2 and Table 3  are two new feature templates. Secondly, feature extraction is carried out to generate training corpus and test corpus which can be recognized by CRF model tool. Using the CRF model generated by the training, the test corpus is tested and a prediction result is obtained. The main corpus used in this paper is the training corpus and test corpus of Changjiang Daily. The corpus is from 1950 to 2005, and the scale is 2564168000 sentences. The coding method is GB code. The corpus content mainly comes from newspaper news. The format consists of a sentence segment consisting of words marked with spaces. The performance of a word segmentation system mainly depends on two aspects: segmentation precision and word segmentation speed. The main factors that affect the speed of word segmentation are the structure of word segmentation dictionary and word segmentation algorithm. In different applications, the performance requirements of the word segmentation system have different emphases. It can be seen from  Table 5  that the results of the CRF system are better than those of other models under the same conditions as the training corpus and the test corpus. The experimental results show that conditional random field is an efficient segmentation method. And then use these tools and the corpus carried out a number of experiments. In the experiment, only some feature information is used, and most of the features are extracted from the training corpus, we have achieved good results.
paper_391	Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. Globally, tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The WEKA 3.6.1 tool was used for classification, clustering, association and attribute selection. The maximum confidence attained in the general association is 0.98. But, in the class attribute, it is 0.72. The literacy status of the mother has high information gain with the value 0.046. Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. Thus, women receive doses of tetanus toxoid to protect their birth against neonatal tetanus  [1] . Tetanus is caused by a toxin produced during the anaerobic growth of Clostridium tetani. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The information is rich and massive. Data mining techniques are more effective that has used in healthcare research. In this study, we use different data mining techniques that have been tested on TT dataset. Preprocessing solves issues about noise, incomplete and inconsistent data. The next phase is the transformation of the preprocessed data into a suitable form for performing the desired data mining task. However, not all of the patterns are useful. The methodology of this study was the practical research method applied on the Tetanus Toxoid data of the Ethiopian DHS 2011. The EDHS of 2011 dataset was used as a source for this study and WEKA 3.6.1 machine learning tools are used. The data used in this investigation are the TT immunization data. It has a dimension of 7033 rows and 12 columns. Finally, data have been saved in ". csv" file formats and stored as an ". Classification is one of the data mining techniques and it is used to group the instances which belong to same class  [8] . How does this classification work? The approaches are; (a). Decision tree (J48) approach It is a flowchart-like a tree structure. And the topmost node in a tree is the root node. The training tuples are described by m attributes. Each of the tuples represents a point in an m-dimensional space. Bayesian classifiers have also exhibited high accuracy and speed when applied to large databases. This assumption is called class conditional independence. Multilayer preceptor Multilayer preceptor is a simple two-layer neural network classifier with no hidden layers. Confusion matrix; is used in this study, include accuracy, sensitivity, specificity, and precision. Training and testing are performed k times. Thus, in this study, we have used five clusters. "How does the k-means ( ) algorithm work?" The k-means algorithm defines the centroid of a cluster as the mean value of the points within the cluster. The InfoGainAttributeEval with Ranker T used with respect to the class attribute and visualization was done in the attribute selection. From the selected 7037 mothers, 3351 of mothers received TT Immunizations. (Table 1)  As can be seen in  Figure 2 , the accuracy rate of the classifiers on training data is relatively high. A multilayer perceptron is the best classifier in our data set. Briefly described as follows: Cluster 0-This group have 1534 (27%) instances of which consist of mothers who live in the rural, no access to radio, no access to TV, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 1-This group consists of 1004 (18%) instances of which mothers who live in the rural, no access to radio, access to TV yes, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, head of household male, tt-vaccinated yes. Cluster 2-This group consists of 2063 (37%) instances of which mothers who live in the rural, no access to radio, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated No. Cluster 3-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 4-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, orthodox in religion, Amhara in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education no education, women age 35-49, mother marital status married, hh_head Female, tt-vaccinated yes. csv" file and features were described using WEKA performance.
paper_402	The BPNN and SFS were used interchangeably to identify the relevant features that contributed with the response variable. As a result, the BPNN with the selected features was able to interpret more accurate results (r 2 = 0.991) than the model with all the features (r 2 = 0.816). The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete. ANN emerged as one of soft computing paradigms that have been successfully applied in several engineering fields  [1] . Specifically, ANN has been used to solve a wide variety of civil engineering problems  [2] [3] [4] . The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete  [14] [15] [16] [17] . Afterwards, several authors began developing ANN models for the prediction of compressive strength of high performance concrete  [18] [19] [20] [21] . In this study ANN is employed with other machine learning techniques to identify the parameters that capture the compressive strength of UHPC using data collected from the literature. As a result, BPNN proved to be more accurate than SMD in the prediction of compressive strength and slump flow of UHPC. ANN) until the model's error function increases. There are two types of SFS classes -mainly filter method and wrapper method  [28] , where Zhou et al. [29]  used the Markov Blanket with a wrapper method to select the most relevant features of human motion recognition. Four sets of open human motion data and two types of machine learning algorithms were used. Moreover, Rodriguez-Galiano et al. Four types of machine learning algorithms were used as wrappers for the SFS. There are two types of ANN models: (1) feed forward; and (2) feed backward. Equation  1 shows the linear combination of mapping weights from each input neuron, via wires, to the hidden neurons. SFS reduces the dimensionality of data by selecting only a subset of measured features to create a prediction model. There are two types of search algorithms: sequential forward selection and sequential backward selection. In this study, the previously verified ANN model was used as the objective function and the forward selection was used in selecting the relevant features. Figure 1  shows the algorithm SFS uses when performing forward selection. Figure 2  shows the plot of all the scenarios with the minimum point circled at 11 neurons. Table 2  tabulates the percentage of features that were used during the 200 trials. In this study, four variables (Cement, Sillica Fume, Flyash, and Water) were selected as the most relevant features for the prediction model. The selected features, using SFS, were analyzed by the previous BPNN model. Table 3  shows the statistical measurements calculated for both cases. The LSR model is a linear function and its form is shown in  (2) . fc = Œ∏1C + Œ∏2SI + Œ∏3FA + Œ∏4W (2) This study was conducted to detect the correlation between the material constituents of UHPC and its compressive strength. 2) The use of ANN with selected input parameters improved the accuracy of prediction of compressive strength of UHPC and reduced the computational effort.
paper_418	An important aspect of descriptive time series analysis is the choice of model for time series decomposition. Also, when there is no trend (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The models most commonly used to describe time series data are additive, multiplicative and mixed models. For short series, the cyclical is embedded in the trend  [2] . They do not depend on the level of the trend  [3] . Here again, no statistical test was provided for the choice. An important aspect of descriptive time series analysis is the choice of model for time series decomposition. This is an indication that the seasonal variation equals a certain percentage of the level of the time series. In such cases it is appropriate to use a multiplicative model. Oladugba et al  [9]  gave brief description of additive and multiplicative seasonality. This is an improvement over what is in existence. The method adopted in this study is the Buys-Ballot procedure in descriptive time series. The row and overall variances contain both trending parameters and seasonal indices for additive and mixed models. formula_5 Source: Iwueze and Nwogu (2014), Nwogu el al (2019). formula_10 When there is no trend ( 0 b = ) we obtain from  (Table 1)  . j j j X S a e = + (27)  From table 2, we observed that the estimates of the trend parameters and seasonal indices are not the same for both additive and mixed. Table 3  that when there is no trend i.e. (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The data of 120 observations has been arranged in a Buys-Ballot table as monthly data (s = 12) and for 10 years (m = 10). The actual and transformed series are given in figures 3.1 and 3.2. The expressing of a linear trend and seasonal indices for an additive model is given as Estimates of trend parameters and seasonal indices are discussed. Also, when there is no trend ( ) 0 b = , the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models.
paper_428	developed a model for the transport of Williamson fluid in an annular region  [10] . found numerically convergent solutions of two dimensional flows of non-Newtonian fluids along chemically reactive species  [11] . MHD is very useful to analyze the interaction of electrically directing fluids. It play significant role in field of technology. studied unsteady combined convection flow in a cavity in presence of nanofluid  [41] . outlined mixed convection boundary layer flow influenced by thermo-diffusion  [42] . Chaudhary and Jain studied the impact of mass transfer, radiation and hall on MHD mixed convection flow of viscoelastic fluid in an infinite plate  [43] . General example of porous wedge is sand, soil, sandstone and foams. Deka   is wedge angle parameter. The nondimensional form of the given system of partial differential equations is obtained by introducing the following stream function and the similarity variables  [76] . E 0 = E 6 , E‚Ä≤ 0 = 0, F 0 = 1 (9) E‚Ä≤ ‚àû = 1, F ‚àû = 0 Where E 6 is injection/suction parameter. This With the boundary conditions E 0 E 6 , $ 0 0, F 0 1, $ ‚àû 1, F ‚àû 0 (13) The resulting system in Eq. (11) (12) (13)  is solved numerically with the help of 5 th order Runge-Kutta-Fehlberg method. Further details about the obtained numerical solutions are presented in the next section. The transformed governing equations  (11) (12)  subjected to boundary conditions  (13)  are solved numerically by employing the fifth order Runge-Kutta-Fehlberg method. [78] , Yih  [79]  and Rashidi et al. The agreement of our work with the prior results is stable. Figures (1-2)  illustrate the influence of wedge angle parameter < with on velocity and temperature profile. This eminent phenomenon is known as Lorentz force that squeezes the momentum boundary layer. Furthermore an increase in Œª may cause increase in temperature of flow. Figure 6  drafts the non-dimensional velocity E‚Ä≤ for different values of suction parameter E 6 . Figures 7-8  illustrate the behavior of thermal radiation and Prandtl number on fluid flow region with M= E 6 = O = < =1. The important conclusions of the analysis are 1. The non-dimensional velocity profile increases by increasing the wedge angle parameter <.
paper_432	The mathematical analysis method used. The Cotangent Bundles * of manifold d provides the basic model of a symplectic manifold. The Hamiltonian is natural energy function on the cotangent bundles. Then the Cotangent Bundle has a dual space * . Time -dependent smooth Hamiltonian on Œ§ * , the cotangent bundles of . Generically, the corresponding Hamiltonian system `(%) =& H '%, % ). Given any , manifold , of dimensionn, with -. We see that(E > ) ,‚Ä¶, (E ) is a basis of T 5 * (M). b) Let exp ‚à∂ ‚Üí 3 > be given by exp(% =J ‚Ä¢" ‚Ä¶ ‚àà 3 > . Let 0 ‚à∂ 3 ‚Üí 0 be the projection map. From which the required inclusions follow easily. Let be a lie group‚Ä¢ ‚àà o‚ãÉ ~‚àû‚Ä¢. 8 ' : K V ‚Üí R s> ( K ), satisfying R o 8 ' u 0 ' and the equivariance property. 8 ' ( K , n, ' ) = 8 ' ( K, n ). ' Let be a Lie subgroup of a Iie group , and 3 a manifold on which acts. ), the left multiplication of : n`. The twisted product √ó ¬∞ is the quotient of √ó 3 by the twist. Now consider a G on a manifold. ≈°J% ¬± ‚àà , with isotropy subgroup = ¬≤ . The symplectic reduction of the cotangent bundles * Q has more structure than a symplectic manifold. Define the map, ‚àÖ : ¬¥s > (0) ‚Üí * ( / ) by, for every 0 ‚àà ¬µ * Q. Then ‚àÖisc , invariant surjective submersion and descends to a symplectic homeomorphic. Note that ‚àÖ is "injective mod '', meaning that ‚àÖ(¬± 1 ) =‚àÖ(¬± 2 ) if and only if ¬± > =n.
paper_444	Then, an uncertain resource constrained model is built. The other constraint is the resource constraint in which the demand of resource shall not exceed the total supply of resource for each resource type at any time. However, the cost that effected by the increased quantities of human resource was not taken into account. Its constrains were the resource and priority rules. The construction of this paper is organized as follows. Node 1 and node represent the start and the end of the project, respectively. (1) This paper only considers renewable resources. (3) Assume that the finish time of each activity is a decision variable. (4) No interruption is allowed for each activity in progress. " : The set of underway activities at time #. Constraint ‚óã 4 reflects that for any time # and each resource type , the demand for resources shall not exceed the total supply of resources with at least given confidence level < = . Constraint ‚óã 5 illustrates the total cost of project has two pasts. Constraint ‚óã 6 shows the range of decision variables. Then excepted value of G is formula_1 Theorem 2. [16]  Let G A , G P , ‚ãØ , G ) be independent uncertain variables with regular uncertainty distributions H A , H P , ‚ãØ , H ) , respectively. Model (1) is equivalent to the following model. is an uncertain variable, and the inverse uncertainty distribution of ? is formula_6 By Theorem 1, we know that the excepted value of ? is formula_7 Since ‚Ñ≥6‚àë ‚àà8 9 ‚â§ + ; ‚â• < = , then, ‚Ñ≥6‚àë ‚àà8 9 ‚àí ‚àí ‚â§ 0; ‚â• < = . By Theorem 2, we have ‚àë ‚àà8 9 ‚àí ‚àí Œ¶ MA (1 ‚àí <) ‚â§ 0Ôºå  (12)  i.e. formula_10 Therefore, the model (1) is equivalent to the model (2). The type of resources chosen in this case is human resource. The duration time, cost and resource requirement of activities are presented in  Table 1 . The manager tends to the demand of resource shall not exceed the total resource supply with at least given confidence level 0.9 for any time # and each resource type . The cost per time unit of additional resource ! The goals of project are both completion time minimization and expected total cost minimization. The constrains are recourse constraint and precedence constraint. The decision variable is finish time of each activity. The information of the activities.
paper_462	The goal of postgraduate education for master's degree in clinical medicine is to cultivate high-level medical and health professionals. Since 1998, the postgraduate education for master's degree in clinical medicine has been piloted in China. After 17 years of experiment and exploration, great progress has been made in clinical medical degree education. However, there are still difficulties in linking up with professional qualification certification, slow internationalization process and postgraduate students. There are many problems such as insufficient practical ability training. "5 + 3" new training mode of training in combination. Chongqing Medical University was founded in 1956. Since 1979, it has enrolled doctoral and master's degree students. In 1981, it was approved as the first authorized unit of doctoral and master's degree in China. In 1998, it became the first batch of doctoral and master's degree in clinical medicine in China. Pilot units for professional degrees. Since 1998, the school has vigorously developed professional degree postgraduate education  [1] . Since 2009, the school has been focusing on reforming the training mode of clinical master degree postgraduates (hereinafter referred to as clinical master). It not only guarantees the quality of professional degree postgraduate training, but also supplies a large number of high-quality talents for related industries. Educational development and other key problems of clinical master training. The organic cohesion of syndromes has effectively improved the quality of clinical master training. After completing the dissertation defense and meeting the award requirements, the graduate certificate and master's professional degree certificate can be obtained. Effective management during the transition period. In order to speed up the development of professional degree postgraduate education, the school has formulated an enrollment policy conducive to the development of professional degree  [9] . Our school has made corresponding reforms in the curriculum system of clinical master's degree. The reformed curriculum system not only meets the training objectives of clinical master, but also meets the requirements of theoretical knowledge for trainees. Secondly, the curriculum of Master of Clinical Science is adapted to the requirement of training students' theoretical knowledge and foreign language. It is closely related to clinical practice. The secondary clinical colleges are responsible for both the health bureau, the training of residents and the school, and the training of postgraduates with professional degrees. Students are brought into the "two levels, two stages" training after they enter school. Requirements for the first stage of training. System for Clinical Ability Clinical competence is the core of clinical master training. At present, the country lacks a quantitative index system for clinical competence evaluation system. School research and explore more rigorous clinical training and assessment methods. The clinical master emphasizes on examining the clinical competence of postgraduates. At the same time, the cost of training clinical masters has increased substantially. Compared with academic degree postgraduates, the training objectives of professional degree postgraduates are obviously different. Postgraduates can devote all their energy and time to clinical ability training. Thirdly, we should formulate an enrollment system conducive to the development of professional degrees. The number of professional degree postgraduates enrolled by tutors in clinical departments is clearly required. All tutors working in clinical departments must enroll professional degree postgraduates. By promoting the combination of clinical master training and regular training, the problem of clinical master qualification certification has been solved, and a new "5 + 3" training mode has been constructed. It provides a mature reference model for the docking of professional degree postgraduate education and industry access standards in other fields in China. The curriculum system is guided by the improvement of vocational competence and solves the contradiction between curriculum learning and clinical competence training. The two systems complement each other and organically combine to realize the training of clinical master and regular training. Since 2009, the school began to explore the reform of the training mode of clinical master in an all-round way. It has overcome many obstacles and steadfastly promoted the reform. It has basically realized the seamless connection between the degree education of clinical medicine specialty and the certification of professional qualification. Over the past five years, the reform has achieved fruitful results and basically achieved the expected goals. The main results are as follows: Dozens of brothers such as Fudan University learn from our experience. In the past five years, a total of 2063 clinical masters enrolled in the school have been integrated into the training. Many excellent students have become an indispensable new force in clinical colleges. They have achieved the ultimate goal of training "doctors who can really see a doctor". The employment rate of graduates has been guaranteed to be 100% for a long time. In December 2013, the school sponsored the National Symposium on the Reform of the Training Model of Clinical Master's Degree Postgraduates. Our school has 10 affiliated hospitals of Grade A and 28 key national clinical specialties. Since the implementation of the reform of the training mode of clinical master, the combination of degree education and vocational education in our school has become closer. Since the reform of training mode, the enrollment of clinical masters in our university has been increasing rapidly. In the past five years, a total of 2063 clinical masters have been enrolled in the standardized resident training system. We should build "five guarantees", innovate the management system and mechanism, and ensure that the reform is in place. Through a series of reforms, it has achieved relatively ideal results and accumulated rich experience in reform.
paper_476	(3) Through the proper splitting and loading of orders, staff scheduling, and area adjustment of delivery station, the optimal delivery frequency can be achieved under the balance of cost and resource operational efficiency. In order to reduce the operating costs of logistics enterprises on the basis of ensuring service levels, one should first properly allocate the amount of orders and splitting ratios to achieve an economic increase in the delivery frequency. In addition, it is important for the logistics enterprises to consider the constraints such as delivery resources and consumer satisfaction to achieve the appropriate decision of delivery frequency. What's more, it is also crucial to reasonably arrange vehicle loading, staff scheduling and distribution station leasing for the logistics enterprises. at various stages in the delivery system, which will more likely to result in uneconomical performance. The order volume, the orders' splitting ratio, the operational efficiency of delivery resource and the delivery costs will affect the delivery frequency. System dynamics models not only offer a different Operational Efficiency: A Case Study of Jingdong Logistics perspective while whole system approach to transport planning, but also demonstrate to policy makers the importance of these feedbacks and lagged responses  [1] . There are various of applications of system dynamics in the area of transportation  [2, 3] . Fan Xuemei et al. That research explored three scenarios, which are joint delivery, autonomous delivery, and third-party delivery, which also pointed out that, in order to improve delivery efficiency, enterprises should adequately consider relevant factors such as own resources, competitors' delivery strategies, and urban transport policies before determining delivery methods. [5]  pointed out that joint delivery can effectively improve the efficiency of urban delivery. It proposed an assessment framework for joint delivery. That research took Lyon in France as an example with the using of radar map to visually show CO2 emissions, risk values, delivery costs, traffic impact and delivery time of joint delivery under different scenarios. Wang Wei  [17]  analyzed the advantages and disadvantages of self-operated, outsourced, and federated modes in ecommerce delivery model, and constructed a system dynamics model for the combination selection of delivery modes by selecting six core factors, which can provide decision-making advice for managers to improve the delivery efficiency. Hongtao Yang and Jianbang Du  [19] [20] [21]  uses system dynamics to analyze the problems of supply chain and delivery efficiency from the perspectives of transportation routes, drivers and cooperation agreements, and its sensitivity analysis provides some reference to this paper for the corporate in this case to improve the delivery efficiency. Lin Wanting [22]  considers the phenomenon of unfilled orders, information leakage, and product damage which exists in crowdsourcing delivery, then built system dynamics model from five dimensions of reliability, responsiveness, assurance, empathy and security, which provide decision-making advice for enterprise delivery efficiency management. This paper uses Jingdong Logistics (JDL for short) as research objects. It provides reference for express companies to determine the delivery frequency. As the first step, this paper defines the research boundary of JDL delivery system. The relationship of the workflows in the delivery system is shown in  Figure 1 . Sorting staffs consist of sorters and on-site logistics personnel. The sorter is the person who operates on the sorting equipment. The on-site logistics personnel is the person who puts the sorted goods into a mail packages and places them on a pallet and then sends it to the transportation vehicles. This article considers the utilization efficiency for the facilities, personnel, and technology such as turnover rate of transportation vehicles, the utilization rate of delivery site, and the delivery personnel loading rate, etc., which are put into the sorting, transportation, and terminal delivery links. The utilization rate of transportation vehicles is calculated by dividing the actual traffic volume at each sorting center by the vehicle capacity. The personnel load rate in the process is calculated by dividing the actual workload by the authorized workload. The increase in the delivery frequency will result in the following two changes: first, the increase in consumer satisfaction; second, the corresponding changes in the cost and resources operational efficiency. As is shown in  Figure 2 , 58 causal loops are formed. Among them, the positive loop represents that there is a mutually reinforcing relationship between the factors, while the negative loop indicates that there is a balanced relationship between factors (Such as: Delivery frequency --> + Consumer demand response ability --> + Impact of demand response capacity --> + Consumer satisfaction --> + Order quantity due to change in satisfaction --> + Total daily order quantity --> + Per batch delivery order quantity-->+ Sorting center cargo volume-->+ Number of on-site logistics personnel--> -Onsite logistics personnel operating time-->+On-site logistics personnel costs-->+ Sorting personnel costs-->+ Sorting costs-->+ Total cost --> Total profit --> + Delivery facility input --> + Delivery frequency). Second  Third  Fourth   1  60  30  40  20  2  30  30  20  20  3  10  30  30  20  4  -10  10  30  5  ---10  Table 4 . Setting reason for each scheme. The delivery frequency in scenarios 2 and 3 was increased by 9.5% over that in scenario 1. When the volume of orders increased from 1496 to 5300, the increased percentage of total delivery cost showed a downward trend followed by an upward trend. The minimum value of 3.9% appeared in the order volume of 3086-3154 units. The total delivery cost in scenario 4 was increased by 25% in average than that in scenario 1. As the order volume increased, the increased percentage of total cost tended to decrease in fluctuation. The difference of average unit sorting costs among scenarios 1, 2 and 4 decreased as the order increased, which is because the change in the sorting costs under different order quantities was small and the difference in the delivery order quantity increased. Figures 12 and 13  show that the transportation cost in scenario 2 was increased by an average of 14% than that in scenario 1. The transportation cost in scenario 4 was increased by an average of 45% over scenario 1. The transportation cost in scenario 1 showed a rising trend with the increase of orders. When the order volume was 2406, 2530 and 4700, the transportation cost rose rapidly. As a result, the overall efficiency of the delivery staff was reduced. At the same time, the increase in the delivery frequency also improved the rental cost of the delivery station. Figures 16, 17 and 18  demonstrate that the delivery frequency hade different effects on different transportation vehicles. The vehicle utilization efficiency for the three sorting centers in scenario 4 increased with the order volume since only one transportation vehicle was required to be dispatched from each sorting center within the current order volume. In some cases, the area utilization efficiencies in Scenario 1 and Scenario 3 were the same. In addition, scenario 4 can effectively ease the utilization of area and personnel in the delivery site. The change of delivery frequency has different effects on the resource operational efficiency in different stages of the delivery process. Recommendation I: one should properly allocate the amount of orders and splitting ratios to achieve an economic increase in the delivery frequency. Recommendation II: one should consider constraints such as delivery resources and consumer satisfaction to achieve the appropriate decision of delivery frequency. With different delivery frequency, JDL's delivery resources and consumer service quality are different. Therefore, JDL needs to consider different constraints when making delivery frequency decisions. Recommendation III: One should reasonably arrange vehicle loading, staff scheduling and distribution station leasing. The delivery frequency has different effects on the resource operational efficiency in different delivery stages. Therefore, JDL should combine the simulation results with the actual situation before making the delivery frequency decision.
paper_479	Handwriting is also a key element in document examination as it leaves a forensic document examiner with the task of determining who the writer of a particular document is and this is achieved through the likelihood ratio (L R) paradigm. A forensic document examiner is saddled with the task of document authenticity. The L R theory takes its stance on odds-form from Bayes ' theorem. Let us presume that there are two opposing ideas. In ' competing, ' it can be said that they are mutually exclusive, but may not necessarily include all possible alternatives. Consider, as an example, a situation where the question is whether a specific writing belongs to a particular person. One proposition is 'The suspect is the author of the document in question'. For instance, someone might have fabricated the writing with some special skills. Strength of proof serves an integral part of this problem. [19]  described a statistical model for the writer verification task to determine if two documents were written by the same writer, their model followed the scenario proposed by  [17]  thus suffers from the same weakness  [20]  presented one conceivable way to deal with gauge a probability proportion in near handwriting analysis was delineated. Assessment of signature handwriting evidence through score-based likelihood ratio based on comparative measurement of relevant dynamic features was carried out by  [22, 9]  presented score-based approaches to calculating forensic probability ratios that have been established as becoming more progressively common in forensic literature. Figure 1  shows the graphical representation for the handwriting modeling. Back Propagation Neural Network (BPNN) served the sake of supervised learning of the system. We target is set for each character in the handwriting. The target will help to know which handwriting is original and disguised. The strength of neural computation comes from linking neurons in a network. The neural network is a parametrized system because the weights are variable parameters. The weighted number of the inputs is the activation of the neuron. Transfer function can add non-linearity to the network. Each character variable has a weight W i which shows its contribution in the training process. The feature information extracted from student handwriting through LBP was entered in NN through input layer and the participation of each class of character variables is calculated by the hidden layer of the network using: formula_1 net j is ith type of factor, n is the number of factors in net i , & ' ! weight-recognition factor with weight ! " This study used a network concealed with n input points, k 1 and k 2 hidden, and m output units. The weight from input points i and two hidden unit j is ! " Weight from second hidden unit i and output unit j is ! ) Weight from the constant 1 and two hidden unit j is #$",*"  formula_2 After randomly selecting the weights of the network, the backpropagation algorithm is used to measure the necessary corrections. Base on decision law i.e. If L R value greater than 1 H p is true If L R value less than 1 H p is false. It has also helped in lowering the rate of disagreement (L R 7 and 10) in the investigation process as compared to existing methods. A more elaborate and collated result table is presented in  Table  2 . Statistically the estimation of the interval is considered to be robust over the estimation point, therefore we do consider the estimation of the interval and take into account the decision condition, and we conclude that both the down and top intervals must be of the same sign; that is, either both positive or both negative. Thus there must be agreement in sign otherwise there is disagreement. Full L R void of nuisance parameters is needed for most forensic investigators. Several factors have to be put in place to estimate a full L R for forensic handwriting investigations.
paper_492	Based on known functions of neuroscience the neural network that performs serial parallel conversion and its inverse transformation is presented. This function is the behavior of so-called short-term memory. It is human intelligence that gave birth to the technology. But the structure of our brain has not changed since tens of thousands of years ago. Acceptance and generation of time series data of the nervous system of long-term memory is carried out in consistent with short-term memory. The same is true for the recognition process. The same applies to general figures. Many East Asian recognize kanji as a combination of parts. Deductive logical development is desired. Finally, it shows the hierarchically connected neural network that can process for general time series data. The divided subsequence is defined the basic subsequence. For simplicity, assume time series data consisted of 10 type elements from a 0 to a 9 . Example shown in  Figure 1  shows that the time series data (1) arranged by randomly selecting the elements is divided into five basic subsequences (2). The dividing is done by the following procedure. (1) The first element is the beginning of the first subsequence. (2) If the same element exists in already divided subsequence. In this example a6 is the concerned element. The subsequences divided by above procedure are defined as the basic subsequence. Figure 2  shows the affinity with the neural circuit. When the first data c 0 is received activate the bottom. Four portions are activated in the  Figure 2 . By receiving c 2 , c 3 and so on the activated portions become narrow and narrow. It may be seen as the output of the connected AND logic element. Its output is the result of the serial parallel conversion, it is the result of AND logic of the output of the activated elements. The elements involved in the conversion are still activated at the time of output. This operation is a generation of (learned) time series data. that can be said a conversion of parallel to serial triggered by the first data. When the state transition diagram of the  Figure 3  is seen as a serial parallel conversion, the bottom is inputs connected to such sensory organs. After the first data reception, the connected elements are activated as described above. That is, the operation with no reaction within waiting time is invalid and aborted. Both state transition diagram is shown in the existing  Figure 3 . For each basic subsequence a neural network that accepts and generates the basic subsequence is considered. This neural network is called a basic unit. In the  Figure 4  activity changes of each unit on processing the time series data of eating behavior is shown. It can be said as "stance to the event" that most animals have  [2] . The movement will be mentioned in the next chapter. Neuroscientist Damasio calls "image" the internal representation built in the nervous system by stimulation inside and outside the body. However, recognition is limited to the area of the vicinity that the sensory organ catches. They exchange information with their peers through squeals and gestures to enhance their ability to survive as a species. In order to be able to exchange information, it is necessary to transmit and reproduce the act. The ability to imitate fellow's action is indispensable. Imitating is first step of learning the behavior of the fellows who is transmitting information. The nervous system which is involved in the imitating function is called mirror neurons. In addition, it spreads to the story beyond space and time. And you'll think of the shape of the cake and the action of lifting the chocolate plate and mouthing it. If there is a problem in the episodic memory, it causes difficulties in social activities. Two types of time series data can be considered in the above situation. Like talking about your childhood while eating cake. In order to avoid confusion, the episode must be corrected by reality. Figure 5  is an illustration for showing the state change of each part in the neural network. In neuroscience, the morphism corresponds to synapses and axons, and is responsible for the transmission of information between objects. Next, we introduce a function called functor between different categories. Therefore, the existence of the above functor would be considered naturally. Thus, the behavior of both categories can be migrated to each other. In this paper, short-term memory and long-term memory are both regarded as time series data brought about by activity in the brain. Of course, the description does not convey all the features of the hippocampus. Each unit has the same function, but the connections between the units are not the same. This process is close to the rehabilitation process of the brain that has had a stroke. The bud is also felt in the judgment and the movement in our daily life.
paper_507	The main aim of present study is to integrate the result of our study with spatial data, soil parameters, land inventory and used the output as a user friendly application using GIS which could predict the future susceptibility of region to landslide and% contribution of each factor for the same. ArcGIS 9.3, ERDAS and Excel software have been used for zonation, and statistical analysis respectively. Database of this information layer is used to train, test, and validate the ANN model. A three-layered ANN with an input layer, two hidden layers, and one output layer is found to be optimal. A frequently used definition of landslide is "movement of mass of rock, earth or debris down a slope" in the words of Cruden  [1] . As per the official figures of United Nations International strategy of disaster reduction (UN/ISDR) and Centre for Research on Epidemiology of Disasters CRED for the year 2006, landslide ranked 3 rd in terms of number of death among the top ten natural disasters  [2] . The recent landslide of 2012 took a heavy toll on life and property, many people lost their lives, and thousands of tourist were stranded due to disrupted communication services  [5] . The material can be rock, debris, earth or a mix and movement can be fall, topple, slide, spread and flow. These map divides the land into homogeneous areas or domain and their ranking according to degree of actual hazard potential caused by mass movement  (Guzzetti et. al, 1999 and varnes 1984)    [7, 8, 9] . The result obtained i.e. Identification of factors which affects to the landslide. Determination of the extent to which the various factors contribute to landslide. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area (Mohammad Onagh, 2012). slope, aspect, lithology, rainfall, land cover etc. were generated using ERDAS and ARCGIS v. 9.3. DEM (Digital elevation model) was obtained from BHUVAN. The factors in relevance to the landslide susceptibility analysis of Uttarkashi are: The maps depicting various features of Uttarkashi are in Raster Form i.e. The back-propagation training algorithm is trained using a set of examples of associated input and output values. The ANN is a black box model is a multi-layered neural network, which consists of an input layer, hidden layers, and an output layer. A neural network consists of a number of interconnected nodes. Architecture of neural network (source:  (Lee, 2009) ). In the present study we selected 107 points and all the six factors namely soil depth, soil type, rock type, land cover, slope and elevation and a excel database is created. The dataset is categorized into 60% training and 40% validation. The dataset is categorized into 60% training and 40% validation. However, this is possible only if we know the coefficients called weights. For this three-layered feed-forward network was implemented using the MATLAB software package. For a new dataset the weights are unknown. We have set the error to 0.01 also referred to as goal. The number of epochs was set to 3,000. Weightage of different factors are shown in table 1. The results are compiled below. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area  [13] . slope, aspect, lithology, rainfall, land cover etc. were generated using ERDAS and ARCGIS v. 9.3. An artificial neural network technique was used. The dataset is categorized into 60% training and 40% validation. With the further advancement in such type of study, we could interpret results for future from past records, if the site is inaccessible, or the test results are erroneous. The result of present study with spatial data, soil parameters, land inventory and presented by a landslide hazard zonation map and a user friendly application using GIS that could predict the future susceptibility of region to landslide and percentage contribution of each factor for the same.
paper_1	This helps in solving the problem of losing vital information that is generated from the social media. Clustering is a descriptive task of data mining. A cluster is a collection of data objects that are either similar to one another in the same group or dissimilar to objects in other groups. A number of techniques can be used to do clustering. The quality of clustering also depends on both the similarity measure used by the method and its implementation  [10] . The system design methodology used was incremental prototyping. Requirements and Architectural Design can be done up front and then each prototype developed as the project progresses. The third step involved using the data already preprocessed above to train the prototype. The relationship between the data items can be established using the k-nearest neighbor technique. The fourth step was testing the system. The prototype was then subjected to testing using the test data. This is a collection of data whose class labels are already known. They are part of the data that was used to train the system but its results are already known. They were used to confirm that the system indeed accurately did the classification given some data items. Finally, the model was used to classify a new user into a group. The illustration of the proposed prototype is given below. This is summarized in the chart below. The Na√Øve Bayes Classifier was also tested to evaluate its accuracy, precision and recall  [9] . A total of 200 tweets were extracted and used for this test which was summarized in a confusion matrix. The results below illustrate a summary of what was obtained when 200 tweets were used to test the Na√Øve Bayes Classifier. This classifier was doing the classification using the unigrams. From this analysis the classifier performed above average with an accuracy of 71.4%. Precision and recall were however average. The classifier then grouped the users into different categories based on various tweets that they posted on the task. Most of the information that passes through social media was being used majorly for social interaction. The study has proved that it can actually be used constructively in learning in various institutions.
paper_2	In addition to this, the current traffic light systems encourage extortion by corrupt traffic officials as commuters often violate traffic rules because of the insufficient time allocated to their lanes or may want to avoid a long waiting period for their lanes to come up. The developed system uses a microcontroller of PIC89C51 microcontroller duly interfaced with sensors. The density of the vehicles is measured in three zones i.e., low, medium and high based on which timings were allotted accordingly. Traffic congestion is a common occurrence in many major cities across the world, especially in third world cities, and this has caused untold hardship to commuters in these cities in diverse ways  [1] . Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. Sometimes higher traffic density at one side of the junction demands longer green time as compared to the standard allotted time. But the system only increases the level of traffic congestion during peak hours. As a result of this a lot of time is wasted in the process. The proposed system would use a microcontroller of PIC family duly interfaced with sensors, to change the junction timing automatically to accommodate movement of vehicles, thereby, avoiding unnecessary waiting time at the junction. The density of the vehicles is measured in three zones i.e., low, medium, high based on which timings were allotted accordingly. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. Sometimes higher traffic density at one side of the junction demands longer green time as compared to standard allotted time  [4] . In a bid to overcome this challenge  [4]  adopted an approach whereby a camera is placed on the top of the signal to get a clear view of traffic on the particular side of the signal so that it will capture the image. The image captured in the traffic signal is processed and converted into grayscale image then its threshold is calculated based on which the contour has been drawn in order to calculate the number of vehicles present in the image. After calculating the number of vehicles we will came to know in which side the density is high based on which signals will be allotted for a particular side. An intelligent traffic lights control system using a Fuzzy Logic approach was developed by  [5] . However, this system has the disadvantage of the controller since it depends on the preset quantification values for fuzzy variables  [6]  conducted a cross sectional study targeting traffic control in the city of Nairobi's Central Business District and its surroundings. Data was analyzed and presented using descriptive statistics; tables and graphs by using excel 2003. The simulation runs results showed that the adaptive algorithms can strongly reduce average waiting times of cars compared to the conventional traffic controllers. This is converted by a bridge rectifier to a dc voltage. 0.7 volts for silicon diodes). For convenience, a capacitor of 1000uF is used. The PIV of the diode is calculated using the relation formula_1 Where Vp = peak voltage of transformer Vrms = root mean square voltage The Vrms of the transformer is given as 12v formula_2 Recall PIV ‚â• 2Vp Therefore, PIV of the selected diode is given as 2 x 16.9706 = 33.9412v For current, the maximum current of the transformer I (max) = 500mA The forward current of the diode I f is given as:   formula_3 Another purpose of an optocoupler is to prevent rapidly changing voltages or high voltages on one side of a circuit from distorting transmissions or damaging components on the other side of the circuit. Figure 4  shows the circuit diagram of the system. The IR sensors are used to sense the number of vehicles on the road. The Vero board is also called a strip board. It is a widely used type of electronic prototyping board characterized by a 0.1-inch rectangular grid holes with parallel strips of copper cladding running in one direction all the way across one side of the board. The PIC Microcontroller was programmed using Embedded C language. The codes are as shown in the Appendix. Simulation was done via Proteus software.
paper_3	In the beginning, a related work on the architecture trends of Web-based AEHSs is presented. Then, a description of the aspects of the MATHEMA is done regarding both its pedagogical and technological part. The teaching strategies are based on  [9]  learning cycle and learning style model. The aim of this article is to highlight the main contributions of the architecture of the MATHEMA in the improvement of the functionality of AEHSs. The rest of the paper is organized as follows: In the section 2 related works for various models of architecture of AEHSs is presented. In the section 3 the architecture of the AEHS MATHEMA is presented in detail. For the AEHSs various models of architecture have been developed so far on which the designers of these systems are based on. In this section some models that are most suited to the MATHEMA system are presented. The AHAM architecture model proposed by  [6]  is shown in the  Figure 1 . The AEHSs fit so nicely in this model. When designing a course it is important to first list the concepts. The next thing to do is to determine dependencies between the concepts. This leads to a structure of prerequisite relationships. The prerequisites are most common in educational material. For each type of relationship (e.g., prerequisites) some generic adaptation rules can be defined. The rules together form the adaptation model in AHAM. This is done by means of a teaching model which consists of pedagogical rules. AEHSs applications need to maintain a permanent user model. These dimensions form the axes on which both an AEH problem and its solution can be represented. Proper's architecture is a combined architecture of SCORM LMS and AEHS. Their system derives from SCORM 2004 Sample RTE Version 1.3.3 that is based on SCORM 2004 specification. Thus its architecture is a typical of a SCORM compliant LMS. They use the Apache Tomcat 5.5 as Web and application server and the MySQL 5 as database server. System retrieve course files initially from a zip file, which contains a manifest xml file and all the html and media required files. The DM structure is exported by the manifest file and is stored into Java Object Files. However additional data about the course is stored into the database. The adaptive navigation techniques have been applied to it via Java servlets and Java Server Pages (JSP). In the  Figure 3  the architecture of the WELSA  [14]  system is presented. [12] proposed a framework for AEHS (  Fig. 4 ) based on JSP, Java servlets, and Javabeans. The Interaction Analyzer is responsible for acquiring information on learner's behavior. ULUL-ILM  [1]  is an AEHS that focuses on student's learning styles. The result of that analysis is called domain model. For supporting features, it uses the (JSP) pages, Java servlets, JavaBeans, JavaScripts, and Apache Struts technologies. The content loaded to the MySQL database is accessed via JDBC API. A servlet is a Java class that implements the Java servlet API, a protocol to respond to HTTP requests. These servlets are complete programs that are capable of creating JSPs. This allows for much more flexibility in creating the page than XML. Oddly enough, servlets do not face any of the problems faced by classical CGI programming because a servlet has a lifecycle. Servlets dynamically create JSP pages according to student requirements. JavaBeans technology comes into play. This technology is a portable, platform-independent component model that lets developers write components and reuse them everywhere. (3) Protects your intellectual property by keeping source code secure. The Apache Struts is a formalized framework for the architecture style design pattern Model View Controller (MVC). Figure 5  shows the architecture of the AEHS MATHEMA. Bellow the operation of the units will be presented and analyzed. The domain knowledge of the AEHS MATHEMA is the basis of the system's adaptation. Content is on separate pages, enabling it to be re-used. The model that supports the AEHS MATHEMA is the overlay model. Figure 6  shows a snapshot of the page showing the characteristics of the learner model where the learner can be informed about them but also modify them. This is an innovation on the architecture of AEHSs. It also offers additional information about the navigation techniques that it supports. knowledge of the cognitive goal. Figure 7  shows a snapshot of the meta-adaptation result. The learner has the ability to change his or her navigation technique. A snapshot of a meta-adaptation result. The pages displayed to the learner are dynamically generated. Figure 8  shows the user interface, as seen by each user on his or her remote computer, sent to him or her by the presentation module. Figure 9  shows a snapshot of the page responsible for selecting preferences of the learner. MATHEMA supports the learner to find the most suitable peer for the formation of a collaboration team. Figure 10  shows a snapshot of adaptive group formation in the MATHEMA. This module is responsible for monitoring and supporting synchronous communication between learners via a chattool. Assessment of the pedagogical and technological aspects of the AEHS MATHEMA has been carried out. Research has shown that high school students increase their performance by studying through the AEHS MATHEMA  [13] . The evaluation of the system was carried out by students of the Department of Informatics and Telecommunications of the University of Athens, Greece. Most of AEHSs presented in this paper are based on JSP, Java servlets, and Javabeans technologies to implement their functions, as implemented by AEHS MATHEMA. The architecture of AEHSs becomes more complex as more and more functions are implemented. MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and metaadaptive navigation, interactive problem solving, and adaptive group formation.
paper_21	The proposed scheme ZSISMP is validated on all BCH codes of known minimum distance. An efficient solution of this problem is the use of error correcting codes. For these codes, only a lower bound is known and the minimum distance is known only for some lengths and special cases  [2-3-4-5-6-7-8] . The section 3 presents the proposed scheme ZSISMP. The section 4 presents the main results. For these codes, only a lower bound is known but the true value is still unknown for large codes. This section summarizes the most important ones. This approach consists in adding to all-zero codeword a level of noise and considering the minimum distance as the smallest level of noise from which the Soft-In decoder fails in correction. It is well known that for BCH (n=2 m -1, Œ¥) codes, the multiplier permutations defined on {0, 1,..., n‚àí1} by k 2 ¬µ : i‚Üí2 k i (mod n) with 1‚â§k‚â§m-1 are stabilizers. This section presents the proposed scheme for finding the lowest weight in BCH codes. For finding the minimum distance of BCH codes. Output: -d as estimated minimum distance of BCH (n, k, Œ¥) All results have been done using a simple configuration machine: Intel (R) Core (TM) i3-4005U CPU @1.70GHz RAM 4GO and are made by running the considered algorithm in 1day for each code. It is known also, that All the narrow-sense primitive binary BCH codes of length 255 have their minimum distance equal to their designed distance except BCH (255, 63, 61), and BCH (255, 71, 59). In order to validate the proposed method, it is applied on all BCH codes of known minimum distance presented in table 1. the obtained results show that the minimum weight found by the proposed method is equal to the true value of the minimum distance of all BCH codes of length up to 255. Therefore, the proposed method is validated for BCH codes of lengths up to 255. The table 2 summarizes the obtained results. These results demonstrate that the proposed scheme outperform greatly the famous Zimmermann algorithm. The table 3 presents a comparison between the proposed scheme and MIM-RSC method  [24] . In this paper, we have proposed a new efficient scheme to find the minimum distance for large BCH codes.
paper_31	From 1990 to 2017, the morbidity of chronic diseases with high incidence in China had been increasing continuously  [1] . Therefore, it is crucial to effectively prevent and control chronic diseases. With the development of Internet, people's life has been profoundly changed. Since the end of last century, some Chinese hospitals and universities have carried out some researches in this field. Meanwhile, some also have applied various wireless sensor technology to medical monitoring system. In the same year, American scholar Jutra had founded teleradiology. Therefore, the word "Telemedicine" arose. At the end of the last century, the rapid development of semiconductor technology drove the revolution of information technology. This is the second stage of the development of WITMED. During this stage, information technology was connected with medical health care more closely. Most data were transferred by satellite and ISDN (integrated service digital network). At this stage, telemedicine had made considerable progress. In recent years, with the popularization of mobile terminals (mobile phones, tablets) and the rapid development of the Internet, Telemedicine starts to combine with big data, artificial intelligence, cloud computing, cloud services and other technologies. This is the third stage of the development of WITMED. Energy-efficient Zigbee-based wireless sensor network (WSN) occupies a major role in emergency-based applications  [6] . Haji Bagheri Fard present methods based on deep learning algorithms to achieve high classification accuracy  [8] . Zigbee has been implemented on the Health Care Profile. In this telemedicine stage, it gradually focused more at community, families and individualized care  [11] . What's more, the coverage of 4G network and the transmission distance of WIFI are often easy to cause the loss of transmission data. (2) The problem of power consumption. (3) The problems of unitary monitoring data. (4) The problem of data processing. Finally, the processed data will be sent back to patients and their family doctors. According to the statistics of Professor Yang Hongying, there are 212 million patients with chronic disease in China at present. Then the coordinator transmits the data collected to the intelligent medical system. The system architecture diagram is as follow: Fast and correct data acquisition is the basis of the platform's efficient operation. The process of reading and transmitting data is a loop. The data acquisition structure charts The related knowledge base in  Figure 3  can be regarded as an expert system. Data analysis roadmap is shown as follows: If there are X words appearing in the taglib and N-X words not appearing in it, the matching degree of the information is (N-X) / N * 100%. Therefore, it is obvious that the matching degree is a fraction smaller than 100%. Finally, the request is inserted into the message queuing sequence by the matching degree. With the definitions above, f x, y can turn to be: f x, y ax 1 a y Œµ , namely f x, y y a x y Œµ . Definition 2: let {X1, Y1}, {X2, Y2} is concept form of {X, Y}, {X2, Y2b {X1, Y1}. Definition 3: The concept similarity is calculated as:  formula_1 Patients' health information involves their privacy. Therefore, it is crucial to guarantee the security of data transmission in the process of information transmission. This system adopts AES (Advanced Encryption Standard) encryption algorithm which is widely used at present. The calculation is shown as  Figure 7 : The intelligent system realizes data acquisition, data encryption and processing, and big data analysis.
paper_38	The study adopted the descriptive research design. Pearson Product Moment Correlation (PPMC) Coefficient and Multinomial Logistics Regression (MLR) were the statistics used to answer the four research questions used. Education is an essential issue regarding the development of any country in the world. It is a progressive development of knowledge and skills of examinees through stages of teaching and learning at various levels  [1] . At the inception of the Kaduna State University  (KASU)  in 2005, a total number of 409 students were admitted out of which 199 were for the Faculty of Science. In the 2017/2018 academic session, a total number of students admitted was 4,031, and 1,632 was admitted into the Faculty of Science. Students admitted into any of the Faculty of Science undergraduate degree programmes in the Kaduna State University must have been subjected to serious academic scrutiny. There have been various perspectives presented by researchers and psychologists about what is academic performance and its importance. Formula 1 is used for calculating the CGPA. formula_0 Most universities in Nigeria have been using 5.0 as their perfect CGPA score. As such, the CGPA is considered to be a good predictor indicator of a student's academic performance. The rest of this paper is organised as follows: section 2 is the review of related literature, section 3 presents the methodology, section 4 discusses the results obtained, and the last section presents the conclusion and recommendation. Over the years in Nigerian tertiary institutions, there has been rife with complaints about students' poor academic performance. The objectives of this study are to: i. The following research questions directed the study: 1. How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? It is a yardstick that is used to ascertain the competences of a student from which his abilities could be determined. There are a lot of definitions of students' performance based on previous works of literature. However, most of the studies mention graduation being the measure of students' success. Several studies have criticised the use of UTME and PUTME as an imperfect instrument for predicting academic performance of students. However, the following review of literature examined the relationships between UTME and PUTME scores as a predictor for the academic performance of students have revealed contradictions in their findings. A sample of 214 students records was used for data collection. The records of students who graduated in the 2009/2010 and 2010/2011 academic sessions from seven faculties were obtained using the stratified random sampling technique. Graduates with the best academic performance from the preliminary programmes were those admitted through the certificate programme. The authors used records of students admitted via the JAMB UTME from a Nigerian private university. The sample population consisted of students who were admitted during the 2010/2011 academic sessions but have graduated at the end of the 2012/2013 academic session. The author further recommended the need for the PUTME exercise to be strengthened to have a fruitful admission of candidates. One major shortcoming of virtually all the studies reviewed is their scope in time and spread/coverage. The design adopted in the study is the correlational ex-post facto, which is used to measure the degree of association between two or more variables or sets of scores. The Faculty of Science consists of nine undergraduate B. Sc. The sample distribution is as shown in  Table 1 . The session with the largest sample size was 2014/2015 with 710 (21.8%) students. The semester examinations were mostly essay type questions. However, the OL results obtained from the database was for students admitted during 2010/2011 to 2014/2015 academic sessions. The stanine grades in the OL results obtained at either NECO or WAEC were collected and coded as shown in  Table  2 . The total score for five relevant subjects in OL is then computed and coded together with the UTME and PUTME scores which are as shown in  Table 3 . The coding for the CGPA is also shown in  Table 4 . It was used in this research study. The data were regrouped and analysed by academic session and programme of study. There are three types of linear relationship that may exist between these two variables namely positive linear correlation, negative linear correlation and no correlation. MLR can have interactions between nominal and continuous independent variables to predict the dependent variable. The results of the data analysis are presented in tables according to the research questions that guided the study in this section. For the Computer Science programme, as shown in  Table 5 , the correlation coefficient indicated a low negative correlation in UTME-CGPA (-0.092) and PUTME-CGPA (-0.001) and a low positive correlation for OL-CGPA (0.072). Similarly, there exists a low negative correlation in UTME-CGPA (-0.082) and PUTME-CGPA (-0.038) and a low positive relationship in OL-CGPA (0.089) for the Mathematics programme. In the Physics programme, there exists a low positive relationship in OL-CGPA (0.016), PUTME-CGPA (0.028) and a low negative relationship in UTME-CGPA (-0.031). How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? Table 6  is the likelihood ratio test results for the Computer Science, Mathematics and Physics degree programmes. Likelihood Ratio Tests is a statistical test of the goodness-offit between two models. For the Computer Science programme, the slopes (B) of OL in all the CGPA categories are positive. What is the relationship between OL results, UTME scores and Post-UTME scores of students and their firstyear CGPA across the academic session, 2010/2011 to 2014/2015? PPMC was used to analyse the data for this research question. The summary of  Table 8  shows the correlations of OL and CGPA scores, UTME and CGPA scores and PUTME and CGPA scores for all the programmes used for the analysis. In the 2010/2011 academic session, the correlation coefficient indicated a low positive correlation in OL-CGPA (0.198), UTME-CGPA (0.189), and PUTME-CGPA (0.232). In 2011/2012, OL-CGPA (-0.114) and PUTME-CGPA (-0.043) display low negative correlation while UTME-CGPA (0.114) has low positive correlation. The OL-CGPA (-0.071), UTME-CGPA (-0.090), and PUTME-CGPA (-0.040) in the 2014/2015 session signifies a low negative correlation. Table 9  is the likelihood ratio test results for all academic sessions ranging from 2010/2011 to 2014/2015. Table 10 , on the other hand, shows the results of the Parameter estimates for the MLR for each academic session. For the students admitted during the 2010/2011 academic session, the slopes (B) of OL in all the CGPA categories are positive. Conclusively, the slope (B) of PUTME in the CGPA category of '3 rd Class' is positive and the rest negative. This indicates that the relative strength of UTME is higher than those with a CGPA category of 'Fail' and the rest of the CGPA categories are negative which indicates otherwise. This predictor holds true especially for students who are in the CGPA category of '2 nd Class Lower' and '1 st Class' respectively.
paper_57	This paper attempts to overcome stagnation problem of Ant Colony Optimization (ACO) algorithms. In this case some heuristic information which guide search process is useful. It is a multi-agent meta-heuristic approach and was first purposed by M. Dorigo et al. [2]  as Ant system (AS) algorithm. The aim is to find the shortest path. On their way back they use the same way from which abundant loops has been removed, but the amount of pheromone (1) ‚àÜœÑ k ij (t) they produced is inversely proportional to the tour length L k (t). where œÅ ‚àà (0,1) is the pheromone persistence (1 -œÅ is evaporation rate) and m is the number of ants. how long the acquired information will be available. An ant in each node has to make a decision which arc to take. Heuristic values Œ∑ ij affect probability only at the beginning when pheromone values are low. If q ‚â• p k ij (t), then choose the next node randomly. Genetic algorithms (GA) were proposed by  Holland (1975) . The original GA is known as simple genetic algorithm (SGA). The population size is given by the number of genomes, i.e. Mutation (ii) mimics random gene changes. In each genome each gene is changed with the equal probability. If more such nodes occur, random selection is applied. If no such node exists, another gene is randomly picked up from the list. If more of such nodes exist, random selection is applied. Crossover operation makes sense only if both child strings differ from their parents. No string is allowed to take the same genetic operation more than once. Prior the genetic operations all loops are removed from the tours. At first mutation is applied. It is applied on random selected tour T k (t) in random selected node. If mutation is not feasible, another node is chosen. After all mutation operations are performed, crossover operations are applied. Parent strings are random selected. After all genetic operations are executed fitness evaluation and pheromone update are scheduled. Genetic operations do not have to be necessarily feasible. The task is to find the shortest path between start node n s = 1 and end node n e = 80. For each setting 500 trials were performed. It can be seen that the higher number of mutation operations, the better the performance is (Tables 2). To prevent interference, no mutation operation was allowed. The results vary  (Fig. Beyond 60% threshold ants foraging behavior is suppressed by crossover overload. This is caused by the search space dimension. It is too large for ten ants to meet. The mean value of the cycle when the best value was found is 109.081 with standard deviation 2.617. Limit of crossover is 60% of crossover rate. Mutation operation causes better results than crossover operation. The higher amount of mutation operations the higher the performance gain is.
paper_78	Each formation has its own composition, structure, distribution range and environment, and all of them should be protected effectively. Quantitative methods, such as numerical classification and ordination, are significant in ecological analysis of plant communities  [1] . It is applicable in ecology because the description of ecological systems is not always possible in terms of a binary approach  [4] . Ecological communities, such as Glycyrrhiza uralensis communities, have great variations in their species composition respond more or less independently to environmental variables. Because of this, methods based on fuzzy mathematics might be more appropriate in ecological analysis  [6, 7] . Medicinal plant is important natural resource and significant for people health in many countries and regions, such as China, Japan, Korea, India and so forth  [8, 9] . Licorice (Glycyrrhiza uralensis) is one of the most popular Chinese herbal medicines and a significant resource plant species. Like licorice, most medicinal plants are harvested in the wild and the extended use has led to some medicinal species endangered and tending to be extinct, and the protection of such medicinal species has been urgent  [12, 13] . The studies on medicinal plants and their communities are the basis for their conservation and restoration. Classification is the process of dividing plots into clusters (communities) so that plots in the same cluster are as similar as possible, and plots in different clusters are as dissimilar as possible. Fuzzy C-means clustering is a soft classification technique  [5, 7]  in which a plot can belong to more than one cluster, and associated with each plot is a set of membership levels. The Fuzzy C-means clustering attempts to divide a finite collection of n elements (plots) formula_0 into a collection of C fuzzy clusters with respect to some given criterion. Given a finite set of data matrix (species √ó plots), the method calculates a list of C cluster centres  formula_1 formula_2 Where i = 1, 2, ‚Ä¶, N = the number of plots; j = 1, 2, ‚Ä¶, C = the number of clusters; U = {U ij } = the matrix of membership values, U ij is the membership of plot i in cluster j; V is a matrix of cluster centers; m is fuzzifier which determines the level of cluster fuzziness (1 ‚â§ m < ‚àû). A large m results in smaller memberships u ij and hence, fuzzier clusters. (dA ij ) 2 is the distance index: formula_3 X i is the vector of attribute measurements in plots, usually a vector of ordination scores; V j is the centre of cluster j, if A is a unit matrix, then formula_4 Based on the algorithm above, the procedure of fuzzy C-means clustering is as follows: (1) Selecting a common ordination method, we use Detrended Correspondence Analysis (DCA), and do ordination analysis for species data. formula_5 formula_7 Based on the new membership values U, we go back to the fourth step and calculated the next turn V j , (dA ij )  2  and U ij iteratively, and until the membership values become approximately stable. We can use U to identify the relationships among plots and communities directly. However for clustering purpose, a plot should belong to the cluster in which it had the maximum membership value. Based on a general survey of Glycyrrhiza uralensis and its community distribution, five study regions, Chifeng (in inner Mongolia), Hengjinqi (in inner Mongolia), Minqin (in Ganshu), Aletai (in Xinjing) and Kashi (in Xinjing), were selected as sampling sites  [7] . The coverage, mean height, individual number for shrub and herb species was measured in each plot. Fuzzy C-means clustering is a nonhierarchical method which provides C clusters and their plot compositions  (Table  1) . The community has a total cover of 75%, a shrub layer cover of 20% and an herb layer cover of 65%. The community has a total cover of 70%, a shrub layer cover of 5% and an herb layer cover of 70%. The average cover of Glycyrrhiza uralensis in this community is 35% with a density of 5700 ha -1 . The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 60%. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 59500 ha -1 . The community has a total cover of 65%, a shrub layer cover of 10% and an herb layer cover of 60%. The average cover of Glycyrrhiza uralensis in this community is 37% with a density of 4900 ha -1 . The community has a total cover of 60%, a shrub layer cover of 5% and an herb layer cover of 58%. It is distributed from 400 to 700 m in hills with slope 10 -30¬∞ in sunny and semi-sunny slope and sandy soil. The community has a total cover of 75%, a shrub layer cover of 10% and an herb layer cover of 70%. Theoretically, fuzzy C-means clustering is the only one soft clustering method and can deal with much imprecise and incomplete information and has advantages in solving non-linear problems and in studying complex system  [2, 17] . Therefore, fuzzy C-means clustering should describe ecological phenomena and rules better  [5, 6] . This study proved that fuzzy C-means clustering is fully usable in classification analysis of plant communities  [1, 19] . Glycyrrhiza uralensis communities recognizing by fuzzy C-means clustering varied greatly in species composition, structure and distribution area  [20] . These communities are important not only for protection of medicinal plant species but also for conservation of ecosystems and their environments in semi-arid and arid regions in North China  [23, 24] . The classification results by fuzzy C-means clustering are reasonable according to vegetation classification system of China  [25] [26] [27] .
paper_96	With the rapid development of China's transportation, the frequency of traffic accidents is also high. This humane technology innovation is the specific embodiment of environmental science and technology aesthetic theory in the field of scientific and technological innovation. It has a positive and promoting role to the development of transportation and blind-man welfare in China. During last summer, I went blind orphanage as a volunteer, and personally experienced the inconvenience and hardship of the blind when they travel. In fact, this "warning system of embedded electronic guide dog avoiding obstacles" is similar to a car anti-collision avoidance system. In this way, the design of obstacle-avoiding early warning system of embedded electronic guide dog has the following advantages. It has a simple design, and small volume, so that it is convenient to carry the "dog". Its operating system is thought from the perspectives of practical application, development tool, instantaneity, technical services, and price, etc. Its main control unit hardware uses the ARM microprocessor as the core, processing and handling the signal of each sensor. [3]  The alarm system of guide dog adopts the combination of light and sound. Sound will produce auditory stimulus for the blind. The full name of ARM is Advanced RISC Machines. The ARM architecture follows the principle of reduced instruction set computer (RISC). It extensively uses the registers with a fast speed of instruction execution. And its way to addressing is much easier and more flexible, and its operating efficiency is high. Also, it could complete most of the data manipulation in the register. At present, the embedded RTOS (Real Time Operate System) mainly includes two categories--commercial and free. Thereinto, the commercial RTOS includes WINCE and VxWotks; while the free RTOS has Linux (uCLinux and RT Linux included) and uC/OS -II. These are the most remarkable features that uCLinux owned. The reference model is shown in  Figure 2 . of USB reference model  [4] The traffic safety problem of the blind is an urgent issue to solve. This humanized technology innovation is the embodiment of environmental science and technology aesthetics theory in the field of science and technology innovation practice.
paper_134	The most jobbery ways during olive oil production consist of mixing other oils such as maize, sunflower, Canola and corn into the olive oil. Data mining takes advantage of the progress made in artificial intelligence and statistics. These methods are very damaging, costly and time-consuming. In recent years, nondestructive methods have been considered in purification. Dielectric properties are one of the most important physical properties of agricultural and food products. If the dielectric coefficient is bigger, it will be a better insulating property  [6] . One of the main reasons for this low consumption is the high price of this oil. The great benefit of mixing vegetable oils with lubricant makes it hard to find the type of product. Also, processing of data and results were presented in fourth section  [11] . (2010) conducted a research on olive oil for detecting adulteration using a dielectric spectrum. They used a 4V sine voltage in the range of 10Hz to 1 MHz to determine the dielectric properties of a binary mixture of olive oil. They also used the partial least squares model (PLS) to detect oil falsification. The results of the PLS calibration model showed good results for the adulterated oils with soybean oil as a counterfeit oil. Soltani et al (2010) used a non-destructive capacitive method to detect the amount of banana fruit juice. Experiments were carried out on the day of laying, on the third day, on the sixth day, on the ninth and twelfth days after the laying. The results showed a significant difference between DC / DV ratio during storage period. 70% of the data was for network training, 15% for validation and 15% for the network testing. In this study, a recent study has been carried out to identify the authentication of olive oil. In this article the experiment was done by olive, sunflower, Canola and corn oil. The device used consists of the Arduino board, ICL8083 and AD8302. The USB port is used to communicate or send and receive information between the device and the computer. The Arduino board is used on the Uno device, which uses the Atmega2560 microcontroller. One of the new ideas in identifying and categorizing patterns is a backup vector machine or SVM. Backup machines have very valuable properties that make it suitable for pattern recognition. Figure 4  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn. Using this technique, the amount of RMSE for the samples were equal to 3.56, 2.45, 3.05 and the amount R 2 was equal to 0.92, 96 and 94, respectively. Figure 5  shows the results of adulterated oil boosted tree regression. The results were predicted and modeled using regression methods. Figure 2  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn Using this technique, the amount of RMSE for the samples were equal to 3.65, 2.46, 3.31 and the amount R 2 was equal to 0.92, 96 and 93, respectively. Interestingly, olive-Canola oil samples predicted with high accuracy in all techniques. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud. Also device used can classify samples and use for other oils. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud.
paper_139	At the present time web contains many indistinguishable documents. From the proposed system, we have relevant information with more accuracy. Step2: Pick the minimum degree vertex 'v' in the set-up and include in the set U. Step3:While U doesn't include all vertices Step3A: Include the entire isolated vertex which is adjacent to the vertex 'v' to U. Step3B: Find the adjacent vertex 'u' to 'v' which is not in U and has maximumdegree. Update 'u' to U. Step3C: Update the value of degree for all adjacent vertices of 'u'. Iterate throughall adjacent vertices if possible. We have 3 nodes (B, 1 and 5) and we can pick the minimum degree node. Here all nodes have an equal degree  (3) . Now the set U consists of the nodes A, B. 4.2 Search the node B, the unvisited adjacent node is from B as 2 and 7. Now the set U consists of the nodes A, B, 2. 4.3 Now travel around the node 2, the unvisited adjacent node is from 2 as 1 and 4. After inclusion of the node 1 the set U consists of the nodes A, B, 2, 1. 4.4 Discover the node 1, the unvisited adjacent node is from 1 as only 3. Now the set U consists of the nodes A, B, 2, 1, 3. In search engine generate relevant information but most of the time the information is not redundant.
paper_145	The present study was based on data collected from 900 respondents of both urban and rural areas of Bangladesh. Among them 44.3 percent were overweight and obese. Majority (70.6%) were diabetic patients. With the increase in age significant increase in prevalence rate of obesity was observed. Higher prevalence rate was also observed among housewives. Higher prevalence of obesity was noted among females. The prevalence of overweight and obesity has increased rapidly over the last decades especially in developed countries  [1] [2] [3] . In 2016 WHO estimated that globally approximately 1.9 billion adults (age > 20 years) were overweight and more than 650 million adults were obese  [4] . The major contribution of obesity is to lead the increase in the prevalence of chronic diseases and cancers  [7] [8] [9] [10] . Behavioral factors have significant effects on metabolic risk. The study was based on data collected from both urban and rural people of Bangladesh. The investigated diabetic patients were 544. But during investigation some of them were found as diabetic patients. The latter information were provided by the diabetic patients. Some of the variables observed were qualitative in character and some were quantitative. The analysis was done by using SPSS [version 20.0]. The respondents were classified as underweight  [BMI < 20] , normal [BMI,  20 -25] , overweight [BMI < 30] and obese [BMI 30+]. Factor analysis was done to identify the factors for obesity. Maximum (43.1) of the respondents were overweight and 20.9 percent of them were rural people. Obesity was observed among 15.3 percent people and overweight respondents were 34.0 percent. There were 58.9 percent males among the respondents and 47.2 percent of them were normal. The corresponding figure among females were 37.3 percent. However, compared to males more females were obese. Similar normal group was noted among the other group of respondents. Majority (52.9%) of the respondents were of age 50 years and above and 48.5 percent of them were normal [  Table 5 ]. Levels of obesity was significantly associated with levels of ages [P (œá2 ‚â• 18.34) = 0.008]. Higher proportion of respondents (23.2%,  Table 7 ) were businessmen and 45.5 percent of them were normal. The lower income (< 20,000.00 Taka) group of people were more (34.2%) and 48.4% of them were normal [  Table  8 ]. The data indicated that 54.4% respondents had income less than 30,000.00 taka. Significant association was noted between the level of obesity and the level of income [P(œá2 ‚â• 64.994) = 0.00]. These two groups of people were 450 and 164 of them were overweight. Again, those who were not doing any physical labor (23. Table 10  showed that 67.6 percent underweight respondents were affected by diabetes. There was no significant association between level obesity and prevalence of diabetes [P(œá2 ‚â• 0.851) = 0.837]. The association between smoking habit and level of obesity was significant [P(œá2 ‚â• 20.189) = 0.0.002]. This was done by factor analysis. So, the inclusion of variables were satisfactory. These three variables were more important for the variation in the level of obesity. The coefficients of the components were presented in  Table 13 . The respondents were investigated mostly by the doctors and nurses from their working places. Around 50 percent respondents were overweight and obese. Higher (71.6%) prevalence rate of diabetes was noted among the overweight and obese group of respondents. Similar finding was also noted in another study  [21] . The prevalence of overweight and obesity were significantly associated with age, religion, education, occupation, marital status, income and smoking habit. Around 50.6 percent people of urban area were overweight and obese. This result was also similar as was observed in another study  [21] . The analysis was done from the collected information of 900 respondents. They were classified as underweight (BMI <20), normal (BMI, 20 -<25), overweight (BMI, 25 -< 30) and obese (BMI = 30+). The percentages of these four groups of respondents were 7.6, 43.1.34.0 and 15.3, respectively. Most the respondents were in normal and overweight groups. Again, prevalence of diabetes was more among these groups. This finding is similar to that observed in both home and abroad  [23] [24] [25] . The public health authority can play a decisive role for the above steps.
paper_212	Epidemiological disease models have been used to explain the dynamics of HIV/AIDS in the population from the early 1900s. We conclude that the simulated model reflects reality. On June 5, 1981 a mysterious disease was recognized among MSM in the USA. In 1982 the CDC identified the same disease among IVDU, hemophiliacs and Haitian residents. It was characterized by its etiological agent HIV in 1983,  [1] . A serological test, was then made available. In 1984, Robert Gallo discovered that HIV was responsible for AIDS. In 1984, several HIV and AIDS cases were documented in Kenya. This accounts for 4% of new pediatric infections worldwide. Among the expectant women there are 13,000 new HIV infections among children. The number of those that died account for 7% of the global total. There are various questions still left unanswered to date on the HIV epidemic. In 2010 several authors came up with a model to predict HIV transmission in China in 2002  [3] . In this model, there were no forms of intervention. The formulated model was used to forecast the number of PLWHA. The number of HIV infections in 2010 was predicted to approximately 1000000. The group most affected would be the 31-40 years group. A stochastic differential equation SI model with demographic stochasticity has already been developed  [8] . They used the Milstein method to simulate for analysis. In their works they build a disease spread prediction model based on the SIR model and applied parameter values to a stochastic model based on Gillespie's algorithm. This study seeks to incorporate a stochastic aspect in the deterministic SIR epidemiological model. By contrast, in a deterministic process, there is no randomness  [12] . The Classic SIR model The Kermack-McKendrick theory illustrates individuals grouped as susceptible and removeds only  [13] . The transmission and infection rates were considered to be variant. The model explored how altering transmission dynamics affected the model as a whole. The SIR model explained how the epidemic manifests in all the compartments. The graphical representation of the developed stochastic model is shown;  Figure 1 . The stochastic SIR model. The assumption is that the population is finite and is sub-divided into categories of finite discrete compartments. Assuming that the resulting state is . A propensity function is the probability of one event occurring in the time interval ! Discrete evolution is modelled in discrete time. The data was obtained from NACC for HIV/AIDS cases. The means and variances of the simulated and natural data were computed. A stochastic SIR model was simulated with a mean step size of 0.006336446. 537 tau steps were made in the model. Curves produced are illustrated below. where C represents the variance of D The calculated value found was 64.958. The critical value was 47.4. Since the calculated value is greater than the critical value, the decision rule is to reject the null hypothesis. Therefore, the conclusion is that the simulated data model fits the natural data model. In this study, a simulation was carried out on the SIR model to explain the trajectory of the disease by employing a stochastic element using Gillespie's simulation algorithm. The simulated curves were compared to HIV/AIDS data. The simulated curves were found to resemble the data available in reality. Mathematical modeling is an area that requires more research.
paper_214	Objectives of decision-making process aimed at adopting the best solution from many possible alternatives. Optimizing the use of a certain class of models for finding the best solutions. Among the methods most commonly used are decision analysis and mathematical programming  [1, 2] . Each alternative are attached estimates and probabilities of achievement. Solving the problem is to build decision tables or decision trees, from which it selects the best alternative. Apply mathematical programming problems which lead to the formalization of a mathematical relationship between decision variables and purpose. It is the only method that can be applied to unstructured problems. These are translated into algorithms which are executed by a computer system. This led to consideration of simulation as one of the most powerful tools in decision making. Simulation becomes a technical coordination of procedures using the computer. The solution offered is one spot that has no counterpart in the real system. Trying to solve them in a particular category determined tackling by a standard method employment. In complex cases, the problem breaks down into subproblems more manageable, easier structured. Modeling takes expression of reality by means of abstract entities possessing quantitative and qualitative attributes. Intuition, creativity and experience allow decision-makers compare alternatives; predict outcomes of each alternative separately. After the final resolution of the model, select the best alternative is chosen implementation plan. The assessment in turn depends on the search method. Structural problems use mathematical formulas and analytical method to achieve an optimum solution. In order to improve efficiency the best solution search algorithms are used. When the number of alternatives is too large, then testing some or all of the possible solutions is possible by using an incremental search method. For complex problems, solving is carried progressing from one situation to another, until a final statement, which is the solution. Methods called heuristics, based on a thorough analysis of the issue. Basically successive tests are performed, the search progressing from a solution to another. Implementation is the phase that involves the integration model chosen solution in context and simulating the real system. Personal data is data that relates to the behavioral aspects of decisionmakers in making decisions. The data source, internal or external, data is extracted and managed by a management database. The management of the database depends on the organization of data. In most cases there is SGBS transactional relational data system and a management database for multidimensional data warehouses created. The data dictionary is a catalog of all data from the database. It contains data definitions, data sources and their intrinsic significance. The data dictionaries are permitted operations to add new data, deletion or retrieval of existing information according to certain criteria. The most common data dictionary used in the first phase of decision making is data mining to identify their problems and opportunities. The SQL language is used, which accepts requests for data from other systems. It is the component that differentiates interactive decision support systems to other systems. The models are domain-specific and models can be classified into strategic, tactical and operational models models. The models are used currently in operational and transactional system that aims of the organization. In building a data warehouse is based on the analysis of data. Extract information in order to obtain information for decision making. Basically there are two steps: designing and populating data. Conceptual models are multidimensional and designed to organize data necessary decision-making process on issues.
paper_216	Amongst the non-parametric approaches used to improve the accuracy of the model in this study is the Wavelet-based pricing model. The study was carried out using simulated stock prices of 1024 observations. Derivatives includes; Forwards, futures, options and swaps. The ones traded on the exchange are standardized and regulated. The Monte Carlo method, which is based on repeated computation and random sampling can be used for pricing American options  [1] . In the Kenyan market, derivatives are yet to be developed. This study therefore prices a European option using two nonparametric methods and a parametric method. This model derives the closed form solution for pricing of a European options that is why it is used as a benchmark. The Wavelet based pricing model is another nonparametric method alternative used to price derivatives  [4] . The remainder of the paper is structured as follows: Section 2 discusses the existing literature; Section 3 presents an overview of the Black-Scholes model and Wavelet based pricing model. Section 4 describes the data, shows the empirical results and performance measures of the models. Lastly, section 5 concludes the study. The simple closed form solution of European options was derived during the financial crisis  [2] . This model has been discussed extensively in order to improve its pricing biases and to impose more practical assumptions. The Black Scholes model does not correctly price in high volatility markets  [5] . Employing the wavelet method to de-seasonalize prices of electricity  [9] . The de-noising ability of wavelets was also recognized in  [7] . Evidence have been provided to support that wavelet based local linear approximation  [11] . European options can also be priced using the Shannon wavelet  [14] . Another example is the application of the wavelets in nonparametrically estimation of a diffusion function  [15] . Wavelets can also be used in calibrating parameters with long memory processes  [16] . c is the call option price, p is the put option price and N (.) is the cumulative distribution function. ( is the bilateral inverse Laplace transform. Interest rate r is assumed to be constant. When T -t = 1, Œò (v) is the risk-neutral MGF for the rate of return per unit of time. This needs to be approximated by wavelets. On this criterion, the Franklin hat function performs very well because it is symmetric, smooth and piecewise continuous. This function also emulates the probability density function of asset returns. To estimate the risk neutral MGF of the return per unit time, we use the Franklin hat function f (t) expressed as; In this study Monte Carlo simulation was used to generate 1024 stock prices. Other complex options include; Bermuda options and exotic options. Moreover, hedging performance may be evaluated with respect to each of the two models.
paper_219	This paper introduces a framework to determine data quality on enterprise networks for net-centric and net-ready initiatives as introduced by the US Department of Defense (DoD). quality of service (QoS). The QDS attribute brings distinction to the resultant data quality of the network's quality of service. There is also a need to assess the quality of data sharing across the enterprise network. The resultant minimax value correlates to the lowest performing attributes of the framework. The presented framework offers decision support tools to enable agencies to allocate limited resources towards improving the performance of their net-centric service offerings to the enterprise network. At the same time, developers are also facing a new host of challenges from increasing cyber threats. All of this is in addition to the historical problems of network management and quality of service. The value of data quality can be used to indicate the priority of data delivery to the consumer. Not all data is the same; some is more relevant to the users' needs when compared across all the data. Section 2 of this paper provides a brief overview of the origins of net-centric and net-ready policies. Section 3 defines a new model for valuing the quality of data by measuring user satisfaction. Section 4 presents a method for applying the new model attributes to evaluate data quality across the enterprise. These enterprise catalogs and registries contain discovery metadata entries for an individual data asset or group of data assets  [6] . Untrusted data can introduce error, uncertainty, and delay into the military decision process  [8] . The performance of the IT must be quantifiable with threshold and objective values that are traceable to measures of effectiveness (MOEs)  [3] . Sampling users is the preferred and direct method for measuring QoE. The question arises how to get the users' opinions. Usually the MOS is formed from arithmetic mean of a population of user opinions. One possible split is to consider groups of users based on their shared mission or objectives. develop or enhance the prediction model of QoE for each attribute. For a sample of size n consider v strata with sample s k in each stratum, i.e. ‚àë i=1 to v n i = n and s = U k=1 to v s k . where n ‚â§ N and s = {1, 2,‚Ä¶, n}. The QDS takes into account the effects of environmental conditions on sensor performance for given design parameters. QDS is a subjective rating from the perspective of the end user  [11] . Traditionally these QoE MOS ratings were undertaken by panels of experts. But cost and time to use panels of experts to assess MOS has resulted in seeking alternative approaches. To produce a subjective rating MOS, or QoE, the ITU-T P.862.1  [13]  is used to map raw PESQ to the final rating. The reason to use the full reference is to capture environmental conditions resulting in the most accurate predictions of ratings. A standard to address the rating of motion imagery (i.e. In  [17]  and  [15]  there are equations to estimate the interpretability for still imagery and video, respectively. Over the last several decades there have been many papers on the topic of QoS. But just for completeness a brief explanation of QoS is given here. QoS is in essence an engineering optimization problem where the objective is to maximize users' satisfaction while minimizing cost of delivery of the supporting network services. And the service level agreement (SLA) is the users' agreement with network provider(s) on acceptable ranges for the metrics. User-satisfaction provides a true gauge of a network QoS  [11]  and the subjective assessment of that satisfaction is provided by QoE. However there are a number of challenges to QoE discussed in  [19]  and  [20] . To understand cause and effect it is ideal to have the full reference i.e. As stated in  [20]  QoE is likely to be biased by negative responses, tendency to higher responses from unhappy consumers. These models of QoE based on QoS objective measures are often referred to as "QoE\QoS" correlators  [21]  and  [12] . Models have been developed to correlate the QoS with QoE for multimedia applications  [12] . For both models  [21]  showed QoE had an exponential model in terms of their QoS objective metric(s). A number of authors  [12, 15] The measure of data relevance expresses the utility provided by the data towards the consumer's objective(s). A scalar measure is required to support evaluating the degree of relevance on the enterprise performance. The three principal elements of data relevance and their respective effects on the QoE for data relevance are now discussed, as depicted in  Fig. Intrinsic data relevance represents the relative value (i.e. QoE) that the data would provide to the consumer assuming perfect discovery and delivery. Intrinsic relevance reflects three properties of the data: form, spatial, and temporal. Enterprise data systems can offer multiple forms of data to the consumer (e.g. overhead, side-view, rear-view, distant, near). The goal of the tagging and discovery process is to connect highly relevant data with an authorized consumer. metadata) with those of the producer. Alternatively the data relevance user satisfaction to match the other attributes can be improved. Consider a simple example in  Table 1  to illustrate the minimax in a game theory context. Player one has three action options U, D, N to choose from. Thus, the minimax strategy is Player one move of N and Player two move of R with a payoff vector (4, 3). f d (s) = u HT . The x-axis of the chart in  Fig. 3  is organized to reflect the connection between QDS and QoS. The final enterprise data quality, as shown in  Fig. 3 , as formula_6 In  Fig. This paper introduced a framework to determine data quality on enterprise networks for DoD net-centric and net-ready initiatives. The framework's data quality model consists of three attributes: quality of data at the source, data relevance, and network QoS.
paper_241	Transformers are the key equipment in electrical power transmission. C transmission, power transformer is one of the most important equipment. It is expensive uninterrupted and desired to be kept in good condition always to have supply. Typically, Power systems are built to allow continuous generation, transmission and consumption of energy. differential current) and can be prevented using differential protection and microcontroller based relay protection. The protection techniques employed differential relay mechanism with Arduino. At all times, the Arduino senses the condition of the transformer. [4] Ochieng designed a microcontroller based power transformer protection system which uses a current sensor as the interfacing instrument between the current transformer and the pic16f690 microcontroller. The current sensor acs712x series was used in the project as the interfacing instrument between the power transformer and the pic16f690 microcontroller. The differential protection scheme is concerned with the faults that arise from associated difficulties such as the magnetizing inrush current and saturation. It is transient in nature so it lasts for just a few seconds and does not cause any permanent damage to the transformer. The power transformer to be protected is connected to the main AC supply and delivered power (P=IV) to the output terminal. The power transformer primary and secondary currents are measured by the current transformers, which are to be protected by the Arduino and the circuit breaker. The step-down transformer is used to step down the primary or source voltage from 220 to 12V. This 12V is regulated to 5V (using L7805CV voltage regulator) which is the required voltage for the Arduino. The current transformers are each connected to the primary and secondary side of the step-up transformer and are used to reduce the currents to a measureable value that can be sensed by the current sensors. For the load with both the 200W and 60W bulbs the current values and difference were larger than with each connected separately.
paper_251	Cloud computing is associated with a new paradigm for the provision of computing infrastructure and services. Secondly, Now computing power or connected computing power (with network) has more demanding and significant role in almost all areas of epoch including market analysis, searching, map, accounting, medical, trading, shopping, rescue operations and many more, the list is endless. Advancement of electronics and telecommunication field has done the job. Secondly most of the resources are idle i.e. Hence the utility types of computing paradigm will play an import role. Cloud offers instant service (software, platform or infrastructure) to requisite dynamically. Proposed agent based approach has provides the efficient and accurate solutions for efficient scheduling and monitoring in cloud computing. In the cloud computing. They are able to communicate i.e. social in nature, mobile i.e. autonomous and distributive in nature. They are following with respective functionality in the proposed system 1. Codenvy -To develop an / are application i.e. SaaS (Software as a service). Platform as a Service (PaaS). For this Cloudbees service provider has been integrated onto the developed SaaS application. New Relic -To develop the core functionality of the proposed system. In this the java agent has been customized to meet the monitoring and scheduling of the SaaS services. These are following 1. Service scheduling delay 2. Better Provisioning of the SaaS 4. For SaaS development Codenvy has been subscribed. In which jsp based application has been develop and deployed on cloudbees PaaS. Cloud bees service integrating a SaaS deployment on it and delivering through agent. Measuring the performance of the proposed analytical approach (influenced from Aneka) in cloud services such as public Cloud bees. Number of tasks submitted at instant i (Ni) 2. Time to execute the task 3. Table 2  shows the response time of the deployed SaaS obtained results and has been compared with existing agent based method (in seconds) . Average user satisfaction is same as to availability and scalability of the proposed system. Hence proposed system performs better than  [2]  and any other methods as shown in figure 3. Proposed mechanism has influences from the working of Aneka framework. For evaluation of the propose agent based method data set (jsp) has been developed using jsp web pages and deployed onto the cloud evaluating the elasticity and its monitoring. The developed java web application (SaaS) has been developed with the help of codenvy SaaS developed platform.
paper_272	Parameters of a vacuum interrupter are essential. Three synchronous generators 6.6KV/8MW each have been connected to three power transformers, rating 6.6KV/1250KVA in refinery power plant. The name vacuum arc is really incorrect, indeed, it's a contradiction  [2]  "If there is a vacuum there is no arc, and if there is an arc there is no vacuum". J=2*10 6 (A/m 2 ) Joule heating as thermionic emission Where √ò is the thermionic work function. K is Boltzmann's constant (1.37x10-23 Joules/K) The constant A is approximately 6x10 5 for most metals The total above processing time was measured by Harris approximately (20-250¬µs)  [15] . If the high frequency accompanying re-ignitions and voltage escalation in one phase couple into other two phases, the process of virtual current chopping can occur  [6] . It is important to appreciate that while current chopping and voltage escalation can occur in a single -phase circuit, virtual current chopping is specifically a 3-phase characteristic: the effects of normal current chopping, multiple re ignition and over voltage escalation in one phase can generate surge over voltages in the second and third phases. A high frequency current governed by the circuit parameters flows. Restrike overvoltage due to the multiple re-ignition of circuit breaker when a switching interrupter process is initiated before current zero. These two high frequency transients and the voltage loop are associated with the current chop and the immediate re ignition of current. formula_0 From equation  (4)  express in dimensionless from the current in the inductor of any parallel RLC circuit, with any degree of damping. We note that the only parameter involved is Œ∑. So that a family of generalized curves can be drawn from equation  4 for different values of Œ∑ with dimensionless quantity -t`, as abscissa. This has been done in Figure. Where Œ∑ =0.5, the sine function changes from a circular to a hyperbolic function. Suppose Vo =13.8‚àö2 KV So. Zo = ‚àöL/C = 5X104 ohms Œ∑. Chopping current times (400us-800us). Transient over-voltages (450us -480us-500us).
paper_294	Cassava has played an important role as a staple crop in the feeding of the Tiv people. The study sampled 377 out of the population of 20,000. The snowball sampling technique was used in selecting the subjects for the study. Mean and standard deviation statistics was used in answering the research questions. The study found that the Tiv people had various strategies they adopted in managing postharvest losses of cassava. This is the very core of storage and preservation. Audiovisual heritage are quite essential that the public library system must not afford to elude its storage not only for users but also for posterity. Cassava has played an important role as a staple crop in the feeding of the Tiv people. Essentially, all these were part of their management strategies for postharvest losses of cassava. Apparently, if this is allowed to continue, the consequences cannot be foretold in the near future. Specifically, the study sought to: 1. Identify Tiv management strategies of postharvest losses of cassava 2. Determined whether public library has impacted on Tiv management strategies of postharvest losses of cassava. storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. What are the Tiv management strategies for postharvest losses of cassava? In what ways does the public library impact on Tiv management strategies for postharvest losses of cassava? What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? Public library is a library that is established and managed with public funds. Public libraries are established wholly or partly from public funds. They are not restricted to any class of persons in the community but freely available to all. Public libraries anywhere they are established, is for the purpose of development. They are a reservoir of society's intellectual history, the custodians of people's knowledge and information. This is their most crucial function of all. Therefore, it will be appropriate only if the public library will acquire audiovisual materials, which will be most suitable in meeting the needs of the indigenous Benue farmers. A direct loss is disappearance of food by spillage, or consumption by insects, rodents, and birds. Peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom 4. Peeling the cassava, fermenting, sieving and squeezing it into lumps known as Akpu among others methods of processing. The snowball sampling was adopted in selecting the subjects. The researchers did this until they arrive at the sample size of 680. The instrument used for data collection was Questionnaire constructed by the researchers. Section A of the questionnaire contained respondents' bio-data, which included sex and occupation. Data were analyzed using mean and standard deviations. Data was collected using 4 point rating scale instrument. Importantly, on each research question, data were collected on related items in the instrument. The collected data were analyzed using mean and standard deviation. What are the Tiv management strategies for postharvest losses of cassava? To answer the research question, data were collected on Tiv management strategies for postharvest losses of cassava. The collected data were analyzed and presented in  Table 1 . From  Table 1 , it can be seen that the mean rating scores of all the items are above 2.50. In what ways does the public library impact on Tiv management strategies for postharvest losses of cassava? What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? To answer the research question, data were collected relating to challenges faced by public libraries on acquisition and storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. The collected data were analyzed and presented in  Table 3 . Table 3 , it can be seen that the mean rating scores of all the items are above 2.50. Based on the results in  Table 1 , the findings from the study revealed that processing cassava into Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom, Mtuhem (Base), Akpu, Kweesi or Akweesa Akom and Kamu Akom are the Tiv management strategies for postharvest losses of cassava. The research discovered that the Tiv people had various strategies they adopted in managing postharvest losses of cassava. Incidentally, the public library's inability to do so was due to lack of adequate funding among many other factors. The study was carried out to investigate the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. The study found out that public library does not impact on Tiv management strategies of postharvest losses of cassava through storage of audiovisual materials on the subject matter.
paper_298	The study investigated the awareness and use of library catalogue for accessing information resources by users of Federal University of Kashere (FUK) Library. Four research questions guided the study. The instrument used for data collection was questionnaire. Data generated was analyzed using simple percentage and descriptive statistics. Poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. Guidelines on how to use catalogue and library in general should be prepared and issued to registered users. The library ensures that the resources acquired are well organized to allow easy access by the library users  [1] . In this environment, the user is both the primary searcher of the system and the user of the information resources  [2] . Students' skills in search catalogue can be created through library tours, orientations, seminars, lectures, library staff, handbooks and use of library studies. Kumar revealed that 26.99% respondents use the OPAC fortnightly and 22.84% weekly. The author attributed the reason for the poor usage to lack of user education programme  [9] . In essence, students use the catalogue to enable them conduct research in the library. It is on this note that this study investigates the use of library catalogue by undergraduate student in the university library. Therefore, the study investigates the access and use of library catalogue by students of Federal University of Kashere, Gombe State. The study is designed to achieve the following objectives: a. To find out the students awareness about library catalogue as access point to library collection. To find out methods employ by students to consult library catalogue to search for information resources. c. To ascertain the extent of use of the library catalogue by students to access information resources. d. To identify the constraints associated with the use of library catalogue. The study is designed to answer the following research questions: a The survey research method was adopted for the study, and questionnaire was the instruments use for collecting data. Thus, 272 respondents were selected for the study based on 95% confidence level and 5% confidence interval of Sample Size Calculator. The completed and returned questionnaires were analyzed using descriptive statistics, percentage and frequency distribution. The gender breakdown is presented on  Table 1 . The question on the demographic information of the respondents was on gender presented in  Table 1 . It shows that majority of the respondents 188 (69%) were male, followed by females 84 (31%). This shows that majority of library users in the Federal University of Kashere Library were male. Table 2  revealed that majority of the respondents 262 (96.3%) were aware of the existence of the library catalogue in the university library. This findings correspond with that of Oghenekaro on the "use of library catalogue in Nigerian University Libraries: A focus on Redeemer's University Library", that majority of respondents 225 (89.6%) were aware of the existence of catalogue in the library  [1] . 26 (10%) of the respondents were aware of the existence of OPAC as a retrieval tools in the library. This indicates that majority of the respondents were not aware of online public access catalogue in the library and they only use manual catalogue for information retrieval. Their responses were presented in the  Table 4  below. Table 5  indicated that majority of the respondents 158 (60%) used the library catalogue regularly when searching for materials in the library. The study found that majority of the respondents were aware of library catalogue in the library including OPAC  [12] . Table 7  revealed that 118 (45%) of the respondents use catalogue to access information for assignment, followed by those that use it to retrieve information for research 96 (37%). The results are presented in  Table 8 . Table 8  focused on the challenges associated with catalogue use for information retrieval. Higher number of the respondents indicated poor computer skills hindering navigation when searching OPAC 96 (37%) and inability to locate materials on the shelves, indicated in the catalogue as being available 58 (22%) were the challenges affecting the use of catalogue as an information retrieval tools. This finding corroborates with that of Ogbole & Morayo which revealed that (41.1%) of the respondents rarely use OPAC. The study revealed that majority of the university library users were male. Higher number of them got their awareness through library staff and above average used the library catalogue regularly. Another source of frustration was reasonable number of the respondents indicated poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. This will enable them gain the needed skills to use the library catalogue maximally when searching for information resources. Because most challenges associated with the use of the OPAC often have to do with lack of ICT skills on how to make use of it. c. Guidelines on how to use catalogue and library in general should be prepared and issued to registered users.
paper_305	Mitigation of credit risk is a key aspect of portfolio management in any financial institution. In this paper, we report on the results of a MSc. Loans constitute the cornerstone of the banking industry's financial portfolios. Furthermore, a loan officer's credit decision or recommendation for loan worthiness is subjective. Therefore, a knowledge discovery tool is needed to assist in decision making regarding the application. Further, the study champions the use of open source software tools in business intelligence applications. The development of machine learning models and tools has been welcomed as one of the most exciting in business settings. This calls for the use of more efficient and effective loan screening tools and procedures. Automated techniques have progressively become popular in contemporary loan appraisal processes. A decision stump is a decision tree with only a single root node. It works as follows: 1. Looks at all possible thresholds for each attribute 2. Selects the one with the max information gain 3. Figure 1  illustrates the combination criterion. In our study, the variable k was bi-valued and took on either 'accept' or 'reject' values and K was set at 2. The model was specified in terms of K ‚àí1 log-odds that separate each class from the base class K. Train all K decision stumps iii. Select the single best classifier at this stage iv. Combine it with the other previously selected v. classifiers vi. Reweight the data vii. Learn all K classifiers again, select the best, combine, viii. Repeat until you have T classifiers selected The development platform used for this project mainly included the following open source software products: The system was implemented on a Java platform comprising of the JDK compiler, netbeans IDE developer, weka API and the exe4j executable file converter. The model was built using the training dataset and tested using three strategies. We report on cross validation as under. Separate data into fixed number of partitions (or folds) ii. Classify and obtain performance metrics. v. Classify until each partition has been used as the test set. Calculate an average performance. This strategy relies on two separate files, one for training and the other for testing. The accuracy returned by the training set is 19 correctly classified instances out of 20 instances. This gives an accuracy of 19/20=95% Class =Accept: The number of correctly classified instances is 12 and that of instances classified as belong to the class is 13. This gives a precision value of 7/7=1 Class =Accept: The number of correctly classified instances is 12 and the number of instances belonging to the class is 12. This gives a recall value of 12/12=1 Class =Reject: The number of correctly classified instances is 7 and the number of instances belonging to the class is 8. This gives a recall value of 7/8=0.88  Figure 2 . Test Split ROC graph P(true | false) calculated as 1-d/c+d The ROC area has the following indicators: . Indicates a perfect prediction . After a successful implementation of the stated system, the following were the key outcomes: This is a strategy where the training file was portioned into complementary data sets called the training set and the validation set. Further, the system can be improved by creating a web-based interface or porting it to a distributed architecture platform.
paper_310	We are now living in the 21 st century. Now, smart phone has become the most essential thing in our daily life. Here we are using Bluetooth communication, interface microcontroller and android application. We are using Arduino software to interface the Bluetooth module with microcontroller. According to commands received from android the robot motion can be controlled. Here in the project the Android smart phone is used as a remote control for operating the Robot. The controlling device of the whole system is a Microcontroller. Bluetooth module, DC motors are interfaced to the Microcontroller. b) Develop a robot which will be helpful for travelling. f) Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. Microcontroller will act as the brain of the robot. The robot movement will be decided by the microcontroller. In this system we will be using microcontroller named Arduino UNO which contains ATMEGA 328P microcontroller chip (  Figure 1 ). Arduino has it own programming burnt in its Read Only Memory (ROM). Cprogram is very easy to implement for programming the Arduino UNO. The smart phone is the transmitter of this circuit. It sends the data to microcontroller through Bluetooth module. It also helps to send the instruction of forward, backward, left, right to the microcontroller. Actually, the smart phone is used as a remote of this system. Motor driver IC is used to control the dc motors. Here we use programming language 'C' for coding. The program is burnt in the microcontroller using burner software. The program is stored in the EEPROM of the microcontroller, which is present in the Arduino board. In android application when we press a button, a corresponding signal is sent through the Bluetooth to Bluetooth module (HC-05) which is connected with the Arduino board. There are two steps of the programming. First set up section where we define all the variables. As seen from the  Figure 6 . A DC power supply is required to run the system. The DC power supple feeds the Microcontroller and the Bluetooth module. Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. The Bluetooth module is connected with the Arduino UNO board for the connection with the user. Then through the data cable we insert the commands in the microcontroller ATMEGA 328P. The functions of the given instructions are operated by the microcontroller. The instructions are sent by the smart phone. We can easily control the movements of the dc motor. This is indeed a cost-effective and efficient project. The robot can be used for surveillance.
paper_333	The researcher selected two Enset leaf diseases viz. Bacterial Wilt and Fusarium Wilt disease and collected 430 Enset leaf images from Areka agricultural research center and some selected areas in SNNPR. Food security is a challenge in many developing countries like Ethiopia. Nations in our country are still struggling to make use of available resources so as to combat hunger. This is commonly known as false banana, the Ethiopian banana or the Abyssinian banana. Locally the plant is called Enset and botanically, it is named as Ensete ventricosum (Welw.) Ethiopia is one of the grand producer of Enset in African continent countries. There are several issues and diseases which tries to decline the yield with quality. The remaining part of this paper is organized as follows. The types of common Enest diseases are discussed in Section II. In Section III, we present the architecture of proposed system. Experimental results are reported in Section IV. Section V presents the conclusion and discusses of future works. The feature of normal and diseased Enset image features was extracted to train kernel support vector machine. Figure 2  shows the architecture of the proposed system A. Dataset Partitioning The images of the dataset are collected from different parts of southern Ethiopia. Different results were found by using different multiclass support vector machine classifiers. In order to measure the performance of the classifiers K-fold (in this case k is 10) cross validation method was used. The detail classification result is shown in  figure 4 . This can help the farmers to produce a good quality and quantity Enset product.
paper_389	In this paper, a large number of experiments have been carried out using conditional random fields. The experimental corpus has been tested by Changjiang Daily for many years. Word probability, the paper explores the probability characteristic of word location. Peng F establishes a Chinese character segmentation model based on CRF. In addition to using some common features, But also used a lot of domain knowledge. Common Chinese character tagging method is based on the Chinese characters appear in the words of different locations marked different labels. Enter the sentence as "This is Wuhan." In order to reduce the computation cost, we adopt some rules to eliminate some unnecessary paths. 2, marking O cannot appear behind the mark I, can only be O or B. 3, Mark B can only be followed by the mark I. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. Combinations: A combination of 2 or 5 different word or string features at different positions. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. Then the training corpus is trained to generate a CRF model, and some training parameters such as iteration number are added in the training process. Using the CRF model generated by the training, the test corpus is tested and a prediction result is obtained. Finally, the evaluation program is used to evaluate the prediction results and get the evaluation results. The corpus is from 1950 to 2005, and the scale is 2564168000 sentences. The coding method is GB code. The corpus content mainly comes from newspaper news. The format consists of a sentence segment consisting of words marked with spaces. The performance of a word segmentation system mainly depends on two aspects: segmentation precision and word segmentation speed. The main factors that affect the speed of word segmentation are the structure of word segmentation dictionary and word segmentation algorithm. In different applications, the performance requirements of the word segmentation system have different emphases. The experimental results show that conditional random field is an efficient segmentation method. And then use these tools and the corpus carried out a number of experiments. In the experiment, only some feature information is used, and most of the features are extracted from the training corpus, we have achieved good results.
paper_391	Globally, tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The WEKA 3.6.1 tool was used for classification, clustering, association and attribute selection. The maximum confidence attained in the general association is 0.98. But, in the class attribute, it is 0.72. The literacy status of the mother has high information gain with the value 0.046. Multilayer perceptron classifier has the lowest average error at 32.72% compared to others. Thus, women receive doses of tetanus toxoid to protect their birth against neonatal tetanus  [1] . Tetanus is caused by a toxin produced during the anaerobic growth of Clostridium tetani. Globally tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually, accounting for up to 25% neonate's death in some African countries  [3] [4] [5] . Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The information is rich and massive. In this study, we use different data mining techniques that have been tested on TT dataset. The  Figure 1 (Adapted from  [7] ) shows, the basic phases of the knowledge discovery from data, we have undergone. Preprocessing solves issues about noise, incomplete and inconsistent data. Those patterns that remain represent the discovered knowledge. The EDHS 2011 was conducted by the Central Statistical Agency (CSA) in collaboration with Measure DHS and ICF International under the auspices of the Ethiopian Ministry of Health (EMoH). The data used in this investigation are the TT immunization data. It has a dimension of 7033 rows and 12 columns. This data processed and arranged for the purposes of training and testing. Finally, data have been saved in ". csv" file formats and stored as an ". Classification is one of the data mining techniques and it is used to group the instances which belong to same class  [8] . How does this classification work? And the classification step that is, where the model is used to predict class labels for given data  [7] . The approaches are; (a). Decision tree (J48) approach It is a flowchart-like a tree structure. And the topmost node in a tree is the root node. The training tuples are described by m attributes. Each of the tuples represents a point in an m-dimensional space. These k training tuples are the k "nearest neighbors" of the unknown tuple  [7, 10] . They can predict class membership probabilities like, the probability that a given tuple belongs to a particular class  [10, 11] . This assumption is called class conditional independence. This is for assessing how "accurate" your classifier is at predicting the class label of tuples. Understanding them will make it easy to grasp the meaning of the various measures. Training and testing are performed k times. Thus, in this study, we have used five clusters. "How does the k-means ( ) algorithm work?" The k-means algorithm then iteratively boost the within-cluster variation. From the selected 7037 mothers, 3351 of mothers received TT Immunizations. The 5680 of mothers were from rural Ethiopia, and more of them (3484) were in the age range from 25-34. (Table 1)  As can be seen in  Figure 2 , the accuracy rate of the classifiers on training data is relatively high. Briefly described as follows: Cluster 0-This group have 1534 (27%) instances of which consist of mothers who live in the rural, no access to radio, no access to TV, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 1-This group consists of 1004 (18%) instances of which mothers who live in the rural, no access to radio, access to TV yes, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, head of household male, tt-vaccinated yes. Cluster 2-This group consists of 2063 (37%) instances of which mothers who live in the rural, no access to radio, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated No. Cluster 3-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 4-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, orthodox in religion, Amhara in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education no education, women age 35-49, mother marital status married, hh_head Female, tt-vaccinated yes. Multilayer perceptron classifier has the lowest average error at 32.72% compared to others.
paper_402	This paper presents machine learning algorithms based on back-propagation neural network (BPNN) that employs sequential feature selection (SFS) for predicting the compressive strength of Ultra-High Performance Concrete (UHPC). The BPNN and SFS were used interchangeably to identify the relevant features that contributed with the response variable. The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete. Specifically, ANN has been used to solve a wide variety of civil engineering problems  [2] [3] [4] . The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete  [14] [15] [16] [17] . Other researchers proposed different mixtures by adding fly ash and sand to reduce the amount of cement and silica fume, and acquire an optimum mix that is both economical and sustainable  [25, 26] . The objective of the study was to develop an ANN and SMD model to predict both the compressive strength and the consistency of UHPC with two different types of curing systems (steam curing and wet curing). As a result, BPNN proved to be more accurate than SMD in the prediction of compressive strength and slump flow of UHPC. Meaning, the model does not produce any analytical model with a mathematical structure that can be studied. In addition prediction of compressive strength of high strength and high performance concrete was addressed by other researchers  [20, 21] . There are several machine learning techniques, in the literature, that assist researchers in identifying the underlying covariates impacting the prediction model. The reduction in the covariate domain improves the accuracy of the fitting model, decreases its computation time, and facilitates a better understanding of the data processing  [27] . [29]  used the Markov Blanket with a wrapper method to select the most relevant features of human motion recognition. Moreover, Rodriguez-Galiano et al. Four types of machine learning algorithms were used as wrappers for the SFS. There are two types of ANN models: (1) feed forward; and (2) feed backward. Equation  1 shows the linear combination of mapping weights from each input neuron, via wires, to the hidden neurons. SFS is composed of two components: the objective function, which is the criteria the algorithm follows when selecting the features (i.e. There are two types of search algorithms: sequential forward selection and sequential backward selection. Therefore, 11 neurons is, approximately, the number of neurons that is sufficient enough for BPNN to facilitate an accurate ANN model for the collected dataset. The SFS algorithm was run 200 times to capture all possible combinations of independent features when using ANN. Table 2  tabulates the percentage of features that were used during the 200 trials. In this study, four variables (Cement, Sillica Fume, Flyash, and Water) were selected as the most relevant features for the prediction model. The selected features, using SFS, were analyzed by the previous BPNN model. Table 3  shows the statistical measurements calculated for both cases. It was observed that the r 2 and NMSE before and after selection yielded 81.6% and 99.1%, respectively, and 0.0594 and 0.026, respectively. A linear regression model was developed using the Least Square Regression (LSR) method, where the analytical model consisted of the previously selected features. Table 4  shows the coefficient values, with their corresponding symbols, for each UHPC constituent with the statistical measurements of the LSR model. The LSR model is a linear function and its form is shown in  (2) . fc = Œ∏1C + Œ∏2SI + Œ∏3FA + Œ∏4W (2)
paper_418	This paper examined the challenges in choosing between additive and mixed models in time series decomposition. Also, when there is no trend (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The models most commonly used to describe time series data are additive, multiplicative and mixed models. For short series, the cyclical is embedded in the trend  [2] . They do not depend on the level of the trend  [3] . The higher the trend, the more intensive these variations are. This is an indication that the seasonal variation equals a certain percentage of the level of the time series. In such cases it is appropriate to use a multiplicative model. Oladugba et al  [9]  gave brief description of additive and multiplicative seasonality. They pointed out that any of the tests for constant variance can be used to identify a series that admits the additive model. This is an improvement over what is in existence. The method adopted in this study is the Buys-Ballot procedure in descriptive time series. 2 (a) The column means mimic the shape of the trending parameters and contain seasonal indices for additive model. formula_5 Source: Iwueze and Nwogu (2014), Nwogu el al (2019). That is when ( 0 b = ), it is clear from  For mixed model, we obtain using the expression in  Table 1  . formula_10 When there is no trend ( 0 b = ) we obtain from  (Table 1)  . j j j X S a e = + (27)  From table 2, we observed that the estimates of the trend parameters and seasonal indices are not the same for both additive and mixed. Table 3  that when there is no trend i.e. (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The data of 120 observations has been arranged in a Buys-Ballot table as monthly data (s = 12) and for 10 years (m = 10). The expressing of a linear trend and seasonal indices for an additive model is given as Estimates of trend parameters and seasonal indices are discussed. Also, when there is no trend ( ) 0 b = , the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models.
paper_428	The theory of non-Newtonian fluids has attracted several researchers owing to its enormous applications in engineering and industrial sector. In Non-Newtonian fluids, the most frequently encountered fluids are pseudoplastic fluids, and Navier-stokes equations alone are insufficient to describe the rheological properties of these fluids, therefore, to overcome this defect, several rheological model like Ellis model, Power law model, Carreaus model and Cross model are presented, but little attention has been compensated to the Williamson fluid model and estimated to explain the rheological properties of pseudoplastic fluids. Williamson analyzed the flow of pseudoplastic materials and presented model to described the behavior of pseudoplastic material and explain convenient importance of plastic flows, and also recognized that viscous flows are very varied from plastic flows  [1] . developed a model for the transport of Williamson fluid in an annular region  [10] . MHD is very useful to analyze the interaction of electrically directing fluids. It play significant role in field of technology. studied unsteady combined convection flow in a cavity in presence of nanofluid  [41] . outlined mixed convection boundary layer flow influenced by thermo-diffusion  [42] . Chaudhary and Jain studied the impact of mass transfer, radiation and hall on MHD mixed convection flow of viscoelastic fluid in an infinite plate  [43] . General example of porous wedge is sand, soil, sandstone and foams. Deka   is wedge angle parameter. E 0 = E 6 , E‚Ä≤ 0 = 0, F 0 = 1 (9) E‚Ä≤ ‚àû = 1, F ‚àû = 0 Where E 6 is injection/suction parameter. This With the boundary conditions E 0 E 6 , $ 0 0, F 0 1, $ ‚àû 1, F ‚àû 0 (13) The resulting system in Eq. (11) (12) (13)  is solved numerically with the help of 5 th order Runge-Kutta-Fehlberg method. Further details about the obtained numerical solutions are presented in the next section. The transformed governing equations  (11) (12)  subjected to boundary conditions  (13)  are solved numerically by employing the fifth order Runge-Kutta-Fehlberg method. [78] , Yih  [79]  and Rashidi et al. The agreement of our work with the prior results is stable. Figures (1-2)  illustrate the influence of wedge angle parameter < with on velocity and temperature profile. It is observed that velocity increases by increasing the wedge angle parameter < , but the thermal boundary layer thickness is decreased. This eminent phenomenon is known as Lorentz force that squeezes the momentum boundary layer. Furthermore an increase in Œª may cause increase in temperature of flow. The important conclusions of the analysis are 1. The non-dimensional velocity profile increases by increasing the wedge angle parameter <.
paper_432	The mathematical analysis method used. The Cotangent Bundles * of manifold d provides the basic model of a symplectic manifold. If are local coordinates in neighborhood of a point ‚àà . Then the Cotangent Bundle has a dual space * . Generically, the corresponding Hamiltonian system `(%) =& H '%, % ). Given any , manifold , of dimensionn, with -. The Cotangent Bundle Œ§ * contains the following classes of Lagrangian submanifolds; The fibers of Œ§ * . b) Let exp ‚à∂ ‚Üí 3 > be given by exp(% =J ‚Ä¢" ‚Ä¶ ‚àà 3 > . Then exp is locally trivial fibrations with fiber the integers ‚Ä†. Let 0 ‚à∂ 3 ‚Üí 0 be the projection map. This is a locally trivial fibrations with fiber the two points set. From which the required inclusions follow easily. Let be a lie group‚Ä¢ ‚àà o‚ãÉ ~‚àû‚Ä¢. 8 ' : K V ‚Üí R s> ( K ), satisfying R o 8 ' u 0 ' and the equivariance property. 8 ' ( K , n, ' ) = 8 ' ( K, n ). ' ), the left multiplication of : n`. The twisted product √ó ¬∞ is the quotient of √ó 3 by the twist. Now consider a G on a manifold. ≈°J% ¬± ‚àà , with isotropy subgroup = ¬≤ . The symplectic reduction of the cotangent bundles * Q has more structure than a symplectic manifold. Then ‚àÖisc , invariant surjective submersion and descends to a symplectic homeomorphic. The map ‚àÖ is a sort of pushforward, though a ¬≥ is not injective. Note that ‚àÖ is "injective mod '', meaning that ‚àÖ(¬± 1 ) =‚àÖ(¬± 2 ) if and only if ¬± > =n.
paper_444	Then, an uncertain resource constrained model is built. Furthermore, the equivalent form of the above model is given and its equivalence is proved. Its constrains were the resource and priority rules. For solving the above model, the equivalent form of the model is provided and the proof is given. The construction of this paper is organized as follows. Finally, some concluding statements will be covered in Section 4. (1) This paper only considers renewable resources. (4) No interruption is allowed for each activity in progress. " : The set of underway activities at time #. Therefore, managers always required to make trade-off between the total cost and the completion time. As to a pair of activity and 2, activity 2 start after its predecessor activity is finished. Constraint ‚óã 6 shows the range of decision variables. [16]  Let G be an uncertain variable with regular uncertainty distribution H. If the excepted value exists, then +IGJ = K Œ¶ MA (<) A = <. (2) For instance, let G be a linear uncertain variable, it has inverse uncertainty distribution H MA = (1 ‚àí <)N + <O. Then excepted value of G is formula_1 Theorem 2. Model (1) is equivalent to the following model. is an uncertain variable, and the inverse uncertainty distribution of ? is formula_6 By Theorem 1, we know that the excepted value of ? is formula_7 Since ‚Ñ≥6‚àë ‚àà8 9 ‚â§ + ; ‚â• < = , then, ‚Ñ≥6‚àë ‚àà8 9 ‚àí ‚àí ‚â§ 0; ‚â• < = . By Theorem 2, we have ‚àë ‚àà8 9 ‚àí ‚àí Œ¶ MA (1 ‚àí <) ‚â§ 0Ôºå  (12)  i.e. The duration time, cost and resource requirement of activities are presented in  Table 1 . The cost per time unit of additional resource ! The goals of project are both completion time minimization and expected total cost minimization. The constrains are recourse constraint and precedence constraint. The decision variable is finish time of each activity. The information of the activities. By describing the increased quantities of resource as uncertain variables, an uncertain resource constrained project scheduling problem is discussed in this paper. To solve this model, we transformed the uncertain model into equivalent form and proved it. In future research, We can also focus on more types of project scheduling problems based on uncertainty theory.
paper_462	The goal of postgraduate education for master's degree in clinical medicine is to cultivate high-level medical and health professionals. Since 1998, the postgraduate education for master's degree in clinical medicine has been piloted in China. However, there are still difficulties in linking up with professional qualification certification, slow internationalization process and postgraduate students. There are many problems such as insufficient practical ability training. "5 + 3" new training mode of training in combination. Chongqing Medical University was founded in 1956. Since 1979, it has enrolled doctoral and master's degree students. In 1998, it became the first batch of doctoral and master's degree in clinical medicine in China. Pilot units for professional degrees. Since 1998, the school has vigorously developed professional degree postgraduate education  [1] . Since 2009, the school has been focusing on reforming the training mode of clinical master degree postgraduates (hereinafter referred to as clinical master). Educational development and other key problems of clinical master training. The organic cohesion of syndromes has effectively improved the quality of clinical master training. The above policy support ensures that our clinical master can fully participate in clinical practice skills training and ensure the quality of training. After completing the dissertation defense and meeting the award requirements, the graduate certificate and master's professional degree certificate can be obtained. To ensure the quality of clinical master's training, the school has set up a guidance group composed of the first tutor and rotating responsibility tutor. Effective management during the transition period. At the same time, the workload of guiding professional degree postgraduates is directly linked to the promotion of their professional titles. Our school has made corresponding reforms in the curriculum system of clinical master's degree. The reformed curriculum system not only meets the training objectives of clinical master, but also meets the requirements of theoretical knowledge for trainees. It is closely related to clinical practice. System The secondary schools awarded professional degrees in clinical medicine in our university are all the standardized training bases for residents in Chongqing. The secondary clinical colleges are responsible for both the health bureau, the training of residents and the school, and the training of postgraduates with professional degrees. Students are brought into the "two levels, two stages" training after they enter school. Requirements for the first stage of training. System for Clinical Ability Clinical competence is the core of clinical master training. How to objectively and effectively assess the clinical competence of postgraduates is the key to ensure the quality of clinical medicine degree award. At present, the country lacks a quantitative index system for clinical competence evaluation system. School research and explore more rigorous clinical training and assessment methods. The clinical master emphasizes on examining the clinical competence of postgraduates. If the reviewers are hired, some experts will evaluate the papers according to the requirements of scientific degree papers. At the same time, the cost of training clinical masters has increased substantially. Postgraduates can devote all their energy and time to clinical ability training. Thirdly, we should formulate an enrollment system conducive to the development of professional degrees. The number of professional degree postgraduates enrolled by tutors in clinical departments is clearly required. All tutors working in clinical departments must enroll professional degree postgraduates. The curriculum system is guided by the improvement of vocational competence and solves the contradiction between curriculum learning and clinical competence training. The two systems complement each other and organically combine to realize the training of clinical master and regular training. The school has reformed the single tutor system and explored the establishment of clinical master tutor group system. It has overcome many obstacles and steadfastly promoted the reform. It has basically realized the seamless connection between the degree education of clinical medicine specialty and the certification of professional qualification. Over the past five years, the reform has achieved fruitful results and basically achieved the expected goals. The main results are as follows: Dozens of brothers such as Fudan University learn from our experience. It has been highly valued by the Ministry of Education and the Health Planning Commission, and has the realistic conditions for its promotion and implementation throughout the country. In the past five years, a total of 2063 clinical masters enrolled in the school have been integrated into the training. Many excellent students have become an indispensable new force in clinical colleges. They have achieved the ultimate goal of training "doctors who can really see a doctor". The employment rate of graduates has been guaranteed to be 100% for a long time. In the 2011 "Forum on Reform and Development of Medical Education", the principal of the school presented the reform experience to the conference and won the unanimous praise of the broad masses of colleagues. In December 2013, the school sponsored the National Symposium on the Reform of the Training Model of Clinical Master's Degree Postgraduates. Our school has 10 affiliated hospitals of Grade A and 28 key national clinical specialties. There are 9694 open beds in each affiliated teaching hospital, 57 resident standardized training bases, covering all the specialty categories of clinical medicine. Since the reform of training mode, the enrollment of clinical masters in our university has been increasing rapidly. In the past five years, a total of 2063 clinical masters have been enrolled in the standardized resident training system. The problem of docking professional degree education with industry access standards has been solved. It hopes that through continuous exploration and reform, it can play a role in attracting more brothers.
paper_476	This paper uses JD Logistics as an example to quantitatively analyze the relationship mentioned above. at various stages in the delivery system, which will more likely to result in uneconomical performance. At present, most express companies are operating different delivery frequencies in different regions. There are various of applications of system dynamics in the area of transportation  [2, 3] . Fan Xuemei et al. [5]  pointed out that joint delivery can effectively improve the efficiency of urban delivery. It proposed an assessment framework for joint delivery. The system dynamics has a good applicability in analyzing the delivery efficiency, and some researches have achieved a series of results. This paper uses Jingdong Logistics (JDL for short) as research objects. First, it analyzes the boundary and causality of its delivery system. Then, it establishes a simulation model for the operation of the delivery system on the base of system dynamics. Each delivery operation mainly includes: storage, ferry, sorting, transportation and terminal delivery. The relationship of the workflows in the delivery system is shown in  Figure 1 . According to investigation results, the cost of storage and ferry in JDL only accounted for about 8% of the average cost in daily delivery. The number of employees is affected by factors such as order quantity, transportation efficiency, delivery frequency, and the number of transport vehicles. Performance wage is determined by the actual amount of work performed during the delivery process. Sorting staffs consist of sorters and on-site logistics personnel. The sorter is the person who operates on the sorting equipment. These factors present a complex, nonlinear, and inverse relationship between each other. As is shown in  Figure 2 , 58 causal loops are formed. According to the influencing factors above, a simulation model of the system dynamics has been built as shown in in  Figure  3 . This model passed the mechanical error checking and dimension consistency testing by VENSIM. The main parameters involved in the model are as follows: (   Table 1 , and the explanations are shown in  Table 2 . In this paper, four different schemes of delivery frequency were set up for simulation analysis. Different delivery frequencies have different splitting ways of total order quantity per day. Second  Third  Fourth   1  60  30  40  20  2  30  30  20  20  3  10  30  30  20  4  -10  10  30  5  ---10  Table 4 . Setting reason for each scheme. The delivery frequency in scenarios 2 and 3 was increased by 9.5% over that in scenario 1. When the volume of orders increased from 1496 to 5300, the increased percentage of total delivery cost showed a downward trend followed by an upward trend. The minimum value of 3.9% appeared in the order volume of 3086-3154 units. The reason is that in scenario 3 the sorting center dispatched more vehicles in advance, which increased the cost of transportation. The total delivery cost in scenario 4 was increased by 25% in average than that in scenario 1. As the order volume increased, the increased percentage of total cost tended to decrease in fluctuation. In this situation, the delivery frequency should be increased accordingly. Within the orders volume ruled in this paper, the delivery frequency and the cumulative delivery costs are positively correlated. Compared to scenario 1, scenario 4 had an average increase of 315 yuan per day in total sorting costs. This is because that under the current total delivery volume, the order quantity after splitting in the scenario 4 was higher. Figures 12 and 13  show that the transportation cost in scenario 2 was increased by an average of 14% than that in scenario 1. The transportation cost in scenario 4 was increased by an average of 45% over scenario 1. The transportation cost in scenario 1 showed a rising trend with the increase of orders. When the order volume was 2406, 2530 and 4700, the transportation cost rose rapidly. The main reason is that in this case the number of transportation vehicles at the sorting center was fixed. As a result, the overall efficiency of the delivery staff was reduced. At the same time, the increase in the delivery frequency also improved the rental cost of the delivery station. Figures 16, 17 and 18  demonstrate that the delivery frequency hade different effects on different transportation vehicles. Figures 19, 20, 21  and 22show that the delivery frequency had different effects on terminal delivery operations. In some cases, the area utilization efficiencies in Scenario 1 and Scenario 3 were the same. This is because the difference of orders volume in the largest batch between two scenarios was small. As shown in  Figures 21 and 22 , the overall utilization efficiency of delivery station A was approximately 5% higher than that of delivery station B. This is because that the ratio between the order quantity and the actual number of delivery personnel at the delivery site was not equal. With the same delivery frequency, the orders splitting ratio influences delivery cost and resource utilization efficiency significantly. With the same delivery frequency, different order splitting ratios affect delivery cost and resource operational efficiency. In addition, the average split of order quantity can effectively reduce the tension of delivery resources. Therefore, JDL needs to consider different constraints when making delivery frequency decisions. When transportation vehicles are in tight supply, the delivery frequency should be increased when the order volume is 2530.
paper_479	Handwriting is an integral part of our life that can predict who we are because the style of writing is unique for every person. Handwriting has remained one of the most frequently occurring patterns that we come across in everyday life. A forensic document examiner is saddled with the task of document authenticity. The L R theory takes its stance on odds-form from Bayes ' theorem. Let us presume that there are two opposing ideas. In ' competing, ' it can be said that they are mutually exclusive, but may not necessarily include all possible alternatives. Consider, as an example, a situation where the question is whether a specific writing belongs to a particular person. One proposition is 'The suspect is the author of the document in question'. Such two theories are mutually exclusive, but there may be other reasons even if they are far-fetched. For instance, someone might have fabricated the writing with some special skills. The propositions involved should be relevant and the latter case does not seem to be applicable. The definition of proof analysis is then reflected in the proportion of probabilities. The forensic examiner provides a summary of the evidence needed for belief based on the evidence and the fact that prior beliefs about H p and H d have been quantified. Extended writing samples such as a paragraph of writing as well as signatures were considered. Strength of proof serves an integral part of this problem. Figure 1  shows the graphical representation for the handwriting modeling. Original and disguised handwriting were gotten from each writer over a period of six months and a skilled forger was asked to forge these writings. The handwritings were preprocessed using the Otsu method after which they were segmented into different words using the Sobel edge detection algorithm. Back Propagation Neural Network (BPNN) served the sake of supervised learning of the system. We target is set for each character in the handwriting. The target will help to know which handwriting is original and disguised. The neural network is a parametrized system because the weights are variable parameters. The weighted number of the inputs is the activation of the neuron. Transfer function can add non-linearity to the network. weight-recognition factor with weight ! " Weight from second hidden unit i and output unit j is ! ) Weight from the constant 1 and two hidden unit j is #$",*"  formula_2 After randomly selecting the weights of the network, the backpropagation algorithm is used to measure the necessary corrections. Each scanned, segmented and clustered characters and alphabets which were collated over a period of six months were trained to learn the pattern for each writer. Base on decision law i.e. If L R value greater than 1 H p is true If L R value less than 1 H p is false. A more elaborate and collated result table is presented in  Table  2 . Thus there must be agreement in sign otherwise there is disagreement. Full L R void of nuisance parameters is needed for most forensic investigators. Several factors have to be put in place to estimate a full L R for forensic handwriting investigations.
paper_492	This function is the behavior of so-called short-term memory. It is human intelligence that gave birth to the technology. But the structure of our brain has not changed since tens of thousands of years ago. In this paper, a new neural network and its behavior are presented based on the above idea. In this paper, the model of the transfer of data between different nervous systems is shown using the concept of category theory. The same is true for the recognition process. The same applies to general figures. Deductive logical development is desired. The divided subsequence is defined the basic subsequence. For simplicity, assume time series data consisted of 10 type elements from a 0 to a 9 . Example shown in  Figure 1  shows that the time series data (1) arranged by randomly selecting the elements is divided into five basic subsequences (2). The dividing is done by the following procedure. (1) The first element is the beginning of the first subsequence. (2) If the same element exists in already divided subsequence. In this example a6 is the concerned element. Figure 2  shows the affinity with the neural circuit. It is a neural network having an input consisting of a plurality of bits are shown. When the first data c 0 is received activate the bottom. For the next data c 1 additionally activates the elements which has been activated by the first data c 0 . Because the elements activated by c 1 is randomly connected to input, not all elements activated by c 0 are additionally activated. Four portions are activated in the  Figure 2 . By receiving c 2 , c 3 and so on the activated portions become narrow and narrow. The number and their position of the activated elements is the conversion output corresponding to the serial input  [1] . The elements involved in the conversion are still activated at the time of output. This operation is a generation of (learned) time series data. that can be said a conversion of parallel to serial triggered by the first data. After the first data reception, the connected elements are activated as described above. That is, the operation with no reaction within waiting time is invalid and aborted. Both state transition diagram is shown in the existing  Figure 3 . For each basic subsequence a neural network that accepts and generates the basic subsequence is considered. This neural network is called a basic unit. It can be said as "stance to the event" that most animals have  [2] . The movement will be mentioned in the next chapter. Neuroscientist Damasio calls "image" the internal representation built in the nervous system by stimulation inside and outside the body. And argues that an evolved animal has at the heart of the nervous system an elaborate network, which is the brain  [3] . Animals in the early stages of evolution will spend most of their living time obtaining food and avoiding danger. However, recognition is limited to the area of the vicinity that the sensory organ catches. They exchange information with their peers through squeals and gestures to enhance their ability to survive as a species. In order to be able to exchange information, it is necessary to transmit and reproduce the act. The ability to imitate fellow's action is indispensable. Imitating is first step of learning the behavior of the fellows who is transmitting information. In addition, it spreads to the story beyond space and time. "Tomorrow I'd like to buy a birthday cake at Store A to celebrate you with everyone." "Let's ask store A to put a chocolate plate on the cake with your name on it." And you'll think of the shape of the cake and the action of lifting the chocolate plate and mouthing it. If there is a problem in the episodic memory, it causes difficulties in social activities. Two types of time series data can be considered in the above situation. Like talking about your childhood while eating cake. In order to avoid confusion, the episode must be corrected by reality. In neuroscience, the morphism corresponds to synapses and axons, and is responsible for the transmission of information between objects. Next, we introduce a function called functor between different categories. Therefore, the existence of the above functor would be considered naturally. The process of migration can be described mathematically using free functor or free construction functor. It is a short-term memory to be involved in the recognition and manipulation of the thing in the vicinity of the animal itself. Consistency is required when the objects in which both memories are involved are the same. Of course, the description does not convey all the features of the hippocampus. Each unit has the same function, but the connections between the units are not the same. The bud is also felt in the judgment and the movement in our daily life. From the viewpoint of neuroscience, even if there are no "parts" equivalent to the basic unit in the process of stimulation from the sensory organs propagating to the cerebral cortex, axons parallel to the propagation direction connect between the layers, and it can be said that it is a passage of serial parallel mutually converted information.
paper_507	In present study we used the already existing topographical maps, satellite imageries and field work. The main aim of present study is to integrate the result of our study with spatial data, soil parameters, land inventory and used the output as a user friendly application using GIS which could predict the future susceptibility of region to landslide and% contribution of each factor for the same. ArcGIS 9.3, ERDAS and Excel software have been used for zonation, and statistical analysis respectively. Efficiency of the application will be calculated by the help of previously acquired data of the study area at different places and then the reliability of the application will be judged. Uttarakhand is an appropriate choice for the study since the newly developed state has been fighting with the catastrophe and is making front paper headlines for landslide at Vishnuprayag, Baldora, Lambagharchatti, Jharkula, phatabyung, and Amiya landslides  [4] . These map divides the land into homogeneous areas or domain and their ranking according to degree of actual hazard potential caused by mass movement  (Guzzetti et. al, 1999 and varnes 1984)    [7, 8, 9] . The result obtained i.e. Identification of factors which affects to the landslide. Determination of the extent to which the various factors contribute to landslide. Uttarkashi falls under the physiographic division (s) Rohilkhand plains, Nepal Himalayas, Ganga, Yamuna daob, Siwalik range, Kumaun Himalaya, Dhaoladhar range. There are 793 villages with area drained by major river (s), Yamuna, Ganga. Annual average rainfall observed is 1750.50mm and mean temperature 16¬∞C. In the present paper landslide hazard zonation map has been prepared for the Rishikesh-Uttarkashi-Gaumukh-Gangotri. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area (Mohammad Onagh, 2012). Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. were generated using ERDAS and ARCGIS v. 9.3. DEM (Digital elevation model) was obtained from BHUVAN. The factors in relevance to the landslide susceptibility analysis of Uttarkashi are: Digitization for the various shape files has then being done by retrieving the concerned shape file and the map form which digitization has to be done. The shape files created using the raw data are: The ANN is a black box model is a multi-layered neural network, which consists of an input layer, hidden layers, and an output layer. A neural network consists of a number of interconnected nodes. The dataset is categorized into 60% training and 40% validation. The data is categorized into training, testing and validation, and all the six factors namely soil depth, soil type, rock and a excel database is created. The dataset is categorized into 60% training and 40% validation. In these points we categorized them into two categories landslide prone and non-landslide prone. For this three-layered feed-forward network was implemented using the MATLAB software package. Here, "feed-forward" denotes that the interconnections between the layers propagate forward to the next layer. For a new dataset the weights are unknown. The number of epochs was set to 3,000. Weightage of different factors are shown in table 1. The results are compiled below. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area  [13] . Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. were generated using ERDAS and ARCGIS v. 9.3. An artificial neural network technique was used. The dataset is categorized into 60% training and 40% validation. The region that is prone to landslide has been depicted by 1 and the region that is not prone to landslide has been depictyed by 0. The result of present study with spatial data, soil parameters, land inventory and presented by a landslide hazard zonation map and a user friendly application using GIS that could predict the future susceptibility of region to landslide and percentage contribution of each factor for the same.
