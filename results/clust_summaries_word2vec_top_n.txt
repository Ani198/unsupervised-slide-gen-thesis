paper_1	Through Social media, people are able to write short messages on their walls to express their sentiments using various social media like Twitter and Facebook. Through these messages also called status updates, they share and discuss things like news, jokes, business issues and what they go through on a daily basis. These numerous amounts of information can be extracted, processed and properly utilized in areas like marketing and electronic learning. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. Clustering uses unsupervised learning technique in finding similarities between data according to the characteristics found in the data and grouping similar data objects into clusters. It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . Typically development starts with the external features and user interface, and then adds features as prototypes are developed. The first task was to retrieve details of each of the students from their twitter accounts using an extension script which is part of the twitter Application Programming Interface. The second task involved identifying the right kind of data to use for training the expected prototype as well as testing it. The third step involved using the data already preprocessed above to train the prototype. The machine learning method used was unsupervised learning in which the system was given the data so that it automatically analyzes and creates clusters from the data  [3] . They were used to confirm that the system indeed accurately did the classification given some data items. This involved picking the details of a new student from twitter and trying to predict the class hence group that he should join. The results reported in this paper were obtained from a series of evaluations that were done on the classifier on different parameters including functionality, usability, accuracy, precision and recall. The first task was to determine if the prototype achieved its overall goal which is grouping students through social media for electronic learning. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . They also confirmed that the system simplified the process of group formation and made inclusivity of distant students in the groups possible. A good number of learners indicated that they would continually use the system for the purposes of group formation and discussion. These measures can be improved if large amounts of data are used to train the classifier before being used to do actual classification. Through social media a better and more efficient way can be used to enable online learning generally and group formation specifically. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. It was able to extract tweets from various social media accounts based on a given hash tag (task) and then pass them to a Naïve Bayes classifier as input. The classifier was also able to assign other or new users groups also according to their tweets and the learning that the system had undergone. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_2	In addition to this, the current traffic light systems encourage extortion by corrupt traffic officials as commuters often violate traffic rules because of the insufficient time allocated to their lanes or may want to avoid a long waiting period for their lanes to come up. This research is aimed at tackling the afore-mentioned problems by adopting a density based traffic control approach using Jakpa Junction, one of the busiest junctions in Delta State, Nigeria as a case study. The signal timing changes automatically based on the traffic density at the junction, thereby, avoiding unnecessary waiting time at the junction. The sensors used in this project were infra-red (IR) sensors and photodiodes which were placed in a Line of Sight configuration across the loads to detect the density of the traffic signal. These include loss of man-hours, accident, missed opportunities, noise pollution, air-pollution, increased fuel consumption, increased tendency to violate traffic rules, and in some cases extortion by corrupt traffic control officials. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. The proposed system would use a microcontroller of PIC family duly interfaced with sensors, to change the junction timing automatically to accommodate movement of vehicles, thereby, avoiding unnecessary waiting time at the junction. A traffic control model using signaling system in a discrete cross-road with NE-555 timer circuit was implemented by  [3] . The design had a provision for pedestrians to request for crossing the road as and when required by pressing a switch. This model of traffic signaling system is now being implemented across several metro and second tier cities of India. Most of the crossings handle the automated traffic signaling using fixed duration intervals between the Red, Yellow, Green and Pedestrian Pass Signal. The uniqueness of this model lies in the implementation of on-demand Pedestrian-Pass signaling, thereby transforming the design into dynamic controller. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. In a bid to overcome this challenge  [4]  adopted an approach whereby a camera is placed on the top of the signal to get a clear view of traffic on the particular side of the signal so that it will capture the image. After calculating the number of vehicles we will came to know in which side the density is high based on which signals will be allotted for a particular side. A major drawback of this system is that it may not provide a reliable count of the vehicles upon which density is based. However, this system has the disadvantage of the controller since it depends on the preset quantification values for fuzzy variables  [6]  conducted a cross sectional study targeting traffic control in the city of Nairobi's Central Business District and its surroundings. This approach involves breaking down a system into smaller units to enable the designer get more insight into the system. Since the peak inverse voltage of the diodes has to be greater than the peak secondary voltage of the transformer, the 1N4007 silicon diode with peak inverse voltage (PIV) of 1000 Volts was used in the circuit. A capacitor is needed to effectively carry out the filtering of the rectified AC signal to eliminate ripples  [9]  and can thus be calculated using the equation below. Another purpose of an optocoupler is to prevent rapidly changing voltages or high voltages on one side of a circuit from distorting transmissions or damaging components on the other side of the circuit. This is rectified by the bridge rectifier, filtered by the capacitors to remove ripples and regulated by the voltage regulators to produce fixed value of 5 volts which is supplied to the system. IR sensors are placed on the intersections on the road at fixed distances from the signal placed in the junction. According to the IR count, microcontroller takes appropriate decisions as to which road is to be given the highest priority and the longest time delay for the corresponding traffic light. It is a widely used type of electronic prototyping board characterized by a 0.1-inch rectangular grid holes with parallel strips of copper cladding running in one direction all the way across one side of the board. The leads were then soldered to the copper tracks on the other side of the board to make the desired connections. From the test carried out on the circuit, it was observed that the LEDs with the same color have equal timing, and that each pole of the four traffic light controlling poles, switches sequentially and repetitively until the circuit is disconnected from power. Jakpa Junction, in Effurun, Delta State, Nigeria is increasingly becoming chaotic by reason of the recurring traffic gridlock it experiences. The sensors help in keeping count of vehicles entering roads and the microcontroller subsequently allots time delay thereby giving accurate priority to each road. This new design would further reduce time delay, improve efficiency and reduce accidents by incorporating the following modifications: (1) The Airport -PTI road should have a two -way traffic light to pass the vehicles coming from Airport road to PTI road.
paper_3	The aim of this article is to present the general architecture trends of Web-based Adaptive Educational Hypermedia Systems (AEHSs) and to give a complete description of architecture of the AEHS MATHEMA. Also, the formative evaluation of AEHS MATHEMA by students of the Department of Informatics and Telecommunications of the University of Athens, Greece, has shown, with the exception of other things, that all its functions are useful and easy to use. They build a personalized model of the learner and apply this model to adapt the content and/or the appearance of the hypermedia, according to the learner's specific characteristics, such as the cognitive goal, level of knowledge, pre-existing or prior knowledge, interests, preferences, stereotypes, cognitive preferences and cognitive or learning style  [4] . (c) Adaptive Navigation Support: It adapts the link structure in such a way that the learner is guided towards interesting and relevant information, kept away from nonrelevant information either by suggesting the most relevant links to follow or by providing adaptive comments to visible links. (d) Meta-adaptive Navigation Support: It selects or suggests the most appropriate adaptive navigation technique that suits the given learner best relatively to the given context, either by observing and evaluating the success of each technique in different contexts and the resulting learning from these observations, or by assisting the learner in selecting the navigation technique that best suits to him or her. (f) Intelligent Analysis of Learner's Solutions: It uses intelligent analysers that not only tell the learner whether the solution is correct but also it tells him/her what exactly is wrong or incomplete. (g) Example-based Problem Solving Support: It helps the learners in solving new problems, not by articulating their errors, but by suggesting them relevant successful problem solving cases, chosen from their earlier experience. (h) Adaptive Collaboration Support; Adaptive Group Formation and/or Peer Help: These techniques support the collaboration process either just like the interactive problem solving support systems assist an individual learner in solving a problem, or they use knowledge about possible collaborating peers in order to form a matching group relatively to the kind of the collaborative task. AEHS MATHEMA (Meta-Adaptation Technology Hypermedia for Electro-Magnetism Approach) combines the constructivist, socio-cultural and meta-cognitive teaching model and supports personalized and collaborative learning. The general purpose of the AEHS MATHEMA is to support learners through an interactive and constructivist educational environment in order to construct their knowledge in Physics and Technology, individually and/or collaboratively, and to overcome their misconceptions and/or learning difficulties AEHS MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and meta-adaptive navigation, interactive problem solving, and adaptive group formation. More specifically it supports: (1) adaptive presentation according to learner's learning style; (2) four adaptive navigation techniques which are direct guidance (uses a "next" button), link hiding (hide nonrelevant links), link annotation (use colors to indicate relevance), and link sorting (like in search engines); (3) a meta-adaptive navigation technique in order for the learner selects the more appropriate navigation technique that best suits him or her; (4) an interactive problem-solving method through personalized and collaborative problem-solving activities that exploits the teaching approaches of experimentation through simulations, guided inquiry and discovery, investigations and peer collaboration; and (5) a method of adaptive group formation for peer matching. Most of the Internet based systems use a variation of conditional reasoning where a decision is made to organize the text differently from fragments or select from a group of whole pages or even groups of pages. The link structure of a hyper-document can also be modified by color-coding, or sorting according to specific criteria based on student preferences or abilities. engine performs the actual adaptation by adapting or dynamically generating the content of nodes and the destination and class of links in order to guide each individual user differently. Today almost all AEHSs follow the general architecture of the AHAM reference model (but possibly with a different kind of rule or reasoning language and engine). ADAPT  [7]  is an European Community funded project that aims to rectify the situation described in the introduction, by investigating current adaptive practices in various AEH environments and identifying the design patterns within them. The ADAPT project has identified high level design dimensions for AEHSs, which are: (1) context of use (CU), (2) content domain (DM), (3) instructional strategy (IS), (4) instructional view (IV), (5) learner model (LM), (6) adaptation model (AM), and (7) detection mechanism (DE). Proper's architecture is a combined architecture of SCORM LMS and AEHS. As shown in simplified form, in the  Figure 2 , they adopt the typical SCORM Run Time Environment (RTE) structure adding an adaptation module and extending the preexistent Domain and User Models. Thus the prototype involves four main modules: (1) The Domain Model (DM) that represents the domain knowledge of the system. 3 The Adaptation Module (AM) which interacts with DM and UM in order to provide adaptive navigation to the course, and (4) The RTE Sequencer that is triggered by course navigating controls and interacts with DM and delivers the appropriate educational content to the learner. Thus its architecture is a typical of a SCORM compliant LMS. System retrieve course files initially from a zip file, which contains a manifest xml file and all the html and media required files. WELSA does not store the course Web pages but instead generates them on the fly, following the structure indicated in the XML course and chapter files. This dynamic adaptation mechanism reduces the work-load of authors, who only need to annotate their LOs with standard metadata and do not need to be pedagogical experts (neither for associating LOs with learning styles, nor for devising adaptation strategies). The only condition for LOs is to be as independent from each other as possible, without crossreferences and transition phrases, to insure that the adaptation component can safely apply reordering techniques. The Adaptation Decision Model is responsible for deciding what the system should do in terms of presentation and navigation adaptation on the basis of the conclusions draw by the Interaction Analyzer, parameters from the Learner Model and information from the Application Model. As JavaBeans are components of an application in the context of JavaServer Pages or servlets, they are suitable to implement the Presentation Generator and the Adaptation Decision Model. Additionally, the JavaBeans offer advantages of separating the programming logics from presentation, as for the Presentation Generator and the final page composition. The system then enables to present and recommends a variety of learning contents adaptively towards each of the student's learning style identified in the student model through the adaptation model. The system then analyzes the learning content on each of the learning material, and then comes up with the generated teaching strategies by means of the teaching strategy generator and fragment sorting. The course player in ULUL-ILM enables the system to adaptively presents the content with various teaching strategies towards each of student's learning style. MySQL is the database of choice since there is extensive experience on Campus and provides the ability to implement adaptation rules. These servlets are complete programs that are capable of creating JSPs. A servlet allows a programmer to utilize whatever functions a programmer needs including conditional branching and loops. It is light weight, which means that it does not require a large amount of processing power on creation of the servlet instance, and it accesses databases using JDBC which offers a secure way of accessing many wellestablished database brands. Servlets are capable of eliciting student responses on more than one question and analyzing them to find out the strengths and weaknesses of that student to direct them towards remediation. In the context of JSP pages, JavaBeans components contain business logic that returns data to a script on a JSP page, which in turn formats the data returned from the JavaBeans component for display by the browser. The didactic design of the AEHS MATHEMA supports learners in constructing knowledge, choosing and achieving their learning goals, recognizing what they have already learned and what they are capable to do, and judging personal progress of their learning. It also offers appropriate teaching strategies to match students' learning styles, develop critical thinking and self-regulation, as well as collaborative activities, encouraging them to actively participate. Moreover, offers facilities in recognizing learners' misconceptions and assisting them in self-correction through reflection, and providing multiple representations of learning. The domain knowledge is structured in a way that supports the ability of the system to choose the educational material, depending on the learner's requirements and current status. In the AEHS MATHEMA, the hierarchical representation of the domain model is adopted by the ELM-ART  [5] , as follows: The first level defines the cognitive objectives, the second the basic concepts and the third the pages with the corresponding educational material. The characteristics that the system maintains in learner model for each learner are: name, gender, username, password, learning style, level of knowledge for each cognitive goal (general performance), level of knowledge for each basic meaning of the cognitive goal (quantitative and qualitative characterization), concepts that have been successfully evaluated for each cognitive goal, current page, current teaching method, current navigation technique, navigation history, meta-adaptive navigation status, initial Web experience and knowledge level (pre-existing knowledge) in the current cognitive goal, ways of presenting feedback messages, preference for assistance by Advisor in terms of its navigation, course sequence, and phases of the guided dialogues in interactive problem solving process in which each learner participates. The learner during the study selects the cognitive goal he or she wishes to study, following his or her personal course in the educational content, exploits the system's assistance, navigating and studying, is informed about the information that the system maintains, who can change them by intervening and directing the system's adaptation in this way, in terms of curriculum sequencing, adaptive presentation and navigation and adaptive group formation, selects his or her learning style, selects the navigation technique, activates and deactivates the curriculum sequencing, selects the way that the feedback messages are presented, changes his or her knowledge level for each basic concept of the cognitive goal, and activates-deactivates the Advisor. The adaptation engine decides on adaptive presentation and navigation in relation to Student, Domain and Didactic models. The system, during the learner's study, monitors his or her interactions with the system and his or her assessment and accordingly updates the links to the course material provided and/or adapts learning material to learner according to his or her learning style. The adaptive navigation techniques that it supports are: direct guidance, link hiding, link annotation, and link sorting. It is the second level meta-adaptation engine that is responsible for monitoring the cognitive improvement of the learner in the cognitive goal that he or she studies, after n successful evaluations of his or her knowledge in basic concepts, showing him or her advantages and disadvantages of the four navigation techniques that it supports. In AEHS MATHEMA, meta-adaptive navigation works as follows: The first time that the learner enters the system, he or she is asked to state his or her Web experience and level of knowledge in the cognitive goal he chose to study (preexisting knowledge). Following the learner's statement, the system suggests the most appropriate navigation technique to him or her, taking into account the level of his or her Web experience and his or her level of knowledge in the subject he or she chose to study as follows: (1) Direct guidance: Little or no Web experience and little or no knowledge of the cognitive goal. The meta-adaptation engine proposes to the learner to change the navigation technique if he or she wishes after n successful assessments of the basic concepts of a cognitive goal he or she studied, showing him or her the pros and cons and additional information on each of four navigation techniques, and the learner decides whether to change it or not. This module is responsible for transferring the training material to the presentation unit, for giving feedback, for creating guided dialogues and for evaluating the learner knowledge. It includes the content of the JSP pages, Java servlets responsible for feedback and creation of guided dialogs in problem-solving, Java servlets for creating evaluation questionnaires, and so on. It is responsible for what will be presented to the learner as a result of the processing of the information that arrives from the adaptation engine, the meta-adaptation engine, adaptive group formation module, and application module. In this module there is the basic servlet, which deals with the presentation of the training material and other JSP pages and servlets, such as the page and the corresponding servlet to inform the learner about the characteristics of all learning styles, the servlet for the curriculum sequencing, the servlet for selecting and presenting the concept that the student selects to study according to his or her learning style, etc. More specifically it: (1) initializes and updates the Student Model; (2) collects and stores the data that the learner answers to open-ended questions, entries in tables, registers in notes, etc; (3) monitors the interactions between the learner and the system to get information about his or her choices in terms of the questionnaires, the phases of guided dialogs, the links, the options for meta-adaptive navigation, the visited pages, and more. MATHEMA supports the learner to find the most suitable peer for the formation of a collaboration team. Following a study  [13]  conducted on the formation adaptive groups in the MATHEMA, it was decided that: when the system creates a priority list of candidate peers for an interested learner, in the first and second position of the priority list, the algorithm will place the candidate collaborators with a concrete or abstract style and with the same active or reflective dimension of their learning style as follows: If the learner has a concrete learning style, then the algorithm will place the candidate collaborators with a concrete learning style in the first position, and in the second position, the candidate collaborators with an abstract learning style. After creating the list, the system informs the learner that his or her most important candidate collaborator is at the top of the list, while the less important is at the end of the list. The collaboration protocol that uses the synchronous communication tool is as follows: (1) The learner declares willingness to collaborate either by selecting his or her partner from the priority list of candidate collaborators or by declaring a desire for collaboration so that others who would like to work with him or her can choose it, while activating the synchronous communication tool. (2) Learners negotiate for collaboration and, if they come in agreement with each other, state it in the system to not include them in the priority list of other learners who would like to work with them. The  Figure 11  shows a snapshot of a dialog between the learners Giannis and Mary taking place via the Synchronous Communication Tool (chat-tool). Regarding adaptive and intelligent techniques of the AEHS MATHEMA mentioned above, the students having evaluated them consider that the adaptive presentation of different teaching strategies is quite useful (93.0 percent) and fairly easy to use (86.0 percent), the different ways of navigating are quite useful (100.0 percent) and handy (90.7 percent), the meta-adaptive navigation is quite useful (88.4 percent) and fairly easy to use (86.0 percent), the adaptive group formation of collaborative teams to assist them in collaborative problem solving is useful (83.7 percent), but less user-friendly (58.1 percent). MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and metaadaptive navigation, interactive problem solving, and adaptive group formation. The main contributions of the architecture of the MATHEMA in the improvement of functionality of AEHSs are the meta-adaptation technique, the adaptive group formation technique, the problem-solving technique, and the synchronous communication protocol that it supports. The evaluation of the AEHS MATHEMA was encouraging and rewarding since the evaluators considered that almost all system functions are quite useful and easy to use. Taking into account the observations and recommendations of the evaluators, some system functions, such as this offered by the asynchronous communication tool, have been improved.
paper_21	In this paper, we present the powerful scheme ZSISMP (Zimmermann Self Invertible Stabilizer Multiplier Permutation) to attack the hardness of the minimum distance search problem of BCH codes. This scheme consists in evaluating the minimum distance of the reduced dimension sub code fixed by a Self Invertible Stabilizer Multiplier Permutation by Zimmermann algorithm. The proposed scheme ZSISMP is validated on all BCH codes of known minimum distance. The use of this efficient local search had yield to determine the error correcting capability of many BCH codes of length 1023 and 4095. In  [10] , Augot and Sendrier found idempotent codewords of minimum weight for several primitive narrow-sense BCH codes. The artificial intelligence Simulated Annealing presented in  [16] , Tabu Search  [17] , Hill-Climbing  [18] , Genetic Algorithm  [18] [19] , Ant Colony Optimization  [20] , Metropolis Algorithm  [21] , was shown to be useful to attack the difficulty of the minimum distance search problem for BCH Codes. This improvement has yield to a fast convergence of the Simulated Annealing by reducing the number of iterations, as well as obtaining good results in comparison with the previous works presented in  [17-18-19-21] . Unlike classical techniques based on exhaustive or partial enumeration of codewords, Berrou in  [23]  has presented an efficient approach based on the notion of Error Impulse response of a Soft-In decoder. This approach consists in adding to all-zero codeword a level of noise and considering the minimum distance as the smallest level of noise from which the Soft-In decoder fails in correction. In  [24] , the authors proposed an efficient method, by applying the MIM method on some sub code randomly extracted from the considerer BCH code. The proposed method MIM-RSC, has allowed an efficient local search and therefore finding the true minimum distance of some BCH codes of length 1023 and 2047 as well as obtaining good results in comparison with the previous works presented in  [17-18-19-20-21-22 ]. From these stabilizers, we take only a Self Invertible stabilizer if it exist and by using a mathematical tool, we find the sub code fixed by this involution and then we evaluate the minimum distance by using the famous Zimmermann algorithm. This section presents the proposed scheme for finding the lowest weight in BCH codes. The proposed scheme works as follows: Inputs: formula_0 Step 1: Find the sub code SC fixed by σ Step 2: Find the estimated minimum distance d of SC by using the Zimmermann algorithm. Output: -d as estimated minimum distance of BCH (n, k, δ) This section presents a validation of the proposed method on BCH codes of known minimum distance and its application for finding the minimum distance of BCH codes of unknown minimum distance. This section presents also a comparison between the proposed scheme and previous work on minimum distance for BCH codes. All results have been done using a simple configuration machine: Intel (R) Core (TM) i3-4005U CPU @1.70GHz RAM 4GO and are made by running the considered algorithm in 1day for each code. It is well known that the smallest primitive narrow-sense BCH code whose minimum distance is greater than its designed distance is BCH (127, 43, 29) and it is the only one for this length. It is known also, that All the narrow-sense primitive binary BCH codes of length 255 have their minimum distance equal to their designed distance except BCH (255, 63, 61), and BCH (255, 71, 59). In order to find the minimum distance of some large BCH codes, the proposed scheme is applied by using a simple machine of the configuration given above.
paper_31	With the continuous improvement of human living conditions and the aggravation of global aging level, chronic diseases such as hypertension and diabetes continue to plague human health. This paper takes chronic diseases as the research object, and proposes a design of intelligent medical system for chronic diseases based on semantic matching by the adaptation of ZigBee technology in the front-end data acquisition. Through the ZigBee wireless sensor network, this system sends the physiological parameters collected by various medical sensors to the intelligent medical system, and innovatively proposes semantic matching algorithm to solve the queuing problem of data transmission, to ensure the accuracy of data transmission. It will cause many problems: firstly, it is unable to check the patient's continuous vital signs data; secondly, it is impossible for patients to get their own condition on time; thirdly, it is liable to doctors' misjudgment. In-depth interviews with 91 employees in different positions from 11 grassroots medical and health institutions since July 2019 show that the efficiency of mobile medical care in the surveyed areas has appeared  [2] . With the means of the Internet, with the carrier of the mobile terminal, intelligent medical treatment founded on the units of family is a new trend. Therefore, it is of great significance to develop an intelligent data analysis platform to record, store, and share and handle various personal health signs in time through wireless transmission. Due to the late development of information technology in China, our Wise Information Technology of med (WITMED) starts later than foreign countries. For example, the General Hospital of the People's Liberation Army (PLAGH) communicated and discussed some diseases with foreign experts through satellites. The third Military Medical University studied on the home digital medical monitoring project, which designs a network system to monitor human physiological parameters using mobile terminals. For example, some domestic hospitals use Bluetooth technology to monitor the ward room. And some scholars combined Bluetooth technology with GSM short message to realize the collection and remote monitoring of physiological parameters such as blood pressure  [12]  and pulse. Chronic diseases can easily cause damage to patients' vital organs such as the brain, heart, and kidneys  [3]  to seriously affects working ability and quality of life, and its high medical expenses increases the burden of economy on society and families. With the emergence of various new telecommunication technology, scholars kept providing health care using all sorts of communication technology. Traditional phone calls were adopted as the main way of communication between patients and doctors. At this stage, telemedicine has gradually transitioned to WITMED, and developed from precision medicine to disease prevention  [5] . Detecting physiological signals in the daily life of the population can effectively grasp the health status of the people in the area, and provide an important reference for the diagnosis and prevention of chronic diseases  [10] . In this telemedicine stage, it gradually focused more at community, families and individualized care  [11] . There are some defects of the current WITMED (Wise Information Technology of med) system as follows: (1) The inadequate reliability of data transmission. Chronic diseases patients have to be long-term monitored, however, whether using a WIFI or GPRS, high power consumption will be arouse. At present, ad-hoc network cannot work no matter using GPRS or WIFI devices, and only one kind of vital-sign data can be transmitted at a time. The current WITMED system focuses only on data acquisition instead of processing uploaded data and returning the result. The growing trend of WITMED system shows that a variety of health signs data will be monitored, transmitted, processed and analyzed in real time, the system can process the uploaded data. As the speedy development of the escalation of people's living standard, China has gradually entered the aging society with an increasing number of patients with chronic diseases and a descent trend of age  [14] . Since 2014, the central and local governments have formulated a series of policies around intelligent health care, pointing out the direction for the development and construction of it. Attracted by the broad prospect of smart medicine, BAT and other Internet enterprises have participated in the smart medical industry. Alibaba founded Alibaba health  [15] ; Tencent launched the first AI+ medical product Tencent Meiying in 2017; many large enterprises integrated medical resources through mergers and acquisitions to lay out the intelligent medical industry chain. The system architecture diagram is as follow:  Fast and correct data acquisition is the basis of the platform's efficient operation. Data analysis roadmap is shown as follows:  After collecting patients' physiological information from the node, various physiological sensors will continuously upload them to the coordinator. Finally, the request is inserted into the message queuing sequence by the matching degree. The general algorithm of this part is as follows: Define priority function: f x, y ax by ε, with a basic definition as follows: x: semantic matching degree, 0≤x≤1 y: a request arriving at the serial position, 0<y≤1 a is defined as the weight of semantic matching degree, 0<a<1 b is defined as the weight of serial position of Request arrives, 0<b<1, a+b=1 ε is defined as the disturbance value, 0≤ε≤0.1, with the default as 0. Generally, a is close to µ= 1 2 , formula_0 Therefore, the process of queuing algorithm based on Semantic matching is as follows: With the hypothesis that a= 1 2 (to take one half of the sum) x= 5 10 (there are 10 participles in semantic information, of which 5 are matched), y 1 (Before a new message arrives, assumes that the request queue has 10 participles waiting, then: f x, y y a x y ε =0.294, therefore, the priority value of the message is 0.294. An improved SVM calculation method is as follows: Definition 1 (conception): Define a tuple F= (X, Y, Z), in which x is the collection of all objects is the object property set, Z (X*Y), Z is a subset of X*Y. The column mixed operation formula is shown as  Figure 6 : (4) key plus calculation Key plus calculation operates the 128 bit key is XOR bit by bit with the data in the state matrix. The process of key plus calculation can be regarded as the result of Bitwise XOR or byte level or bit level operation. The calculation is shown as  Figure 7 :   After logging in the system, enter the sign input interface and put on the sensor, the system will automatically input the relevant data collected, as shown in  Figure 8 : After inputting the data of relevant signs, the system will automatically generate diagnosis & treatment files and health assessment according to the relevant data. The method of backstage data giving warnings and putting forward corresponding disposal measures by big data processing as shown in  Figure 9  is better than clustering, and the choice of Space vector model is more accurate than such models as Zigebee Health Care Profile chosen in the references. In this study, a design scheme of intelligent medical system based on semantic matching is proposed, with the front-end data acquisition which adopts ZigBee technology, and a semantic matching algorithm is innovatively proposed to solve the queuing problem of data transmission to ensure the accuracy of data transmission. Through ZigBee wireless sensor network, the system sends the physical parameters collected by various medical sensors to the information control terminal, and the improved spatial vector model is employed to process the uploaded data in the process of data transmission by the intelligent data analysis platform, and the security of data transmission is ensured by adopting the AES encryption algorithm in the whole process. The intelligent system realizes the data collection of the whole process of outpatient service, examination and treatment, which can provide data support for the precise treatment of diseases.
paper_38	The study aimed to determine if any of the entry requirements such as Ordinary Level (OL) results, Unified Tertiary Matriculation Examination (UTME) scores or Post-UTME (PUTME) scores could predict an outstanding academic performance of first-year undergraduate students admitted into the Faculty of Science in the Kaduna State University, Kaduna. A purposive sample of nine hundred and forty-three (943) first-year students constituted the population for the study were drawn from Computer Science, Mathematics and Physics undergraduate degree programmes from the Faculty of Science of the university who were admitted from the 2010/2011 to 2014/2015 academic sessions. The results revealed that with a weak correlation, OL is a good predictor on the CGPA, a dependent variable, for academic performance which holds true for students who are in the CGPA category of '1st class' and '2nd Class Lower' respectively. It concluded that the use of OL and UTME as instruments is not enough to select candidates for admission and therefore recommended that other instruments such as senior secondary school mock examinations need to be included as part of the entry requirements in the admission criteria. Each student is expected to have at least five credit passes in not more than two sittings in Mathematics, English Language and three other science-based subjects such as Chemistry, Biology, Physics and Geography at the Ordinary Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria Levels (OL) of either Senior School Certificate Examination (SSCE) which is conducted by West Africa Examination Council (WAEC) and National Examination Council (NECO). Also, each student is expected to have obtained at least the minimum score required in the Unified Tertiary Matriculation Examination (UTME), conducted every year by the Joint Admissions Matriculation Board (JAMB) since 1979. In Nigerian universities, an academic performance frequently is defined in connection with semester examination performance. The CGPA score takes into consideration students' tests, assignments, practicals, examinations and sometimes lecture attendance. The focus of this study is to predict full-time undergraduate students' first-year Cumulative Grade Point Average (CGPA), which is one of the variables for measuring the academic performance by using entry requirements, such as Ordinary Levels and UTME, for Faculty of Science in Kaduna State University (KASU), Kaduna -Nigeria. Students' academic records show that after admissions, some students perform poorly even after going through a series of screening of their OL results, and writing of UTME and PUTME examinations before offering them admissions. This study aims to investigate the relationship between the entry requirements into any of the Faculty of Science undergraduate full-time degree programmes in the Kaduna State University and the students' academic performance at the end of the first year of study. Determine if any of the following entry qualification used for admission, OL results only, UTME scores only or Post-UTME scores (average of OL results and UTME scores) best predict the academic performance of students in the 100 level CGPA examinations; ii. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? Academic performance or sometimes known as an academic achievement is defined by  [10]  as "Knowledge attained or skill developed in the school subjects, usually designated by test scores or by marks assigned by teachers". A student's academic performance usually is measured in either examinations or continuous assessment tests, and this is expressed in various ways depending on what the scores should be used for. The numerous ways of reporting academic performance include raw scores, percentages, transformed scores, or even as categorical variables such as Excellent, Merit, Very Good, Pass, First Class, Distinction, A1, B2, C4, F9, and others. In Nigeria, students are admitted into universities using their scores in the UTME as well as Post-UTME (PUTME) subject to having at least five OL credit passes in relevant subjects obtained in not more than two examination sittings including the English Language. The underlying assumption made in such selection is that those admitted by satisfying the admission criteria will be successful in the successive academic activities attached to their studies. However, the following review of literature examined the relationships between UTME and PUTME scores as a predictor for the academic performance of students have revealed contradictions in their findings. Reference  [11] , in his study, monitored the performance of science education students admitted through Post UME screening in 2005/2006 academic session. The author's findings in his study showed that there was a consistent decline in the number of students admitted using the PUTME which cannot do better than UTME in influencing students' academic performance as the outstanding and weak students formed the upper 12.5% and lower 12.5% while the remaining 75% consists of the average students. The authors in  [8]  comparatively analysed the academic performance of graduates admitted through UTME and preliminary programmes (Certificate, Basic Studies and School of Science Laboratory Technology [SSLT]) in the University of Port Harcourt. Their results showed that in all the faculties with the exemption of Agricultural Science and Engineering, the graduates admitted through the preliminary programmes performed significantly better than their counterparts admitted through the UTME. Graduates with the best academic performance from the preliminary programmes were those admitted through the certificate programme. Reference  [1]  used the Pearson Product Moment Correlation Coefficient (PPMCC) to predict the academic performance of first-year students in four departments in the University of Abuja from 2008/2009 to 2010/2011 academic sessions using UTME, PUTME and CGPA. Partial Correlations Coefficient (PCC) was used in addition to PPMCC in  [12]  to predict the student's final grade in from a sample population of 306 students of Faculty of Health Sciences and Technology at the Enugu Campus of the University of Nigeria that had their final results ready and approved by Senate at the end of 2012. The author's study found that the use UTME score was a very poor predictor of students' final grades and thereby recommended that less emphasis should be placed on UTME scores as a criterion for admission of candidates into universities. The study by  [13]  found a significant relationship between students' scores in three examinations, namely: UTME, PUTME and 100-Level Psychology course, Faculty of Agriculture, Federal University of Agriculture, Makurdi, and thus concluded that the UTME has predictive validity for performance in the university. In the same vein,  [14]  tested the predictive power of the Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria JAMB UTME in predicting students' performance in the university's semester examination by using a regression model. Their results suggested that the JAMB UTME had positive but low indices of predictive validity, which varied across the academic sessions from 2005/2006 to 2013/2014 and all programmes of study except for four departments. In contrast to the studies from the earlier mentioned authors,  [15]  investigated the relationship between 276 students' performance in the entrance examination and their performance in Mathematics in two selected Colleges of Education (CoE) in Osun and Oyo states each. The data obtained were semester results in Mathematics during 2010/2011 to 2012/2013 sessions and their grades in Mathematics from any of the public entry examinations known in Nigeria such as UTME, WAEC, NECO, and National Business and Technical Examinations Board (NABTEB). The author concluded that the entry qualification or the entrance examination performance could not individually predict Mathematics performance at the CoE. A population of 1650 students admitted into the university during the 2011/2012 academic session from Faculties of Arts, Education, Science and Social and Management Science was used to obtain their UTME, PUTME scores along with their GPA for eight semesters. For instance,  [12]    This study aimed to investigate which of the University's entry requirements used for the admission process best predicts the academic performance of students in the 100 level CGPA examinations. The Predictive Correlational Ex-Post Facto design was identified to be the most appropriate for the study since the results (CGPA, UTME and OL) of students in the Faculty of Science were used in reaching conclusions about the whole prediction of academic performance. The population of the study was limited to consist of all students admitted into three Faculty of Science undergraduate degree programmes of Kaduna State University for five academic sessions from 2010/2011 to 2014/2015 using the OL, UTME and CGPA results. Out of the total sample of 3,255 students admitted between 2010/2011 to 2014/2015 academic sessions, the programme with the highest sample size is Microbiology with 491 (15.1%), followed by Biological Sciences (13.9%) whereas the programme with the least sample size is Industrial Chemistry (5.7%). The instrument used to derive the data for this study were: the JAMB UTME scores from 2010 to 2014 and OL grades in the five relevant subjects which were the pre-admission criteria and first-year CGPA results obtained from the semester examinations from 2010/2011 to 2014/2015 academic sessions. The data used in the study are the OL grades, and JAMB UTME scores obtained from the University's central database, and the academic standing of first-year CGPA results collected from the various departmental examination officers (DEOs) with the approval from the Dean of the Faculty of Science. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? In  Table 6 , the likelihood ratio Chi-Square of 18.723, 17.661 and 12.401 for Computer Science, Mathematics and Physics programmes with a significant value of 0.227, 0.281 and 0.414 tells us that the model as a whole does not predict the dependent variable, i.e., CGPA. A Multinomial Logistic Regression (MLR) was performed to model the relationship between the predictors and membership in the six groups, i.e., CGPA categories (Fail, Pass, 3 rd Class, 2 nd Class Lower, 2 nd Class Upper and 1 st Class) for each degree programme. These showed that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. For Mathematics and Physics students the relative strength of OL, UTME and PUTME on CGPA performance are statistically insignificant except for the slope (B) of OL in the CGPA category of '1 st Class' for Mathematics students, which statistically significant. What is the relationship between OL results, UTME scores and Post-UTME scores of students and their firstyear CGPA across the academic session, 2010/2011 to 2014/2015? Table 8  shows the summary of correlations coefficient between OL-CGPA, UTME-CGPA and PUTME-CGPA aimed at all the degree programmes for the academic sessions ranging from 2010/2011 to 2014/2015. How well do OL results, UTME scores and Post-UTME Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria scores of students predict their first-year CGPA across the academic session, 2010/2011 to 2014/2015? For all the academic sessions as shown on the results, the first equation intercept is the log of the ratio of the possibility of a student having a 'Pass' degree to the possibility of that student having a 'fail' degree. These show that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. The relative strength of OL, UTME and PUTME on CGPA performance of students admitted in the 2010/2011 session is not statistically significant except for the slope (B) of OL in the CGPA category of '2 nd Class Upper', which is statistically significant. As for the students admitted in the other sessions, 2011/2012 to 2014/2015, the relative strength of OL, UTME and PUTME on CGPA performance are statistically insignificant except for the slope (B) of UTME in the CGPA category of '2 nd Class Upper' and '1 st Class' for those admitted in the 2012/2013 academic session, which is statistically significant. The primary purpose of this study is to investigate if OL results, UTME and PUTME scores do predict the academic performance among first-year undergraduate students in the Faculty of Science. Based on the analysis and results using MLR and PPMC for each programme and each academic session, it is evident that OL, UTME or PUTME could not individually significantly predict the academic performance of students in Faculty of Science. However, by combining all the criterion variables, that is OL, UTME, and PUTME, as one variable and performing the PPMC and MLR, findings show that OL is a good predictor on the dependent variable for academic performance with a weak correlation of 0.068 which is statistically significant at 0.04 level. Although OL and UTME are still necessary as instruments for admission, it is recommended that the University be advised to include some other instruments such as senior secondary school mock examinations results for selecting candidates into any of the undergraduate degree programmes in the Faculty of Science. Also, there is a need to do a further study by including some more variables, such as age and senior secondary school mock examinations results, as criteria to significantly predict the academic performance of students successfully. The authors in  [21] , in their study also recommended the need of potential researchers to compare the OL, UTME and Post UTME terms and scores across Nigerian Universities for standardisation and a model for educational development in the twenty first century.
paper_57	This problem has been addressed by Genetic operations (GO) incorporated into ACO framework. Crossover and mutation operations have been adapted for use with ant generated strings which still have to provide feasible solutions. [2]  as Ant system (AS) algorithm. During the search process each ant sets off from ant colony (start position) and moves to search food (destination). As ants are passing the terrain (graph) they mark used routes (arcs of the graph) by chemical substance called pheromone. On their way back they use the same way from which abundant loops has been removed, but the amount of pheromone (1) ∆τ k ij (t) they produced is inversely proportional to the tour length L k (t). formula_0 formula_1 T k (t) is the tour generated by ant k, Q is a constant and tuple (i, j) denotes beginning and termination node of an arc. At the beginning when no pheromone values are available heuristic values η ij takes dominance. Later the ant uses probability selection rule to choose the next arc according to formula_3 where p k ij (t) is probability the ant k chooses the arc (i, j) from the neighborhood N k i of node i except the node visited previously. The probability p ij (t) of choosing the particular arc (i, j) depends on pheromone τ ij  (t)  and the heuristic η ij values which are associated with the arc (4). formula_4 Symbols α and β are weight parameters and represents balance between ant's gathered knowledge and the user preferred area. Heuristic values η ij affect probability only at the beginning when pheromone values are low. Let's mention ant colony system (ACS) with pseudo-random proportional rule  [3]  in which random uniformly distributed variable q ∈ (0,1) is compared with a tunable parameter formula_5 else the probability selection rule (3) is applied; random selection applied to AS rank  [4]  where random selection rate r is the probability of random selection and represents a user parameter which adjusts the balance between exploration and exploitation; prevention of quick convergence (i) and stagnation avoidance (ii) mechanisms applied to AS  [5] . The stagnation avoidance mechanism (ii) is based on the comparison of a randomly generated quantity q ∈ (0,1) with probability p k ij (t) of selected arc. In ACO adaptation the first and the last node is excluded from mutation. For feasibility reason the replacement node n r (new gene) is such a node from the node n i neighborhood N i , to which an arc from n i predecessor n p to n i successor n s exists  (Fig. If more such nodes occur, random selection is applied. If no such node exists, another gene is randomly picked up from the list. In ACO algorithm crossover position is represented by a common node of parental strings except the first and the last node  (Fig. If more of such nodes exist, random selection is applied. In GA many selection (vi) mechanisms are available, like roulette-wheel selection, tournament selection, stochastic universal sampling or reward-based selection  [7] . Since optimization process is primary done by ants cooperative behavior, the selection process has purely random concept and genetic operations serve just for selection pressure decrease. The above described genetic operations have been applied to one of the best performing ACO algorithms of Kumar, Tiwari and Shankar (ACO KTS )  [5] . At the end of each cycle t, when all the ants finish their tours T k (t), genetic operations are applied on the T k (t) strings which represents the list of nodes. Prior the genetic operations all loops are removed from the tours. If more candidates by which the selected node can be replaced occur, the new node is random chosen from the candidates. If mutation fails on all nodes of the tour, another tour is chosen. If crossover operation is not feasible, another second string is selected. If no tour has common node with the first selected tour, another first tour is selected and the random selection process is repeated. Since genetic operations may produce strings with loops, in ACO framework prior and immediately after each genetic operation a loop removal procedure is performed. After all genetic operations are executed fitness evaluation and pheromone update are scheduled. Feasibility of genetic operations depends on the graph and generated tours. For this purpose ACO GO algorithm has embedded user feedback which represents a ratio between accomplished and required genetic operations. The value for the number of cycles represents three macro cycles of ACO MC  [9]  for the same graph and parameters. The reference value of n [%] was received without any genetic operation and is 5.6 (Table 2, row 1). with no crossover pair certain amount of mutation operations should be spread out among more paths, but with 2 crossover pairs concentration of mutation operation on less paths tends to perform better. This behavior may be caused by the execution order of the GO: crossover is applied after mutation, thus crossover may re-distribute mutation substrings between more paths. Genetic operations where nearly always feasible; ratio accomplished / required mutation operations is 100% and for the crossover operation over 99%. Beyond 60% threshold ants foraging behavior is suppressed by crossover overload. As the crossover rate increases, ratio accomplished / required operation decreases  (Fig. This can be explained by the nature of the mutation operation which creates new paths whilst crossover operation can only combine already existing solutions.
paper_78	Classification is the process of dividing plots into clusters (communities) so that plots in the same cluster are as similar as possible, and plots in different clusters are as dissimilar as possible. It classified 100 plots into 12 clusters, representing 12 Glycyrrhiza uralensis communities, e.g. Glycyrrhiza uralensis + Stipa bungeana. It is distributed from 380 to 605 m in hills with slope 10 -20° in sunny and semi-sunny slope and chestnut soil. The community has a total cover of 75%, a shrub layer cover of 20% and an herb layer cover of 65%. The average cover of Glycyrrhiza uralensis in this community is 25% with a density of 3600 ha -1 . Glycyrrhiza uralensis + Polygonum bistorta. It is distributed from 300 to 500 m in hills with slope 20 -30° in sunny，semi-sunny and semi-shady slope and sandy chestnut soil. Its disturbance intensity is medium and heavy. The community has a total cover of 70%, a shrub layer cover of 5% and an herb layer cover of 70%. The average cover of Glycyrrhiza uralensis in this community is 35% with a density of 5700 ha -1 . Glycyrrhiza uralensis + Ephedra przewalskii + Cancrinia discoidea. It is distributed from 300 to 500 m in hills with slope 20 -30° in sunny， semi-sunny and semi-shady slope and sandy chestnut soil. Its disturbance intensity is medium and heavy. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 60%. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 59500 ha -1 . It is distributed from 280 to 500 m in hills with slope 15 -25° in sunny and semi-sunny slope and sandy soil. Its disturbance intensity is medium and heavy. The community has a total cover of 65%, a shrub layer cover of 10% and an herb layer cover of 60%. The average cover of Glycyrrhiza uralensis in this community is 37% with a density of 4900 ha -1 . It is distributed from 350 to 650 m in hills with slope 15 -35° in sunny and semi-sunny slope and sandy soil. The community has a total cover of 60%, a shrub layer cover of 5% and an herb layer cover of 58%. The average cover of Glycyrrhiza uralensis in this community is 33% with a density of 5500 ha -1 . Glycyrrhiza uralensis + Astragalinae triloa + Stipa sareptana. It is distributed from 400 to 700 m in hills with slope 10 -30° in sunny and semi-sunny slope and sandy soil. The community has a total cover of 75%, a shrub layer cover of 10% and an herb layer cover of 70%. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 6000 ha -1 . Glycyrrhiza uralensis + Aneurolepidium chinense +Stipa sareptana. It is distributed from 400 to 750 m in hills with slope 15 -35° in sunny and semi-sunny slope and sandy soil. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 65%. The average cover of Glycyrrhiza uralensis in this community is 29% with a density of 4100 ha -1 . Glycyrrhiza uralensis + Festuca logae + Stipa sareptana. It is distributed from 400 to 800 m in hills with slope 20 -35° in sunny and semi-sunny slope and sandy soil. The community has a total cover of 80%, a shrub layer cover of 10% and an herb layer cover of 75%. The common species are Artemisia parvula, Scorzonera divaricata, Roegneria kamoji, Potentilla bifurca, Carex duriuscula, and Ranunculus japonicas. The average cover of Glycyrrhiza uralensis in this community is 30% with a density of 4000 ha -1 .
paper_96	In the program, a new type of "warning system of obstacle avoidance of embedded electronic guide dog" has been developed on the basis of careful analysis of all kinds of present anti-collision warning systems, which has a core micro-controller, 32-bit ARM7 microprocessor, and takes the embedded operating system uCLinux as its platform. Such warning system of obstacle avoidance of embedded electronic guide dog can effectively eliminate the impact of the traffic environment and the subjective factors of the blind, warning in advance for the travelling blind in time, effectively avoiding obstacles such as vehicles, to reduce traffic accidents caused by the their blindness. In fact, this "warning system of embedded electronic guide dog avoiding obstacles" is similar to a car anti-collision avoidance system. The design of obstacle-avoiding early warning system of embedded electronic guide dog is an inheritance and innovation, based on the design of traffic information collecting system, which is generally used in vehicle anti-collision at present. [1]  In the design process of the obstacle-avoiding early warning system of embedded electronic guide dog, we firstly has carried on a comprehensive evaluation and analysis for each scheme, and then put forward a new idea that it introduces the open source embedded real-time operating system uCLinux, with a high-powered ARM core as the core processor. In this way, the design of obstacle-avoiding early warning system of embedded electronic guide dog has the following advantages. Also, it is easy to extend the functions, which is convenient for field operations, greatly reducing the cost of the traffic information collecting system in the obstacle-avoiding early warning system of electronic guide dog. What's more, the design of obstacle-avoiding early warning system of embedded electronic guide dog has utilized the core processor, based on the realization of the system function, and development conditions. Moreover, the obstacle-avoiding early warning system of embedded electronic guide dog also selects USB protocol to transfer data, saving the collected data in the hardware after being managed. The overall design plan of obstacle-avoiding early warning system of embedded electronic guide dog mainly is the velocity and distance measurement module, the power circuit module, the LED screen show module, USBcommunication circuit module, and clock circuit module. The hardware of the obstacle-avoiding early warning system of embedded electronic guide dog can reference the Figure 1:  Figure 1 : Schematic diagram of hardware system structure  [4]   In the anti-collision early warning system, referenced in the design of obstacle-avoiding early warning system of embedded electronic guide dog, there mainly exits four steps, that is, information collection, information processing, information judgment, and warning information. The selection of anti-collision warning system for the design of obstacle-avoiding early warning system of embedded electronic guide dog should start from the characteristics of the highway network and the street network construction in our country, combing with the characteristics of the obstacle-avoiding early warning system of electronic guide dog, as well as the construction of our country's highway and street traffic integrated management system. We should optimize the choice of the obstacle-avoiding early warning system technique of embedded electronic guide dog, based on China's national situation. Therefore, in the design process of the obstacle-avoiding early warning system of embedded electronic guide dog, after the comprehensive and comparative analysis of the anti-collision warning techniques, we have compared the optimization principles of the anti-collision warning techniques, and finally adopt the approach of millimeter wave radar sensor + ARM microprocessor. The design of obstacle-avoiding early warning system of embedded electronic guide dog also adopts the generally used embedded processor. The embedded system will further compile all of its procedure codes, including the operating system code, driver's code and application code, into a whole paragraph of executable code and in inserted them into the hardware. UCLinux aimed at the micro-control field, designing the Linux system, which is specially designed for the CPU without MMU (such as the S3C44B0X adopted in this project), and it has done a lot miniaturization work for the Linux kernel. In addition, the design of obstacle-avoiding early warning system of embedded electronic guide dog also makes a full use of USB. USB is a new kind of computer peripheral communication interface standard, which abandons the defects of the traditional computer series/parallel, with advantages of the reliability of data transmission, hot plug (plug and play), convenient extension, and low cost. The reference model is shown in  Figure 2 . of USB reference model  [4]   Today, with the rapid development of transport and vehicles, the frequency of traffic accidents has been being high, which not only greatly restricted the development of transport, but also made people travel in great danger. The warning system of embedded electronic guide dog avoiding obstacles studied in the project can effectively eliminate the influence of traffic environment and blind subjective factors, send the pre-trip alarm for the blind in time, effectively avoid obstacles such vehicles, to reduce traffic accidents caused due to the blindness.
paper_134	A set of 16 samples of olive oil, sunflower, canola and corn oil which mixed with different ratio of Authentication, were used for calibration and evaluation of developed system. Both of these areas work in model identification and data classification issues and will, in effect, be directly used in data mining, and both groups are active in identifying and using neural networks and decision trees  [2] . The purpose of the development of these methods is to estimate the quantitative and qualitative characteristics of the materials rapidly, non-destructively and reliably  [4] . Among the non-destructive methods, the dielectric method has advantages due to the relationship between the steady-state dielectric with some qualitative materials, the cost less than other methods, high speed and high efficiency in the construction of systems Different recognition of quality has attracted the attention of many researchers  [5] . Typically, the conductor plates of the capacitor are made of aluminum, zinc and silver, and among them, a dielectric can be placed in the air or other material. Based on the dielectric method, when a material is placed in the alternating electric field, the positive and negative charged particles in it will constantly tend to move in the electric field. The association of these properties with many quantitative and qualitative components of the product, high speed, low cost and high efficiency of its use as a new and non-destructive technology has been considered by many researchers to measure a wide range of qualitative and quantitative characteristics of agricultural products  [8] . The great benefit of mixing vegetable oils with lubricant makes it hard to find the type of product. The second pertain to data mining algorithms; third part related to samples and used methods in the article. They used a 4V sine voltage in the range of 10Hz to 1 MHz to determine the dielectric properties of a binary mixture of olive oil. In addition, PCA has been used to classify virgin oil samples, separately from fake oils. The results of the PLS calibration model showed good results for the adulterated oils with soybean oil as a counterfeit oil. Also, the charts sorted by the method showed clear performance for all oil samples and easily categorize them in different clusters. From the result of this study it can be seen that the dielectric spectrum can be used to detect fake oils with different types of oils with a percentage of their mixture below 5%  [12] . The relationship between the rates of fruit dielectric constant in a frequency of 1 kHz to 10 MHz was investigated. The results of the experiment showed that the dielectric constant is strongly affected by the size and volume of the fruit and also decreases with the increase in the fruit juice, which is clearer in a frequency of 1 MHz. They used 4-volt sin voltage with a frequency of 120 MHz and a rectangular parallel capacitor, which obtained capacitive capacitors (DCs) and voltages (DVs). This parameter had a good relationship with all the quality properties of eggs (elevation of the cell, height of albumin, etc.). Using this parameter and egg mass, they extracted regression models and reported that the application of this method to the egg production line and its grading based on quality properties requires more research  [14] . According to the researches that carried out in relation to the determination of the quality and content of agricultural products and food industries, it can be concluded that different methods have been developed to counteract the adulteration in these products. Then different classification algorithms by MATLAB software and various techniques such as support vector regression were done and finally output dates were processed. The samples of sunflower oil, canola oil and corn oil which are known as adulterated oils were also obtained from national markets. The dielectric experiments started on the dielectric parameters of olive oil one day after the preparation of the sample. The samples were blended with sunflower canola and corn oil, and they made up a total of 9 classes with varying percentages (from 60 to 100% in 5% increments), each of which was shed separately, according to the desired amount, of 100 centimeters inside the sensor. Due to the high flow of data, the ch340g chip on the Arduino board is used to measure dielectric parameters as well as a device that can detect the purity of olive oil. Arduino has a variety of ports, controls, side panels, and screen connectivity, while it may be smaller, less expensive, and possibly faster than a microcomputer system. In particular, SVM does not have the problem of local optimization in its training, it builds the categorizer with maximum generalization, determines its structure and topology optimally, and functions nonlinear differentiation easily with low computing using It forms the concept of an inner product in the Hilbert space. According to the results, Most R 2 related to Olive-Canola (0.90) and the least amount of R 2 pertain to Olive-Sunflower (0.86). Also, in regard to the amount of RMSE, result has shown 4.81, 2.54, and 4.38 for Olive-Sunflower, Olive-Canola and Olive-corn, respectively. A pair of matched logarithmic amplifiers provides the measurement, and their hard-limited outputs drive the phase detector. Figure 2  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn Using this technique, the amount of RMSE for the samples were equal to 3.65, 2.46, 3.31 and the amount R 2 was equal to 0.92, 96 and 93, respectively. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud.
paper_139	Much effort made towards in investigates mechanism with identical detection algorithms, still the retrieved web documents with outmodedlink. In this proposed system, we are successfully identifying and minimize the redundant information and like link in web documents. Retrieving relevant information from web without redundancy is more challenge task nowadays where in web mining communities  [3, 4] . Utilizing customary data recovery  [5]  and information mining systems it get to the known and obscure data from the Web content. Traditional web mining algorithms handle with structured document  [7] [8] [9] [10] [11] [12] [13] [14] [15]  than the advanced methodology of mining algorithm can dealthe entire heterogeneous document comprises of images  [9] , graphs, videos  [16] , etc. A query is searched in a web search tool to recover some significant and required data for the client, either the search query is known or unintelligible to the client, it generally to reply with relevant data rather than redundant, however we can't guarantee that the reply for the query about the significance and redundancy. Once the input query is requested, the search engine generate the document with multiple web pages along with the links, the user will be unaware about the content of the web pages, the extracted web documents contain multiple web pages either be redundant or not. The Document retrieved must follow some constraints which have less time & space requirements, based upon the criteria the extracted web document must be preprocessed, for preprocessing & information selection, need to apply some techniques such as stop word removal, Stemming of word, phrasing, normalization of tokens. Initially calculate measure for all the vertices and maintain the set U which contains a minimum and maximum degree for all vertices and isolated measure vertex. After applying the above steps the entire vertex without redundant information available in the set U. Pseudo Code forThe Proposed Algorithm KTMIN-JAK-MAXAM Step1: Compute degree measure for all vertices in the setup. Step2: Pick the minimum degree vertex 'v' in the set-up and include in the set U. Step3:While U doesn't include all vertices Step3A: Include the entire isolated vertex which is adjacent to the vertex 'v' to U. Step3B: Find the adjacent vertex 'u' to 'v' which is not in U and has maximumdegree. Case I:Regular set-up Connected Regular Setup Case I: A Consider the following connected set-up G 1 in figure 2, having 12 nodes having 3 degree in all vertices along with redundant links. Apply the proposed KTMIN-JAK-MAXAM ALGORITHMto G 1 By step 1: deg(All Nodes of G 1 ) = 3 By step 2: Mark the node A as visited and put it onto the set U. By step 3: There is no isolated vertex in the given graph G 1 By step 4: 4.1 Investigate any unvisited adjacent node from A. Future work aims to create a finite automata tool to produce only relevant and without redundant information of web documents in data mining.
paper_145	An upward shift of prevalence rate was observed among the higher educated respondents. Overweight and obesity were more among urban residents compared to rural residents and they were thirty two percent more exposed to overweight and obesity. Higher prevalence of obesity was noted among females. The proportion of overweight and obese was higher among them who did not do any physical labor. According to factor weights it was noted that the important factors for variation in the level of obesity were mainly gender variation followed by occupation, education and type of work. In 2016 WHO estimated that globally approximately 1.9 billion adults (age > 20 years) were overweight and more than 650 million adults were obese  [4] . Obesity is generally associated to a significantly higher risk of arterial hypertension, diabetes mellitus (DM), hepatic steatosis, hyperdyslipidemia and renal failure  [5, 6] . It had been observed in some research findings that youth who do not meet guidelines for dietary behavior, physical activity and sedentary behavior have greater insulin resistance than those who do meet guideline  [12] . For this reasons, World Health Organization considers the epidemic a worldwide problem which requires public health intervention  [13]  that act on different factors associated with overweight and obesity as well as technological changes that have lowered the cost of living of the people so that people can avail sufficient food with required protein. The aim of this paper was to identify the socioeconomic factors responsible for obesity and overweight among some rural and urban people of Bangladesh. The important factors for obesity and overweight were identified by factor analysis, where largest factor weight indicated the most important variables  [14, 15]  responsible for obesity. However, among this latter group of respondents also there were 91 diabetic patients. All variables were transformed to nominal form by assigning numbers to do the factor analysis. The variables included for factor analysis were residence of the respondents, their age, gender, marital status, religion, level of education, occupation, type of work, monthly income and smoking habit. The respondents were classified as underweight  [BMI < 20] , normal [BMI,  20 -25] , overweight [BMI < 30] and obese [BMI 30+]. Besides the factor analysis, the association of different socioeconomic variables with level of obesity were investigated. It was observed from the analysis that among 900 respondents 7.6 percent were underweight [  Table 1 ] and 19.1 percent of them were from rural area. Obesity was observed among 15.3 percent people and overweight respondents were 34.0 percent. The differences in proportions of level of obesity according to residential area were not significant [P (χ2 ≥ 5.128) = 0.528] which indicated that respondents for different levels of obesity were similar for both urban and rural areas. However, urban people were more exposed to overweight and obesity by thirty two percent compared to rural people [O. R.= 1.32]. They were in more risk of overweight and obesity by 51 percent compared to males [O. R.= 1.51] Obesity and severe obesity were observed almost similar among Muslims and Non-Muslims [  Table 3 ]. But the O. R.= 1.07 indicated that both the religious groups were similarly exposed to overweight and obesity. Among the investigated respondents 92.6 percent were currently married and 43.1 percent of them were normal  [Table  4 ]. But if classification of respondents was done into two groups, one group of ages <40 years and another group of ages 40 years and above, both the groups were almost similarly exposed to overweight and obesity [O. R.= 0.81]. Maximum normal group of respondents (53.8%) was observed among agriculturists. Maximum (25.5%) respondents of obesity was noted among housewives. But, all other professional groups of respondents had the similar risk of obesity compared to servicemen [O. R. = 1.04]. More respondents of normal group of people were observed (49.0%) among them who had income 20,000.00 -< 30,000.00.This group of people were 20.2 percent. More overweight people was observed among them who had income 30,000.00 -< 40,000.00 taka followed by the group of people who had income 50,000.00+. The value of O. R= 0.40 for higher income group of people compared to other income level did not indicate that rich people had more chance to become obese and overweight. It was noted that [  Table 9 ] 50 percent respondents were involved in official work with or without physical labor. Now, let us observe the association of level of obesity and prevalence of diabetes. But overweight and obese respondents were 62 percent more exposed to diabetes compared to other groups of respondents [O. R.=1.62] In one study  [20] , it was reported that smoking was one of the factor to increase the level of obesity. Among the smokers 47.2 percent were normal and 37.2 percent were overweight. The association between smoking habit and level of obesity was significant [P(χ2 ≥ 20.189) = 0.0.002] and smokers were 21 percent more exposed to overweight and obesity compared to non-smokers [O. R. = 1.21]. Thus, we were in search of identification of most important variables to explain the variation in the levels of obesity in the present data  [14] . The variables which were included in the analysis were sufficient to explain the variation as KMO = 0.633, χ2 = 256.371, p-value = 0.000. The significant multiple regression analysis using one of the included variable as dependent variable and others as explanatory variables also justified the inclusion of the variables for factor analysis. From the analytical results it was seen that all the p-values were less than 0.05 which indicated that the inclusion of variables for factor analysis were justified. From the results of the communality of the variables it could be concluded that 85.6 percent variation of the variable marital status would be explained by the extracted factors  [22] . The factor analysis extracted 5 components as these components explained 73.309 percent variation in the observations of obesity. From the factor analysis it was noted that the coefficients of the variables gender was highest followed by occupation, education and type of work. The second component showed that the most important variables to explain the variation in the data of obesity were residence followed by religion and income. The third component showed that the variable income was important for explaining the variability in the obesity. This component explained 10.86 percent variation of the obesity. The respondents were investigated mostly by the doctors and nurses from their working places. The investigated respondents were divided into 4 groups according to their level of obesity, where levels of obesity were decided by their levels of BMI. Around 50 percent respondents were overweight and obese. Higher (71.6%) prevalence rate of diabetes was noted among the overweight and obese group of respondents. The prevalence of overweight and obesity were significantly associated with age, religion, education, occupation, marital status, income and smoking habit. Factor analysis also indicated that some of the socioeconomic variables were responsible for increased rates of overweight and obesity. The factor analysis showed that sex, occupation, education and type of work were more important to explain the variation in the level of obesity. These are: a) Halt the rise in body weight by encouraging people so that they can take healthy home made food and avoid restaurant food / first food.
paper_212	The models developed however faced considerable challenges ranging from inaccurate representation of natural data for deterministic models, to methods of forecasting such as statistical extrapolation which assumes that current conditions will prevail which is not always the case. This algorithm is able to give a statistically correct of the course of a disease with initial conditions to begin with and propensity functions to update the system. On June 5, 1981 a mysterious disease was recognized among MSM in the USA. In 1982 the CDC identified the same disease among IVDU, hemophiliacs and Haitian residents. In the same year, it was identified that it attacks the immune system of the host, incapacitating them to heal subsequently leading to death. In 1984, Robert Gallo discovered that HIV was responsible for AIDS. This virus, being highly transmittable is analyzed and capturing how it is transmitted is crucial in attempting to model the disease. However, this is not always the case as there exist individual differences in the ability to transmit and acquire HIV that remain unexplained  [2] . The following year, 26 new cases of HIV were recorded from sex workers and the NAC was established. Sex workers have the highest percentage of HIV prevalence at 29.3% according to the Kenya HIV estimates report-National Aids Control Council. There are various questions still left unanswered to date on the HIV epidemic. These questions are encompassed in the modeling of the HIV immunology, the HIV dynamics as well as the AIDS dynamics such as the dynamic distribution of the disease in the population and its likely magnitude. This study employs mathematical modeling tools in the transmission probability of HIV and analyses done on how the cumulative number of infected individuals responds as well as the AIDS death probability and how the cumulative cases of removed individuals responds to this probability. The following reviews consider models developed for HIV/AIDS data that either differed too greatly with other model estimates or still fail even with developments on the model. This model approximated that there would be 6000000 cases of HIV and 400000 cases of AIDS in China if there were no forms of interventions implemented. In 2007 the government of China alongside UNAIDS made an estimate of 700000 cases of HIV and 85000 cases of AIDS in China at the time, which is much lower that the estimates made by Liu  [4] . The group comprising of the largest individuals predicted to be living with HIV/AIDS was approximately 650000. In several concentrated epidemics, HIV prevalence estimates do not match reported cases and mortality estimates do not match reported deaths, even after adjusting. Even with the 2013 updates of Spectrum where adjustments were made in the parameter values empirically to improve the fit to program data, the estimates given by Spectrum still differed with data available. They considered and analyzed a two stage SI model that allowed for random variation in the demographic structure of the population with the population size changing at different times which had an exponentially distributed rate of infection. The Gillespie's algorithm based SIR model concept considered the Gillespie algorithm, Euler alongside other CME based exact methods which showed that Gillespie's algorithm had the least execution time  [9] . Other authors have made contributions to mathematical epidemiology by performing simulations that explain the process of disease spread. According to Koopman, deterministic differential equation models cannot capture the real-life representation because no matter how finely they divide populations into geographic and social space, the infectious population is spread out to cover the entire space. The inability of differential equation models to capture stochastic effects therefore has been demonstrated by studies done by Koopman  [11] . There is need to come up with a stochastic mathematical model that better expresses the changing number of HIV/AIDS cases. This study seeks to incorporate a stochastic aspect in the deterministic SIR epidemiological model. This paper will contribute and build on to the existing literature on modeling disease dynamics in the population with the model tested on HIV/AIDS data 1985-2018 to investigate if the simulated values would reflect results that are close to reality. This paper will help bridge the gap between conceptual epidemiological models and its simulated version by providing a developed version of an SIR model that solves one inherent problem that deterministic models do not reflect the natural data. The Classic SIR model The Kermack-McKendrick theory illustrates individuals grouped as susceptible and removeds only  [13] . The initial conditions changed over time and demographics not being included such that change over time was described as; (1) formula_0 The Kermack-McKendrick theory was later developed to a version where they tackled the problem of endemics  [14] ,  [15] . They set the transmission and infection rates as invariant for all ages and this allowed the inclusion of an infectives class. This transformed the theory to the basic SIR model such that when demographics were included becomes (4) (5)  (6)  where N denotes the total host population. denotes the birth rate and death rate denotes the infection rate denotes the recovery rate t denotes time point Model development The Gillespie algorithm was used to simulate a statistically correct trajectory given initial SIR conditions. The SIR model explained how the epidemic manifests in all the compartments. denotes the rate of birth denotes the rate of non-AIDS death denotes the rate of infection denotes AIDS death rate denotes model's time step Gillespie's procedure The Gillespie simulation procedure was developed to produce a statistically correct course for finite well-mixed populations  [16, 17] . The interaction between states is made possible by events outlined in this model as birth, infection, non-AIDS death an AIDS death. , , This Gillespie's algorithm based stochastic SIR model generates a statistically correct trajectory from the initial vector as , , where i=s, i, r S+I+R=N i denotes the population size of the state at time t denotes a function characterized by two quantities as a state change vector and a propensity function. Continuous-time Markov chains are the basic tool for building discrete population epidemic models. The Markov property lets us specify a model by giving the transition probabilities-defined as rates-on a small interval between the compartments. Considering the fact that the propensity functions require to be in probability form, we explore this assumption further by defining and interpreting it. A Markov chain is interpreted here then, as a stochastic discrete-valued model with the Markov property that future states of a process depend on the current state. Continuoustime Markov chains are the basic tool for building discrete population epidemic models. The Markov property lets us specify a model by giving the transition probabilities-defined as rates-on a small interval between the compartments. The wait times between one event and the next can assume an exponential distribution ∆ ~+ ,-   In order to assess how the simulated data performs against natural data, a modified chi-square test was used. The SIR model as well as Gillespie algorithm could continue to be applied other areas such as viral marketing and behavioural science as has already been done successfully.
paper_214	The paper presents the usage of databases that store business data into a warehouse star model that permits to create queries using SQL language and business intelligence tools. This kind of model allow to the decision maker to create complex reports and graphs based on the columns from the dimension tables and measures from fact tables, that can be the base for creating alternatives and scenarious acording to the economical indicators. Building alternatives and scenarious is an elaborate task and must have a background in existing data structured in databases that have a special structure of dimensions and fact tables. This data warehouse star model allow complex analyses such as rollup, drill down, slice and dice through the dimensions and fact tables by using special tools such as online analythical processes and complex queries based on views and snapshots. The business environments require analyses on large amount of data, big data and necessitate advanced tools to query through numerous criterias and also to create different realistic scenarious that allow choosing one option, so the business manager can use the right tool to gain economic advantage. The optimal solution or satisfactory is obtained using either algorithms or formulas within optimization models, either by experiencing various possible alternatives in a process simulation. Such testing may be actions that can be made explicit in the framework model; enable better decision-making structure problem, allowing exploration of information flows and operational procedures without interfering with the actual operation of the system; using cybernetic control system, which underlies decision making in practice; There are a large number of parcel simulation program. Limits of simulation include: support of simulation model is simplified, built pursuing one goal, one key criterion. always taking into account the unique factors of a problem specific results can not be transferred to other issues; results are difficult to interpret, being dependent on random factors; no matter how powerful your computer is, the optimal solution is difficult to obtain a model that has many equations and a significant number of parameters. These limitations have led to the use of simulation only when the interactions between components of the system are complex, when factors random have a significant and requires a large number of observations on the behavior data, the problem can not be solved by an algorithm or experiments direct. Quantitative mathematical models are embedded in base models, managed by subsystem a management model that requires separate users from the physical aspects of data processing and storage that extract, create, delete and modify models. Interaction replaces classical execution, procedural, with a performance conducted by decider according to the stages of solving a problem decisions that necessitate different inputs. The activity of the coordinations of inputs is done in most cases with specialized software systems that create analytical databases or modeling languages. In the first case, the user is provided customized views of data stored by performing a diverse set of operations on transactional data. The next logical of optimization and forecasting, simulation assists with the running complex patterns, resulting variables whose analysis highlights the value adopated lead to a decision. Besides maintaining traditional information representation formats like charts, maps and diagrams used currently to represent multidimensional data there are used new types of dynamic graphs. After analyzing the results achieved and the objectives of their reporting, signaled differences and after identify problems it was reveal the need to take action. Solving the result of communication between all stakeholders, sharing the general manager responsibilities both at decisionmaking levels and the corresponding subproblems defined. For example, after the first phase, the scope may relate to excessive spending decisions of a functional department, inventories too high or adoption of a draft research and evaluation on the introduction of computers  [3, 4] . Intuition, creativity and experience allow decision-makers compare alternatives; predict outcomes of each alternative separately. From model design and solution choice there is a strict demarcation, certain activities may be conducted during both phases, and return of election phase in phase. It receives relevant and substantiated elements on activity in the real system and builds models for solving future decision making on the basis of current assessments.Of the foregoing that a decision support system provides a filtering of information provided to decision makers and indicates certain restrictions. Basically, it helps the decision maker during operation and defining the problem, generating satisfactory solutions and retention strategy. For an interactive decision support system architecture includes the following subsystems: Data management subsystem Subsystem management models User subsystem dialog Data management subsystem consists of the following elements: database management system oxidase data, data dictionary and declarative query language. The database contains no internal data, external data and personal data.Internal data consist from the current activities of the organization and operations of various functional departments image. Whatever the nature of their data is stored in relational databases, transactional system data or data warehouse, built on subjects of interest. In current systems, the company's intranet, are increasingly present data accessible through web browsers and multimedia items such as maps, images, sounds. In most cases there is SGBS transactional relational data system and a management database for multidimensional data warehouses created. The data dictionaries are permitted operations to add new data, deletion or retrieval of existing information according to certain criteria. The subsystem management model consists of the following components: base models, the management models, dictionary and processor execution models and integration patterns Base models contain the set of models that make it possible to analyze the facts and the choice of options in terms required by the user. Design is the stage where the data warehouse model is chosen, depending on the complexity of the system real user requirements and data structure existing in the company (databases, Excel spreadsheets...) Building a data warehouse there are three models: type star, snowflake patterns type and constellation type models. The models may change depending on the context, presenting the data in a structure bed, easily designed and accessible to end users. In such a model is highlighted: quantitative data centralized called measures of activity quantitative criteria for centralized aggregation  ,  referred sizes  relational table that stores the measures identified by   the facts dimensions is called table  Tables where aggregation criteria has explicit codes,  called type tables list. Data warehouse star The eastern type constellation when several schemes that use the same type star catalogs. The advantages of using databases for Decision Support Systems in businesses involves creating numerous queries by the decision maker and also allow to create many scenarious and variants to choose from. In the business area is crucial to store data into a warehouse such as a star modell and create many queries and reports so the decision maker can see the trends of the economical indicators and helps him to make a decision that can improve those numbers or it can be useful to implement a strategy for a further development  [1, 3] . The databases permits flexibility in creating queries by choosing columns from dimension tables and measures from the fact table and so the decision maker may choose what economical indicators is useful to analyse and also permits to create different types of graphs such as columns, pies, scatter points or webs. By simply analysing these reports and graphs helps the decision maker to create different scenarious by changing target indicators and corelate with the economical indicators that can't be changed  [2, 5] . The methods specific to the databases such as SQL language and the Business Intelligence tools allow businesses to explore data and to create alternatives that helps to choose the optimal variant according to the economical restrictions that came from the business environment. It is very important to store data into a warehouse model and then to create future strategies that can help a company to expand or to grow the profit in a realistic way based on complex analyses and alternatives.
paper_216	However, it exhibits some systematic biases or unrealistic assumptions like the log-normality of asset returns and constant volatility. This model is found as promising alternative as far as pricing of European options is concerned, due to its varied volatility of the underlying security and estimation of the risk neutral MGF. The MSE and RMSE of Wavelet model is 0.208546 and 0.456669 respectively which is much lower than that of Black-Scholes model and therefore in conclusion, Wavelet model outperforms the other model. Derivatives are instruments whose value depend on an underlying asset. An option is a type of derivative that gives the option holder the right but not obligation to buy or sell an underlying security at a specified price, at any time on or before a given date in the future as agreed on. The holder of a put option has the right but not the obligation to sell underlying security under some predefined terms of an agreement which includes; strike or exercise price, maturity date and option volume. European-style options can be exercised at the maturity date only while American options can be exercised at any time prior to or on the maturity date. The Black-Scholes option pricing formula which has been used as the benchmark to price European options in most of the previous researches due to its simplicity and low computational demand  [2] . This Black-Scholes formula remains the most widely used model in pricing options though it has some known biases which include volatility smiles and skewness. The Black Scholes model has been developed under some assumptions such as the risk-free rate and volatility of the returns are known and constant, the returns of the underlying security are normally distributed, markets are efficient (market movements cannot be predicted). In the real market, the asset returns follow a leptokurtic distribution which is in contrast to the Black-Scholes model where returns are assumed to be log-normally distributed. Also, in practice the volatility should vary in the market and as a result of the Black Scholes' assumption of constant volatility it results to volatility smiles. This model derives the closed form solution for pricing of a European options that is why it is used as a benchmark. According to the literature, this model is the latest theoretical contribution to the option pricing and it is better at capturing the volatility smiles which is as a result of the Black-Scholes' assumption on volatility constant. This research therefore compared the performance of Wavelet based pricing model and Black-Scholes model in the valuation of a European option. The simple closed form solution of European options was derived during the financial crisis  [2] . The derivation of the pricing formula is independent on the parametric form of the underlying security's price dynamics and hence the success of this model depends on the accurate capture of dynamics of the underlying security's price process. According to the researches as far as pricing derivative securities is concerned, Wavelet based option pricing model is the latest option pricing model in the literature  [6] . The approximation of the implied risk-neutral MGF of the underlying security (asset) returns was focused by developing a non -parametric option pricing model called Wavelet model  [4] . The following are some examples of these advantages; when there is presence of jumps in the underlying process MGF is more tractable, the option's obtained MGF is a continuous function, statistical moments of the asset distributions can be obtained using MGF and the risk neutral MGF can be used to estimate the out of sample options that has maturity dates which are different. The following are some of the applications of wavelet method in finance and economics as pointed out in  [6]  and  [7] ; Wavelets can be used in multi-scaling analysis. Algorithm has a superior performance in de-noising financial data with high frequency by using the first principal component to de-noise frequency variations of a business cycle with wavelets  [12] . Lastly, wavelets can be used to estimate parameters of the models which are unknown using wavelets in pricing of an American derivative security by levy process  [13] . Where;  formula_0 From the Black-Scholes formula above, we see that the strike price (K), risk free interest rate (r), time to maturity (T), the volatility (") and the current price of the stock (S0) are required. Based on some general assumptions like independent and identically distribution (iid) for asset returns and that the moment generating function is defined well, the wavelet based pricing model can be expressed as follows; formula_1 Where, t is the current time, is the underlying asset price at time t, # $ is the time t price for a European call option written on asset, K is the strike price and T is the future maturity date. The underlying asset dynamics and investor expectation in option prices is captured by the MGF Θ !$ + of the logarithmic returns /0 7 . To estimate the risk neutral MGF of the return per unit time, we use the Franklin hat function f (t) expressed as;  In this study, for us to compare the pricing performance of the three models, we used the commonly used statistical criteria. The focus on this study was on pricing of a European Call option using two models, one parametric model (BS Model) and one non-parametric models (Wavelet Pricing Model). In order to estimate the risk neutral Moment Generating Function of the underlying security from option prices, we applied the methodology of wavelet. Therefore, one of the most significant input of this study is that the wavelet -based pricing model is an alternative model for pricing options and other derivatives on the same underlying asset with varying times to maturity and different strike values. We recommend more further investigation on the nonparametric models since most of the studies have focused on the parametric model especially Black-Scholes model. In this study, we also focused on pricing European call options and therefore we recommend an extension of the approach to pricing more complex options like American options which have no general closed form analytical solution.
paper_219	This paper introduces a framework to determine data quality on enterprise networks for net-centric and net-ready initiatives as introduced by the US Department of Defense (DoD). It is proposed to add two new attributes pertaining to data sharing performance to QoS: data relevance (DR) and quality of data at source (QDS); and further a method to evaluate these new attributes. The DR attribute is introduced; it is important in enabling enterprise data consumers to sort, filter and prioritize data. The minimax decision model is chosen to meet the design philosophy that little advantage to the overall enterprise network performance will result from further investment in high performing attributes prior to balancing performance across all three model attributes. As a result, an ever-increasing amount of data is being created for military operators and commanders through the evolution of sensor capability and increased situational awareness data sources. The ability to generate data combined with the ability to store data is beginning to outpace the capacity of the bandwidth necessary to simultaneously share all data amongst all the operational stakeholders. A fundamental shift in paradigm is required to ensure the stressors of conducting military operations are supported through data (of both high-quality and high-relevance) and, not burdened by attempts to manage its excess. The US Department of Defense (DoD) introduced the terms net-centric and net-ready to describe the mechanisms by which operators can search and discover information within the bounds of data. Net-ready also introduced concepts to improve access through mechanisms of data entry and network management; all in pursuit of increasing support to military operations. To implement net-ready, system developers add data tagging, search algorithms, additional communications paths, and a suite of tools to exploit the new forms of data organization to help operators sift more rapidly through data to find relevant information. Successful policy implementation amongst other factors requires system analysis methods that assist acquisition agencies in targeting limited developmental resources to areas of greatest impact to the overall mission objectives. This paper introduces such a method in support of achieving maximum data quality for military enterprise networks: a quantitative mechanism by which the value of different net-ready implementation options can be evaluated and graded. Importantly, a model for valuing enterprise data quality is introduced to bridge the gap between measure of technical performance and operational benefit. Thus the term data relevancy (DR) is introduced into the model for valuing data quality in the context of net-centric / net-ready. Vice Admiral Arthur K. Cebrowski, U.S. Navy, and John J. Garstka proposed that adoption of network-centric operations across the military enterprise would result in benefits at the strategic, operational, and structural levels and bring forth "a much faster and more effective warfighting style"  [2] . This new warfighting style is Net-Centric Warfare (NCW): an "information superiority-enabled concept of operations that generates increased combat power by networking sensors, decision makers, and shooters to achieve shared awareness, increased speed of command, higher tempo of operations, greater lethality, increased survivability, and a degree of self-synchronization"  [1] . The DoD introduced four criteria that must be satisfied for "Data Sharing in a Net-Centric Environment" via DoD Directive 8320.02 in 2004  [4]  which later in  [5]  expanded to the seven listed in this section. This directive and the subsequent series of 8320 series documents identify "the cornerstones of Net-Centric Data sharing"; data shall be visible, accessible, understandable, and trusted  [6] . In the years following, the Chairman of the Joint Chiefs of Staff Instruction (CJSCI) 6212.01 was released that set-forth the procedures for development and certification of a Net-Ready (NR) Key Performance Parameter (KPP); the NR KPP process later being subsumed into the Joint Capabilities Integration and Development System (JCIDS) process  [3]    [7] . The NR KPP specifies the attributes required of data sharing Information Technology (IT) introduced into the net-centric operational environment: (1) IT must be able to support military operations (SMO), IT must be able to be entered and managed on the network (EMN), and (3) IT must effectively exchange information (EI)  [3] . Making data accessible requires providing authorized users the ability to view, transfer, download, or otherwise consume a data asset once discovered  [6] . Making data understandable requires alignment of terminology, data protocols, data formats, and data meaning between produced and consumer  [6] . Alignment can be achieved via direct negotiation or-more practically-via the adoption of commonly referenced standards such as those listed indicated by the global information grid (GIG) Technical Guidance Federation  [6] . A consumer's trust in a data asset is dependent on multiple facets: assessment of the data asset authority, clear identification of the data asset source, tagging with appropriate security metadata, and maintaining a full pedigree of the data asset throughout the full process  [6] . To satisfy the attribute of support to military operations, IT deployed to the operational environment must support identifiable net-centric operational tasks and mission objectives  [3] . non-generic); the required performance of the connections be identified by quantifiable and testable measures of performance (MOPs); and the connectivity must be managed by a structured methodology  [3] . The specific data elements and assets exchanged with external networks as part of executing net-centric operational tasks are specified by the exchange information attribute  [3] . Each net-centric information exchange defined MOPs that are measurable and each information exchange must also identify how the four criteria for net-centric data sharing (visible, accessible, understandable, and trusted) are satisfied for authorized consumers across the enterprise  [3] . Evaluation of the data sharing enterprise requires a holistic view that considers the net-centric attributes of the data simultaneously with the quality of service for the data network. Additionally, the interdependence between cyber security and net-centric principles are indicated in the most recent update to the DoD's instruction for enterprise data sharing  [5] . The combined consideration for each of these areas yields a newly defined model for the data sharing enterprise comprised of three equally important attributes: data relevance, quality of data at source, and quality of service for the enterprise network. 1  illustrates the mapping relationship for each enterprise attribute to various DoD objectives of net-centric, net-ready and cyber-security. Thus to avoid bias consider unequal sampling of the user population by using login authentication to identify users to form strata of homogenous users. The survey is not just restricted to a complete enterprise system but can be performed in the early design phases of prototypes and help analyze operational performance of an enterprise attribute as a function of its objective measures, i.e. A novel application to networks of a common survey practice is proposed to use the Horvitz-Thompson (HT) estimator  [10]  for determining the value of the total score of the population of size N. HT is commonly used because of its versatility as an unbiased estimator of the total for a sampling/sample with or without replacement. Where it is not easy to design a stratified sampling plan a stratification post collection of n samples without replacement can be constructed. An important part of assessing the end-to-end performance of a data system is consideration of the inherent quality of the originating data prior to its entry into the network; to be referred to as the quality of data at source (QDS). An example of such an estimate for audio (speech wireless, VOIP, fixed, clean) is use of objective measures to form the perceptual evaluation of speech quality (PESQ) model. There are three levels of reference used in determining the models for estimating the subjecting ratings: full reference (undistorted service is available for comparison with distorted outcome), partial (or reduced) reference, and no reference. The reason to use the full reference is to capture environmental conditions resulting in the most accurate predictions of ratings. Video and audio have models for QoE but other data types still require development of objective measure models to predict their QoE subjective ratings. The prevailing method for assessing the quality of a still image is based on the ability to perform certain levels of object recognition with scoring defined by the national imagery interpretability rating scale (NIIRS)  [17] . User-satisfaction provides a true gauge of a network QoS  [11]  and the subjective assessment of that satisfaction is provided by QoE. Examples include understanding relationships between objective measures of QoS like jitter, throughput, and latency to be able to control the QoE. A number of authors  [12, 15]    The measure of data relevance expresses the utility provided by the data towards the consumer's objective(s). Highly relevant data must be visible to the consumer, understood by the consumer, and provide support to the consumers' military operations (as depicted in the mapping of  Fig. A simplistic rejection of non-relevant data is insufficient given the negative effect that excess amounts of data can have on human decision makers, even when the data in the network is restricted to only relevant data  [23] . The use of the QoE estimate is proposed to provide a subjective rating of the overall relevance of data shared on the enterprise. The measure of tagged data relevance corresponds to how accurate the data producer was in expressing the intrinsic data relevance through the application of metadata that is understandable, relatable, and unambiguous to the data consumer. The metadata taxonomy needs to be sufficiently diverse to express the essential characteristics of the data product but not so overly detailed that the data tagging approaches the size and complexity of the data itself. This requires a thorough understanding of the wants, needs, and priorities of the consumer to realize maximum value of tagged relevance, and therefore a higher QoE. The measure of discovered data relevance is an indication of how well the enterprise system enables a consumer to differentiate the data product offerings accessible via the network according to the level of relevance and value to the mission objectives. To support a high level of discovered relevance, The taxonomy available to the consumer must be sufficient to explicitly discriminate the desired data features from the undesired. A poorly performing taxonomy would be one that prompts extraneous definition of detail or that includes terminology with such subtle variation as to lack the mutual exclusivity necessary to select between one term or the other. Just as the performance of tagged relevance is improved by an understanding of the consumer by the producer, discovered relevance benefits from awareness that the consumer has of the full range of data offerings of the enterprise and how they may be described. Without a strong mutual awareness, a consumer may prematurely end their discovery process with the first piece of seemingly relevant data believing that it is the best or perhaps the only product available to them. This is why an overall QoE value for data relevance is important as it indicates the relative success of the discovery process across multiple groups of users at finding data with intrinsic relevancy suitable to meet their needs. We first discuss the design philosophy for the overall assessment of the data quality for the enterprise: (1) The enterprise data quality is determined by the attribute with the lowest user satisfaction (QoE). For example, if the quality rating is fair for data relevancy then funds to provide an excess bandwidth to have an excellent QoS can be spent elsewhere as the overall data quality at best is going to be fair. Thus, aside from determining the value of enterprise data quality for purposes of data prioritization, we also have an analytical tool to identify areas of improvement and to allocate resources more effectively across the overall system. The formal definition of Wald minimax model is  And for QoS the objective measures could be latency, packet loss, jitter, and sequence of packets. percentage of spatial and temporal coverage between consumer desired and actual data, and percentage of ontology alignment user and producer. 3 , a particular value of enterprise data quality is given at the start, but from start one could continue to improve QoS and QDS under Option A with no increase in the value of enterprise data quality, whereas Option B modifying the system delivering DR does increase the overall enterprise data quality. This paper introduced a framework to determine data quality on enterprise networks for DoD net-centric and net-ready initiatives. Thus, support is indicated for further research in the development of objective measures using the definition of data relevance elements presented and to determine models for predicting QoE ratings as a function of these objective measures.
paper_241	So the use of transformers protection in electric power systems is very crucial and critical as they are required for efficient transportation of electricity to consumers for long term. This paper is concerned with the differential current protection scheme of transformer using a differential relay technique with arduino Uno microcontroller as a differential relay responsible for comparing the differential current values and sends trip signal to the relay (acting as circuit breaker) to open the circuit when there is fault in the protected zone as a result of imbalance in the differential current values. The system is efficient in transformer protection, gives better isolation, has accurate fault detection and quick response time to clearing faults The demand of electricity in our modern day as led to advancement in electrical power systems, which is reflected in the development of all the power system device generators, different sizes of transformers, transmission lines and the protection equipment. The system is capable of sustaining a variety of environmental and operating impacts that resemble normal operating conditions which may include lightning striking the transmission lines, excessive loading, deterioration or breakdown of the equipment insulation resulting to power systems experiencing occasional faults such as phase to phase or phase to ground faults, over current, over voltage or even temperature based faults. The Differential current protection scheme is based on the principle that the input power to the transformer under normal condition is equal to the output power and is concerned with having a differential or balanced current between the primary and secondary side which would be digitally displayed  [2] . If it finds any error then it sends commands to the circuit breakers to trip the main potential transformer and the buzzer gives an alert. The fundamental idea of the differential current protection scheme is to provide protection to the transformer if any fault occurs in the protected zone that will cause an imbalance in the differential currents. When there is an occurrence of internal fault an imbalanced or non-zero differential current will flow through the operating coil of the differential relay which would be substituted with a microcontroller that sends a trip signal to the relays needed to open the circuit. The differential protection scheme is concerned with the faults that arise from associated difficulties such as the magnetizing inrush current and saturation. The magnetizing inrush current is a phenomenon that occurs during brief initial state of energization of the transformer even when the secondary side has no load connected to it and has its current a lot higher than the rated current  [6] [7] [8] . The power transformer primary and secondary currents are measured by the current transformers, which are to be protected by the Arduino and the circuit breaker. The current transformers are each connected to the primary and secondary side of the step-up transformer and are used to reduce the currents to a measureable value that can be sensed by the current sensors. As a differential current comparator it is used to compare the current values from the outputs of both Current sensors at the primary and secondary sides of the power respectively, computes their resultant difference, which would process an instruction to send a trip signal to the relay depending on its programming and the time required to clear a fault. Acting as a control unit, used for monitoring and controlling the working activities of the protection zone and display values of voltage and currents sensed by the voltage sensor and current transformers respectively. The differential protection of a transformer is implemented using Arduino Uno microcontroller as a decision making device that sends a trip signal to the relay (acting as circuit breaker) whenever there is faults (internal or external faults). This paper seeks to design an alternative method of transformer protection with a digitally displayed, Arduino based system that will intelligently monitor faults which may arise due to current imbalance of the transformer and prompts a safety measure to protect the transformer.
paper_251	Clouds provide an infrastructure for easily usable, scalable, virtually accessible and adjustable IT resources that need not be owned by an entity but can be delivered as a service over the Internet. The cloud concept eliminates the need to install and run middleware and applications on users own computer by providing Infrastructure, Platform and Services to users, thus easing the tasks of software and hardware maintenance and support. Heterogeneity in the computational requirement, dynamic choice and infrequent usages types of resources of the users in modern era has main challenge for service provider (application developer and hardware manufacturer). Secondly, Now computing power or connected computing power (with network) has more demanding and significant role in almost all areas of epoch including market analysis, searching, map, accounting, medical, trading, shopping, rescue operations and many more, the list is endless. Hence demand of users is heterogeneous in nature so that varieties of application (hardware & software) have been developed to achieve the highest user satisfaction. Purchasing or licensing of all such required items (devices & applications) is not feasible to the organization or individuals in terms of the cost and installation. More comprehensive concept about cloud computing has been narrated and drafted by National Institute of Standard & Technology (NIST): According to NIST-"Cloud computing is a paradigm for facilitating expedient, on-demand network access to a shared cluster (pool/collection) of configurable computing power and resources (like applications, services, networks, servers, and storage,) that can be expeditiously provisioned and exemption with least management endeavor or without service provider interaction. Cloud computing is another example of technological advancement which offers dynamic provisioning of the utility on rent basis to the subscriber. A cloud service provider has deployed and manages to sufficient number of resource that has been shared to the entire subscriber as per the load requirement of the individuals. To achieve this task cloud service provider required highly efficient scheduling approach and the proper monitoring of the services provisioned or will be provision to subscriber. In the cloud computing. Hence agent based solution has been proposed to meet the requirement of the modern cloud computing with pace of dynamic provision to insure shrink in shrink out (elasticity) of the cloud service provider to achieve highest scalability and reliability in extent of the maximum availability of the service to the requisites. For implementation and evaluation of proposed approach public cloud has been chosen due to cost effective experimental setup. Outcome the results shows that the provisioning of SaaS (Software as a service) and its monitoring using agent has gives better result which is more efficient than existing approach. For developing proposed agent based system three types of public cloud and their services has been selected as test bed for better evaluation and measurement of the accuracy of the propose system. For this Cloudbees service provider has been integrated onto the developed SaaS application. Fault tolerance While author  [1]  and  [2]  proposed an agent based solution to solve the above listed QoS parameter that greatly affect the performance of cloud service especially SaaS. Following goals has been achieved or solved with integrating of the Mobile Agent to Cloud Computing service realization 1. To Evaluate and delivered the cloud computing services (SaaS) using agent (for better and fast delivery) using public cloud such as "New Relic and cloud bees". Deploying a web services under SaaS paradigm and evaluate the effectiveness of the web application in the cloud environment with the help of agent. Cloud bees service integrating a SaaS deployment on it and delivering through agent. Provisions of service and resources in cloud PaaS is an important function that provides analytical statistics about the current view of cloud (running instance for a user or group of users). Model for Proposed Work Our proposed work is to schedule and monitor cloud SaaS application onto the cloud and evaluate the performance of the same using proposed agent based. Resources: Algorithm for Scheduling (influenced from Aneka) developed onto the Cloudbees Proposed Algorithm Step1: Register user login Step2: Sign in the user login Step3: Select multi monitoring agent in cloud environment (for each subc_request with QoS constraints) Step 4: Mapping of resources between cloud client, cloud agent and cloud service providers. For evaluating the performance of the proposed agent system, the obtained results have been compared with  [2]  in which author has proposed "MABOCCF" the realization of the federation of different cloud (cloud interoperability) using agent. Proposed multi agent based solution has influenced from [2] but it's not the realization of cloud federation rather it has to evaluate the scheduling and monitoring of the SaaS (task) application in public cloud's (cloud federation not interoperability). Table 3  shows the user satisfaction of the deployed SaaS obtained results and has been compared with existing malty agent based method  Rather than performing analytical calculation, proposed system has been compared with the author's MABOCCF and NMABOCCF method. for their granting/releasing) and their exact monitoring in the cloud in context of public cloud computing service provider such as Cloudbees. Proposed agent based solution for guaranteed better elasticity and their efficient monitoring of the resources in the cloud which helps to gather analytical statistics of the resources currently held and will be used such a memory, number of instances and CPU. For evaluation of the propose agent based method data set (jsp) has been developed using jsp web pages and deployed onto the cloud evaluating the elasticity and its monitoring. To deploying created SaaS application in the cloud a PaaS service has been required to be subscribed, for this cloudbees PaaS service has been chosen. Then for monitoring and scheduling with software agent New Relic service has been used to customized the agent functionality to meet the propose systems requirement. Following few areas has been chosen as future work as derivative of the proposed multi agent based solution where the current work can be taken further.
paper_272	This paper addresses the description of all parameters and evaluates switching processing times for three circuit breakers, VD4 /ABB Vacuum type 6.6KV/1500A/20KV/40KV-IEC60-071 " for maximum switching times 2.7ms". Circuit switching system -electroplates including some devices utilizing vacuum interrupters have been viewed as eliminating both of switching surge, arcing currents and high frequency rates for any interval times (microseconds' Process). In addition the inside electroplates some characteristics can be described as followings: Non-sustained disruptive discharge voltages In general, the above characteristics are not involving associated with only vacuum switchgear itself a lone, but when connects the switchgear with power distribution transforms or synchronous motors, there are also many negative interactive processes for instance hysteresis currents, eddy currents and high voltage capacitive inside each transformer (Parasites electrostatics charged) feedback to the interrupter process. K is Boltzmann's constant (1.37x10-23 Joules/K) The constant A is approximately 6x10 5 for most metals According to the Harris model and classification who was classified "the positive ions initiated from ionization process in an Anode plate and generates joule heating to a cathode plate surface during switching process"; i. Ionization zone ii. The term multiple re-ignitions refers to the series of alternate re-ignitions and high frequency (typical several hundred KHz)interruptions usually resulting in an increasing train of voltage peaks; this overall phenomena is usually defined as voltage escalation  [3, 4] . It is important to appreciate that while current chopping and voltage escalation can occur in a single -phase circuit, virtual current chopping is specifically a 3-phase characteristic: the effects of normal current chopping, multiple re ignition and over voltage escalation in one phase can generate surge over voltages in the second and third phases. Pre-striking of the breaker in picking up a transformer load is somewhat similar to the multiple re-ignition event which occurs on opening a breaker  [7] . Restrike overvoltage due to the multiple re-ignition of circuit breaker when a switching interrupter process is initiated before current zero. Multiple repeated ignitions mean an over voltage magnitude is a straightforward concept: as the amplitude of any overvoltage increases, the probability of breakdown in vacuum or breakdown of solid insulation increases. The process of current chopping is the premature suppression of 50Hz or 60Hz circuit current before normal current zero due to instability of the arcs in a vacuum interrupter  [6] [7] . Although the current in the vacuum interrupter can chop to zero almost instantaneously (fraction of a microsecond), the current in the load inductance-3ph coils in the power transformer cannot attain zero value instantaneously. When current chop occurs, the energy stored in the effective load inductance is transferred to the available loadside capacitance to produce the so called chop overvoltage, given by Ic√   According to the equation  1    EMTP which is called electromagnetic transient program or others such as SIMULINK/MATLAB are excellent numerical tools that allow for depth studies of switching transients in industrial as well as utility power systems. Since the high frequency characteristics of the power system equipment are depend on stray capacitances that mentioned above and inductances also and they in turn depend on physical dimensions, equipment layout, materials, cable lengths, all these can be represented as follows  [17] ; The figure represents TRV & Re-ignitions computed in the system when switching off power transformer load that the transient voltage approximately (350µs-450 µs) and shows very high frequency sub transients both going in the negative direction of the voltage -Left side. The analysis shows that the effect of damping in an oscillatory circuit can be described in terms of a single parameter, designed ŋ, or its reciprocal ʎ, which is the ratio of the resistance to the surge impedance of the circuit: This fact permits the construction of generalized damping curves for different values of ŋ, from which the solutions to many practical problems can be extracted with about the same effort as one expand in using a table logarithms. Normally our analysis for the switching process basically on parallel RLC circuit as following; L= Indicative load of stator winding coils C= parallel parasite `s capacitors -see the introduction R= evaluating resistance that can be damping oscillating. This has been done in Figure. By calculating the inductor current in parallel RLC circuit under conditions of a subsidence transient, but have a far wider application. The electrostatic charges -static charged -parasitism are almost depleted in the a mount current begin to decrease in the same time of first peak value which generates chopping currents, moreover the formula of the transition processing rate of du/dt or di/dt have to be considered. The other processing steps as followings: a) Chopping current b) Restrikes voltages c) Prestrikes voltages d) Multi reignitions e) Voltage escalation f) Post-arc currents g) NSDD (non-sustained disruptive discharges) It is often misunderstood how all these phenomena are created and what are their real causes.
paper_294	Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. This study therefore, looks at the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. It was therefore, recommended that concerned policy makers should make policies that will allow public libraries to acquire store and preserve audiovisual materials on various indigenous knowledge for development to ensue; and for posterity; and that governments and other stakeholders should ensure that the public library sector is adequately funded and manpower is adequately provided as well. The Tiv had different management strategies of postharvest losses of cassava (Akom or Alogo) that includes dried cassava chips (kpor), imyorun or imorun (gari) processing, fermented cassava (akpu) and production of base (mtuhem) among others, which were potent managing postharvest losses of cassava. Public libraries are supposed to be conscious of ensuring that these postharvest losses management strategies are captured, acquired stored in audiovisual format. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. Despite these postharvest losses management strategies, the different varieties of food stuffs processed from cassava are threatened to extinction due to the flooding of expensive western food stuffs in the markets; and the indigenous local markets where the products could be sold are not accessible due to lack of good access roads. In addition, these indigenous postharvest losses management strategies seem to be facing total extinction due to lack of documentation and storage of audiovisual materials on them. The study intends to investigate the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. Determined whether public library has impacted on Tiv management strategies of postharvest losses of cassava. Apparently, in order to achieve this mission, public libraries preserve a variety of information materials including print materials such as books, monographs, serials and periodicals; and non-print materials such as audio or audiovisual materials such as cassettes, microfiche and films among others. It involves among other things collecting information bearing materials from different sources, organizations and sometime even initiating and encouraging programs that would enable them capture traditional, socio-cultural and economic activities as well as appropriate information formats for posterity  (Agber & Mngutyô, 2013) . Importantly, among the various formats of information materials the public library acquires, audiovisual materials are most suitable for meeting the needs of users in Africa particularly in Benue State. Suffice it to say that the public library is established to serve everybody, it therefore, entails that it is established to serve those who cannot read and write alike. Therefore, it will be appropriate only if the public library will acquire audiovisual materials, which will be most suitable in meeting the needs of the indigenous Benue farmers. The Tiv people used to make a ridge round their houses and plant cassava on it; and when the cassava grew up, it became a cassava fence surrounding the house. Moreover, other varieties of cassava in Tiv land of Benue State include: Akpu from which akpu are processed, Yakpe, Genyi, Wari and Gyo-Akom among others. The variety of foods that are made from the roots and the nutritious leaves are reasons why cassava cultivation is expanding worldwide  (Lebot, 2009) . Therefore, the Tiv people develop different ways of processing it into durable forms soon after it is harvested, which forms part of management strategies for postharvest losses. Postharvest priorities across the globe have evolved considerably over the past four decades, from being exclusively technical in their outlook, to being more responsive to consumer demand. Consumer-driven trends which have contributed to this shift include rising incomes in urban areas, changing dietary habits, more women in the work-place, reduced time for meal preparation and growing demand for safety, quality and convenience  (Rolle, 2006) . Harris and Lindblad (1978)  asserted that postharvest begins when the process of collecting or separating food of edible quality from its site of immediate production has been completed. The food need not be removed any great distance from the harvest site, but it must be separated from the medium that produced it by a deliberate human act with the intention of starting it on its way to the table. Apparently, postharvest losses therefore mean any change in the availability, edibility, wholesomeness or quality of the food that prevents it from being consumed by people. In pursuance to boost agricultural development in Benue State, postharvest losses must be managed; and to achieve this, the Tiv people developed different management strategies for postharvest losses of cassava. Cassava has a short lifespan after harvest and as a result, the Tiv people process it into various forms for easy storage as a stratagem for postharvest loss management. Incidentally, these methods of cassava processing among the Tiv people practiced as strategies for the management of postharvest losses of cassava were passed down from one generation to another orally. The public library owes the people a duty to go into the hinterlands of the Tiv nation in Benue State to collect audiovisual materials on these postharvest losses management strategies. This would have been achieved by capturing of audio narrations of elderly cassava farmers by recording, as well as organizing custodians of the knowledge to shot informative video clips and snap shot for video slides aimed at educating the younger generation and storing the materials for posterity. Section C also consisted of 7 variables of ways public library has impacted on Tiv management strategies for postharvest losses of cassava and Section D consisted of 5 variables of challenges faced by public libraries in acquisition and storage of audiovisual materials on Tiv management strategies of postharvest losses of cassava in Benue state. Consequently, for the cassava farmers who had no western education, the research assistants read the questionnaire to their hearing and gave interpretation in Tiv language, and the options they selected were ticked for them. These research assistants were asked to administer and retrieve the questionnaire through personal contact to avoid delays associated with mailing and multiple filling. This means that peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips), peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour), peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom, slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base), peeling the cassava, fermenting, sieving and squeezing it into lumps known as Akpu, peeling the cassava, grating, squeezing (demoisturizing) it, mixing with salt and other ingredients and frying with palm or groundnuts oil known as Kweesi or Akweesa Akom and peeling the cassava, grating, squeezing and sieving the starch and preparing it for food as Kamu Akom are the management strategies for postharvest losses of cassava. Table 2  shows the mean rating scores of all the items are below 2.50, which implies that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). To answer the research question, data were collected relating to challenges faced by public libraries on acquisition and storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. This implies that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. Table 2 , findings of the study revealed that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). Findings from the study based on the result in  Table 3  showed that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. Therefore, it was recommended that concerned policy makers should make policies that will allow public libraries to acquire store and preserve audiovisual materials on various indigenous knowledge for development to ensue; and for posterity. Moreover, governments and other stakeholders should ensure that the public library sector is adequately funded and manpower is adequately provided as well. The study found out that public library does not impact on Tiv management strategies of postharvest losses of cassava through storage of audiovisual materials on the subject matter. Importantly, if public library has acquired and stored audiovisual materials on Tiv management strategies for postharvest losses of cassava, cassava farmers would have accessed these information materials to improve on various ways of managing postharvest losses of cassava in Benue State of Nigeria. The study concludes that public library must make it a point of duty to start acquiring, storing and preserving information materials on indigenous knowledge, particularly in audiovisual format, since this is the only way the library will be useful to the host communities.
paper_298	The study investigated the awareness and use of library catalogue for accessing information resources by users of Federal University of Kashere (FUK) Library. Survey research design was used, registered users of the University Library between 2016-2019 numbering 920 formed the population of the study where a sample of 272 (30% sample) was used based on 95% confidence level and 5% confidence interval of Sample Size Calculator. The findings of the study revealed that majority of the respondents were aware of library catalogue existence; they were more informed about card catalogue usage than OPAC for retrieving information resources. Poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. The study recommended that the library management should organize a periodic user education, orientation and sensitization programmes for the undergraduate users to create awareness and enable them gain the needed skills to use the library catalogue maximally when searching for information resources. Guidelines on how to use catalogue and library in general should be prepared and issued to registered users. Regular shelf reading should be done so as to establish right contact between library users and library materials. The university library goals and objectives are to provide adequate and relevant information resources both in print and online for university community to support teaching, learning and research (these for undergraduates students may refer to class work, assignments, research/project work, term papers, seminar presentation by providing relevant information and services provision for effective and efficient achievement of academic pursuit). University library provides well stocked information resources and trained personnel to organize available information materials and assist faculty members and student users in the retrieval and use of these resources. The traditional goals and objectives of the library catalogue are to enable users to search a library's collection to find items pertaining to specific titles, authors, or subjects. Libraries were traditionally known to provide access to library information materials through card catalogues and book catalogue as the primary information locating tools. Following advancement in ICT and subsequent development of Online Public Access Catalogue (OPAC), the traditional concept of access to library resources which many scholars identified to be prone to numerous challenges has changed. Students' skills in search catalogue can be created through library tours, orientations, seminars, lectures, library staff, handbooks and use of library studies. KumarandVohra investigated the use of Online Public Access Catalogue by the users at Guru Nanak Dev University Library, Amritsar (Punjab) and discovered that majority of the respondents 68.7% were not aware regarding OPAC, 12.5% stated the reason to be lack of assistance from library staff and slow speed. 91% respondents used the title search approach and 83.04% used the author search approach, User also indicated that the information regarding the problem faced by the respondents while using the OPAC like 74.39% faced by the problem lack of proper guidance about OPAC followed by 67.47% lack of awareness, 36.33% satisfied with the OPAC and its services  [7] . Consequently, AmkpaandIman emphasized that the success or failure of students to locate resources in the library depends on the skill acquired though the library instruction progamme  [8] . Aguolu andAguolu observed that students use catalogue mostly for educational purposes and have really helped in conducting and disseminating information resources in the library  [10] . It must contain different types of materials, very rich in nature, comprehensive in coverage with adequate bibliographical tools describing the location of each item, which is significant to the whole concept of library and librarianship. However, it is observed sometimes that the bibliographic tools that supposed to lead or guides user to the location of a particular item in the library are either found in adequate, misleading, totally not provided or somehow incomplete. This finding is in agreement with Clifford & Zaccus whose study on "users attitude towards the use of library catalogue in two selected University Libraries in South West Nigeria" revealed that number of the male respondents is higher than female respondents  [11] . Librarians in discharging their responsibilities should inform library users by communicating the availability of new technology and the way it operates to them. The awareness of respondents would depend on the way the library informs their users about the use and functions of library catalogue revealing the available resources and its location in the library. shows that majority of the respondents 178 (68%) were aware of the card catalogues as a access point / retrieval tool for searching for information resources in the library. 26 (10%) of the respondents were aware of the existence of OPAC as a retrieval tools in the library. This indicates that majority of the respondents were not aware of online public access catalogue in the library and they only use manual catalogue for information retrieval. This development is not healthy for university library in the 21 st century, where information resources should accessed from different locations within and outside the university using modern ICT facilities like OPAC. The finding of the study reveals that the creation of awareness on use of catalogue through library orientation, university website and notice board is significantly low. This implies that majority of respondents became aware of information retrieval tools through library staff and user education programmes these are more formal sources of awareness about library catalogue function and use. The result proves that the management of the university library provides awareness opportunity to users for retrieval and utilization of information resources, except that they need to put more emphasis during library orientation to add to existing ones. Table 5  indicated that majority of the respondents 158 (60%) used the library catalogue regularly when searching for materials in the library. 24 (9%) of the respondents never used the catalogue but they were aware of it existence in the library as indicated on table 2 above. The study found that majority of the respondents were aware of library catalogue in the library including OPAC  [12] . The reason for this could be attributed to the fact that users did not have the ICT skills to manipulate the OPAC and they solicit the help of library assistants or even friends to retrieve information materials in the library. Higher number of the respondents indicated poor computer skills hindering navigation when searching OPAC 96 (37%) and inability to locate materials on the shelves, indicated in the catalogue as being available 58 (22%) were the challenges affecting the use of catalogue as an information retrieval tools. This indicates that good of the respondents had difficulties using the library catalogue because the respondents lack sound ICT skills that could enable them use the OPAC. ICT skill is needed by the respondents to be able to browse the library OPAC. Again, the later challenge could be attributed to lack proper shelf reading by the library staff, which made users not to locate material indicated available in the library by the catalogue and not visible on its shelf. The difficult interface of OPAC poses a big challenge to undergraduates who are the target users of OPAC in any academic library. It is disturbing to discover from the study, that most of the respondents were aware of the card catalogues as access and retrieval tool for searching for information resources in the library. Unfortunately, most of the respondents were yet to be conscious of the existence of the OPAC, do not understand its operations and rarely use the service despite the huge investment of library resources in the production. Another source of frustration was reasonable number of the respondents indicated poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. This will enable them gain the needed skills to use the library catalogue maximally when searching for information resources. b. Librarians should organize orientation and sensitization programmes that will create awareness, and encourage the university community to effectively use OPAC to ensure proper use of library information resources. c. Guidelines on how to use catalogue and library in general should be prepared and issued to registered users. d. Regular shelf reading should be done so as to establish right contact between library users and library materials and avoid misplacement or wrong shelving of information resources.
paper_305	This is primarily due to difficulties in uncovering uncertainties in information provided by credit applicants and also due to lack of reliable automated techniques that would improve the efficiency of manual underwriting procedures. The results obtained proved that such a mechanism can be applied to augment manual credit appraising processes, especially where large volumes of applications are to be processed within limited timeframes. Despite the increase in consumer loans defaults and competition in the banking market, most of the Kenyan commercial banks are reluctant to use artificial intelligence technologies in their decision-making routines. Generally, bank loan officers rely on traditional methods to guide them in evaluating the worthiness of loan applications. A checklist of bank rules, conventional statistical methods and personal judgment are used to evaluate loan applications. Given the absence of objectivity, such judgment is biased, ambiguous and nonlinear and humans have limited capabilities to discover useful relationships or patterns from a large volume of historical data. This is the first empirical research of its kind in our country that addresses in a systematic way the issue of using Meta classifiers in loan ap-plications. The general objectives of this study were to: 1) Implement the meta learning algorithm -LogitBoost to develop as system for evaluating credit applications to support loan decisions in Kenyan financial institutions 2) Outline some of the challenges of using the learning algorithm in the decision-making process for the banking industry in Kenya 3) Champion the applicability of Java as an open source software in business intelligence applications From time immemorial in the banking sector, banks have relied on the personal assessment of loan risks or on the traditional statistical methods to predict the default of loans instead of using a standardized evaluation tool. These traditional methods often require a great deal of subjective input from underwriters, making them un-reliable and often lack empirical and scientific backing. Further, the study considered a binary output from the classifier, hence dependent variable can only take on accept or reject values with an emphasis on the banking industry in Kenya; though the results can easily be generalized to institutions elsewhere. Loan appraisal decisions can easily extend beyond the "accept" or "reject" kind of classifications to include such other spectral values as "fairly good", "good" and so on. Although the classifier takes this into account through voting -in which those values that meet certain thresholds are promoted to either of the classification values, most of such incidences are minimal and can be handled through judgmental procedures by re-examining those peculiar cases and applying policies as laid out. The ongoing changes in the banking industry, in the form of new credit regulations, the need for innovative marketing strategies, the ever increasing competition and the constant changes in customer borrowing patterns; call for frequent adjustments to credit management in order to remain competitive. The traditional credit appraising techniques based on a hybrid mixture of manual and statistical techniques such as indices and reporting, credit bureau references, post screening, fact act, multiple credit accounts and initial credit line, the manual input are definitely inadequate in modern times. Automated techniques have progressively become popular in contemporary loan appraisal processes. One of the earliest automated procedures uses statistical tools which have fallen short of the inherent challenge for today's commercial banks is their desire to understand large amounts of information and reveal useful knowledge to improve decision-making. Today, lenders are making increased use of new and innovative techniques -the key being data mining and machine learning to evaluate loan applications for business and financial prospects  [2, 3] . Credit appraisal often amounts to making a decision whether to grant or to reject an application. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced  [4] [5] [6] . One such algorithm that effectively addresses these issues is the LogitBoost Meta classifier -based on the log of the odds ratio for the dependent variable  [7, 8] . Although the model was reported to perform relatively better than models developed using other approaches; as part of the limitations and recommendation, they suggested that such a model is usually a black box and more insight the model parameters was required to make it more effective. There have been various other attempts to deal with the loan appraisal problem using various techniques  [10]  to varied degrees of success. The solution to the problem was an adaptation of ensemble machine learning strategies where a 'weak' classifier, commonly referred to as a base classifier was boosted through a series of adjustments through weighting and re-sampling to develop a better learner which was an additive aggregate of individual learners. The boosting method was developed around the Probably Approximately Correct (PAC) model that entails transforming 'weak learners' into 'strong learners'. The reported technique derives from the intuitive understanding that instead of putting all the effort on finding highly accurate base classifiers, it becomes sufficient or even desirable to use a set of weaker hypotheses. Resulting classifier is a simple threshold on a single feature a) Outputs a +1 if the attribute is above a certain threshold b) Outputs a -1 if the attribute is below the threshold In this study, 'majority voting' was adopted for combining hypothesis from different learners. In majority voting, to predict the class of a new item, each base classifier got to vote for either the 'accept' or the 'reject' class. A accept classification for a loan decision meant pointed to a successful application while a reject classification pointed to the alternative. It can be proven (as discussed here-under), that under the assumption that all individual classifiers have the same prediction rate and that the distribution of the data correctly classified by each base classifier is independent and random, this is the best possible strategy. In our study, the variable k was bi-valued and took on either 'accept' or 'reject' values and K was set at 2. The model was specified in terms of K −1 log-odds that separate each class from the base class K. a) With K attributes , there are K different decision stumps to choose from b) At each stage of boosting i. given reweighted data from previous stage ii. Repeat until you have T classifiers selected The development platform used for this project mainly included the following open source software products: The Java Development Kit (JDK) which is a Sun Microsystems product released under the GNU General Public License (GPL) was one of the packages used especially for the compilation of the source files. This strategy is similar to the use of two files as discussed earlier but relies on the learner to automatically partition a given data set into two given a split percentage Options: -F -R -I 15 Number of performed iterations: 15 Time taken to build model: 0.06 seconds Time taken to test model on training data: 0.01 seconds The results were interpreted along the following parameters for all the various training and testing strategies. Test Split ROC graph ROC was developed during the World War II to statistically model false positives and false negatives of radar detections. The value converted to 1 decimal place, these values indicate a perfect classification After a successful implementation of the stated system, the following were the key outcomes:  Three options were investigated for training the algorithm namely: a) The use of single file both for training and testing the model through stratified cross validation. The model accuracy using this procedure was 86.86% making it a fairly reliable strategy b) The use of separate training and testing data sets returned an accuracy of 95% making it a relatively better strategy c) The use of a ratio to determine the size of the training and testing files from one data set returned an accuracy of 88.64% Therefore, it implies from these findings that the use of separate files for training and testing of the model returns the best model accuracy and hence should be adopted. The trained model was subjected to 20 instances of unclassified data which had been carefully selected from a portion of the training and through analysis returned 19 correctly classified instances resulting in a predictive accuracy of 95%  Three suggestions are likely improve the model and hence the predictive accuracy of the learner: The training and testing procedures can be done severally with different input parameters and file sizes to settle on the most effective set for different learning processes. A cost matrix can be fined as part of the training procedure that penalizes wrong classifications especially the true negatives for this study. Finally, as stated earlier in the introduction, it is not prudent to completely rely on an automated credit appraising as some cases might require subjective interpretation and personal judgment. As a conclusion, the reported work indeed confirmed that: 1) Machine learning procedures can be applied in financial modeling applications to augment manual underwriting techniques 2) These procedures can greatly improve the efficiency of such techniques because of their ability to handle large items of data generating very useful statistics 3) This work can be improved through the use of enhanced data set pre-processing procedures, the use of a cost matrix as well as parameter tuning to settle on the most effective set for various data mining requirements.
paper_310	Android application based smart phones are becoming each time more powerful and equipped with several accessories that are useful for Robots. We present a review of robots controlled by mobile phone via moving the robot upward, backward, left and right side by the android application such as Arduino, Bluetooth. We derived simple solutions to provide a framework for building robots with very low cost but with high computation and sensing capabilities provided by the smart phone that is used as a control device. Here we use mainly Arduino UNO (ATMEGA 328P), Bluetooth module (HC-05). The project aims in designing a Robot that can be operated using Android mobile phone. The controlling of the Robot is done wirelessly through Android smart phone using the Bluetooth feature present in it. The data received by the Bluetooth module from Android smart phone is fed as input to the controller. Our system aims to achieve the target to design a system that can provide following functionalities with a simple and easy-to-use interface: a) Develop an android application that will act as an remote of a robot. c) Here the focus is on the latest technology of android and robot also called as 'Mobot'. f) Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. g) The Bluetooth module is connected with the Arduino UNO board for the connection with the user. h) Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. The system consists of following parts: a) Arduino UNO (ATMEGA 328P) b) Bluetooth module (HC-05) c) Smart phone d) Motor driver (l293D) e) Arduino software (version 1.8.1) The basic building blocks of the project have been described below: Microcontroller will act as the brain of the robot. In this system we will be using microcontroller named Arduino UNO which contains ATMEGA 328P microcontroller chip (  Figure 1 ). Arduino has it own programming burnt in its Read Only Memory (ROM). The Bluetooth module will act as an interface between Smartphone and microcontroller. Bluetooth module will give the commands given by smart-phone to the microcontroller. The advantage of this project is that the application software designed for android phones is kept simple but attractive with all necessary built-in functions. Arduino software (  Figure 5 ) is used to put the instruction of whole functions of this system to the microcontroller. The program is stored in the EEPROM of the microcontroller, which is present in the Arduino board. In android application when we press a button, a corresponding signal is sent through the Bluetooth to Bluetooth module (HC-05) which is connected with the Arduino board. Motor driver switches accordingly the data bit, if the data bit is low then the corresponding pin of the motor driver doesn't work else highbit then thecorrespondingpinof the motor driver is on. The working principle of the circuit has been elaborated with the help of a block diagram, of the system interconnection as shown in  Figure 6 . The Bluetooth module receives the signal sent from an android smart-phone, where the application software coded in C language is installed. The microcontroller, thereby, sends instructions, which when executed, helps in functioning of the motor driver. Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. The Bluetooth module is connected with the Arduino UNO board for the connection with the user. Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. Our proposed project consists of the following three sections: a) Input section b) Microcontroller section c) Output section In our android application base Bluetooth controlled robotic car, the user interacts with the system with a smart phone. When user sends any data to the Arduino board then the corresponding pin of Arduino goes to high state and switches the motor driver ic in the on mode. This device is connected with the Arduino board (microcontroller section) by the means wirelessly i.e. These commands help the microcontroller to interface with the Bluetooth module HC-05 and also with the motor driver IC L293D. Here the Bluetooth module act as a receiver which receives the instruction from the smart phone (remote or transmitter). Then the microcontroller decides the operation for the instruction which is coming from the smart phone. The Bluetooth module can operate below the 10 m range, which we would try to extend in future. Until we send any instruction to the microcontroller the motors remain stop. With few additions and modifications, this robot can be used in army for detecting and disposing hidden land mines.
paper_333	The diagnosis of diseases on the plant is a very important to provide large quantity and good qualitative agricultural products. Enset is an important food crops produced in Southern parts of the Ethiopia with great role in food security. There are several issues and diseases which try to decline the yield with quality. Particularly, diagnosis of potential diseases on Enset is based on traditional ways. This study presented a general process model to classify a given Enset leaf image as normal or infected. Bacterial Wilt and Fusarium Wilt disease and collected 430 Enset leaf images from Areka agricultural research center and some selected areas in SNNPR. The proposed model demonstrated with four different kernels, and the overall result indicates that the RBF Kernel achieves the highest accuracy as 94.04% and 92.44% for bacterial wilt and fusarium wilt respectively. Around 80% to 85% of people in Ethiopia are dependent on agriculture; among these more than 20% of them depend on Enset crop production in the country. The government committed 15% to 17% of expenditures to the sector; it covers 41% of gross domestic product (GDP), over 90% of export value and directly supports 80% of the populations' livelihoods  [1] . Enset crop is related to and resembles the banana plant which is an indigenous plant classified under the monocarpic genus Enset and family Musaceae. There are several issues and diseases which tries to decline the yield with quality. Particularly, diagnosis of potential diseases on Ethiopian banana is based on traditional ways and due to limited research attention given to Enset crop production. There is a potential need for technology supported alternative systems to support the manual identification of Enset crop diseases so as to optimize the accuracy for remedial action. If the Enset crop diseases can be minimized then definitely the production will significantly be increased to contribute to the country's economy towards a new boost. Therefore, the implementation of IT based solution in the sector will have a paramount importance to facilitate the activities such as economic, social and ecologic development in the country by increasing efficiency in both quality and quantity of Enset crops production, to sustain dependability of customer preferences and to preserve the ecology  [2] . In our country few researchers found the promising solutions to different plant diseases diagnosis such as maize, rose flower, coffee and others using computer vision and machine learning techniques  [3, 4, 5] . Section V presents the conclusion and discusses of future works. So far a number of fungal, nematode, viral and bacterial diseases were reported to cause damage at different degrees of intensity that was mainly explored by Quimio research finding since 1992. Among various diseases, Enset bacterial wilt and Fusarium Wilt is considered as the most dangers one that reduces Enset yield  [6, 7, 8] . Test Result For the experimentation a total of fifteen experimental setups which is color, texture morphological and a combination of each and a multiclass Kernel support vector machine model: Linear Kernel, polynomial Kernel, RBF kernel and Quadratic Kernel was used.
paper_389	Experiments are carried out to analyze the influence of the choice of conditional random field model parameters and the selection of Chinese character annotation sets on the experimental results. Furthermore, the condition of random field model can be used to add the advantages of arbitrary features, and some new features are added to the model. For example, Xue N divides Chinese characters into four categories according to their different positions in Chinese words, and then divides them into Chinese characters by using the maximum entropy model. Zhou J built a hybrid method of Chinese word segmentation around CRF model. Common Chinese character tagging method is based on the Chinese characters appear in the words of different locations marked different labels. For example, "O" can be used to represent individual Chinese characters, "B" means Chinese characters appear in the head, "I" means Chinese characters appear in the middle or the end of the word. We first set three candidate marks "O", "B" and "I" for each Chinese character and add a start node "BOS" to the head, An end node "EOS". According to the meaning of "O", "B" and "I", the rules are summarized as follows: 1, the sentence of the first Chinese character tag can not be I, the last character of the mark can not be B. Finally, according to the mark of each Chinese character, the mark result is "OOBIO", so the result is "这 / 是 / 武汉 /." As mentioned earlier, the CRF makes it easy to add any feature in the observed sequence to the model, so that not only the transfer and emission characteristics of the traditional HMM sequence model can be incorporated into the model, but also some other The feature information associated with the observation sequence or with the language itself is added to the model. The probability of the i-character of the position as the prefix: pY> 95% 昨狭抨第竞耽 3 Z The probability of the i-character as the suffix: pZ> 95% 丸役袄侣丸瞰 4 R The probability of the i-character individual word of the position: 85% <= pR <= 95% 吧枚刘磊躺却 5 U Position of the i-character as a prefix of the probability: formula_0 The probability of the i-character as the suffix: 85% <= pV<= 95% 貌型胁帘午岸 7 D The probability of the i-character position of the individual word: pD <= 5% 言辽改信申仪 8 E The probability of the i-character of the position as the prefix: pE <= 5% 业络王场姆杨 9 F The probability of the i-character as the suffix: pF<= 5% 增隐晋香浓伊 In order to deal with the long-distance information, this paper takes the context distance as 2. The feature template in  Table 2  is a classification of Chinese characters, the template well dealing with numbers, letters and punctuation marks such as error-prone not logged in the above table, the  Table 1 feature  template is a basic feature template, Use word-based. The probabilities of the position probabilities in  Table 3  are extracted from the training corpus and the probability of each position is calculated according to the following probability formula: P (individual word) = number of occurrences of the individual idiom of the word / total number of occurrences of the word * 100% (1) P (prefix) = number of occurrences of the word as a prefix / total number of occurrences of the word * 100% (2) P (suffix) = number of occurrences of the word as a suffix / total number of occurrences of the word * 100%  3 After many experiments and comparisons, the probability of more than 85% or probability of less than 5% of the word as a location probability feature, but also on the selected word for some filtering, the elimination of some unnecessary words, such as At the same time, select the probability of greater than or equal to 85% of the word is subdivided into the probability of 85% <p <95% and p> 95% of the two sets. In this paper, CRF automatic word segmentation experiments, the use of features include the following two: a single word characteristic: a position on the word characteristics. CRF word segmentation process of Chinese word segmentation shown in  Figure 2 : In order to construct the CRF model, we must first use a standardized process to convert the original corpus into a standard form. The standard corpus form used here dictates that each line in the corpus contains only one word, and the information associated with the word is followed by a tab stop followed by the word. Secondly, feature extraction is carried out to generate training corpus and test corpus which can be recognized by CRF model tool. The format is as follows: each line includes a word and some characters and markings related to the word, characters and characteristics, And between the feature and the tag are separated by tabs. In addition, automatic word segmentation system should also be easy to expand, maintainability and portability; to support different regions, different application areas of different application goals; vocabulary and processing functions, processing methods can be flexible combination of loading and unloading, thereby enhancing the system Processing precision and processing speed; also, to build a "information processing with the modern Chinese word segmentation standard" to match the common or common modern Chinese word segmentation. Table 4  below shows the results of the CRF word segmentation system on the Yangtze River Daily Test Set and the contribution of each feature template to the results. Moreover, since the word position probability feature is extracted completely from the training corpus, some of the participle criterion information of the corpus is extracted to a certain extent, so that when the test set is tested, Corpus. In order to compare the availability of the conditional random field model, we also established two word segmentation models: Hidden Markov Model (HMM) segmentation model and Maximum Entropy (MEM) segmentation model. (CRF) word segmentation model, the experiment uses the combination of "feature template one", "feature template two" and "feature template three" in the common daily closed test set Test, the performance comparison of the results shown in  Table 5 . This chapter first briefly introduces the CRF tools, experiment corpus and standard of experimental evaluation in Chinese word segmentation experiments. The experiment not only demonstrates the influence of the choice of conditional random field model parameters and Chinese character annotation set on the experimental results, but also verifies the validity of the new features and the feasibility of the new method. As the Yangtze River Daily corpus is from the newspaper news, for the news corpus in the special format, such as title, poetry, weather forecasting, etc., to our model training has a certain impact, so if the corpus in the handling of these disturbances, Our model should have better performance.
paper_391	Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. Globally, tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually. Thus, the aim of this study was to identify the best classifier, and to predict the pattern from the TT data set using the data mining algorithms technique. The data for this study were the Tetanus Toxoid data set from the Ethiopian Demographic and Health Survey (EDHS) 2011, and analyzed using the Knowledge discovery process of Selection, Processing, Transforming, mining, and interpretation. These results suggest that among the machine learning algorithm tested, multilayer perceptron classifier has the potential to significantly improve the conventional classification methods for use in EDHS data of Tetanus toxoid. Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. Thus, women receive doses of tetanus toxoid to protect their birth against neonatal tetanus  [1] . Infection is acquired through environmental exposure of any broken skin or dead tissue such as a wound or when the umbilical cord is cut to the spores of the bacteria. Globally tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually, accounting for up to 25% neonate's death in some African countries  [3] [4] [5] . As of December 2012, maternal and neonatal tetanus persist as a public health problem in 30 countries, mainly in Africa and Asia  [2] . In sub-Saharan Africa, up to an estimated 70,000 newborns die each year in the first four weeks of life due to neonatal tetanus  [5] . Ethiopia has one of the highest neonatal tetanus morbidity and mortality rate in the world due to low tetanus toxoid immunization coverage coupled with some 90% of deliveries taking place at home in unsanitary conditions. In Ethiopia in 1999 WHO has estimated about 17,875 neonatal tetanus cases and 13406 NNT deaths which made the country to contribute to 4.6% of the global NNT deaths  [3] . Ethiopia's Expanded Program on Immunization (EPI) started in 1980 and remains the single most important component of primary health care supported by the Ministry of Health. The vaccine to prevent Maternal Neonatal Tetanus (MNT) introduced as part of routine immunization programs in over 100 countries by the end of 2011. Vaccination coverage with at least two doses of tetanus toxoid vaccine estimated at 70% in 2011 and an estimated 82% of newborns protected against neonatal tetanus through immunization  [3] . However, until now maternal and neonatal tetanus persist as public health problems in 36 countries, mainly in Africa and Asia. The TT vaccination schedule in Ethiopia for childbearing women follows the schedule recommended by WHO for developing countries  [6] . Immunizing the mother prior to childbirth with TT protects both her and her newborn against tetanus and antenatal care is the main programmatic entry point for routine TT immunization. A pregnant woman should receive at least two doses while pregnant unless she already has immunity from previous TT vaccinations. Five doses of TT can ensure protection throughout the reproductive years and even longer. In this study, we have used several data mining techniques; classification, clustering, association and outlier detection techniques over the nominated Tetanus Toxoid immunization dataset of the EDHS 11. The main objective of this project was to identify the best classifier, and to predict the pattern from the TT data set using the data mining algorithms and tools for tetanus toxoid vaccination and to connect the technical field public health and medical field to serve the community. Knowledge discovery from data for prediction of the tetanus toxoid immunization among the women of childbearing age in Ethiopia following the standard process, guiding us in the analysis process, and exposing those aspects that could otherwise be neglected. The goal of interpreting and evaluating all the patterns discovered is to keep only those patterns that are interesting and useful to the user and discard the rest. The EDHS 2011 was conducted by the Central Statistical Agency (CSA) in collaboration with Measure DHS and ICF International under the auspices of the Ethiopian Ministry of Health (EMoH). For this particular study, the dataset was requested and accessed from DHS website https://dhsprogram.com after formal online registration and submission of project title and detail project description. Data values and attributes were modified, added and/or deleted, filtered, recorded, dropped the missing values, transformed and attributes are integrated in order to be used by the machine learning techniques in the analysis step for the study. The classification methods used in this study is to classify data according to their classes putting the data in a single group that belongs to a common class. Where each branch represents an outcome of the test, each internal node denotes a test on an attribute, and each leaf node holds a class label. This approach uses divide and conquers algorithm to split a root node into a subset of two partitions till leaf node that occurs in a tree. K-Nearest Neighbor Classifiers K-Nearest Neighbor is one of the simplest classifier which discovers the unidentified data point using the previously known data points, that is, nearest neighbor  [10] . When given an unknown tuple, a k-nearest-neighbor classifier searches the pattern space for the k training tuples that are closest to the unknown tuple. True   In K-fold cross-validation, the initial data are randomly partitioned into k mutually exclusive folds, D1, D2 … DK, each of approximately equal size. To discover acceptable classes using Simple K-Means based on the principle of maximizing the similarity between objects in the same class i.e., intra-class similarity and minimizing the similarity between objects of different classes i.e., inter-class similarity  [7] . First, it randomly selects k of the objects in D, each of them initially represents a cluster mean or simply center. This is to identify the frequency of the selected attribute occurring together with access to Tetanus toxoid vaccination, based on a threshold called support, identifies the frequent attribute sets. Another threshold is Confidence, which is the conditional probability than an attribute appears in a transaction using the Apriori algorithm. This shows that the results of the accuracy and performance of learning machines on the tetanus toxoid vaccination data set are hence reliable and can be used as good indicators of the ability of the classifier for detection. The only attribute of the algorithm we are interested in adjusting here is the "Num clusters field", which tells us how many clusters into five based on the instruction given. Briefly described as follows: Cluster 0-This group have 1534 (27%) instances of which consist of mothers who live in the rural, no access to radio, no access to TV, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 1-This group consists of 1004 (18%) instances of which mothers who live in the rural, no access to radio, access to TV yes, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, head of household male, tt-vaccinated yes. Cluster 2-This group consists of 2063 (37%) instances of which mothers who live in the rural, no access to radio, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated No. Cluster 3-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 4-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, orthodox in religion, Amhara in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education no education, women age 35-49, mother marital status married, hh_head Female, tt-vaccinated yes. As explained in the    The literacy status of the mother has high information gain (0.046) and followed by the literacy status of the mother (0.041) and the least information gained was head of household (0.00000147). (Figure 3 ) In this study, the data mining tool and algorithm like (J48, k-nearest, and Bayes) is used for selecting the training and test data, for classification, and K-means approach for clustering, single-dimension association rule to identify the best association. These results suggest that among the machine learning algorithm tested, multilayer perceptron classifier has the potential to significantly improve the conventional classification methods for use in medical data.
paper_402	This paper presents machine learning algorithms based on back-propagation neural network (BPNN) that employs sequential feature selection (SFS) for predicting the compressive strength of Ultra-High Performance Concrete (UHPC). The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. Several types of machine learning algorithms such as Artificial Neural Network (ANN) have been used in different fields for the development of models that predict response parameters (experimental dataset) using certain independent input parameters. Mainly, ANN was utilized to model the nonlinear behaviour of fatigue and creep of Reinforced Concrete (RC) members  [5] [6] [7] [8] . Recently, research interest has revolved around the development of ANN models to interpret the behaviour of structural materials such as steel, concrete, and composites  [9] [10] [11] [12] [13] [14] . The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete  [14] [15] [16] [17] . The evolution of UHPC has lead structural engineers to improve the compressive strength, ductility, and durability of heavy loaded reinforced concrete (RC) structures. Several researchers have been investigating the mechanical behaviour of UHPC and its applications over the last four decade, where it was founded that UHPC exhibits a compressive strength that would range from 150 to 810 MPa  [22, 23] . Other researchers proposed different mixtures by adding fly ash and sand to reduce the amount of cement and silica fume, and acquire an optimum mix that is both economical and sustainable  [25, 26] . However, most of the aforementioned mixtures result in exhausting a large amount of resources and performing tests on many batches, while barely predicting the strength of UHPC  [19] . The objective of the study was to develop an ANN and SMD model to predict both the compressive strength and the consistency of UHPC with two different types of curing systems (steam curing and wet curing). Despite the statistical advantages of ANN, it has been long regarded as a black box that evaluates functions using input covariates and yielding outputs. Therefore, ANN should be utilized in detecting the dominant input parameters that have direct association with the ANN model. This technique makes use of ANN's complex computation and allows the SFS tool to select and remove the influential and redundant parameters, respectively. As a result, the Rain Forest machine learning algorithm used with SFS showed promising results, where only three features were sufficient enough in predicting the most accurate results. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. The input neurons are responsible for containing the independent parameter presented by the user, the wires represent the randomly generated matrices called weights that manipulate the function's slope or steepness, the hidden neurons map the weights variables using an activation function, and the bias units control the output function's shift, either upward or downward. In this study, the previously verified ANN model was used as the objective function and the forward selection was used in selecting the relevant features. The ANN numerical solver, Levenberg-Marquardt, was verified by testing different number of neurons using a basis like the normalized mean square error (NMSE) to measure the error. The increment started from one neuron and ended with 15 neurons, where the model was analyzed 10 times, for each increment, because the Levenberg-Marquardt algorithm locates the local, and not the global, minimum of a function. The SFS algorithm was run 200 times to capture all possible combinations of independent features when using ANN. Based on the results of these trials, the most abundant combination during the SFS analysis, within a 20% threshold, was selected as the important parameters that contribute mostly in the model. The correlation plots between the predicted and experimental results for the ANN models, with and without selected features using SFS, are summarized in  Figure 4(a)  presents the percent deviation, where an arbitrary percent deviation was plotted above and below the perfect fit line with a deviation value of ±20%. As a result, the ANN model with the relevant features was capable of predicting 89.6% of its values within the aforementioned boundaries, as opposed to the ANN model with all the features which predicted 58.7% of its values within the boundaries. fc = θ1C + θ2SI + θ3FA + θ4W (2)     Since the developed LSR model is capable of accurately predicting the experimentally measured compressive strength, a parametric study was conducted, using this model, to study the effect of Fly Ash and Silica Fume on the compressive strength of UHPC. Using Fly Ash quantities that range between 0-200 kg/m 3 and Silica Fume quantities that range between 40-160 kg/m 3 while fixing the quantity of cement at 1400 kg/m 3 and water at 175 kg/m 3 , several plots showing the variation of strength of UHPC were generated as shown in  Figure 5 . The SFS tool was used to select the relevant constituent that impacted have the most impact on the compressive strength of UHPC which are mainly Cement, Sillica Fume, Flyash, and Water. It can be concluded from this study that: 1) The use of ANN with SFS reduced the number of input parameters needed to accurately predict the compressive strength of UHPC mix for the prediction of compressive strength, making it less computationally expensive.
paper_418	The ultimate objective of this study is therefore, to compare the row, column and overall means and variances of the Buys-Ballot table for additive and mixed models. Table 1 shows that the column variances of Buys-Ballot table is constant for additive model but depends on slope and seasonal effects for mixed model. Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model. Also, when there is no trend (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The additive model was adopted when the magnitude of the seasonal pattern does not change as the series goes up and down while the additive model was adopted when the magnitude of the seasonal pattern does not change as the series goes up and down  [5] . The emphasis is to compare the row, column and overall means and variances of the Buys-Ballot table for additive and mixed model when trend-cycle component of time series is linear. Iwueze and Akpanta  [6]  pointed out that an additive model is appropriate when the seasonal standard deviations show no appreciable increase or decrease relative to any increase or decrease in the seasonal means while a multiplicative model is usually appropriate when the seasonal standard deviations show appreciable increase/decrease relative to any increase or decrease in the seasonal means. Linde  [7]  observed that, the differences between the Additive and Multiplicative and the models are (i) for the additive model, the seasonal variation is independent of the absolute level of the time series, but it takes approximately the same magnitude each year while in the multiplicative model, the seasonal variation takes the same relative magnitude each year. Iwueze and Nwogu  [10]  observed that when the trend-cycle component is linear, the column variances of the Buys-Ballot table are constant for the additive model, but contain the seasonal component for the multiplicative model. However, this approach can only identify the additive model (when the column variance is constant), but does not tell the analyst the alternative model when the variance is not constant. The implication of this is that when the test for constant variance says the appropriate model for a study series is not the additive model; an analyst still faces the challenge of distinguishing between mixed model and the multiplicative model. For a series that has linear trend, the row, column and overall means and variances of the Buys-Ballot table for additive and mixed models are obtained by Iwueze and Nwogu  [14] , Nwogu el al  [15]  are given in  Table 1 . For additive and mixed models, 1 (a) The row means mimic the shape of the trending parameters and do not contain the seasonal effect for the additive model. 2 (a) The column means mimic the shape of the trending parameters and contain seasonal indices for additive model. (b) For mixed model, the column means mimic the shape of the trending curves of the original series and contain the seasonal indices. The row and overall variances contain both trending parameters and seasonal indices for additive and mixed models. The column variances of the Buys-Ballot table is constant for additive model, but a function of slope and seasonal indices for the mixed model. These properties of row, column and overall averages and variances of the Buys-Ballot table are what could be used for estimation and assessment of trend parameters, estimation and assessment of seasonal indices and choice of the appropriate for time series decomposition. That is when 0 b =  (Table 1)  For mixed model, we obtain using the expression in  Table  1  ( ) ( )  formula_8 when there is no trend. (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The expressing of a linear trend and seasonal indices for an additive model is given as    This paper has discussed the Buys-Ballot procedure for comparing the row, column and overall means and variances of the Buys-Ballot table for additive and mixed models in time series decomposition when trend-cycle component is linear. Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model. Also, when there is no trend ( ) 0 b = , the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models.
paper_428	The present article examines the influence of thermal radiation on two-dimensional incompressible magnetohydrodynamic (MHD) mixed convective heat transfer flow of Williamson fluid flowing past a porous wedge. In Non-Newtonian fluids, the most frequently encountered fluids are pseudoplastic fluids, and Navier-stokes equations alone are insufficient to describe the rheological properties of these fluids, therefore, to overcome this defect, several rheological model like Ellis model, Power law model, Carreaus model and Cross model are presented, but little attention has been compensated to the Williamson fluid model and estimated to explain the rheological properties of pseudoplastic fluids. Williamson analyzed the flow of pseudoplastic materials and presented model to described the behavior of pseudoplastic material and explain convenient importance of plastic flows, and also recognized that viscous flows are very varied from plastic flows  [1] . attained both numerical and analytical solutions of Williamson fluid transport through stretching surface subject to joule heating, and they observed that both methods have great argument with all parameters of flow  [7] . studied the peristaltic motion of non-Newtonian fluid through asymmetric channels along porous wall by means of various phase and amplitude, and also studied the manipulation of different wave structures on the fluid flow model  [8] . found numerically convergent solutions of two dimensional flows of non-Newtonian fluids along chemically reactive species  [11] . examined MHD motion of nanofluid owing to rotating disk with partial slip  [20] . Jabar addressed the influence of viscous dissipation and joule heating on MHD flow through a stratified sheet subjected to power Law Heat Flux having heat source  [22] . Reddy discussed unsteady MHD transport of rotating fluid past a permeable surface confined by infinite vertical permeable plate and concluded that by increasing rotating parameter the velocity field is also increased  [23] . Mixed convection is a coupled phantasm of two heat transfer mechanisms force convection and natural convection that act simultaneously to transfer heat in a fluid flow. analyzed mixed convection flow with irregular fluid properties through a vertical wavy plate  [33] . studied both forced and natural convection boundary layer transport by perpendicular surface in a stratified medium along connective boundary conditions  [34] . By utilizing Killer box technique numerical solutions of problem of mixed convection axisymmetric flow of air with variable physical properties was obtained by Ramarozara  [36] . Bau investigated the thermal convection in a saturated stratified medium bounded between two parallel eccentric cylinders with the help of a regular perturbation expansion along Daarcy-Rayleigh number; it was observed that the appropriate preference of eccentricity values can maximize the heat transfer inside annulus of various thermal insulators  [37] . investigated flow reversal of mixed convection in a three dimensional channel and concluded that an increase in Richardson number, natural convection dominates the flow and thermal field of combine convection  [39] . Kaya found nonsimilar solutions of steady laminar mixed convection heat transfer flow from a perpendicular cone in a porous medium with influence of radiation, conduction, interaction and having high porosity  [40] . studied unsteady combined convection flow in a cavity in presence of nanofluid  [41] . outlined mixed convection boundary layer flow influenced by thermo-diffusion  [42] . Chaudhary and Jain studied the impact of mass transfer, radiation and hall on MHD mixed convection flow of viscoelastic fluid in an infinite plate  [43] . Ferdows & Liu obtained the similarity solutions of mixed convection heat convey in parallel surface with internal heat production  [44] . Malleswaran & Sivasankaran carried an analysis for mixed convection flow and noticed that the average heat transfer decreases with an increase in Richardson number but in general heat transfer is better at force convection mode than free convection mode  [45] . studied boundary layer flow of fluid in a porous wedge subject to Newtonian heating along heat generation or absorption  [69] . Figure 3  displays the velocity profile for various values of the magnetic parameter M, the ratio of electromagnetic force to the viscous force that quantifies the intensity of applied magnetic field. The steady, incompressible two dimensional boundary layer flow of Williamson fluid past a porous wedge is analyzed numerically using the 5 th order Fehlberg technique.
paper_432	And found some results; The theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds, possibility study of Hamiltonian tubes when the simplistic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models. The Cotangent Bundle * is a smooth manifold itself, whose dimension is 2 . The total space of a cotangent bundles naturally has the structure of a symplectic manifold. We can introduce the Legendre transformation we need some basic facts about the structure of the cotangent bundles * of a nd dimensional differentiable manifold . When we take the basis vectors H =  The Cotangent Bundle Τ * contains the following classes of Lagrangian submanifolds; The fibers of Τ * . Let ∈ and let I; T { * Q → Τ * be the natural inclusion mapping. Then exp is locally trivial fibrations with fiber the integers †. [9]   Let (H, |,`, 0) be a locally trivial fiber space whose total space and base space are path -connected and & a pathconnected topological space. Proof: If such a lift ‹ exists, then diagram is Commutative. 1)  Figure 2 . A , principal bundles is a quintuple (0, , , R , where: 0 V →0 is a , right at action with the property of local triviality: Each point e ∈ has an open neighborhood for which there exists a , -diffeomorphism. [6]  Let be a Lie subgroup of a Iie group , and 3 a manifold on which acts. It is a smooth manifold; in fact × °→ / is the fiber bundles associated with the principal bundles → / via the H on S. The left multiplication commutes with the twist and descends to a smooth on G × °, namely n`. A tube for the action at ± is a −equivariant diffeomorphism from some twisted product × ° to onopen neighborhood of ±in , that maps [J, 0] H to ±. A slice theorem (or tube theorem) is a theorem guaranteeing the existence of a tube under certain conditions. In this part we study the symplectic geometry of cotangent -lifted action induced by a smooth proper action of a Lie group on a smooth manifold. Symplectic manifolds have their origin in the geometric for Hamilton ' s and Lagrange's equations of classical mechanics, where symmetries is the main tool that can be used to simplify the equations of motion. This model is known as the Hamiltonian tubes; it the basis of almost all the local studies concerning Hamiltonian of Lie groups on symplectic manifolds. In the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models. The symplectic reduction of the cotangent bundles * Q has more structure than a symplectic manifold. [17]   Let act freely and properly by cotangent lifts on * Q, and let Jbe the momentum map of the G action (with respect to the canonical symplectic form on * Q. Leta ³ : → / is projection. Then ∅isc , invariant surjective submersion and descends to a symplectic homeomorphic. The map ∅ is a sort of pushforward, though a ³ is not injective. Note that ∅ is "injective mod '', meaning that ∅(± 1 ) =∅(± 2 ) if and only if ± > =n. [12]   The first result of the theory in cotangent bundles reduction, the theory developed for the problem with a single or bit type playing an important role in the solution to the general problem of a singular cotangent bundles reduction for base manifolds, Hamiltonian tubes when the symplectic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models and the first work studying symplectic normal forms in the specific case of cotangent bundles. Conclude that the theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds and found that the phase space is the cotangent bundle T * Q of a configuration space .
paper_444	Resource constrained project scheduling problem is to make a schedule for minimizing of the completion time or total cost subject to precedence rules and resource constraints. To meet the manger goals, it is assumed that the increased quantities of resource are uncertain variables and the finish time of each activity is a decision variable. The goals of the model are to minimize the completion time and the total cost which composed by the activity cost and the additional resource cost. Furthermore, the equivalent form of the above model is given and its equivalence is proved. The resource constrained project scheduling problem (RCPSP) takes into account the balance of makespan and total cost through resource allocation and reasonable activity schedules while the precedence constrains between the activities and resource constrains are all satisfied. In real-word projects, there may be some uncertainty phenomena, for example, overdue materials, the decrease in the number of workers at Grain in Ear season, etc, which result in the resource availability may be changed. In this case, many scholars begin to consider the uncertain resource constrained project scheduling problem. Ying  [8]  proposed a schedule model of flexible work-hour constraint, in which the human resource was dealt with a new constraint to the classical RCPSP and the increased quantities of human resource were real-value variables. Xie  [9]  supposed that the increased quantities of resource were real-value variables, and built a multi-mode resource constrained project scheduling model for minimizing both the project makespan and cost. Chen  [11]  developed a project scheduling problem model under fuzzy resource constrained, of which the fuzzy duration time and fuzzy resource availability were represented by triangular fuzzy number. Liu  [13]  firstly established an uncertain project scheduling model, aiming to minimize the total cost under the constraint that the completion time does not exceed the deadline. Ji and Yao  [14]  recently considered the uncertainty of the duration times and the resources allocation times by assuming them are uncertain variables. Ma  [15]  considered resource constrained project scheduling problem with uncertain durations, and an uncertain excepted value model was built with the objective was to minimize the completion time. Up to now, we have not yet found uncertain resource availability constrained RCPSP in uncertain environment, which is not either randomness or fuzziness. In this paper, we consider uncertain resource availability project scheduling problem based on uncertainty theory, in which the increased quantities of resource are uncertain variables, and the finish time of each activity is a decision variable. Then, we build a multi-objective model which is under resource and precedence rule constrained to minimize the resource cost and the project completion time. In order to solve the problem of time-cost trade-off in project scheduling, we consider a project which is described as an activity-on-the-node network ( , ) , where = {1, 2, ⋯ , } is the set of activities and is the set of pairs of activities with precedence relations. A multi-objective programming model is built under the resource restricts and precedence rules of activities constrains in order to balance the completion time and the total cost of the project. For some time-intensive and heavy-duty projects, managers tend to be completed as quickly as possible and cost to be minimized. The uncertain resource constrained time-cost trade-off problem can be described as following optimization model: formula_0 In the above model, objective ○ 1 is to minimize the project total completion time; Objective ○ 2 is to minimize the expected project total cost which consists of the activity cost and the additional resource cost. Constraint ○ 3 declares that finish-start precedence relation among project activities. Constraint ○ 4 reflects that for any time # and each resource type , the demand for resources shall not exceed the total supply of resources with at least given confidence level < = . If the function Q(R A , R P , ⋯ , R ) ) is strictly increase with respect to R A , R P , ⋯ , R S and strictly decreaseing with respect to R STA , R STP , ⋯ , R ) , then formula_2 holds if and only if formula_3 To solve the model (1), according to the operational law of uncertain variables, we transformed it into its equivalent form, as shown below. formula_4 Proof: Because is an uncertain variable with regular uncertainty distribution Φ , and formula_5 by the definition of uncertain variable  [16] , we know that ? The manager tends to the demand of resource shall not exceed the total resource supply with at least given confidence level 0.9 for any time # and each resource type . The information of the activities. Cost  Preceding activity  1  0  0  0  0  2  1  5  1200  0  3  5  24  28800  2  4  27  16  103680 3  5  30  24  172800 3  6  22  24  126720 4  7  19  9  41040  4  8  24  6  34560  3  9  12  4  11520  8  10  22  20  105600 7  11  7  8  13440  6  12  20  8  38400  7  13  24  17  97920  5  14  12  6  17280  4  15  16  4  15360  7  16  11  6  15840  6  17  10  8  19200  14  18  8  6  11520  2  19  24  11  63360  18  20  13  4  12480  6  21  9  4  8640  12/15  22  3  4  2880  21  23  4  4  3840  7  24  0  0  0  22  With the above demand, we can present the following model:  In the real-life project, due to the influence of uncertain environment, managers should consider the trade-off between the completion time and cost. By describing the increased quantities of resource as uncertain variables, an uncertain resource constrained project scheduling problem is discussed in this paper. Then an uncertain optimal model was built with objective of minimizing the completion time and the cost with resource constrained based on uncertainty theory.
paper_462	To deepen the reform of clinical medical personnel training in an all-round way with the cooperation of medicine and education is the strategic adjustment direction of clinical postgraduate education in China. Through improving the conditions of enrollment, innovating the training mode, adjusting the award marks and improving the system guarantee, it has effectively constructed the professional degree education of master of clinical medicine and standardized resident training. Since the implementation of the new training mode, the quality of postgraduate training has been significantly improved, the employment rate of graduates has been steadily improved, the influence of the school has been expanded, and the experience for relevant units to carry out the reform of postgraduate training mode of clinical medicine master's degree has also been provided for reference. Since 2009, the school has been focusing on reforming the training mode of clinical master degree postgraduates (hereinafter referred to as clinical master). In view of the main problems existing in clinical master education, the school has carried out a series of reforms and explorations on the organic link between clinical master education and standardized resident training (hereinafter referred to as standard training). The reform has opened up a new path and realized the organic link between professional degree education and vocational qualification certification. To solve these problems, the Academic Degree Committee of the State Council officially launched the pilot work of clinical medicine professional degree in 1998. The development of postgraduate education of clinical medicine degree in China is faced with the characteristics of short time, complex training objects, multiple administrative departments involved in medical degree education, and diversified training channels  [2] . In the long-term practice of training clinical master, Chongqing Medical University has gradually explored a new "5+3" training mode which combines clinical master education with regular training, and completed the educational practice of "Innovation and Practice of the Training Mode of Clinical Master Professional Degree in China"  [3] . It has effectively solved the problems of imperfect training mode of clinical master, lack of mature training mode of high-level clinicians, low overall training quality of clinical master, inability of clinical master to apply for the qualification of practicing doctor, difficulty of effective connection with regular training, difficulty of seamless connection between professional degree education and industry admission standards, and inadaptability of current management system and mechanism to professional degree postgraduates. The reform of training mode for clinical master of Chongqing Medical University is premised on defining training objectives, based on innovative training mode, with improving training quality as the core and linking up with professional qualification as the grasp  [4] . The system reform of training mode for clinical master of Medical University has effectively solved the problem of insufficient practical ability of clinical master, and has realized the degree education and professional qualification recognition of clinical medicine specialty. The school accurately grasps the law of professional degree postgraduate education, changes the concept of professional degree postgraduate education, and orientates the training purpose of clinical master as "training doctors who can really see a doctor", aiming to improve the clinical practice ability of postgraduates as the main objective, and is in line with the training goal, thus laying a solid foundation for the organic connection between the two. Thirdly, we allow our clinical master not to take the entrance examination, but to be directly incorporated into the standardized resident training system, so that they can participate in the standardized resident training. After completing the dissertation defense and meeting the award requirements, the graduate certificate and master's professional degree certificate can be obtained. "Four syndromes in one" has greatly saved the resources of education and training, and provided qualifications guarantee for the connection of clinical medical professional degree education and vocational qualification certification. In order to effectively improve the quality of clinical master training and strengthen the management of clinical rotation process, the school has set up postgraduate management offices in clinical colleges, implemented the system of professional degree tutorial group, and established three-level management systems of schools (graduate schools), departments (graduate management offices) and clinical departments (tutorial groups)  [6] . The Graduate Management Department of the Department organically integrates the training of clinical master's degree, the training of seven-year students' master's degree, and the application of resident doctors for master's degree related to the work of degree award, and arranges the clinical training and clinical competence assessment of clinical master's degree as a whole. The tutorial responsibility system is applied in postgraduate training, but after clinical master's enters the clinical rotation, many times are not in the Department where the tutor is located, and there is a problem that no one manages the professional degree students when they rotate in the clinical departments outside the department  [7] . The current single tutorial responsibility system can no longer meet the requirements of clinical rotation training, which is not conducive to the management and supervision of postgraduates. Each clinical rotating Department of postgraduate clinical rotation appoints a teacher qualified as a professional master's tutor as rotating responsibility tutor to ensure that clinical master's is in clinical rotation. In order to consolidate the reform results of the training mode of clinical master, gradually standardize the management system and continuously improve the quality of training, the school continuously improves the rules and systems around the education of clinical master, covering the methods of re-examination admission, the quantitative assessment of the selection of tutors, the training program, the tutor group system, the curriculum, the professional course examination, the assessment of clinical ability, the regulation of publishing papers, the management of research funds and institutions  [8] . In terms of setting up, a complete system of management rules for clinical master's degree has been established, and the experience of reform has been standardized and institutionalized, thus forming a long-term mechanism for linking professional degree education with professional qualification certification. The staff of the Graduate Management Department of the school conducted a thorough investigation on the effect, problems and suggestions of the training mode of clinical master's degree in various clinical departments, and held dozens of meetings to solve common problems. Through the above measures, the problem of low enthusiasm of faculties and tutors in guiding graduate students with professional degrees has been solved, and the construction of professional degree tutors has been accelerated. In view of the fact that the subsidy of clinical master is much lower than that of regular trainees during the clinical rotation period, the school has made many investigations and repeated demonstrations to improve the treatment of Postgraduates during the clinical rotation period and to improve the enthusiasm of students in clinical training  [10] . Aiming at the characteristics of professional degree postgraduate education and adapting to the innovation of its training mode, the school has established a funding system different from academic degree postgraduates, and continuously improves the standards and coverage of grants. Postgraduates with licensed doctor qualification and independent bed management are awarded 500 yuan in schools and hospitals, 500-1000 yuan in departments and tutors, and 2000-3000 yuan in living allowance per month for each clinical master. Therefore, the school has improved the conditions for applying for clinical master's examination, so that it is consistent with the admission conditions of residents' standardized training: undergraduate majors should be clinical medicine, full-time national education series undergraduate graduates, to obtain bachelor's degree. Thirdly, according to the requirements of standardized resident training, and with the cooperation of training bases, starting from medical ethics, medical ethics, laws and regulations, professional ethics and basic clinical skills, we will offer lectures on medical law, applied psychology, humanistic literacy and doctor-patient communication to comprehensively improve the comprehensive quality of clinical master. System The secondary schools awarded professional degrees in clinical medicine in our university are all the standardized training bases for residents in Chongqing. The rotation requirement not only meets the requirements of the state for clinical master, but also closely combines with the regular training. The main reason for this phenomenon is that since the trial implementation of clinical medical professional degree, the state has not evaluated the quality of the pilot units and lacked an effective monitoring mechanism. The school has established a multi-level and whole-process clinical competence assessment system, which is suitable for clinical master's clinical competence assessment and regular training and graduation assessment. The Interim Measures for the Implementation of the Regulations on Academic Degrees of the People's Republic of China promulgated in 1981 stipulates that before the postgraduate's reply, the degree-granting unit shall employ one or two experts from the disciplines concerned with the thesis to review the thesis. The clinical master emphasizes on examining the clinical competence of postgraduates. Before graduation, clinical masters not only have to defend their dissertations, but also have to pass strict clinical competence assessment, which increases the workload of students. The school regularly carries out the training of tutors and managers, explaining in detail the strategic development direction of national postgraduate education and the related policies of clinical master's education in our school. The staff of the Graduate Management Department of the school conducted a thorough investigation on the effect, problems and suggestions of the reform of the training mode of clinical master in various clinical colleges and departments, and organized a meeting to solve some common problems. There are three difficult problems in the training process of master's degree postgraduates of clinical medicine specialty: first, the graduates of clinical medicine specialty with bachelor's degree must work in medical institutions for one year before they can apply for the qualification of practicing physician; second, the graduates with bachelor's degree can't obtain the qualification certificate of practicing physician and can't carry out clinical training; third, some of the graduates with bachelor's degree must work in medical institutions for one year. Qualification certificate students are not allowed to practise medicine in other places in accordance with the Law of Licensed Physicians, so it is difficult to carry out clinical training  [14] . The school has thoroughly reformed the training mode of clinical master, organically docked school education with post-graduation education and lifelong education, promoted the seamless docking of professional degree education with industry access standards, accelerated the internationalization process of higher medical personnel training mode, and formed a new systematic training mode for clinical doctors, which has great application value in the whole country. In view of the characteristics of professional degree postgraduate education, the school grasps the development trend of postgraduate education, and closely combines the admission criteria with the industry admission criteria in all aspects of enrollment, cultivation and award of posts, which ensures the complete docking of personnel training and qualifications, and provides mature experience for the seamless docking of professional degree education and Industry admission criteria in China. The school has continuously innovated the training mechanism and formed a "modular" curriculum system and a quantitative assessment system for clinical competence. Since 2009, the school began to explore the reform of the training mode of clinical master in an all-round way. It has basically realized the seamless connection between the degree education of clinical medicine specialty and the certification of professional qualification. In the 2011 "Forum on Reform and Development of Medical Education", the principal of the school presented the reform experience to the conference and won the unanimous praise of the broad masses of colleagues. The leaders of the Ministry of Education, the Health Planning Commission and more than 400 persons in charge of training units participated in the symposium, which greatly promoted the reform and development of clinical Master's training in China, and played a tremendous leading role in promoting the reform of the training model of clinical Master in the western region and even in the whole country. The article "Construction and Practice of Quality Assurance System for Master of Clinical Medicine Degree Postgraduates" published in "Degree and Graduate Education" won the second prize for Excellent Thesis of "Degree and Graduate Education", and it is also the only prize-winning thesis in the field of medical science. Since the implementation of the reform of the training mode of clinical master, the combination of degree education and vocational education in our school has become closer. Professor Jin Xianqing and Professor Xie Peng were appointed members of the National Medical Graduate Education Steering Committee in 1998 and 2010 respectively. The school assists in formulating the standardized training policy for general practitioners and residents in Chongqing, the construction of bases, the training and assessment system, and teacher training. The "5 + 3" reform of clinical medicine master's degree in Chongqing Medical University has established a "one goal", that is, to train "doctors who really can see a doctor", and established the basic principles of the reform of training mode of clinical medicine master's degree and the training of clinicians. It has effectively solved the problems of imperfect training mode of clinical master, lack of mature training mode of high-level clinicians, low overall training quality of clinical master, inability to apply for the qualification of licensed physician, difficulty in effectively linking up with regular training, and difficulty in seamlessly linking professional degree education with industry access standards. Chongqing Medical University combines many years of clinical master's education practice, highlights its own characteristics, gives full play to the advantages of running a school, and strives to solve the problem of linking professional degree education with vocational qualification certification. The unit pays attention to the degree education of clinical medicine specialty, strengthens the exchange study, improves the quality of medical higher education in our country, realizes the seamless connection between degree education of clinical medicine specialty and professional qualification certification, and explores the new mode of integrating medical education of our country with international practice.
paper_476	The results show that: (1) The cost and resources operational efficiency are closely related to the order splitting ratio under the same delivery frequency; (2) The delivery frequency has different effects on the operational efficiency of the resources in different links. (3) Through the proper splitting and loading of orders, staff scheduling, and area adjustment of delivery station, the optimal delivery frequency can be achieved under the balance of cost and resource operational efficiency. In order to reduce the operating costs of logistics enterprises on the basis of ensuring service levels, one should first properly allocate the amount of orders and splitting ratios to achieve an economic increase in the delivery frequency. As the volume of orders and consumers' demand of rapid delivery services increasing, express companies are required to increase the daily delivery frequency to cope with the pressure of delivery during peak period and meet the consumers' needs effectively. The order volume, the orders' splitting ratio, the operational efficiency of delivery resource and the delivery costs will affect the delivery frequency. System dynamics models not only offer a different Operational Efficiency: A Case Study of Jingdong Logistics perspective while whole system approach to transport planning, but also demonstrate to policy makers the importance of these feedbacks and lagged responses  [1] . At present, the researches on delivery frequency, resource operation efficiency and cost utilization were mainly focusing on the following two aspects: (1) Delivery efficiency improvement through the choice of delivery model, And (2) Delivery link optimization. That research explored three scenarios, which are joint delivery, autonomous delivery, and third-party delivery, which also pointed out that, in order to improve delivery efficiency, enterprises should adequately consider relevant factors such as own resources, competitors' delivery strategies, and urban transport policies before determining delivery methods. That research took Lyon in France as an example with the using of radar map to visually show CO2 emissions, risk values, delivery costs, traffic impact and delivery time of joint delivery under different scenarios. Wang Wei  [17]  analyzed the advantages and disadvantages of self-operated, outsourced, and federated modes in ecommerce delivery model, and constructed a system dynamics model for the combination selection of delivery modes by selecting six core factors, which can provide decision-making advice for managers to improve the delivery efficiency. Barla  [18]  used system dynamics to simulate the links of orders, production, and inventory for apparel delivery companies, and considered how to adjust inventory levels to increase the delivery systems' efficiency and economy when demand fluctuates significantly. Hongtao Yang and Jianbang Du  [19] [20] [21]  uses system dynamics to analyze the problems of supply chain and delivery efficiency from the perspectives of transportation routes, drivers and cooperation agreements, and its sensitivity analysis provides some reference to this paper for the corporate in this case to improve the delivery efficiency. Lin Wanting [22]  considers the phenomenon of unfilled orders, information leakage, and product damage which exists in crowdsourcing delivery, then built system dynamics model from five dimensions of reliability, responsiveness, assurance, empathy and security, which provide decision-making advice for enterprise delivery efficiency management. Next, it uses the simulation model to study the effect of changes in delivery frequency on delivery costs and resource operation efficiency under different quantity of delivery orders. Next, it analyzes the interaction between delivery frequency, cost, and resource efficiency, and builds dynamics simulation model to get the equilibrium of cost and resource operation efficiency under different delivery frequency. Based on field surveys of JDL delivery systems and interviews with operational personnel, on a route daily business JDL delivers orders within the region based on a fixed frequency. According to investigation results, the cost of storage and ferry in JDL only accounted for about 8% of the average cost in daily delivery. The variable costs occurred due to the volume of orders, which include operating cost of the sorting equipment, rental cost of the site, units fixed cost of transport vehicle, fuel costs, toll and so on. In addition, the number of working facilities and the operating time are affected by factors such as order quantity, delivery frequency, sorting equipment efficiency, and unit load of transport vehicles. The number of sorting staffs is influenced by factors such as the amount of cargo, the number of sorting equipment, the worker's efficiency, the sorting time requirements, the area of sorting venues, the number of logistics personnel in the yard, the delivery frequency and so on. This article considers the utilization efficiency for the facilities, personnel, and technology such as turnover rate of transportation vehicles, the utilization rate of delivery site, and the delivery personnel loading rate, etc., which are put into the sorting, transportation, and terminal delivery links. The increase in the delivery frequency will result in the following two changes: first, the increase in consumer satisfaction; second, the corresponding changes in the cost and resources operational efficiency. The first change may prompt the increase of customer orders and corporate income Therefore the company will increase the investment in delivery facilities and equipment, and thus increase the delivery frequency and service capabilities. As for the latter, given JDL's current batch-by-batch delivery mode, the increase in delivery frequency will reduce the fixed cost allocated to each delivery operation. At the same time, the factors such as sorting time requirements, proportion of per-order batch, efficiency of the delivery personnel, and unit fuel consumption of the vehicle, will have a direct impact on the delivery frequency. Among them, the positive loop represents that there is a mutually reinforcing relationship between the factors, while the negative loop indicates that there is a balanced relationship between factors (Such as: Delivery frequency --> + Consumer demand response ability --> + Impact of demand response capacity --> + Consumer satisfaction --> + Order quantity due to change in satisfaction --> + Total daily order quantity --> + Per batch delivery order quantity-->+ Sorting center cargo volume-->+ Number of on-site logistics personnel--> -Onsite logistics personnel operating time-->+On-site logistics personnel costs-->+ Sorting personnel costs-->+ Sorting costs-->+ Total cost --> Total profit --> + Delivery facility input --> + Delivery frequency). Since the current order quantity of each batch is mainly determined by the consumer's shopping habits, which cannot be arbitrarily changed, this paper splits the orders of the first batch (60% of the total orders). Therefore, the simulation calculates the utilization rate of the largest batch after splitting when considering the operational efficiency of resources. In scenarios 2 and 3, when the order quantity was lower than 3,500, the differences between the total delivery costs in Operational Efficiency: A Case Study of Jingdong Logistics different scenarios were very small; when the order quantity was higher than 3,500, the increased rate of cost in scenario 3 was higher for a while than that in scenario 2. The reason is that in scenario 3 the sorting center dispatched more vehicles in advance, which increased the cost of transportation. After the order volume reached 5,100, the total delivery cost in scenario 4 showed a downward trend and reached the minimum. Within the orders volume ruled in this paper, the delivery frequency and the cumulative delivery costs are positively correlated. The four scenarios showed a lower average unit cost when the order quantity was around 4,700 units indicating that this order quantity was a batch of economic orders. Due to the increase in the delivery frequency, the unit average time required to complete the sorting operation became shorter while the number of sorters and the salary of personnel increased. This is because that under the current total delivery volume, the order quantity after splitting in the scenario 4 was higher. The difference of average unit sorting costs among scenarios 1, 2 and 4 decreased as the order increased, which is because the change in the sorting costs under different order quantities was small and the difference in the delivery order quantity increased. Due to the splitting of order, the transportation costs of scenarios 2 and 4 remained unchanged when the order volume was small. When the volume of orders increased to a certain value, the transportation cost of scenario 1 became the highest among the three scenarios. In scenarios 2 and 4 the orders can be met by just increasing the frequency of existing vehicles, however, in scenario 1, new vehicles were needed to meet the transportation needs, which resulted in higher transportation costs. In the aspect of unit average transportation cost, similar trends were also emerged among the scenarios mentioned above. The main reason is that the increase in the delivery frequency improved the demand for the number of delivery personnel and caused the delivery time of delivery personnel to be shorter in a single batch. In terms of unit average terminal delivery cost, all three scenarios have shown a tendency of fluctuating decline, and some of the rises are due to the increase in the costs of delivery personnel and facilities. The difference between the vehicle utilization efficiency in scenarios 2 and 3 increased with order volume until it reached 3600 units. The vehicle utilization efficiency for the three sorting centers in scenario 4 increased with the order volume since only one transportation vehicle was required to be dispatched from each sorting center within the current order volume. This is because the order volume was so small in scenario 4 that in the unit batch delivery personnel was working inefficiently. In addition, scenario 4 can effectively ease the utilization of area and personnel in the delivery site. With the same delivery frequency, the orders splitting ratio influences delivery cost and resource utilization efficiency significantly. Recommendation I: one should properly allocate the amount of orders and splitting ratios to achieve an economic increase in the delivery frequency. With the same delivery frequency, different order splitting ratios affect delivery cost and resource operational efficiency. Therefore, JDL needs to consider the increase in delivery frequency, the increase in delivery costs, the overloading of resources, and the ratio of orders and splits. In addition, the average split of order quantity can effectively reduce the tension of delivery resources. When the delivery frequency is increased from 3 times per day to 4 times per day, scenario 3 should be adopted from the perspective of increasing consumer satisfaction; scenario 2 should be adopted when the area of delivery stations and the delivery personnel are tight. Since the average personnel utilization efficiencies of delivery both stations A and B are lower than 60%, the delivery personnel can be reasonably scheduled to improve the utilization efficiency and consumer satisfaction. In addition, the area utilization efficiency of delivery stations may exceed 100% under large orders. Therefore, according to the actual situation, it is necessary to increase the delivery frequency or expand the area of delivery sites.
paper_479	Handwriting is also a key element in document examination as it leaves a forensic document examiner with the task of determining who the writer of a particular document is and this is achieved through the likelihood ratio (L R) paradigm. Inability to model an individual's handwriting over time has made estimating a full likelihood ratio for comparative handwriting analysis impossible thereby employing nuisance parameters and subjectivity in computation of L R that is not full. This research employed back propagation neural network (BPNN) to model the writing pattern of individuals with input layer as the features of handwriting characters, two hidden layers of three neurons each, activation function sigmoid (s) and an output handwriting. With the help of handwriting model for individual writers, little or no assumptions and no nuisance parameters were employed in achieving full likelihood ratio for comparative handwriting analysis in forensic science. From the research carried out, it can be concluded that modeling an individual's handwriting is a crucial factor in achieving a full likelihood ratio, little/or no inconclusiveness in result reporting and a less degree of disagreements for handwriting identification in a forensic environment. The likelihood ratio paradigm has been studied as a means for quantifying the strength of evidence for a variety of forensic evidence types in handwriting and other types of forensic evidence such as earmark, speech, footprint, fingerprint, glass fragments and DNA  [1] [2] [3] [4] [5] [6] [7] [8] . It is straightforward for a DNA as compared to other areas of forensic evidence because estimation is done for the relative frequency of different DNA profile in relevant populations. For the handwriting evidence, previous literatures reported many factors and the inability of the forensic examiner to model the writing profile of a writer because it is often believed that an individual's writing profile is a latent characteristic that cannot be observed directly, and is not easily modeled because an individual's writing profile cannot have static characteristics and may change over time  [9] [10] [11]  which has resulted to adopt ad-hoc methods to compute likelihood  [9, 12, 13]  when full likelihood ratio is not achievable. The novelty of our proposed implementation relies on natural handwriting samples over a period of six months from known individuals to form the database to model the writing profile for each writer. The probability proportion has been used as a way for measuring the power of confirmation for various legal proof sorts. formula_0 Formally, posterior belief in favour of H p (for example the defendant is guilty) with fair facts are used to make a final decision on the evidence. The forensic examiner provides a summary of the evidence needed for belief based on the evidence and the fact that prior beliefs about H p and H d have been quantified. Likelihood ratio = Posterior ratio / prior ratio Likelihood ratio according to  [14]  are increasingly being adopted to convey expert evaluative opinions to courts. In the absence of appropriate databases, many of these likelihood ratios will include verbal rather than numerical estimates of the support offered by the analysis. The problem of estimating a L R for handwriting has proven to be a non trival task due to the inability to model the writing pattern of an individual and due to the absence of a large database of handwriting and other factors. [17]  based their approach on distribution of measurements from comparing items but their approach was similar to that of marginal L R when full L R was not an option due to the presence of nuisance parameter. In statistics, a nuisance parameter is defined as: ''A parameter of a model where there is no scientific intrigue except for whose qualities are normally required (yet when all is said in done are obscure) to make deductions about those parameters which are of such intrigue [25].'' [19]  described a statistical model for the writer verification task to determine if two documents were written by the same writer, their model followed the scenario proposed by  [17]  thus suffers from the same weakness  [20]  presented one conceivable way to deal with gauge a probability proportion in near handwriting analysis was delineated. The oddity of the methodology depends on producing reproduced composing samples from an accumulation of composing samples acquired from a realized source to frame a database for evaluating the dissemination related with the numerator of a L R . In a related work, similar examination of probability proportion based proof appraisal strategies in both evaluative and analytical procedures was carried out using a sample collected from female and male author. While the utilization of probability proportions in the previous circumstance is currently rather entrenched spotlight on the insightful setting still remains rather past contemplations by and by. This paper features that investigative setting still remains rather past contemplations practically speaking; it is also attested that L R can be useful for analytical procedures bolstered through various simulations  [21] . Assessment of signature handwriting evidence through score-based likelihood ratio based on comparative measurement of relevant dynamic features was carried out by  [22, 9]  presented score-based approaches to calculating forensic probability ratios that have been established as becoming more progressively common in forensic literature. This method is genuinely direct for the score-based L R numerator, which involves creating a list of scores obtained by combining proof objects from the same source. Nonetheless, this method creates uncertainty for the generation of denominator databases -in general, how is the best way to produce a list of scores among two items from different source. The work discussed the reasons for the authors' opinion that interval quantifications for the value of evidence should not be used directly in the Bayesian decision making process to determine the support of the evidence for one of the two competing hypotheses. [24] In a related manner, statistical problem and pitfalls identifiable with forensic likelihood ratio were identified. The feature information extracted from student handwriting through LBP was entered in NN through input layer and the participation of each class of character variables is calculated by the hidden layer of the network using: formula_1 net j is ith type of factor, n is the number of factors in net i , & ' ! Weight from the constant 1 and two hidden unit j is #$",*"  formula_2 After randomly selecting the weights of the network, the backpropagation algorithm is used to measure the necessary corrections. The algorithm can be broken down in the following four steps: Feed-forward computation Backpropagation to the output layer Backpropagation to the hidden layer Weight updates The algorithm is stopped when the value of the error function has become sufficiently small. Due to the complexity of modeling the handwriting of a writer and the absence of industrial size databases from which different handwriting can be described  [17, 9]  estimated a marginal L R when the full L R was not possible also in the presence of some parameters considered to be nuisance. The developed writing model for each writer is one of the criteria to eliminate the presence of nuisance parameters when estimating a full L R . An inconclusive state is declared if and only if L R value = 1 With the BPNN algorithm to model handwriting pattern for each writer and L R estimation described in  [10]  this paper was able to deal with the problem of inconclusiveness as there was no inconclusive results reported in the investigation, determine who the writer of a questioned document is as well as eliminate the inclusion of parameters considered nuisance in investigation because each writer's handwriting profile could be ascertained due to the BPNN training of the Handwriting categories. Several factors have to be put in place to estimate a full L R for forensic handwriting investigations. This research shows that modeling handwriting pattern for a writer is an important factor in achieving a full L R which will eliminate the use of nuisance parameters in the computation process, lower the disagreement rate with respect to the Hypothesis in support of the prosecutor and also produce no inconclusive result after estimation of L R .
paper_492	Based on known functions of neuroscience the neural network that performs serial parallel conversion and its inverse transformation is presented. The activity of the upper neural networks changes in response to the context structure inherent in the time series data and have both function of accepting and generating of general time series data. Eating behavior in animals in the early stages of evolution is also processing time series data, and it is possible to predict behavior although be limited short term by learning the contextual structure inherent in time series data. Although status of nervous system of the animal change according to the recognition by sensory organ and to the manipulation of the object by muscle in the vicinity of the animal itself, the evolved animals have in addition another nervous system so-called long-term memory or episodic memory being involved experience and prediction. By the workings of long-term memory, lot of information are exchanged between fellows, and lot of time series data are conserved by characters in human society. Furthermore retroactively, although there is a quantitative difference in each part between brain of ape who does not speak the language and our brain, but our brain is consisted of same material. In other words, the nervous system of animals called advanced higher animals is locally same as very primitive animal's nervous system. By hierarchical connection of the circuits, it is possible to accept and generate general time series data. As an example, a neural network that can accept and generate multi step time series data such as indispensable for eating behavior is presented. The nervous system related to short-term memory is activated in synchronization with environmental events, but the nervous system involved in long-term memory is highly layered to form an image corresponding to past and future events. Even bacteria like animals in the early stages of evolution must have some eating behavior such as moving relying on light and smell to search for food and determine whether they can be eaten. Whether the eating behavior is evaluated as intellectual behavior aside, it is possible to mimic this degree of behavior by electronic work at the junior high school level by combining sensors and logic ICs. The eating behavior of animals evolved from the aforementioned animals is composed of time series of actions such as extending arms toward the target, opening the palm of hand when approaching the target, closing the palm to grasp the target and bringing to the mouth. The recognition technology of the figure has evolved by the neural network which starts with the perceptron, but it is forced to judge by the relation of the part and the whole as in the example of kanji when the scale of the figure increases. It is desirable that the behavior of any neural network system can be expressed by combinations of the simple action parts of animals in the early stages of evolution. In this chapter, it is presented that arbitrary time series data can be divide into basic sequences as the logical basis of neural networks. Next, by providing a two-way function to the neural network both serial parallel conversion and vice versa on basic sequences is realized. Any time series data consisted finite type of element can be divided into multiple subsequences where the same element does not appear more than once. The neural circuit corresponding to each basic subsequence can be easily configured in the neural network, leading to the realization of processing by hardware of general time series data. For simplicity, assume time series data consisted of 10 type elements from a 0 to a 9 . The output of the element holding the activity when receiving the last c 4 of the time series data is the recognition result of the time series data c 0 c 1 c 2 c 3 c 4 . Therefore, the couplings between the elements are enhanced (Hebb rule) by repeating this conversion, as a result the elements involved in the conversion will be activated by only receiving the first element of the time series data. On the flow direction of data, this parallel to serial conversion is upside down with the serial to parallel conversion described above, but the basis of neural network operation is the same. In the spontaneous operation (here by voluntary muscle) performed reactions such as changes of weight feelings from muscle or joint are accompanied by. The essence of the neural network which performs the parallel serial conversion and the neural network which performs the serial parallel conversion is the same except that the conversion result comes out downward or upward. On other words, both operations of the serial parallel conversion and the reverse conversion are essentially the same on the point both operations are triggered by the first data and proceed waiting for input state change. Since general time series data is consisted of hierarchy of basic subsequences, basic units can process general time series data by identifying the outputs of lower layer basic units as the new time series data. Animal's behavior is considered a time series data consisted of the data that is couple of behavior data and received data. The example shown at the beginning of this chapter is also a time series data that is consisted of extending arms, opening the palm, closing the palm to grasp and so on. The time series data handled in the first 2 steps are consists of various stimuli of the internal and endocrine system, as well as data that captured by sensory organ from the environment in which the animal is placed. If the state of the unit becomes active (hungry), it has been shown to activate one after another the lower layer units that achieves eating behavior in the rest steps. After time sequence data learning, animals are possible to quickly start the operation that continues only by receiving the beginning of the time series data to be corresponding a slight sign. As neural circuits evolve, the movement of each element is captured and accumulated as a context, and it becomes possible to adapt to changes in the new environment. And in the nervous system, the eating behavior shown above will be nothing more than a brief occurrence that appears in a constantly continuing life. The reason why drawn a hierarchy further in the upper part of the  Figure 4 , is to suggest existence of the nervous system that processes intellectual judgment. In brief, images were advantageous even if an organism were not conscious of the images formed within it. The organism would not yet be capable of subjectivity and would be unable to inspect the images in its own mind, execution of a movement; the movement would be more precise in terms of its target and succeed rather than fail. Animals with some evolved sensory organs must recognize, for example, environmental changes from sunrise to sunset as repetitions of a time series data. Imitating is the basis of fellow empathy and group behavior, and will develop to ethical emotions such as mercy and encouragement  [4] . Human beings have adopted language as a means of exchanging information between peers, and began to exchange vast amounts of time series data. The object to be drawn is the change of all things, the joys and sorrows of livings, and their hope for the coming future. And you'll think of the shape of the cake and the action of lifting the chocolate plate and mouthing it. On your birthday day, you may want to see the cake in front of you, identify the cake from store A, which was the one that was the subject of a conversation with your family the day before, and worry about the difference from the previous day's expectations. Because the episode about the cake is remembered, communication is possible and feelings are transmitted. One is time series data based on visual information of the cake in front of you and muscle movements that manipulate chocolate plates, which is produced by the nervous system that animals have from the early stages of evolution. The other is generated by the nervous system that is called " image" by Damasio, is time series data based on the shape of cake produced from family conversations the day before. While the behavior of the former nervous system is reflection of the visual information and movement of the objects nearby, the latter nervous system not only no needs to be synchronized with the former, but also moves independently. However, if the nervous system of the episodic memory is activated by remembering the A shop and the chocolate plate while looking at the cake, the difference between reality and the expectation might become a problem. Stimulus from the sensory organs from the bottom becomes time series data and is transmitted to the upper part. The part shown in red is a part that is particularly activated, the lower red disk is the part activated by the visual data of the chocolate plate that placed in front, the upper red disk is the part that is activating by recalled episode about the chocolate plate. The connection between the two sides is enhanced (Hebb rule) as indicated by a bold line, and visual information is copied to the top to reinforce the episodic memory. When studying objects that are intertwined with portion and whole system such as the nervous system, the idea of category theory can be incorporated to develop the whole without focusing on the details of the object. [  formula_0 The composite of the coupling is considered to be a hierarchical connection of the objects, and the identity, which is considered a special morphism, can correspond long axons extending beyond hierarchies. And the time series data brought by other brain with highly hierarchically connected structure activities is concerning long-term memory. From an engineering point of view, the basic unit can be realized by a small microprocessor, and the neural network consists of multiple randomly connected units. It can be said that the essence of the logic of the operation exists not in the basic unit, but in the connecting situation among basic units selected from randomly initialized connection. There may be cases of errors in the accuracy of the operation compared to the circuit using the existing logic IC because there is a probabilistic part, but the bud of a new strategy might be hidden in the vicinity of the malfunction. From the viewpoint of neuroscience, even if there are no "parts" equivalent to the basic unit in the process of stimulation from the sensory organs propagating to the cerebral cortex, axons parallel to the propagation direction connect between the layers, and it can be said that it is a passage of serial parallel mutually converted information. On the other hand, it is considered that the axon perpendicular to the propagation direction is a connection such as causing self-oscillation related to the generation of serial data.
paper_507	Landslide is defined as a slow to rapid downward movement of instable rock and debris masses under the action of gravity. Landslides are one of the major natural hazards that account for hundreds of lives besides enormous damage to properties and blocking the communication links every year. The main aim of present study is to integrate the result of our study with spatial data, soil parameters, land inventory and used the output as a user friendly application using GIS which could predict the future susceptibility of region to landslide and% contribution of each factor for the same. A frequently used definition of landslide is "movement of mass of rock, earth or debris down a slope" in the words of Cruden  [1] . They are the catastrophic phenomenon taken lives of many a hundred and destroyed the hard earned money, disrupting the communication facilities. As per the official figures of United Nations International strategy of disaster reduction (UN/ISDR) and Centre for Research on Epidemiology of Disasters CRED for the year 2006, landslide ranked 3 rd in terms of number of death among the top ten natural disasters  [2] . As far as Indian scenario is concerned approximately 0.49 million km  2  or 15% of land area of country is vulnerable to landslide hazard and 80% is spread over Himalayas, Nilgiris, Ranchi plateau and eastern ghats  (GSI 2006)    [3] . Uttarakhand is an appropriate choice for the study since the newly developed state has been fighting with the catastrophe and is making front paper headlines for landslide at Vishnuprayag, Baldora, Lambagharchatti, Jharkula, phatabyung, and Amiya landslides  [4] . The traditional practice of Landslide prevention is enabling people with Landslide Hazard Zonation Maps. These map divides the land into homogeneous areas or domain and their ranking according to degree of actual hazard potential caused by mass movement  (Guzzetti et. al, 1999 and varnes 1984)    [7, 8, 9] . In the present study with the help of satellite imageries like DEM from cartosat-1 and topographic maps from GSI (geographical survey of India) a susceptibility map is prepared. Preparation of a landslide hazard zonation map that would divide Uttarkashi into different zones depending upon the factors. Uttarkashi falls under the physiographic division (s) Rohilkhand plains, Nepal Himalayas, Ganga, Yamuna daob, Siwalik range, Kumaun Himalaya, Dhaoladhar range. Therefore, the relation between landslide occurrence and the conditioning parameters used is crucially important for landslide susceptibility mapping. Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. Four control points were selected at the corner of the concerned points, the geo-referencing of these coordinates was done by finding the coordinates from the Google Earth. The factors in relevance to the landslide susceptibility analysis of Uttarkashi are:             The data that has been acquired is raw and is to be converted into the software readable form to be used in the susceptibility analysis. Digitization for the various shape files has then being done by retrieving the concerned shape file and the map form which digitization has to be done. Both the shape file and the map being retrieved in Arc Map, the map is zoomed to a comfortable level such that all features on the map could be easily traced out on the screen itself to create new layers or themes. The shape files created using the raw data are:            An artificial neural network is a "computational mechanism able to acquire, represent, and compute a mapping from one multivariate space of information to another, given a set of data representing that mapping". The purpose of an artificial neural network is to build a model of the data-generating process, so that the network can generalize and predict outputs from inputs that it has not previously seen  [11, 12] . An artificial neural network "learns" by adjusting the weights between the neurons in response to the errors between the actual output values and the target output values. In the present study we selected 107 points and all the six factors namely soil depth, soil type, rock type, land cover, slope and elevation and a excel database is created. The dataset is categorized into 60% training and 40% validation. The data is categorized into training, testing and validation, and all the six factors namely soil depth, soil type, rock and a excel database is created. The dataset is categorized into 60% training and 40% validation. Since ANN does not understand the 'landslide prone' and 'non-landslide prone' region we explain it by giving value '1' and '0' respectively. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer (20), and between the hidden layer (20) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. Precipitation The next objective of our study was to present the weightage of various factors causing landslide. The dataset is categorized into 60% training and 40% validation. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer  (15) , and between the hidden layer (15) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). Other values for landslide susceptibility for the adjoining areas have been calculated using interpolation technique therefore the Rishikesh-Uttarkashi-Gangotri-Gaumukh route has been mapped for landslide Hazard. Using ArcGIS the Landslide susceptibility for whole of the map region can be seen. The study has to led the determination of factors on the basis of past studies and determination of weightage for the chosen six factors namely soil depth, soil texture, rock type, height, slope and land cover. With the further advancement in such type of study, we could interpret results for future from past records, if the site is inaccessible, or the test results are erroneous. Here we have used the already existing topographical maps, satellite imageries and field work integrating them together using GIS and ANN MODEL to create a database that has generated the output for the future use. The result of present study with spatial data, soil parameters, land inventory and presented by a landslide hazard zonation map and a user friendly application using GIS that could predict the future susceptibility of region to landslide and percentage contribution of each factor for the same.
paper_1	Through Social media, people are able to write short messages on their walls to express their sentiments using various social media like Twitter and Facebook. Through these messages also called status updates, they share and discuss things like news, jokes, business issues and what they go through on a daily basis. These numerous amounts of information can be extracted, processed and properly utilized in areas like marketing and electronic learning. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. Clustering uses unsupervised learning technique in finding similarities between data according to the characteristics found in the data and grouping similar data objects into clusters. It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . Typically development starts with the external features and user interface, and then adds features as prototypes are developed. The first task was to retrieve details of each of the students from their twitter accounts using an extension script which is part of the twitter Application Programming Interface. The second task involved identifying the right kind of data to use for training the expected prototype as well as testing it. The third step involved using the data already preprocessed above to train the prototype. The machine learning method used was unsupervised learning in which the system was given the data so that it automatically analyzes and creates clusters from the data  [3] . They were used to confirm that the system indeed accurately did the classification given some data items. This involved picking the details of a new student from twitter and trying to predict the class hence group that he should join. The results reported in this paper were obtained from a series of evaluations that were done on the classifier on different parameters including functionality, usability, accuracy, precision and recall. The first task was to determine if the prototype achieved its overall goal which is grouping students through social media for electronic learning. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . They also confirmed that the system simplified the process of group formation and made inclusivity of distant students in the groups possible. A good number of learners indicated that they would continually use the system for the purposes of group formation and discussion. These measures can be improved if large amounts of data are used to train the classifier before being used to do actual classification. Through social media a better and more efficient way can be used to enable online learning generally and group formation specifically. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. It was able to extract tweets from various social media accounts based on a given hash tag (task) and then pass them to a Naïve Bayes classifier as input. The classifier was also able to assign other or new users groups also according to their tweets and the learning that the system had undergone. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_2	Current traffic light systems use a fixed time delay for different traffic directions and do follow a particular cycle while switching from one signal to another. In addition to this, the current traffic light systems encourage extortion by corrupt traffic officials as commuters often violate traffic rules because of the insufficient time allocated to their lanes or may want to avoid a long waiting period for their lanes to come up. This research is aimed at tackling the afore-mentioned problems by adopting a density based traffic control approach using Jakpa Junction, one of the busiest junctions in Delta State, Nigeria as a case study. The signal timing changes automatically based on the traffic density at the junction, thereby, avoiding unnecessary waiting time at the junction. The sensors used in this project were infra-red (IR) sensors and photodiodes which were placed in a Line of Sight configuration across the loads to detect the density of the traffic signal. These include loss of man-hours, accident, missed opportunities, noise pollution, air-pollution, increased fuel consumption, increased tendency to violate traffic rules, and in some cases extortion by corrupt traffic control officials. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. In these fixed traffic control systems, vehicles have to wait at a road crossing even though there is little or no traffic in the other direction. There are other problems as well, like ambulances getting caught up by a red traffic signal and wasting valuable time  [2] . The proposed system would use a microcontroller of PIC family duly interfaced with sensors, to change the junction timing automatically to accommodate movement of vehicles, thereby, avoiding unnecessary waiting time at the junction. A traffic control model using signaling system in a discrete cross-road with NE-555 timer circuit was implemented by  [3] . The design had a provision for pedestrians to request for crossing the road as and when required by pressing a switch. Most of the crossings handle the automated traffic signaling using fixed duration intervals between the Red, Yellow, Green and Pedestrian Pass Signal. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. In a bid to overcome this challenge  [4]  adopted an approach whereby a camera is placed on the top of the signal to get a clear view of traffic on the particular side of the signal so that it will capture the image. The algorithm implementation was done using Mathworks, MATLAB software, and the results were simulated using a Simulink Tool to create traffic scenarios and comparisons between simple time-based algorithms and the developed system. However, this system has the disadvantage of the controller since it depends on the preset quantification values for fuzzy variables  [6]  conducted a cross sectional study targeting traffic control in the city of Nairobi's Central Business District and its surroundings. For testing the adaptive traffic light controllers, a simulation system using Qt, C++ software integrated with MATLAB tools was developed. However, this method has no mechanisms for capturing traffic density and for providing a pass for emergency vehicles. It is further filtered through a 1000µF capacitor and then regulated using 7805 regulator to get +5V. The bridge rectifier consists of four single phase rectifier diodes connected together in a closed loop to form a circuit that is capable of converting AC voltage to DC voltage  [8] . Since the peak inverse voltage of the diodes has to be greater than the peak secondary voltage of the transformer, the 1N4007 silicon diode with peak inverse voltage (PIV) of 1000 Volts was used in the circuit. A capacitor is needed to effectively carry out the filtering of the rectified AC signal to eliminate ripples  [9]  and can thus be calculated using the equation below. Another purpose of an optocoupler is to prevent rapidly changing voltages or high voltages on one side of a circuit from distorting transmissions or damaging components on the other side of the circuit. This is rectified by the bridge rectifier, filtered by the capacitors to remove ripples and regulated by the voltage regulators to produce fixed value of 5 volts which is supplied to the system. According to the IR count, microcontroller takes appropriate decisions as to which road is to be given the highest priority and the longest time delay for the corresponding traffic light. It is a widely used type of electronic prototyping board characterized by a 0.1-inch rectangular grid holes with parallel strips of copper cladding running in one direction all the way across one side of the board. From the test carried out on the circuit, it was observed that the LEDs with the same color have equal timing, and that each pole of the four traffic light controlling poles, switches sequentially and repetitively until the circuit is disconnected from power. Jakpa Junction, in Effurun, Delta State, Nigeria is increasingly becoming chaotic by reason of the recurring traffic gridlock it experiences. The sensors help in keeping count of vehicles entering roads and the microcontroller subsequently allots time delay thereby giving accurate priority to each road. This new design would further reduce time delay, improve efficiency and reduce accidents by incorporating the following modifications: (1) The Airport -PTI road should have a two -way traffic light to pass the vehicles coming from Airport road to PTI road.
paper_3	The aim of this article is to present the general architecture trends of Web-based Adaptive Educational Hypermedia Systems (AEHSs) and to give a complete description of architecture of the AEHS MATHEMA. Also, the formative evaluation of AEHS MATHEMA by students of the Department of Informatics and Telecommunications of the University of Athens, Greece, has shown, with the exception of other things, that all its functions are useful and easy to use. They build a personalized model of the learner and apply this model to adapt the content and/or the appearance of the hypermedia, according to the learner's specific characteristics, such as the cognitive goal, level of knowledge, pre-existing or prior knowledge, interests, preferences, stereotypes, cognitive preferences and cognitive or learning style  [4] . (c) Adaptive Navigation Support: It adapts the link structure in such a way that the learner is guided towards interesting and relevant information, kept away from nonrelevant information either by suggesting the most relevant links to follow or by providing adaptive comments to visible links. (d) Meta-adaptive Navigation Support: It selects or suggests the most appropriate adaptive navigation technique that suits the given learner best relatively to the given context, either by observing and evaluating the success of each technique in different contexts and the resulting learning from these observations, or by assisting the learner in selecting the navigation technique that best suits to him or her. (f) Intelligent Analysis of Learner's Solutions: It uses intelligent analysers that not only tell the learner whether the solution is correct but also it tells him/her what exactly is wrong or incomplete. (g) Example-based Problem Solving Support: It helps the learners in solving new problems, not by articulating their errors, but by suggesting them relevant successful problem solving cases, chosen from their earlier experience. (h) Adaptive Collaboration Support; Adaptive Group Formation and/or Peer Help: These techniques support the collaboration process either just like the interactive problem solving support systems assist an individual learner in solving a problem, or they use knowledge about possible collaborating peers in order to form a matching group relatively to the kind of the collaborative task. AEHS MATHEMA (Meta-Adaptation Technology Hypermedia for Electro-Magnetism Approach) combines the constructivist, socio-cultural and meta-cognitive teaching model and supports personalized and collaborative learning. The general purpose of the AEHS MATHEMA is to support learners through an interactive and constructivist educational environment in order to construct their knowledge in Physics and Technology, individually and/or collaboratively, and to overcome their misconceptions and/or learning difficulties AEHS MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and meta-adaptive navigation, interactive problem solving, and adaptive group formation. More specifically it supports: (1) adaptive presentation according to learner's learning style; (2) four adaptive navigation techniques which are direct guidance (uses a "next" button), link hiding (hide nonrelevant links), link annotation (use colors to indicate relevance), and link sorting (like in search engines); (3) a meta-adaptive navigation technique in order for the learner selects the more appropriate navigation technique that best suits him or her; (4) an interactive problem-solving method through personalized and collaborative problem-solving activities that exploits the teaching approaches of experimentation through simulations, guided inquiry and discovery, investigations and peer collaboration; and (5) a method of adaptive group formation for peer matching. Most of the Internet based systems use a variation of conditional reasoning where a decision is made to organize the text differently from fragments or select from a group of whole pages or even groups of pages. The link structure of a hyper-document can also be modified by color-coding, or sorting according to specific criteria based on student preferences or abilities. engine performs the actual adaptation by adapting or dynamically generating the content of nodes and the destination and class of links in order to guide each individual user differently. Today almost all AEHSs follow the general architecture of the AHAM reference model (but possibly with a different kind of rule or reasoning language and engine). ADAPT  [7]  is an European Community funded project that aims to rectify the situation described in the introduction, by investigating current adaptive practices in various AEH environments and identifying the design patterns within them. The ADAPT project has identified high level design dimensions for AEHSs, which are: (1) context of use (CU), (2) content domain (DM), (3) instructional strategy (IS), (4) instructional view (IV), (5) learner model (LM), (6) adaptation model (AM), and (7) detection mechanism (DE). Proper's architecture is a combined architecture of SCORM LMS and AEHS. As shown in simplified form, in the  Figure 2 , they adopt the typical SCORM Run Time Environment (RTE) structure adding an adaptation module and extending the preexistent Domain and User Models. Thus the prototype involves four main modules: (1) The Domain Model (DM) that represents the domain knowledge of the system. 3 The Adaptation Module (AM) which interacts with DM and UM in order to provide adaptive navigation to the course, and (4) The RTE Sequencer that is triggered by course navigating controls and interacts with DM and delivers the appropriate educational content to the learner. Thus its architecture is a typical of a SCORM compliant LMS. System retrieve course files initially from a zip file, which contains a manifest xml file and all the html and media required files. WELSA does not store the course Web pages but instead generates them on the fly, following the structure indicated in the XML course and chapter files. This dynamic adaptation mechanism reduces the work-load of authors, who only need to annotate their LOs with standard metadata and do not need to be pedagogical experts (neither for associating LOs with learning styles, nor for devising adaptation strategies). The only condition for LOs is to be as independent from each other as possible, without crossreferences and transition phrases, to insure that the adaptation component can safely apply reordering techniques. The Adaptation Decision Model is responsible for deciding what the system should do in terms of presentation and navigation adaptation on the basis of the conclusions draw by the Interaction Analyzer, parameters from the Learner Model and information from the Application Model. As JavaBeans are components of an application in the context of JavaServer Pages or servlets, they are suitable to implement the Presentation Generator and the Adaptation Decision Model. Additionally, the JavaBeans offer advantages of separating the programming logics from presentation, as for the Presentation Generator and the final page composition. The system then enables to present and recommends a variety of learning contents adaptively towards each of the student's learning style identified in the student model through the adaptation model. The system then analyzes the learning content on each of the learning material, and then comes up with the generated teaching strategies by means of the teaching strategy generator and fragment sorting. The course player in ULUL-ILM enables the system to adaptively presents the content with various teaching strategies towards each of student's learning style. MySQL is the database of choice since there is extensive experience on Campus and provides the ability to implement adaptation rules. These servlets are complete programs that are capable of creating JSPs. A servlet allows a programmer to utilize whatever functions a programmer needs including conditional branching and loops. It is light weight, which means that it does not require a large amount of processing power on creation of the servlet instance, and it accesses databases using JDBC which offers a secure way of accessing many wellestablished database brands. Servlets are capable of eliciting student responses on more than one question and analyzing them to find out the strengths and weaknesses of that student to direct them towards remediation. In the context of JSP pages, JavaBeans components contain business logic that returns data to a script on a JSP page, which in turn formats the data returned from the JavaBeans component for display by the browser. The didactic design of the AEHS MATHEMA supports learners in constructing knowledge, choosing and achieving their learning goals, recognizing what they have already learned and what they are capable to do, and judging personal progress of their learning. It also offers appropriate teaching strategies to match students' learning styles, develop critical thinking and self-regulation, as well as collaborative activities, encouraging them to actively participate. Moreover, offers facilities in recognizing learners' misconceptions and assisting them in self-correction through reflection, and providing multiple representations of learning. The domain knowledge is structured in a way that supports the ability of the system to choose the educational material, depending on the learner's requirements and current status. In the AEHS MATHEMA, the hierarchical representation of the domain model is adopted by the ELM-ART  [5] , as follows: The first level defines the cognitive objectives, the second the basic concepts and the third the pages with the corresponding educational material. The characteristics that the system maintains in learner model for each learner are: name, gender, username, password, learning style, level of knowledge for each cognitive goal (general performance), level of knowledge for each basic meaning of the cognitive goal (quantitative and qualitative characterization), concepts that have been successfully evaluated for each cognitive goal, current page, current teaching method, current navigation technique, navigation history, meta-adaptive navigation status, initial Web experience and knowledge level (pre-existing knowledge) in the current cognitive goal, ways of presenting feedback messages, preference for assistance by Advisor in terms of its navigation, course sequence, and phases of the guided dialogues in interactive problem solving process in which each learner participates. The learner during the study selects the cognitive goal he or she wishes to study, following his or her personal course in the educational content, exploits the system's assistance, navigating and studying, is informed about the information that the system maintains, who can change them by intervening and directing the system's adaptation in this way, in terms of curriculum sequencing, adaptive presentation and navigation and adaptive group formation, selects his or her learning style, selects the navigation technique, activates and deactivates the curriculum sequencing, selects the way that the feedback messages are presented, changes his or her knowledge level for each basic concept of the cognitive goal, and activates-deactivates the Advisor. The adaptation engine decides on adaptive presentation and navigation in relation to Student, Domain and Didactic models. The system, during the learner's study, monitors his or her interactions with the system and his or her assessment and accordingly updates the links to the course material provided and/or adapts learning material to learner according to his or her learning style. The adaptive navigation techniques that it supports are: direct guidance, link hiding, link annotation, and link sorting. It is the second level meta-adaptation engine that is responsible for monitoring the cognitive improvement of the learner in the cognitive goal that he or she studies, after n successful evaluations of his or her knowledge in basic concepts, showing him or her advantages and disadvantages of the four navigation techniques that it supports. In AEHS MATHEMA, meta-adaptive navigation works as follows: The first time that the learner enters the system, he or she is asked to state his or her Web experience and level of knowledge in the cognitive goal he chose to study (preexisting knowledge). Following the learner's statement, the system suggests the most appropriate navigation technique to him or her, taking into account the level of his or her Web experience and his or her level of knowledge in the subject he or she chose to study as follows: (1) Direct guidance: Little or no Web experience and little or no knowledge of the cognitive goal. The meta-adaptation engine proposes to the learner to change the navigation technique if he or she wishes after n successful assessments of the basic concepts of a cognitive goal he or she studied, showing him or her the pros and cons and additional information on each of four navigation techniques, and the learner decides whether to change it or not. This module is responsible for transferring the training material to the presentation unit, for giving feedback, for creating guided dialogues and for evaluating the learner knowledge. It includes the content of the JSP pages, Java servlets responsible for feedback and creation of guided dialogs in problem-solving, Java servlets for creating evaluation questionnaires, and so on. It is responsible for what will be presented to the learner as a result of the processing of the information that arrives from the adaptation engine, the meta-adaptation engine, adaptive group formation module, and application module. In this module there is the basic servlet, which deals with the presentation of the training material and other JSP pages and servlets, such as the page and the corresponding servlet to inform the learner about the characteristics of all learning styles, the servlet for the curriculum sequencing, the servlet for selecting and presenting the concept that the student selects to study according to his or her learning style, etc. More specifically it: (1) initializes and updates the Student Model; (2) collects and stores the data that the learner answers to open-ended questions, entries in tables, registers in notes, etc; (3) monitors the interactions between the learner and the system to get information about his or her choices in terms of the questionnaires, the phases of guided dialogs, the links, the options for meta-adaptive navigation, the visited pages, and more. MATHEMA supports the learner to find the most suitable peer for the formation of a collaboration team. Following a study  [13]  conducted on the formation adaptive groups in the MATHEMA, it was decided that: when the system creates a priority list of candidate peers for an interested learner, in the first and second position of the priority list, the algorithm will place the candidate collaborators with a concrete or abstract style and with the same active or reflective dimension of their learning style as follows: If the learner has a concrete learning style, then the algorithm will place the candidate collaborators with a concrete learning style in the first position, and in the second position, the candidate collaborators with an abstract learning style. After creating the list, the system informs the learner that his or her most important candidate collaborator is at the top of the list, while the less important is at the end of the list. The collaboration protocol that uses the synchronous communication tool is as follows: (1) The learner declares willingness to collaborate either by selecting his or her partner from the priority list of candidate collaborators or by declaring a desire for collaboration so that others who would like to work with him or her can choose it, while activating the synchronous communication tool. (2) Learners negotiate for collaboration and, if they come in agreement with each other, state it in the system to not include them in the priority list of other learners who would like to work with them. The  Figure 11  shows a snapshot of a dialog between the learners Giannis and Mary taking place via the Synchronous Communication Tool (chat-tool). Regarding adaptive and intelligent techniques of the AEHS MATHEMA mentioned above, the students having evaluated them consider that the adaptive presentation of different teaching strategies is quite useful (93.0 percent) and fairly easy to use (86.0 percent), the different ways of navigating are quite useful (100.0 percent) and handy (90.7 percent), the meta-adaptive navigation is quite useful (88.4 percent) and fairly easy to use (86.0 percent), the adaptive group formation of collaborative teams to assist them in collaborative problem solving is useful (83.7 percent), but less user-friendly (58.1 percent). MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and metaadaptive navigation, interactive problem solving, and adaptive group formation. The main contributions of the architecture of the MATHEMA in the improvement of functionality of AEHSs are the meta-adaptation technique, the adaptive group formation technique, the problem-solving technique, and the synchronous communication protocol that it supports. The evaluation of the AEHS MATHEMA was encouraging and rewarding since the evaluators considered that almost all system functions are quite useful and easy to use. Taking into account the observations and recommendations of the evaluators, some system functions, such as this offered by the asynchronous communication tool, have been improved.
paper_21	In this paper, we present the powerful scheme ZSISMP (Zimmermann Self Invertible Stabilizer Multiplier Permutation) to attack the hardness of the minimum distance search problem of BCH codes. This scheme consists in evaluating the minimum distance of the reduced dimension sub code fixed by a Self Invertible Stabilizer Multiplier Permutation by Zimmermann algorithm. The use of this efficient local search had yield to determine the error correcting capability of many BCH codes of length 1023 and 4095. The error-correcting capability of these codes is directly related to their minimum distance. In  [10] , Augot and Sendrier found idempotent codewords of minimum weight for several primitive narrow-sense BCH codes. The artificial intelligence Simulated Annealing presented in  [16] , Tabu Search  [17] , Hill-Climbing  [18] , Genetic Algorithm  [18] [19] , Ant Colony Optimization  [20] , Metropolis Algorithm  [21] , was shown to be useful to attack the difficulty of the minimum distance search problem for BCH Codes. Unlike classical techniques based on exhaustive or partial enumeration of codewords, Berrou in  [23]  has presented an efficient approach based on the notion of Error Impulse response of a Soft-In decoder. This approach consists in adding to all-zero codeword a level of noise and considering the minimum distance as the smallest level of noise from which the Soft-In decoder fails in correction. In  [24] , the authors proposed an efficient method, by applying the MIM method on some sub code randomly extracted from the considerer BCH code. The proposed method MIM-RSC, has allowed an efficient local search and therefore finding the true minimum distance of some BCH codes of length 1023 and 2047 as well as obtaining good results in comparison with the previous works presented in  [17-18-19-20-21-22 ]. From these stabilizers, we take only a Self Invertible stabilizer if it exist and by using a mathematical tool, we find the sub code fixed by this involution and then we evaluate the minimum distance by using the famous Zimmermann algorithm. This section presents the proposed scheme for finding the lowest weight in BCH codes. This section presents a validation of the proposed method on BCH codes of known minimum distance and its application for finding the minimum distance of BCH codes of unknown minimum distance. This section presents also a comparison between the proposed scheme and previous work on minimum distance for BCH codes. All results have been done using a simple configuration machine: Intel (R) Core (TM) i3-4005U CPU @1.70GHz RAM 4GO and are made by running the considered algorithm in 1day for each code. It is well known that the smallest primitive narrow-sense BCH code whose minimum distance is greater than its designed distance is BCH (127, 43, 29) and it is the only one for this length. It is known also, that All the narrow-sense primitive binary BCH codes of length 255 have their minimum distance equal to their designed distance except BCH (255, 63, 61), and BCH (255, 71, 59). The table 3 presents a comparison between the proposed scheme and MIM-RSC method  [24] . In order to find the minimum distance of some large BCH codes, the proposed scheme is applied by using a simple machine of the configuration given above. In the perspectives, we will apply this powerful scheme to construct good large cyclic codes, and adapt this scheme for other linear codes.
paper_31	With the continuous improvement of human living conditions and the aggravation of global aging level, chronic diseases such as hypertension and diabetes continue to plague human health. This paper takes chronic diseases as the research object, and proposes a design of intelligent medical system for chronic diseases based on semantic matching by the adaptation of ZigBee technology in the front-end data acquisition. Through the ZigBee wireless sensor network, this system sends the physiological parameters collected by various medical sensors to the intelligent medical system, and innovatively proposes semantic matching algorithm to solve the queuing problem of data transmission, to ensure the accuracy of data transmission. At present, the main way of chronic disease prevention and treatment is to go to hospitals for regular test, and get the treatment on the basis of the corresponding physical data. It will cause many problems: firstly, it is unable to check the patient's continuous vital signs data; secondly, it is impossible for patients to get their own condition on time; thirdly, it is liable to doctors' misjudgment. In-depth interviews with 91 employees in different positions from 11 grassroots medical and health institutions since July 2019 show that the efficiency of mobile medical care in the surveyed areas has appeared  [2] . With the means of the Internet, with the carrier of the mobile terminal, intelligent medical treatment founded on the units of family is a new trend. Therefore, it is of great significance to develop an intelligent data analysis platform to record, store, and share and handle various personal health signs in time through wireless transmission. Due to the late development of information technology in China, our Wise Information Technology of med (WITMED) starts later than foreign countries. For example, the General Hospital of the People's Liberation Army (PLAGH) communicated and discussed some diseases with foreign experts through satellites. The third Military Medical University studied on the home digital medical monitoring project, which designs a network system to monitor human physiological parameters using mobile terminals. For example, some domestic hospitals use Bluetooth technology to monitor the ward room. And some scholars combined Bluetooth technology with GSM short message to realize the collection and remote monitoring of physiological parameters such as blood pressure  [12]  and pulse. Chronic diseases can easily cause damage to patients' vital organs such as the brain, heart, and kidneys  [3]  to seriously affects working ability and quality of life, and its high medical expenses increases the burden of economy on society and families. With the emergence of various new telecommunication technology, scholars kept providing health care using all sorts of communication technology. Traditional phone calls were adopted as the main way of communication between patients and doctors. At this stage, telemedicine has gradually transitioned to WITMED, and developed from precision medicine to disease prevention  [5] . As an alternative for aiding healthcare systems, sensors and wearable devices are used for monitoring patient physiological data to help guide health services or the self-care of patients  [9] . Detecting physiological signals in the daily life of the population can effectively grasp the health status of the people in the area, and provide an important reference for the diagnosis and prevention of chronic diseases  [10] . In this telemedicine stage, it gradually focused more at community, families and individualized care  [11] . There are some defects of the current WITMED (Wise Information Technology of med) system as follows: (1) The inadequate reliability of data transmission. Chronic diseases patients have to be long-term monitored, however, whether using a WIFI or GPRS, high power consumption will be arouse. At present, ad-hoc network cannot work no matter using GPRS or WIFI devices, and only one kind of vital-sign data can be transmitted at a time. The current WITMED system focuses only on data acquisition instead of processing uploaded data and returning the result. The growing trend of WITMED system shows that a variety of health signs data will be monitored, transmitted, processed and analyzed in real time, the system can process the uploaded data. As the speedy development of the escalation of people's living standard, China has gradually entered the aging society with an increasing number of patients with chronic diseases and a descent trend of age  [14] . Since 2014, the central and local governments have formulated a series of policies around intelligent health care, pointing out the direction for the development and construction of it. Attracted by the broad prospect of smart medicine, BAT and other Internet enterprises have participated in the smart medical industry. Alibaba founded Alibaba health  [15] ; Tencent launched the first AI+ medical product Tencent Meiying in 2017; many large enterprises integrated medical resources through mergers and acquisitions to lay out the intelligent medical industry chain. Finally, the request is inserted into the message queuing sequence by the matching degree. The general algorithm of this part is as follows: Define priority function: f x, y ax by ε, with a basic definition as follows: x: semantic matching degree, 0≤x≤1 y: a request arriving at the serial position, 0<y≤1 a is defined as the weight of semantic matching degree, 0<a<1 b is defined as the weight of serial position of Request arrives, 0<b<1, a+b=1 ε is defined as the disturbance value, 0≤ε≤0.1, with the default as 0. Generally, a is close to µ= 1 2 , formula_0 Therefore, the process of queuing algorithm based on Semantic matching is as follows: With the hypothesis that a= 1 2 (to take one half of the sum) x= 5 10 (there are 10 participles in semantic information, of which 5 are matched), y 1 (Before a new message arrives, assumes that the request queue has 10 participles waiting, then: f x, y y a x y ε =0.294, therefore, the priority value of the message is 0.294. This research proposes an improved space vector model algorithm to ensure whether the patient has a related disease by the similarity between the physical signs and the disease on the basis of calculation of the sign value, continuous sign value and the disease characteristic value. An improved SVM calculation method is as follows: Definition 1 (conception): Define a tuple F= (X, Y, Z), in which x is the collection of all objects is the object property set, Z (X*Y), Z is a subset of X*Y. AES encryption process is shown in  Figure 4 : (3) column Mixed Operation The column mixed transformation is realized by matrix multiplication. The column mixed operation formula is shown as  Figure 6 : (4) key plus calculation Key plus calculation operates the 128 bit key is XOR bit by bit with the data in the state matrix. The process of key plus calculation can be regarded as the result of Bitwise XOR or byte level or bit level operation. After logging in the system, enter the sign input interface and put on the sensor, the system will automatically input the relevant data collected, as shown in  Figure 8 : After inputting the data of relevant signs, the system will automatically generate diagnosis & treatment files and health assessment according to the relevant data. The method of backstage data giving warnings and putting forward corresponding disposal measures by big data processing as shown in  Figure 9  is better than clustering, and the choice of Space vector model is more accurate than such models as Zigebee Health Care Profile chosen in the references. In this study, a design scheme of intelligent medical system based on semantic matching is proposed, with the front-end data acquisition which adopts ZigBee technology, and a semantic matching algorithm is innovatively proposed to solve the queuing problem of data transmission to ensure the accuracy of data transmission. Through ZigBee wireless sensor network, the system sends the physical parameters collected by various medical sensors to the information control terminal, and the improved spatial vector model is employed to process the uploaded data in the process of data transmission by the intelligent data analysis platform, and the security of data transmission is ensured by adopting the AES encryption algorithm in the whole process. The intelligent system realizes the data collection of the whole process of outpatient service, examination and treatment, which can provide data support for the precise treatment of diseases.
paper_38	The study aimed to determine if any of the entry requirements such as Ordinary Level (OL) results, Unified Tertiary Matriculation Examination (UTME) scores or Post-UTME (PUTME) scores could predict an outstanding academic performance of first-year undergraduate students admitted into the Faculty of Science in the Kaduna State University, Kaduna. A purposive sample of nine hundred and forty-three (943) first-year students constituted the population for the study were drawn from Computer Science, Mathematics and Physics undergraduate degree programmes from the Faculty of Science of the university who were admitted from the 2010/2011 to 2014/2015 academic sessions. The results revealed that with a weak correlation, OL is a good predictor on the CGPA, a dependent variable, for academic performance which holds true for students who are in the CGPA category of '1st class' and '2nd Class Lower' respectively. It concluded that the use of OL and UTME as instruments is not enough to select candidates for admission and therefore recommended that other instruments such as senior secondary school mock examinations need to be included as part of the entry requirements in the admission criteria. Each student is expected to have at least five credit passes in not more than two sittings in Mathematics, English Language and three other science-based subjects such as Chemistry, Biology, Physics and Geography at the Ordinary Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria Levels (OL) of either Senior School Certificate Examination (SSCE) which is conducted by West Africa Examination Council (WAEC) and National Examination Council (NECO). Also, each student is expected to have obtained at least the minimum score required in the Unified Tertiary Matriculation Examination (UTME), conducted every year by the Joint Admissions Matriculation Board (JAMB) since 1979. In Nigerian universities, an academic performance frequently is defined in connection with semester examination performance. The CGPA score takes into consideration students' tests, assignments, practicals, examinations and sometimes lecture attendance. The focus of this study is to predict full-time undergraduate students' first-year Cumulative Grade Point Average (CGPA), which is one of the variables for measuring the academic performance by using entry requirements, such as Ordinary Levels and UTME, for Faculty of Science in Kaduna State University (KASU), Kaduna -Nigeria. Students' academic records show that after admissions, some students perform poorly even after going through a series of screening of their OL results, and writing of UTME and PUTME examinations before offering them admissions. This study aims to investigate the relationship between the entry requirements into any of the Faculty of Science undergraduate full-time degree programmes in the Kaduna State University and the students' academic performance at the end of the first year of study. Determine if any of the following entry qualification used for admission, OL results only, UTME scores only or Post-UTME scores (average of OL results and UTME scores) best predict the academic performance of students in the 100 level CGPA examinations; ii. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? Academic performance or sometimes known as an academic achievement is defined by  [10]  as "Knowledge attained or skill developed in the school subjects, usually designated by test scores or by marks assigned by teachers". A student's academic performance usually is measured in either examinations or continuous assessment tests, and this is expressed in various ways depending on what the scores should be used for. The numerous ways of reporting academic performance include raw scores, percentages, transformed scores, or even as categorical variables such as Excellent, Merit, Very Good, Pass, First Class, Distinction, A1, B2, C4, F9, and others. In Nigeria, students are admitted into universities using their scores in the UTME as well as Post-UTME (PUTME) subject to having at least five OL credit passes in relevant subjects obtained in not more than two examination sittings including the English Language. The underlying assumption made in such selection is that those admitted by satisfying the admission criteria will be successful in the successive academic activities attached to their studies. However, the following review of literature examined the relationships between UTME and PUTME scores as a predictor for the academic performance of students have revealed contradictions in their findings. Reference  [11] , in his study, monitored the performance of science education students admitted through Post UME screening in 2005/2006 academic session. The author's findings in his study showed that there was a consistent decline in the number of students admitted using the PUTME which cannot do better than UTME in influencing students' academic performance as the outstanding and weak students formed the upper 12.5% and lower 12.5% while the remaining 75% consists of the average students. The authors in  [8]  comparatively analysed the academic performance of graduates admitted through UTME and preliminary programmes (Certificate, Basic Studies and School of Science Laboratory Technology [SSLT]) in the University of Port Harcourt. Their results showed that in all the faculties with the exemption of Agricultural Science and Engineering, the graduates admitted through the preliminary programmes performed significantly better than their counterparts admitted through the UTME. Graduates with the best academic performance from the preliminary programmes were those admitted through the certificate programme. Reference  [1]  used the Pearson Product Moment Correlation Coefficient (PPMCC) to predict the academic performance of first-year students in four departments in the University of Abuja from 2008/2009 to 2010/2011 academic sessions using UTME, PUTME and CGPA. Partial Correlations Coefficient (PCC) was used in addition to PPMCC in  [12]  to predict the student's final grade in from a sample population of 306 students of Faculty of Health Sciences and Technology at the Enugu Campus of the University of Nigeria that had their final results ready and approved by Senate at the end of 2012. The author's study found that the use UTME score was a very poor predictor of students' final grades and thereby recommended that less emphasis should be placed on UTME scores as a criterion for admission of candidates into universities. The study by  [13]  found a significant relationship between students' scores in three examinations, namely: UTME, PUTME and 100-Level Psychology course, Faculty of Agriculture, Federal University of Agriculture, Makurdi, and thus concluded that the UTME has predictive validity for performance in the university. In the same vein,  [14]  tested the predictive power of the Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria JAMB UTME in predicting students' performance in the university's semester examination by using a regression model. Their results suggested that the JAMB UTME had positive but low indices of predictive validity, which varied across the academic sessions from 2005/2006 to 2013/2014 and all programmes of study except for four departments. In contrast to the studies from the earlier mentioned authors,  [15]  investigated the relationship between 276 students' performance in the entrance examination and their performance in Mathematics in two selected Colleges of Education (CoE) in Osun and Oyo states each. The data obtained were semester results in Mathematics during 2010/2011 to 2012/2013 sessions and their grades in Mathematics from any of the public entry examinations known in Nigeria such as UTME, WAEC, NECO, and National Business and Technical Examinations Board (NABTEB). The author concluded that the entry qualification or the entrance examination performance could not individually predict Mathematics performance at the CoE. A population of 1650 students admitted into the university during the 2011/2012 academic session from Faculties of Arts, Education, Science and Social and Management Science was used to obtain their UTME, PUTME scores along with their GPA for eight semesters. The Predictive Correlational Ex-Post Facto design was identified to be the most appropriate for the study since the results (CGPA, UTME and OL) of students in the Faculty of Science were used in reaching conclusions about the whole prediction of academic performance. The population of the study was limited to consist of all students admitted into three Faculty of Science undergraduate degree programmes of Kaduna State University for five academic sessions from 2010/2011 to 2014/2015 using the OL, UTME and CGPA results. Out of the total sample of 3,255 students admitted between 2010/2011 to 2014/2015 academic sessions, the programme with the highest sample size is Microbiology with 491 (15.1%), followed by Biological Sciences (13.9%) whereas the programme with the least sample size is Industrial Chemistry (5.7%). The instrument used to derive the data for this study were: the JAMB UTME scores from 2010 to 2014 and OL grades in the five relevant subjects which were the pre-admission criteria and first-year CGPA results obtained from the semester examinations from 2010/2011 to 2014/2015 academic sessions. The data used in the study are the OL grades, and JAMB UTME scores obtained from the University's central database, and the academic standing of first-year CGPA results collected from the various departmental examination officers (DEOs) with the approval from the Dean of the Faculty of Science. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? In  Table 6 , the likelihood ratio Chi-Square of 18.723, 17.661 and 12.401 for Computer Science, Mathematics and Physics programmes with a significant value of 0.227, 0.281 and 0.414 tells us that the model as a whole does not predict the dependent variable, i.e., CGPA. A Multinomial Logistic Regression (MLR) was performed to model the relationship between the predictors and membership in the six groups, i.e., CGPA categories (Fail, Pass, 3 rd Class, 2 nd Class Lower, 2 nd Class Upper and 1 st Class) for each degree programme. These showed that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. For Mathematics and Physics students the relative strength of OL, UTME and PUTME on CGPA performance are statistically insignificant except for the slope (B) of OL in the CGPA category of '1 st Class' for Mathematics students, which statistically significant. What is the relationship between OL results, UTME scores and Post-UTME scores of students and their firstyear CGPA across the academic session, 2010/2011 to 2014/2015? Table 8  shows the summary of correlations coefficient between OL-CGPA, UTME-CGPA and PUTME-CGPA aimed at all the degree programmes for the academic sessions ranging from 2010/2011 to 2014/2015. How well do OL results, UTME scores and Post-UTME Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria scores of students predict their first-year CGPA across the academic session, 2010/2011 to 2014/2015? For all the academic sessions as shown on the results, the first equation intercept is the log of the ratio of the possibility of a student having a 'Pass' degree to the possibility of that student having a 'fail' degree. These show that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. Furthermore, the slopes (B) of UTME in the CGPA categories of the '3 rd Class' is negatively signifying that the relative strength of UTME is lower than those with a CGPA category of 'Fail' and the rest are positive which signifies otherwise. The relative strength of OL, UTME and PUTME on CGPA performance of students admitted in the 2010/2011 session is not statistically significant except for the slope (B) of OL in the CGPA category of '2 nd Class Upper', which is statistically significant. As for the students admitted in the other sessions, 2011/2012 to 2014/2015, the relative strength of OL, UTME and PUTME on CGPA performance are statistically insignificant except for the slope (B) of UTME in the CGPA category of '2 nd Class Upper' and '1 st Class' for those admitted in the 2012/2013 academic session, which is statistically significant. The primary purpose of this study is to investigate if OL results, UTME and PUTME scores do predict the academic performance among first-year undergraduate students in the Faculty of Science. Based on the analysis and results using MLR and PPMC for each programme and each academic session, it is evident that OL, UTME or PUTME could not individually significantly predict the academic performance of students in Faculty of Science. However, by combining all the criterion variables, that is OL, UTME, and PUTME, as one variable and performing the PPMC and MLR, findings show that OL is a good predictor on the dependent variable for academic performance with a weak correlation of 0.068 which is statistically significant at 0.04 level. Although OL and UTME are still necessary as instruments for admission, it is recommended that the University be advised to include some other instruments such as senior secondary school mock examinations results for selecting candidates into any of the undergraduate degree programmes in the Faculty of Science. Also, there is a need to do a further study by including some more variables, such as age and senior secondary school mock examinations results, as criteria to significantly predict the academic performance of students successfully. The authors in  [21] , in their study also recommended the need of potential researchers to compare the OL, UTME and Post UTME terms and scores across Nigerian Universities for standardisation and a model for educational development in the twenty first century.
paper_57	This problem has been addressed by Genetic operations (GO) incorporated into ACO framework. Crossover and mutation operations have been adapted for use with ant generated strings which still have to provide feasible solutions. [2]  as Ant system (AS) algorithm. During the search process each ant sets off from ant colony (start position) and moves to search food (destination). As ants are passing the terrain (graph) they mark used routes (arcs of the graph) by chemical substance called pheromone. On their way back they use the same way from which abundant loops has been removed, but the amount of pheromone (1) ∆τ k ij (t) they produced is inversely proportional to the tour length L k (t). formula_0 formula_1 T k (t) is the tour generated by ant k, Q is a constant and tuple (i, j) denotes beginning and termination node of an arc. At the beginning when no pheromone values are available heuristic values η ij takes dominance. Later the ant uses probability selection rule to choose the next arc according to formula_3 where p k ij (t) is probability the ant k chooses the arc (i, j) from the neighborhood N k i of node i except the node visited previously. The probability p ij (t) of choosing the particular arc (i, j) depends on pheromone τ ij  (t)  and the heuristic η ij values which are associated with the arc (4). formula_4 Symbols α and β are weight parameters and represents balance between ant's gathered knowledge and the user preferred area. Heuristic values η ij affect probability only at the beginning when pheromone values are low. Let's mention ant colony system (ACS) with pseudo-random proportional rule  [3]  in which random uniformly distributed variable q ∈ (0,1) is compared with a tunable parameter formula_5 else the probability selection rule (3) is applied; random selection applied to AS rank  [4]  where random selection rate r is the probability of random selection and represents a user parameter which adjusts the balance between exploration and exploitation; prevention of quick convergence (i) and stagnation avoidance (ii) mechanisms applied to AS  [5] . The stagnation avoidance mechanism (ii) is based on the comparison of a randomly generated quantity q ∈ (0,1) with probability p k ij (t) of selected arc. In ACO adaptation the first and the last node is excluded from mutation. For feasibility reason the replacement node n r (new gene) is such a node from the node n i neighborhood N i , to which an arc from n i predecessor n p to n i successor n s exists  (Fig. If more such nodes occur, random selection is applied. If no such node exists, another gene is randomly picked up from the list. In ACO algorithm crossover position is represented by a common node of parental strings except the first and the last node  (Fig. If more of such nodes exist, random selection is applied. In GA many selection (vi) mechanisms are available, like roulette-wheel selection, tournament selection, stochastic universal sampling or reward-based selection  [7] . Since optimization process is primary done by ants cooperative behavior, the selection process has purely random concept and genetic operations serve just for selection pressure decrease. The above described genetic operations have been applied to one of the best performing ACO algorithms of Kumar, Tiwari and Shankar (ACO KTS )  [5] . At the end of each cycle t, when all the ants finish their tours T k (t), genetic operations are applied on the T k (t) strings which represents the list of nodes. Prior the genetic operations all loops are removed from the tours. If more candidates by which the selected node can be replaced occur, the new node is random chosen from the candidates. If mutation fails on all nodes of the tour, another tour is chosen. If crossover operation is not feasible, another second string is selected. If no tour has common node with the first selected tour, another first tour is selected and the random selection process is repeated. Since genetic operations may produce strings with loops, in ACO framework prior and immediately after each genetic operation a loop removal procedure is performed. After all genetic operations are executed fitness evaluation and pheromone update are scheduled. Feasibility of genetic operations depends on the graph and generated tours. For this purpose ACO GO algorithm has embedded user feedback which represents a ratio between accomplished and required genetic operations. The value for the number of cycles represents three macro cycles of ACO MC  [9]  for the same graph and parameters. The reference value of n [%] was received without any genetic operation and is 5.6 (Table 2, row 1). with no crossover pair certain amount of mutation operations should be spread out among more paths, but with 2 crossover pairs concentration of mutation operation on less paths tends to perform better. This behavior may be caused by the execution order of the GO: crossover is applied after mutation, thus crossover may re-distribute mutation substrings between more paths. Genetic operations where nearly always feasible; ratio accomplished / required mutation operations is 100% and for the crossover operation over 99%. Beyond 60% threshold ants foraging behavior is suppressed by crossover overload. As the crossover rate increases, ratio accomplished / required operation decreases  (Fig. This can be explained by the nature of the mutation operation which creates new paths whilst crossover operation can only combine already existing solutions.
paper_78	In the absence of experimentation or domain knowledge, m is commonly set to 2. Glycyrrhiza uralensis + Stipa bungeana. It is distributed from 380 to 605 m in hills with slope 10 -20° in sunny and semi-sunny slope and chestnut soil. Its disturbance intensity is medium and heavy. The community has a total cover of 75%, a shrub layer cover of 20% and an herb layer cover of 65%. The average cover of Glycyrrhiza uralensis in this community is 25% with a density of 3600 ha -1 . Glycyrrhiza uralensis + Polygonum bistorta. It is distributed from 300 to 500 m in hills with slope 20 -30° in sunny，semi-sunny and semi-shady slope and sandy chestnut soil. Its disturbance intensity is medium and heavy. The community has a total cover of 70%, a shrub layer cover of 5% and an herb layer cover of 70%. The average cover of Glycyrrhiza uralensis in this community is 35% with a density of 5700 ha -1 . It is distributed from 300 to 500 m in hills with slope 20 -30° in sunny， semi-sunny and semi-shady slope and sandy chestnut soil. Its disturbance intensity is medium and heavy. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 60%. The common species are Caragana korshinskii, Elaeagnus, mooceroftii, Suaeda prostrate, Artemisias phaerocephala, Saussurea laciniata, Saposhnikovia divariicata, Oxytropis glabra, and Artemisia ordosica. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 59500 ha -1 . It is distributed from 280 to 500 m in hills with slope 15 -25° in sunny and semi-sunny slope and sandy soil. Its disturbance intensity is medium and heavy. The community has a total cover of 65%, a shrub layer cover of 10% and an herb layer cover of 60%. The average cover of Glycyrrhiza uralensis in this community is 37% with a density of 4900 ha -1 . It is distributed from 350 to 650 m in hills with slope 15 -35° in sunny and semi-sunny slope and sandy soil. Its disturbance intensity is heavy. The community has a total cover of 60%, a shrub layer cover of 5% and an herb layer cover of 58%. The average cover of Glycyrrhiza uralensis in this community is 33% with a density of 5500 ha -1 . Glycyrrhiza uralensis + Astragalinae triloa + Stipa sareptana. It is distributed from 400 to 700 m in hills with slope 10 -30° in sunny and semi-sunny slope and sandy soil. The community has a total cover of 75%, a shrub layer cover of 10% and an herb layer cover of 70%. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 6000 ha -1 . Glycyrrhiza uralensis + Aneurolepidium chinense +Stipa sareptana. It is distributed from 400 to 750 m in hills with slope 15 -35° in sunny and semi-sunny slope and sandy soil. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 65%. The average cover of Glycyrrhiza uralensis in this community is 29% with a density of 4100 ha -1 . Glycyrrhiza uralensis + Festuca logae + Stipa sareptana. It is distributed from 400 to 800 m in hills with slope 20 -35° in sunny and semi-sunny slope and sandy soil. Its disturbance intensity is heavy. The community has a total cover of 80%, a shrub layer cover of 10% and an herb layer cover of 75%. The average cover of Glycyrrhiza uralensis in this community is 30% with a density of 4000 ha -1 .
paper_96	In the program, a new type of "warning system of obstacle avoidance of embedded electronic guide dog" has been developed on the basis of careful analysis of all kinds of present anti-collision warning systems, which has a core micro-controller, 32-bit ARM7 microprocessor, and takes the embedded operating system uCLinux as its platform. Such warning system of obstacle avoidance of embedded electronic guide dog can effectively eliminate the impact of the traffic environment and the subjective factors of the blind, warning in advance for the travelling blind in time, effectively avoiding obstacles such as vehicles, to reduce traffic accidents caused by the their blindness. In fact, this "warning system of embedded electronic guide dog avoiding obstacles" is similar to a car anti-collision avoidance system. The design of obstacle-avoiding early warning system of embedded electronic guide dog is an inheritance and innovation, based on the design of traffic information collecting system, which is generally used in vehicle anti-collision at present. Nowadays, the main types of the design of traffic information collecting system, generally used in vehicle anti-collision, are shown as follows, that is, using single chip microcomputer for data analysis, PC-centric (industrial control computer), using DSP for data analysis and so on. [1]  In the design process of the obstacle-avoiding early warning system of embedded electronic guide dog, we firstly has carried on a comprehensive evaluation and analysis for each scheme, and then put forward a new idea that it introduces the open source embedded real-time operating system uCLinux, with a high-powered ARM core as the core processor. In this way, the design of obstacle-avoiding early warning system of embedded electronic guide dog has the following advantages. Also, it is easy to extend the functions, which is convenient for field operations, greatly reducing the cost of the traffic information collecting system in the obstacle-avoiding early warning system of electronic guide dog. What's more, the design of obstacle-avoiding early warning system of embedded electronic guide dog has utilized the core processor, based on the realization of the system function, and development conditions. Moreover, the obstacle-avoiding early warning system of embedded electronic guide dog also selects USB protocol to transfer data, saving the collected data in the hardware after being managed. The overall design plan of obstacle-avoiding early warning system of embedded electronic guide dog mainly is the velocity and distance measurement module, the power circuit module, the LED screen show module, USBcommunication circuit module, and clock circuit module. The hardware of the obstacle-avoiding early warning system of embedded electronic guide dog can reference the Figure 1:  Figure 1 : Schematic diagram of hardware system structure  [4] In the anti-collision early warning system, referenced in the design of obstacle-avoiding early warning system of embedded electronic guide dog, there mainly exits four steps, that is, information collection, information processing, information judgment, and warning information. The selection of anti-collision warning system for the design of obstacle-avoiding early warning system of embedded electronic guide dog should start from the characteristics of the highway network and the street network construction in our country, combing with the characteristics of the obstacle-avoiding early warning system of electronic guide dog, as well as the construction of our country's highway and street traffic integrated management system. We should optimize the choice of the obstacle-avoiding early warning system technique of embedded electronic guide dog, based on China's national situation. Therefore, in the design process of the obstacle-avoiding early warning system of embedded electronic guide dog, after the comprehensive and comparative analysis of the anti-collision warning techniques, we have compared the optimization principles of the anti-collision warning techniques, and finally adopt the approach of millimeter wave radar sensor + ARM microprocessor. The design of obstacle-avoiding early warning system of embedded electronic guide dog also adopts the generally used embedded processor. Due to the operating system is an aggregation of the system program, people don't have to consider the differences between different hardware, and could achieve the target of writing application software through the system interface provided uniformly. The embedded system will further compile all of its procedure codes, including the operating system code, driver's code and application code, into a whole paragraph of executable code and in inserted them into the hardware. UCLinux aimed at the micro-control field, designing the Linux system, which is specially designed for the CPU without MMU (such as the S3C44B0X adopted in this project), and it has done a lot miniaturization work for the Linux kernel. In addition, the design of obstacle-avoiding early warning system of embedded electronic guide dog also makes a full use of USB. USB is a new kind of computer peripheral communication interface standard, which abandons the defects of the traditional computer series/parallel, with advantages of the reliability of data transmission, hot plug (plug and play), convenient extension, and low cost. The warning system of embedded electronic guide dog avoiding obstacles studied in the project can effectively eliminate the influence of traffic environment and blind subjective factors, send the pre-trip alarm for the blind in time, effectively avoid obstacles such vehicles, to reduce traffic accidents caused due to the blindness.
paper_134	A set of 16 samples of olive oil, sunflower, canola and corn oil which mixed with different ratio of Authentication, were used for calibration and evaluation of developed system. Both of these areas work in model identification and data classification issues and will, in effect, be directly used in data mining, and both groups are active in identifying and using neural networks and decision trees  [2] . The purpose of the development of these methods is to estimate the quantitative and qualitative characteristics of the materials rapidly, non-destructively and reliably  [4] . Among the non-destructive methods, the dielectric method has advantages due to the relationship between the steady-state dielectric with some qualitative materials, the cost less than other methods, high speed and high efficiency in the construction of systems Different recognition of quality has attracted the attention of many researchers  [5] . Typically, the conductor plates of the capacitor are made of aluminum, zinc and silver, and among them, a dielectric can be placed in the air or other material. Based on the dielectric method, when a material is placed in the alternating electric field, the positive and negative charged particles in it will constantly tend to move in the electric field. The association of these properties with many quantitative and qualitative components of the product, high speed, low cost and high efficiency of its use as a new and non-destructive technology has been considered by many researchers to measure a wide range of qualitative and quantitative characteristics of agricultural products  [8] . The great benefit of mixing vegetable oils with lubricant makes it hard to find the type of product. The second pertain to data mining algorithms; third part related to samples and used methods in the article. They used a 4V sine voltage in the range of 10Hz to 1 MHz to determine the dielectric properties of a binary mixture of olive oil. In addition, PCA has been used to classify virgin oil samples, separately from fake oils. The results of the PLS calibration model showed good results for the adulterated oils with soybean oil as a counterfeit oil. Also, the charts sorted by the method showed clear performance for all oil samples and easily categorize them in different clusters. From the result of this study it can be seen that the dielectric spectrum can be used to detect fake oils with different types of oils with a percentage of their mixture below 5%  [12] . The relationship between the rates of fruit dielectric constant in a frequency of 1 kHz to 10 MHz was investigated. The results of the experiment showed that the dielectric constant is strongly affected by the size and volume of the fruit and also decreases with the increase in the fruit juice, which is clearer in a frequency of 1 MHz. They used 4-volt sin voltage with a frequency of 120 MHz and a rectangular parallel capacitor, which obtained capacitive capacitors (DCs) and voltages (DVs). This parameter had a good relationship with all the quality properties of eggs (elevation of the cell, height of albumin, etc.). Using this parameter and egg mass, they extracted regression models and reported that the application of this method to the egg production line and its grading based on quality properties requires more research  [14] . According to the researches that carried out in relation to the determination of the quality and content of agricultural products and food industries, it can be concluded that different methods have been developed to counteract the adulteration in these products. Then different classification algorithms by MATLAB software and various techniques such as support vector regression were done and finally output dates were processed. The samples of sunflower oil, canola oil and corn oil which are known as adulterated oils were also obtained from national markets. The dielectric experiments started on the dielectric parameters of olive oil one day after the preparation of the sample. The samples were blended with sunflower canola and corn oil, and they made up a total of 9 classes with varying percentages (from 60 to 100% in 5% increments), each of which was shed separately, according to the desired amount, of 100 centimeters inside the sensor. Due to the high flow of data, the ch340g chip on the Arduino board is used to measure dielectric parameters as well as a device that can detect the purity of olive oil. Arduino has a variety of ports, controls, side panels, and screen connectivity, while it may be smaller, less expensive, and possibly faster than a microcomputer system. In particular, SVM does not have the problem of local optimization in its training, it builds the categorizer with maximum generalization, determines its structure and topology optimally, and functions nonlinear differentiation easily with low computing using It forms the concept of an inner product in the Hilbert space. According to the results, Most R 2 related to Olive-Canola (0.90) and the least amount of R 2 pertain to Olive-Sunflower (0.86). Also, in regard to the amount of RMSE, result has shown 4.81, 2.54, and 4.38 for Olive-Sunflower, Olive-Canola and Olive-corn, respectively. A pair of matched logarithmic amplifiers provides the measurement, and their hard-limited outputs drive the phase detector. Figure 2  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn Using this technique, the amount of RMSE for the samples were equal to 3.65, 2.46, 3.31 and the amount R 2 was equal to 0.92, 96 and 93, respectively. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud.
paper_139	Much effort made towards in investigates mechanism with identical detection algorithms, still the retrieved web documents with outmodedlink. In this proposed system, we are successfully identifying and minimize the redundant information and like link in web documents. Retrieving relevant information from web without redundancy is more challenge task nowadays where in web mining communities  [3, 4] . Utilizing customary data recovery  [5]  and information mining systems it get to the known and obscure data from the Web content. Traditional web mining algorithms handle with structured document  [7] [8] [9] [10] [11] [12] [13] [14] [15]  than the advanced methodology of mining algorithm can dealthe entire heterogeneous document comprises of images  [9] , graphs, videos  [16] , etc. A query is searched in a web search tool to recover some significant and required data for the client, either the search query is known or unintelligible to the client, it generally to reply with relevant data rather than redundant, however we can't guarantee that the reply for the query about the significance and redundancy. Once the input query is requested, the search engine generate the document with multiple web pages along with the links, the user will be unaware about the content of the web pages, the extracted web documents contain multiple web pages either be redundant or not. The Document retrieved must follow some constraints which have less time & space requirements, based upon the criteria the extracted web document must be preprocessed, for preprocessing & information selection, need to apply some techniques such as stop word removal, Stemming of word, phrasing, normalization of tokens. Initially calculate measure for all the vertices and maintain the set U which contains a minimum and maximum degree for all vertices and isolated measure vertex. After applying the above steps the entire vertex without redundant information available in the set U. Pseudo Code forThe Proposed Algorithm KTMIN-JAK-MAXAM Step1: Compute degree measure for all vertices in the setup. Step2: Pick the minimum degree vertex 'v' in the set-up and include in the set U. Step3:While U doesn't include all vertices Step3A: Include the entire isolated vertex which is adjacent to the vertex 'v' to U. Step3B: Find the adjacent vertex 'u' to 'v' which is not in U and has maximumdegree. Case I:Regular set-up Connected Regular Setup Case I: A Consider the following connected set-up G 1 in figure 2, having 12 nodes having 3 degree in all vertices along with redundant links. Apply the proposed KTMIN-JAK-MAXAM ALGORITHMto G 1 By step 1: deg(All Nodes of G 1 ) = 3 By step 2: Mark the node A as visited and put it onto the set U. By step 3: There is no isolated vertex in the given graph G 1 By step 4: 4.1 Investigate any unvisited adjacent node from A. Future work aims to create a finite automata tool to produce only relevant and without redundant information of web documents in data mining.
paper_145	An upward shift of prevalence rate was observed among the higher educated respondents. Overweight and obesity were more among urban residents compared to rural residents and they were thirty two percent more exposed to overweight and obesity. Higher prevalence of obesity was noted among females. The proportion of overweight and obese was higher among them who did not do any physical labor. According to factor weights it was noted that the important factors for variation in the level of obesity were mainly gender variation followed by occupation, education and type of work. In 2016 WHO estimated that globally approximately 1.9 billion adults (age > 20 years) were overweight and more than 650 million adults were obese  [4] . Obesity is generally associated to a significantly higher risk of arterial hypertension, diabetes mellitus (DM), hepatic steatosis, hyperdyslipidemia and renal failure  [5, 6] . It had been observed in some research findings that youth who do not meet guidelines for dietary behavior, physical activity and sedentary behavior have greater insulin resistance than those who do meet guideline  [12] . For this reasons, World Health Organization considers the epidemic a worldwide problem which requires public health intervention  [13]  that act on different factors associated with overweight and obesity as well as technological changes that have lowered the cost of living of the people so that people can avail sufficient food with required protein. The aim of this paper was to identify the socioeconomic factors responsible for obesity and overweight among some rural and urban people of Bangladesh. The important factors for obesity and overweight were identified by factor analysis, where largest factor weight indicated the most important variables  [14, 15]  responsible for obesity. However, among this latter group of respondents also there were 91 diabetic patients. All variables were transformed to nominal form by assigning numbers to do the factor analysis. The variables included for factor analysis were residence of the respondents, their age, gender, marital status, religion, level of education, occupation, type of work, monthly income and smoking habit. The respondents were classified as underweight  [BMI < 20] , normal [BMI,  20 -25] , overweight [BMI < 30] and obese [BMI 30+]. Besides the factor analysis, the association of different socioeconomic variables with level of obesity were investigated. It was observed from the analysis that among 900 respondents 7.6 percent were underweight [  Table 1 ] and 19.1 percent of them were from rural area. Obesity was observed among 15.3 percent people and overweight respondents were 34.0 percent. The differences in proportions of level of obesity according to residential area were not significant [P (χ2 ≥ 5.128) = 0.528] which indicated that respondents for different levels of obesity were similar for both urban and rural areas. However, urban people were more exposed to overweight and obesity by thirty two percent compared to rural people [O. R.= 1.32]. They were in more risk of overweight and obesity by 51 percent compared to males [O. R.= 1.51] Obesity and severe obesity were observed almost similar among Muslims and Non-Muslims [  Table 3 ]. But the O. R.= 1.07 indicated that both the religious groups were similarly exposed to overweight and obesity. Among the investigated respondents 92.6 percent were currently married and 43.1 percent of them were normal  [Table  4 ]. But if classification of respondents was done into two groups, one group of ages <40 years and another group of ages 40 years and above, both the groups were almost similarly exposed to overweight and obesity [O. R.= 0.81]. Maximum normal group of respondents (53.8%) was observed among agriculturists. Maximum (25.5%) respondents of obesity was noted among housewives. But, all other professional groups of respondents had the similar risk of obesity compared to servicemen [O. R. = 1.04]. More respondents of normal group of people were observed (49.0%) among them who had income 20,000.00 -< 30,000.00.This group of people were 20.2 percent. More overweight people was observed among them who had income 30,000.00 -< 40,000.00 taka followed by the group of people who had income 50,000.00+. The value of O. R= 0.40 for higher income group of people compared to other income level did not indicate that rich people had more chance to become obese and overweight. It was noted that [  Table 9 ] 50 percent respondents were involved in official work with or without physical labor. Now, let us observe the association of level of obesity and prevalence of diabetes. But overweight and obese respondents were 62 percent more exposed to diabetes compared to other groups of respondents [O. R.=1.62] In one study  [20] , it was reported that smoking was one of the factor to increase the level of obesity. Among the smokers 47.2 percent were normal and 37.2 percent were overweight. The association between smoking habit and level of obesity was significant [P(χ2 ≥ 20.189) = 0.0.002] and smokers were 21 percent more exposed to overweight and obesity compared to non-smokers [O. R. = 1.21]. Thus, we were in search of identification of most important variables to explain the variation in the levels of obesity in the present data  [14] . The variables which were included in the analysis were sufficient to explain the variation as KMO = 0.633, χ2 = 256.371, p-value = 0.000. The significant multiple regression analysis using one of the included variable as dependent variable and others as explanatory variables also justified the inclusion of the variables for factor analysis. From the analytical results it was seen that all the p-values were less than 0.05 which indicated that the inclusion of variables for factor analysis were justified. From the results of the communality of the variables it could be concluded that 85.6 percent variation of the variable marital status would be explained by the extracted factors  [22] . The factor analysis extracted 5 components as these components explained 73.309 percent variation in the observations of obesity. From the factor analysis it was noted that the coefficients of the variables gender was highest followed by occupation, education and type of work. The second component showed that the most important variables to explain the variation in the data of obesity were residence followed by religion and income. The third component showed that the variable income was important for explaining the variability in the obesity. This component explained 10.86 percent variation of the obesity. The respondents were investigated mostly by the doctors and nurses from their working places. The investigated respondents were divided into 4 groups according to their level of obesity, where levels of obesity were decided by their levels of BMI. Around 50 percent respondents were overweight and obese. Higher (71.6%) prevalence rate of diabetes was noted among the overweight and obese group of respondents. The prevalence of overweight and obesity were significantly associated with age, religion, education, occupation, marital status, income and smoking habit. Factor analysis also indicated that some of the socioeconomic variables were responsible for increased rates of overweight and obesity. The factor analysis showed that sex, occupation, education and type of work were more important to explain the variation in the level of obesity. These are: a) Halt the rise in body weight by encouraging people so that they can take healthy home made food and avoid restaurant food / first food.
paper_212	The models developed however faced considerable challenges ranging from inaccurate representation of natural data for deterministic models, to methods of forecasting such as statistical extrapolation which assumes that current conditions will prevail which is not always the case. The purpose of this paper is to build on the concept of Gillespie's Algorithm based SIR models by developing a stochastic SIR model to simulate disease evolution in the population setting. On June 5, 1981 a mysterious disease was recognized among MSM in the USA. In 1984, Robert Gallo discovered that HIV was responsible for AIDS. This virus, being highly transmittable is analyzed and capturing how it is transmitted is crucial in attempting to model the disease. However, this is not always the case as there exist individual differences in the ability to transmit and acquire HIV that remain unexplained  [2] . The following year, 26 new cases of HIV were recorded from sex workers and the NAC was established. Sex workers have the highest percentage of HIV prevalence at 29.3% according to the Kenya HIV estimates report-National Aids Control Council. Mathematical modeling of HIV is the use of statistical tools and procedures to recognize the general pattern in the transmission of HIV and to translate a problem into a statistical form for subsequent analysis. There are various questions still left unanswered to date on the HIV epidemic. These questions are encompassed in the modeling of the HIV immunology, the HIV dynamics as well as the AIDS dynamics such as the dynamic distribution of the disease in the population and its likely magnitude. This study employs mathematical modeling tools in the transmission probability of HIV and analyses done on how the cumulative number of infected individuals responds as well as the AIDS death probability and how the cumulative cases of removed individuals responds to this probability. The following reviews consider models developed for HIV/AIDS data that either differed too greatly with other model estimates or still fail even with developments on the model. In 2007 the government of China alongside UNAIDS made an estimate of 700000 cases of HIV and 85000 cases of AIDS in China at the time, which is much lower that the estimates made by Liu  [4] . The group comprising of the largest individuals predicted to be living with HIV/AIDS was approximately 650000. In several concentrated epidemics, HIV prevalence estimates do not match reported cases and mortality estimates do not match reported deaths, even after adjusting. Even with the 2013 updates of Spectrum where adjustments were made in the parameter values empirically to improve the fit to program data, the estimates given by Spectrum still differed with data available. A stochastic differential equation SI model with demographic stochasticity has already been developed  [8] . They considered and analyzed a two stage SI model that allowed for random variation in the demographic structure of the population with the population size changing at different times which had an exponentially distributed rate of infection. Despite the fact that a lot of research has been done on modeling disease trajectory, not much literature is available on the use of Gillespie based SIR models to simulate the trajectory of a disease in the population. According to Koopman, deterministic differential equation models cannot capture the real-life representation because no matter how finely they divide populations into geographic and social space, the infectious population is spread out to cover the entire space. The inability of differential equation models to capture stochastic effects therefore has been demonstrated by studies done by Koopman  [11] . This study seeks to incorporate a stochastic aspect in the deterministic SIR epidemiological model. Several execution options have been suggested for the SIR model such as Gillespie's algorithm and agent-based models but they have not been extensively explored in literature. This paper will contribute and build on to the existing literature on modeling disease dynamics in the population with the model tested on HIV/AIDS data 1985-2018 to investigate if the simulated values would reflect results that are close to reality. This paper will help bridge the gap between conceptual epidemiological models and its simulated version by providing a developed version of an SIR model that solves one inherent problem that deterministic models do not reflect the natural data. The Classic SIR model The Kermack-McKendrick theory illustrates individuals grouped as susceptible and removeds only  [13] . The initial conditions changed over time and demographics not being included such that change over time was described as; (1) formula_0 The Kermack-McKendrick theory was later developed to a version where they tackled the problem of endemics  [14] ,  [15] . This transformed the theory to the basic SIR model such that when demographics were included becomes (4) (5)  (6)  where N denotes the total host population. denotes the birth rate and death rate denotes the infection rate denotes the recovery rate t denotes time point Model development The Gillespie algorithm was used to simulate a statistically correct trajectory given initial SIR conditions. The SIR model explained how the epidemic manifests in all the compartments. denotes the rate of birth denotes the rate of non-AIDS death denotes the rate of infection denotes AIDS death rate denotes model's time step Gillespie's procedure The Gillespie simulation procedure was developed to produce a statistically correct course for finite well-mixed populations  [16, 17] . The interaction between states is made possible by events outlined in this model as birth, infection, non-AIDS death an AIDS death. , , This Gillespie's algorithm based stochastic SIR model generates a statistically correct trajectory from the initial vector as , , where i=s, i, r S+I+R=N i denotes the population size of the state at time t denotes a function characterized by two quantities as a state change vector and a propensity function. The Markov property lets us specify a model by giving the transition probabilities-defined as rates-on a small interval between the compartments. Considering the fact that the propensity functions require to be in probability form, we explore this assumption further by defining and interpreting it. A Markov chain is interpreted here then, as a stochastic discrete-valued model with the Markov property that future states of a process depend on the current state. Continuoustime Markov chains are the basic tool for building discrete population epidemic models. The Markov property lets us specify a model by giving the transition probabilities-defined as rates-on a small interval between the compartments. The transition probabilities assigned are defined on an open interval (t, t + ), such that the probability an individual moves from the susceptible compartment to the infectives compartment is  [ 1 ] . The wait times between events can either assume an exponentially distributed wait time or the rate of flow between compartments can assume any of the following distributions depending on the results. In this study, a simulation was carried out on the SIR model to explain the trajectory of the disease by employing a stochastic element using Gillespie's simulation algorithm. Therefore, the implementation of a stochastic factor to an epidemiological model is a useful contribution to mathematical modeling. Recommendation for research would be to explore other variations of the SIR model such as SI, SEIR under Gillespie's algorithm. The SIR model as well as Gillespie algorithm could continue to be applied other areas such as viral marketing and behavioural science as has already been done successfully.
paper_214	The paper presents the usage of databases that store business data into a warehouse star model that permits to create queries using SQL language and business intelligence tools. This kind of model allow to the decision maker to create complex reports and graphs based on the columns from the dimension tables and measures from fact tables, that can be the base for creating alternatives and scenarious acording to the economical indicators. Building alternatives and scenarious is an elaborate task and must have a background in existing data structured in databases that have a special structure of dimensions and fact tables. This data warehouse star model allow complex analyses such as rollup, drill down, slice and dice through the dimensions and fact tables by using special tools such as online analythical processes and complex queries based on views and snapshots. The business environments require analyses on large amount of data, big data and necessitate advanced tools to query through numerous criterias and also to create different realistic scenarious that allow choosing one option, so the business manager can use the right tool to gain economic advantage. The optimal solution or satisfactory is obtained using either algorithms or formulas within optimization models, either by experiencing various possible alternatives in a process simulation. Such testing may be actions that can be made explicit in the framework model; enable better decision-making structure problem, allowing exploration of information flows and operational procedures without interfering with the actual operation of the system; using cybernetic control system, which underlies decision making in practice; There are a large number of parcel simulation program. Limits of simulation include: support of simulation model is simplified, built pursuing one goal, one key criterion. always taking into account the unique factors of a problem specific results can not be transferred to other issues; results are difficult to interpret, being dependent on random factors; no matter how powerful your computer is, the optimal solution is difficult to obtain a model that has many equations and a significant number of parameters. These limitations have led to the use of simulation only when the interactions between components of the system are complex, when factors random have a significant and requires a large number of observations on the behavior data, the problem can not be solved by an algorithm or experiments direct. Quantitative mathematical models are embedded in base models, managed by subsystem a management model that requires separate users from the physical aspects of data processing and storage that extract, create, delete and modify models. Interaction replaces classical execution, procedural, with a performance conducted by decider according to the stages of solving a problem decisions that necessitate different inputs. The activity of the coordinations of inputs is done in most cases with specialized software systems that create analytical databases or modeling languages. In the first case, the user is provided customized views of data stored by performing a diverse set of operations on transactional data. The next logical of optimization and forecasting, simulation assists with the running complex patterns, resulting variables whose analysis highlights the value adopated lead to a decision. Besides maintaining traditional information representation formats like charts, maps and diagrams used currently to represent multidimensional data there are used new types of dynamic graphs. After analyzing the results achieved and the objectives of their reporting, signaled differences and after identify problems it was reveal the need to take action. Solving the result of communication between all stakeholders, sharing the general manager responsibilities both at decisionmaking levels and the corresponding subproblems defined. For example, after the first phase, the scope may relate to excessive spending decisions of a functional department, inventories too high or adoption of a draft research and evaluation on the introduction of computers  [3, 4] . Intuition, creativity and experience allow decision-makers compare alternatives; predict outcomes of each alternative separately. From model design and solution choice there is a strict demarcation, certain activities may be conducted during both phases, and return of election phase in phase. It receives relevant and substantiated elements on activity in the real system and builds models for solving future decision making on the basis of current assessments.Of the foregoing that a decision support system provides a filtering of information provided to decision makers and indicates certain restrictions. Basically, it helps the decision maker during operation and defining the problem, generating satisfactory solutions and retention strategy. For an interactive decision support system architecture includes the following subsystems: Data management subsystem Subsystem management models User subsystem dialog Data management subsystem consists of the following elements: database management system oxidase data, data dictionary and declarative query language. The database contains no internal data, external data and personal data.Internal data consist from the current activities of the organization and operations of various functional departments image. Whatever the nature of their data is stored in relational databases, transactional system data or data warehouse, built on subjects of interest. In current systems, the company's intranet, are increasingly present data accessible through web browsers and multimedia items such as maps, images, sounds. In most cases there is SGBS transactional relational data system and a management database for multidimensional data warehouses created. The data dictionaries are permitted operations to add new data, deletion or retrieval of existing information according to certain criteria. The subsystem management model consists of the following components: base models, the management models, dictionary and processor execution models and integration patterns Base models contain the set of models that make it possible to analyze the facts and the choice of options in terms required by the user. Design is the stage where the data warehouse model is chosen, depending on the complexity of the system real user requirements and data structure existing in the company (databases, Excel spreadsheets...) Building a data warehouse there are three models: type star, snowflake patterns type and constellation type models. The models may change depending on the context, presenting the data in a structure bed, easily designed and accessible to end users. In such a model is highlighted: quantitative data centralized called measures of activity quantitative criteria for centralized aggregation  ,  referred sizes  relational table that stores the measures identified by   the facts dimensions is called table  Tables where aggregation criteria has explicit codes,  called type tables list. Data warehouse star The eastern type constellation when several schemes that use the same type star catalogs. The advantages of using databases for Decision Support Systems in businesses involves creating numerous queries by the decision maker and also allow to create many scenarious and variants to choose from. In the business area is crucial to store data into a warehouse such as a star modell and create many queries and reports so the decision maker can see the trends of the economical indicators and helps him to make a decision that can improve those numbers or it can be useful to implement a strategy for a further development  [1, 3] . The databases permits flexibility in creating queries by choosing columns from dimension tables and measures from the fact table and so the decision maker may choose what economical indicators is useful to analyse and also permits to create different types of graphs such as columns, pies, scatter points or webs. By simply analysing these reports and graphs helps the decision maker to create different scenarious by changing target indicators and corelate with the economical indicators that can't be changed  [2, 5] . The methods specific to the databases such as SQL language and the Business Intelligence tools allow businesses to explore data and to create alternatives that helps to choose the optimal variant according to the economical restrictions that came from the business environment. It is very important to store data into a warehouse model and then to create future strategies that can help a company to expand or to grow the profit in a realistic way based on complex analyses and alternatives.
paper_216	However, it exhibits some systematic biases or unrealistic assumptions like the log-normality of asset returns and constant volatility. This model is found as promising alternative as far as pricing of European options is concerned, due to its varied volatility of the underlying security and estimation of the risk neutral MGF. The MSE and RMSE of Wavelet model is 0.208546 and 0.456669 respectively which is much lower than that of Black-Scholes model and therefore in conclusion, Wavelet model outperforms the other model. Derivatives are instruments whose value depend on an underlying asset. An option is a type of derivative that gives the option holder the right but not obligation to buy or sell an underlying security at a specified price, at any time on or before a given date in the future as agreed on. The holder of a put option has the right but not the obligation to sell underlying security under some predefined terms of an agreement which includes; strike or exercise price, maturity date and option volume. European-style options can be exercised at the maturity date only while American options can be exercised at any time prior to or on the maturity date. This study therefore prices a European option using two nonparametric methods and a parametric method. The Black-Scholes option pricing formula which has been used as the benchmark to price European options in most of the previous researches due to its simplicity and low computational demand  [2] . This Black-Scholes formula remains the most widely used model in pricing options though it has some known biases which include volatility smiles and skewness. The Black Scholes model has been developed under some assumptions such as the risk-free rate and volatility of the returns are known and constant, the returns of the underlying security are normally distributed, markets are efficient (market movements cannot be predicted). In the real market, the asset returns follow a leptokurtic distribution which is in contrast to the Black-Scholes model where returns are assumed to be log-normally distributed. Also, in practice the volatility should vary in the market and as a result of the Black Scholes' assumption of constant volatility it results to volatility smiles. This model derives the closed form solution for pricing of a European options that is why it is used as a benchmark. According to the literature, this model is the latest theoretical contribution to the option pricing and it is better at capturing the volatility smiles which is as a result of the Black-Scholes' assumption on volatility constant. The implied risk neutral moment-generating functions (MGF) is estimated by the wavelet method. This research therefore compared the performance of Wavelet based pricing model and Black-Scholes model in the valuation of a European option. The simple closed form solution of European options was derived during the financial crisis  [2] . The derivation of the pricing formula is independent on the parametric form of the underlying security's price dynamics and hence the success of this model depends on the accurate capture of dynamics of the underlying security's price process. According to the researches as far as pricing derivative securities is concerned, Wavelet based option pricing model is the latest option pricing model in the literature  [6] . The approximation of the implied risk-neutral MGF of the underlying security (asset) returns was focused by developing a non -parametric option pricing model called Wavelet model  [4] . The following are some examples of these advantages; when there is presence of jumps in the underlying process MGF is more tractable, the option's obtained MGF is a continuous function, statistical moments of the asset distributions can be obtained using MGF and the risk neutral MGF can be used to estimate the out of sample options that has maturity dates which are different. The following are some of the applications of wavelet method in finance and economics as pointed out in  [6]  and  [7] ; Wavelets can be used in multi-scaling analysis. Algorithm has a superior performance in de-noising financial data with high frequency by using the first principal component to de-noise frequency variations of a business cycle with wavelets  [12] . Lastly, wavelets can be used to estimate parameters of the models which are unknown using wavelets in pricing of an American derivative security by levy process  [13] . formula_0 From the Black-Scholes formula above, we see that the strike price (K), risk free interest rate (r), time to maturity (T), the volatility (") and the current price of the stock (S0) are required. Based on some general assumptions like independent and identically distribution (iid) for asset returns and that the moment generating function is defined well, the wavelet based pricing model can be expressed as follows; formula_1 Where, t is the current time, is the underlying asset price at time t, # $ is the time t price for a European call option written on asset, K is the strike price and T is the future maturity date. The underlying asset dynamics and investor expectation in option prices is captured by the MGF Θ !$ + of the logarithmic returns /0 7 . The focus on this study was on pricing of a European Call option using two models, one parametric model (BS Model) and one non-parametric models (Wavelet Pricing Model). In order to estimate the risk neutral Moment Generating Function of the underlying security from option prices, we applied the methodology of wavelet. Therefore, one of the most significant input of this study is that the wavelet -based pricing model is an alternative model for pricing options and other derivatives on the same underlying asset with varying times to maturity and different strike values. We recommend more further investigation on the nonparametric models since most of the studies have focused on the parametric model especially Black-Scholes model. In this study, we also focused on pricing European call options and therefore we recommend an extension of the approach to pricing more complex options like American options which have no general closed form analytical solution.
paper_219	This paper introduces a framework to determine data quality on enterprise networks for net-centric and net-ready initiatives as introduced by the US Department of Defense (DoD). The DR attribute is introduced; it is important in enabling enterprise data consumers to sort, filter and prioritize data. The minimax decision model is chosen to meet the design philosophy that little advantage to the overall enterprise network performance will result from further investment in high performing attributes prior to balancing performance across all three model attributes. The ability to generate data combined with the ability to store data is beginning to outpace the capacity of the bandwidth necessary to simultaneously share all data amongst all the operational stakeholders. A fundamental shift in paradigm is required to ensure the stressors of conducting military operations are supported through data (of both high-quality and high-relevance) and, not burdened by attempts to manage its excess. The US Department of Defense (DoD) introduced the terms net-centric and net-ready to describe the mechanisms by which operators can search and discover information within the bounds of data. Net-ready also introduced concepts to improve access through mechanisms of data entry and network management; all in pursuit of increasing support to military operations. To implement net-ready, system developers add data tagging, search algorithms, additional communications paths, and a suite of tools to exploit the new forms of data organization to help operators sift more rapidly through data to find relevant information. Successful policy implementation amongst other factors requires system analysis methods that assist acquisition agencies in targeting limited developmental resources to areas of greatest impact to the overall mission objectives. This paper introduces such a method in support of achieving maximum data quality for military enterprise networks: a quantitative mechanism by which the value of different net-ready implementation options can be evaluated and graded. Importantly, a model for valuing enterprise data quality is introduced to bridge the gap between measure of technical performance and operational benefit. The discussion of quality of data in a communications network is usually limited to the Quality of Service (QoS) which measures user's satisfaction based on network performance metrics like latency and bandwidth. Thus the term data relevancy (DR) is introduced into the model for valuing data quality in the context of net-centric / net-ready. Vice Admiral Arthur K. Cebrowski, U.S. Navy, and John J. Garstka proposed that adoption of network-centric operations across the military enterprise would result in benefits at the strategic, operational, and structural levels and bring forth "a much faster and more effective warfighting style"  [2] . This new warfighting style is Net-Centric Warfare (NCW): an "information superiority-enabled concept of operations that generates increased combat power by networking sensors, decision makers, and shooters to achieve shared awareness, increased speed of command, higher tempo of operations, greater lethality, increased survivability, and a degree of self-synchronization"  [1] . The DoD introduced four criteria that must be satisfied for "Data Sharing in a Net-Centric Environment" via DoD Directive 8320.02 in 2004  [4]  which later in  [5]  expanded to the seven listed in this section. This directive and the subsequent series of 8320 series documents identify "the cornerstones of Net-Centric Data sharing"; data shall be visible, accessible, understandable, and trusted  [6] . In the years following, the Chairman of the Joint Chiefs of Staff Instruction (CJSCI) 6212.01 was released that set-forth the procedures for development and certification of a Net-Ready (NR) Key Performance Parameter (KPP); the NR KPP process later being subsumed into the Joint Capabilities Integration and Development System (JCIDS) process  [3]    [7] . The NR KPP specifies the attributes required of data sharing Information Technology (IT) introduced into the net-centric operational environment: (1) IT must be able to support military operations (SMO), IT must be able to be entered and managed on the network (EMN), and (3) IT must effectively exchange information (EI)  [3] . Making data accessible requires providing authorized users the ability to view, transfer, download, or otherwise consume a data asset once discovered  [6] . Making data understandable requires alignment of terminology, data protocols, data formats, and data meaning between produced and consumer  [6] . Alignment can be achieved via direct negotiation or-more practically-via the adoption of commonly referenced standards such as those listed indicated by the global information grid (GIG) Technical Guidance Federation  [6] . A consumer's trust in a data asset is dependent on multiple facets: assessment of the data asset authority, clear identification of the data asset source, tagging with appropriate security metadata, and maintaining a full pedigree of the data asset throughout the full process  [6] . To satisfy the attribute of support to military operations, IT deployed to the operational environment must support identifiable net-centric operational tasks and mission objectives  [3] . non-generic); the required performance of the connections be identified by quantifiable and testable measures of performance (MOPs); and the connectivity must be managed by a structured methodology  [3] . The specific data elements and assets exchanged with external networks as part of executing net-centric operational tasks are specified by the exchange information attribute  [3] . Each net-centric information exchange defined MOPs that are measurable and each information exchange must also identify how the four criteria for net-centric data sharing (visible, accessible, understandable, and trusted) are satisfied for authorized consumers across the enterprise  [3] . Evaluation of the data sharing enterprise requires a holistic view that considers the net-centric attributes of the data simultaneously with the quality of service for the data network. Additionally, the interdependence between cyber security and net-centric principles are indicated in the most recent update to the DoD's instruction for enterprise data sharing  [5] . The combined consideration for each of these areas yields a newly defined model for the data sharing enterprise comprised of three equally important attributes: data relevance, quality of data at source, and quality of service for the enterprise network. 1  illustrates the mapping relationship for each enterprise attribute to various DoD objectives of net-centric, net-ready and cyber-security. Thus to avoid bias consider unequal sampling of the user population by using login authentication to identify users to form strata of homogenous users. The survey is not just restricted to a complete enterprise system but can be performed in the early design phases of prototypes and help analyze operational performance of an enterprise attribute as a function of its objective measures, i.e. The sample is organized into their various strata with the number of elements in each total number of strata k, in the total population N. The HT estimator for stratification becomes formula_2 and estimate of the stratified mean is formula_3 where m k = ∑ i in s k y i /n is the arithmetic mean of strata k. With each attribute having its MOS value using the stratified estimate in (3). An important part of assessing the end-to-end performance of a data system is consideration of the inherent quality of the originating data prior to its entry into the network; to be referred to as the quality of data at source (QDS). An example of such an estimate for audio (speech wireless, VOIP, fixed, clean) is use of objective measures to form the perceptual evaluation of speech quality (PESQ) model. There are three levels of reference used in determining the models for estimating the subjecting ratings: full reference (undistorted service is available for comparison with distorted outcome), partial (or reduced) reference, and no reference. The reason to use the full reference is to capture environmental conditions resulting in the most accurate predictions of ratings. Video and audio have models for QoE but other data types still require development of objective measure models to predict their QoE subjective ratings. The prevailing method for assessing the quality of a still image is based on the ability to perform certain levels of object recognition with scoring defined by the national imagery interpretability rating scale (NIIRS)  [17] . User-satisfaction provides a true gauge of a network QoS  [11]  and the subjective assessment of that satisfaction is provided by QoE. Examples include understanding relationships between objective measures of QoS like jitter, throughput, and latency to be able to control the QoE. The measure of data relevance expresses the utility provided by the data towards the consumer's objective(s). Highly relevant data must be visible to the consumer, understood by the consumer, and provide support to the consumers' military operations (as depicted in the mapping of  Fig. A simplistic rejection of non-relevant data is insufficient given the negative effect that excess amounts of data can have on human decision makers, even when the data in the network is restricted to only relevant data  [23] . The use of the QoE estimate is proposed to provide a subjective rating of the overall relevance of data shared on the enterprise. The measure of tagged data relevance corresponds to how accurate the data producer was in expressing the intrinsic data relevance through the application of metadata that is understandable, relatable, and unambiguous to the data consumer. The metadata taxonomy needs to be sufficiently diverse to express the essential characteristics of the data product but not so overly detailed that the data tagging approaches the size and complexity of the data itself. The goal of the tagging and discovery process is to connect highly relevant data with an authorized consumer. This requires a thorough understanding of the wants, needs, and priorities of the consumer to realize maximum value of tagged relevance, and therefore a higher QoE. The measure of discovered data relevance is an indication of how well the enterprise system enables a consumer to differentiate the data product offerings accessible via the network according to the level of relevance and value to the mission objectives. To support a high level of discovered relevance, The taxonomy available to the consumer must be sufficient to explicitly discriminate the desired data features from the undesired. A poorly performing taxonomy would be one that prompts extraneous definition of detail or that includes terminology with such subtle variation as to lack the mutual exclusivity necessary to select between one term or the other. Just as the performance of tagged relevance is improved by an understanding of the consumer by the producer, discovered relevance benefits from awareness that the consumer has of the full range of data offerings of the enterprise and how they may be described. Without a strong mutual awareness, a consumer may prematurely end their discovery process with the first piece of seemingly relevant data believing that it is the best or perhaps the only product available to them. This is why an overall QoE value for data relevance is important as it indicates the relative success of the discovery process across multiple groups of users at finding data with intrinsic relevancy suitable to meet their needs. We first discuss the design philosophy for the overall assessment of the data quality for the enterprise: (1) The enterprise data quality is determined by the attribute with the lowest user satisfaction (QoE). For example, if the quality rating is fair for data relevancy then funds to provide an excess bandwidth to have an excellent QoS can be spent elsewhere as the overall data quality at best is going to be fair. Thus, aside from determining the value of enterprise data quality for purposes of data prioritization, we also have an analytical tool to identify areas of improvement and to allocate resources more effectively across the overall system. Formal minimax definition  [24]  is formula_4 where a(i) denotes the actions of the i-th player of n players, a(-i) is actions of all other players except the i-th, and v i is the value function of player i. The formal definition of Wald minimax model is  And for QoS the objective measures could be latency, packet loss, jitter, and sequence of packets. percentage of spatial and temporal coverage between consumer desired and actual data, and percentage of ontology alignment user and producer. 3 , a particular value of enterprise data quality is given at the start, but from start one could continue to improve QoS and QDS under Option A with no increase in the value of enterprise data quality, whereas Option B modifying the system delivering DR does increase the overall enterprise data quality. This paper introduced a framework to determine data quality on enterprise networks for DoD net-centric and net-ready initiatives. Thus, support is indicated for further research in the development of objective measures using the definition of data relevance elements presented and to determine models for predicting QoE ratings as a function of these objective measures.
paper_241	So the use of transformers protection in electric power systems is very crucial and critical as they are required for efficient transportation of electricity to consumers for long term. This paper is concerned with the differential current protection scheme of transformer using a differential relay technique with arduino Uno microcontroller as a differential relay responsible for comparing the differential current values and sends trip signal to the relay (acting as circuit breaker) to open the circuit when there is fault in the protected zone as a result of imbalance in the differential current values. The demand of electricity in our modern day as led to advancement in electrical power systems, which is reflected in the development of all the power system device generators, different sizes of transformers, transmission lines and the protection equipment. Typically, Power systems are built to allow continuous generation, transmission and consumption of energy. The system is capable of sustaining a variety of environmental and operating impacts that resemble normal operating conditions which may include lightning striking the transmission lines, excessive loading, deterioration or breakdown of the equipment insulation resulting to power systems experiencing occasional faults such as phase to phase or phase to ground faults, over current, over voltage or even temperature based faults. The Differential current protection scheme is based on the principle that the input power to the transformer under normal condition is equal to the output power and is concerned with having a differential or balanced current between the primary and secondary side which would be digitally displayed  [2] . If it finds any error then it sends commands to the circuit breakers to trip the main potential transformer and the buzzer gives an alert. The fundamental idea of the differential current protection scheme is to provide protection to the transformer if any fault occurs in the protected zone that will cause an imbalance in the differential currents. When there is an occurrence of internal fault an imbalanced or non-zero differential current will flow through the operating coil of the differential relay which would be substituted with a microcontroller that sends a trip signal to the relays needed to open the circuit. The differential protection scheme is concerned with the faults that arise from associated difficulties such as the magnetizing inrush current and saturation. The magnetizing inrush current is a phenomenon that occurs during brief initial state of energization of the transformer even when the secondary side has no load connected to it and has its current a lot higher than the rated current  [6] [7] [8] . The power transformer primary and secondary currents are measured by the current transformers, which are to be protected by the Arduino and the circuit breaker. The current transformers are each connected to the primary and secondary side of the step-up transformer and are used to reduce the currents to a measureable value that can be sensed by the current sensors. As a differential current comparator it is used to compare the current values from the outputs of both Current sensors at the primary and secondary sides of the power respectively, computes their resultant difference, which would process an instruction to send a trip signal to the relay depending on its programming and the time required to clear a fault. Acting as a control unit, used for monitoring and controlling the working activities of the protection zone and display values of voltage and currents sensed by the voltage sensor and current transformers respectively. The differential protection of a transformer is implemented using Arduino Uno microcontroller as a decision making device that sends a trip signal to the relay (acting as circuit breaker) whenever there is faults (internal or external faults). This paper seeks to design an alternative method of transformer protection with a digitally displayed, Arduino based system that will intelligently monitor faults which may arise due to current imbalance of the transformer and prompts a safety measure to protect the transformer.
paper_251	Clouds provide an infrastructure for easily usable, scalable, virtually accessible and adjustable IT resources that need not be owned by an entity but can be delivered as a service over the Internet. The cloud concept eliminates the need to install and run middleware and applications on users own computer by providing Infrastructure, Platform and Services to users, thus easing the tasks of software and hardware maintenance and support. Heterogeneity in the computational requirement, dynamic choice and infrequent usages types of resources of the users in modern era has main challenge for service provider (application developer and hardware manufacturer). Secondly, Now computing power or connected computing power (with network) has more demanding and significant role in almost all areas of epoch including market analysis, searching, map, accounting, medical, trading, shopping, rescue operations and many more, the list is endless. Hence demand of users is heterogeneous in nature so that varieties of application (hardware & software) have been developed to achieve the highest user satisfaction. Purchasing or licensing of all such required items (devices & applications) is not feasible to the organization or individuals in terms of the cost and installation. More comprehensive concept about cloud computing has been narrated and drafted by National Institute of Standard & Technology (NIST): According to NIST-"Cloud computing is a paradigm for facilitating expedient, on-demand network access to a shared cluster (pool/collection) of configurable computing power and resources (like applications, services, networks, servers, and storage,) that can be expeditiously provisioned and exemption with least management endeavor or without service provider interaction. Cloud computing is another example of technological advancement which offers dynamic provisioning of the utility on rent basis to the subscriber. A cloud service provider has deployed and manages to sufficient number of resource that has been shared to the entire subscriber as per the load requirement of the individuals. To achieve this task cloud service provider required highly efficient scheduling approach and the proper monitoring of the services provisioned or will be provision to subscriber. In the cloud computing. Hence agent based solution has been proposed to meet the requirement of the modern cloud computing with pace of dynamic provision to insure shrink in shrink out (elasticity) of the cloud service provider to achieve highest scalability and reliability in extent of the maximum availability of the service to the requisites. Outcome the results shows that the provisioning of SaaS (Software as a service) and its monitoring using agent has gives better result which is more efficient than existing approach. For developing proposed agent based system three types of public cloud and their services has been selected as test bed for better evaluation and measurement of the accuracy of the propose system. For this Cloudbees service provider has been integrated onto the developed SaaS application. Fault tolerance While author  [1]  and  [2]  proposed an agent based solution to solve the above listed QoS parameter that greatly affect the performance of cloud service especially SaaS. Following goals has been achieved or solved with integrating of the Mobile Agent to Cloud Computing service realization 1. To Evaluate and delivered the cloud computing services (SaaS) using agent (for better and fast delivery) using public cloud such as "New Relic and cloud bees". Deploying a web services under SaaS paradigm and evaluate the effectiveness of the web application in the cloud environment with the help of agent. Cloud bees service integrating a SaaS deployment on it and delivering through agent. Provisions of service and resources in cloud PaaS is an important function that provides analytical statistics about the current view of cloud (running instance for a user or group of users). Model for Proposed Work Our proposed work is to schedule and monitor cloud SaaS application onto the cloud and evaluate the performance of the same using proposed agent based. Resources: Algorithm for Scheduling (influenced from Aneka) developed onto the Cloudbees Proposed Algorithm Step1: Register user login Step2: Sign in the user login Step3: Select multi monitoring agent in cloud environment (for each subc_request with QoS constraints) Step 4: Mapping of resources between cloud client, cloud agent and cloud service providers. For evaluating the performance of the proposed agent system, the obtained results have been compared with  [2]  in which author has proposed "MABOCCF" the realization of the federation of different cloud (cloud interoperability) using agent. Proposed multi agent based solution has influenced from [2] but it's not the realization of cloud federation rather it has to evaluate the scheduling and monitoring of the SaaS (task) application in public cloud's (cloud federation not interoperability). Table 3  shows the user satisfaction of the deployed SaaS obtained results and has been compared with existing malty agent based method  Rather than performing analytical calculation, proposed system has been compared with the author's MABOCCF and NMABOCCF method. This paper proposed and developed an agent based enhanced method for better scheduling (for resource like CPU, memory etc. for their granting/releasing) and their exact monitoring in the cloud in context of public cloud computing service provider such as Cloudbees. Proposed agent based solution for guaranteed better elasticity and their efficient monitoring of the resources in the cloud which helps to gather analytical statistics of the resources currently held and will be used such a memory, number of instances and CPU. For evaluation of the propose agent based method data set (jsp) has been developed using jsp web pages and deployed onto the cloud evaluating the elasticity and its monitoring. To deploying created SaaS application in the cloud a PaaS service has been required to be subscribed, for this cloudbees PaaS service has been chosen. Then for monitoring and scheduling with software agent New Relic service has been used to customized the agent functionality to meet the propose systems requirement. Following few areas has been chosen as future work as derivative of the proposed multi agent based solution where the current work can be taken further.
paper_272	This paper addresses the description of all parameters and evaluates switching processing times for three circuit breakers, VD4 /ABB Vacuum type 6.6KV/1500A/20KV/40KV-IEC60-071 " for maximum switching times 2.7ms". Circuit switching system -electroplates including some devices utilizing vacuum interrupters have been viewed as eliminating both of switching surge, arcing currents and high frequency rates for any interval times (microseconds' Process). In addition the inside electroplates some characteristics can be described as followings: Non-sustained disruptive discharge voltages In general, the above characteristics are not involving associated with only vacuum switchgear itself a lone, but when connects the switchgear with power distribution transforms or synchronous motors, there are also many negative interactive processes for instance hysteresis currents, eddy currents and high voltage capacitive inside each transformer (Parasites electrostatics charged) feedback to the interrupter process. According to the Harris model and classification who was classified "the positive ions initiated from ionization process in an Anode plate and generates joule heating to a cathode plate surface during switching process"; i. Ionization zone ii. Current chopping refers to the prospective over voltage events which can result with certain types of inductive load (power transformer) due to the premature suppression of the power frequency current before normal current zero in the vacuum interrupter. The term multiple re-ignitions refers to the series of alternate re-ignitions and high frequency (typical several hundred KHz)interruptions usually resulting in an increasing train of voltage peaks; this overall phenomena is usually defined as voltage escalation  [3, 4] . Virtual current chopping involves the load current in the other two phases being forced to zero by superimposed highfrequency re ignition coupled current. It is important to appreciate that while current chopping and voltage escalation can occur in a single -phase circuit, virtual current chopping is specifically a 3-phase characteristic: the effects of normal current chopping, multiple re ignition and over voltage escalation in one phase can generate surge over voltages in the second and third phases. Pre-striking of the breaker in picking up a transformer load is somewhat similar to the multiple re-ignition event which occurs on opening a breaker  [7] . Restrike overvoltage due to the multiple re-ignition of circuit breaker when a switching interrupter process is initiated before current zero. Multiple repeated ignitions mean an over voltage magnitude is a straightforward concept: as the amplitude of any overvoltage increases, the probability of breakdown in vacuum or breakdown of solid insulation increases. The process of current chopping is the premature suppression of 50Hz or 60Hz circuit current before normal current zero due to instability of the arcs in a vacuum interrupter  [6] [7] . Although the current in the vacuum interrupter can chop to zero almost instantaneously (fraction of a microsecond), the current in the load inductance-3ph coils in the power transformer cannot attain zero value instantaneously. Since the high frequency characteristics of the power system equipment are depend on stray capacitances that mentioned above and inductances also and they in turn depend on physical dimensions, equipment layout, materials, cable lengths, all these can be represented as follows  [17] ; The figure represents TRV & Re-ignitions computed in the system when switching off power transformer load that the transient voltage approximately (350µs-450 µs) and shows very high frequency sub transients both going in the negative direction of the voltage -Left side. The analysis shows that the effect of damping in an oscillatory circuit can be described in terms of a single parameter, designed ŋ, or its reciprocal ʎ, which is the ratio of the resistance to the surge impedance of the circuit: This fact permits the construction of generalized damping curves for different values of ŋ, from which the solutions to many practical problems can be extracted with about the same effort as one expand in using a table logarithms. Normally our analysis for the switching process basically on parallel RLC circuit as following; L= Indicative load of stator winding coils C= parallel parasite `s capacitors -see the introduction R= evaluating resistance that can be damping oscillating. This has been done in Figure. By calculating the inductor current in parallel RLC circuit under conditions of a subsidence transient, but have a far wider application. The electrostatic charges -static charged -parasitism are almost depleted in the a mount current begin to decrease in the same time of first peak value which generates chopping currents, moreover the formula of the transition processing rate of du/dt or di/dt have to be considered. The other processing steps as followings: a) Chopping current b) Restrikes voltages c) Prestrikes voltages d) Multi reignitions e) Voltage escalation f) Post-arc currents g) NSDD (non-sustained disruptive discharges) It is often misunderstood how all these phenomena are created and what are their real causes.
paper_294	Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. This study therefore, looks at the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. It was therefore, recommended that concerned policy makers should make policies that will allow public libraries to acquire store and preserve audiovisual materials on various indigenous knowledge for development to ensue; and for posterity; and that governments and other stakeholders should ensure that the public library sector is adequately funded and manpower is adequately provided as well. The Tiv had different management strategies of postharvest losses of cassava (Akom or Alogo) that includes dried cassava chips (kpor), imyorun or imorun (gari) processing, fermented cassava (akpu) and production of base (mtuhem) among others, which were potent managing postharvest losses of cassava. Public libraries are supposed to be conscious of ensuring that these postharvest losses management strategies are captured, acquired stored in audiovisual format. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. Despite these postharvest losses management strategies, the different varieties of food stuffs processed from cassava are threatened to extinction due to the flooding of expensive western food stuffs in the markets; and the indigenous local markets where the products could be sold are not accessible due to lack of good access roads. In addition, these indigenous postharvest losses management strategies seem to be facing total extinction due to lack of documentation and storage of audiovisual materials on them. The study intends to investigate the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. Determined whether public library has impacted on Tiv management strategies of postharvest losses of cassava. Apparently, in order to achieve this mission, public libraries preserve a variety of information materials including print materials such as books, monographs, serials and periodicals; and non-print materials such as audio or audiovisual materials such as cassettes, microfiche and films among others. It involves among other things collecting information bearing materials from different sources, organizations and sometime even initiating and encouraging programs that would enable them capture traditional, socio-cultural and economic activities as well as appropriate information formats for posterity  (Agber & Mngutyô, 2013) . Importantly, among the various formats of information materials the public library acquires, audiovisual materials are most suitable for meeting the needs of users in Africa particularly in Benue State. Suffice it to say that the public library is established to serve everybody, it therefore, entails that it is established to serve those who cannot read and write alike. Therefore, it will be appropriate only if the public library will acquire audiovisual materials, which will be most suitable in meeting the needs of the indigenous Benue farmers. The Tiv people used to make a ridge round their houses and plant cassava on it; and when the cassava grew up, it became a cassava fence surrounding the house. Moreover, other varieties of cassava in Tiv land of Benue State include: Akpu from which akpu are processed, Yakpe, Genyi, Wari and Gyo-Akom among others. The variety of foods that are made from the roots and the nutritious leaves are reasons why cassava cultivation is expanding worldwide  (Lebot, 2009) . Therefore, the Tiv people develop different ways of processing it into durable forms soon after it is harvested, which forms part of management strategies for postharvest losses. Postharvest priorities across the globe have evolved considerably over the past four decades, from being exclusively technical in their outlook, to being more responsive to consumer demand. Consumer-driven trends which have contributed to this shift include rising incomes in urban areas, changing dietary habits, more women in the work-place, reduced time for meal preparation and growing demand for safety, quality and convenience  (Rolle, 2006) . Harris and Lindblad (1978)  asserted that postharvest begins when the process of collecting or separating food of edible quality from its site of immediate production has been completed. The food need not be removed any great distance from the harvest site, but it must be separated from the medium that produced it by a deliberate human act with the intention of starting it on its way to the table. Apparently, postharvest losses therefore mean any change in the availability, edibility, wholesomeness or quality of the food that prevents it from being consumed by people. In pursuance to boost agricultural development in Benue State, postharvest losses must be managed; and to achieve this, the Tiv people developed different management strategies for postharvest losses of cassava. Cassava has a short lifespan after harvest and as a result, the Tiv people process it into various forms for easy storage as a stratagem for postharvest loss management. Incidentally, these methods of cassava processing among the Tiv people practiced as strategies for the management of postharvest losses of cassava were passed down from one generation to another orally. The public library owes the people a duty to go into the hinterlands of the Tiv nation in Benue State to collect audiovisual materials on these postharvest losses management strategies. This would have been achieved by capturing of audio narrations of elderly cassava farmers by recording, as well as organizing custodians of the knowledge to shot informative video clips and snap shot for video slides aimed at educating the younger generation and storing the materials for posterity. Section C also consisted of 7 variables of ways public library has impacted on Tiv management strategies for postharvest losses of cassava and Section D consisted of 5 variables of challenges faced by public libraries in acquisition and storage of audiovisual materials on Tiv management strategies of postharvest losses of cassava in Benue state. Consequently, for the cassava farmers who had no western education, the research assistants read the questionnaire to their hearing and gave interpretation in Tiv language, and the options they selected were ticked for them. These research assistants were asked to administer and retrieve the questionnaire through personal contact to avoid delays associated with mailing and multiple filling. This means that peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips), peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour), peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom, slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base), peeling the cassava, fermenting, sieving and squeezing it into lumps known as Akpu, peeling the cassava, grating, squeezing (demoisturizing) it, mixing with salt and other ingredients and frying with palm or groundnuts oil known as Kweesi or Akweesa Akom and peeling the cassava, grating, squeezing and sieving the starch and preparing it for food as Kamu Akom are the management strategies for postharvest losses of cassava. Table 2  shows the mean rating scores of all the items are below 2.50, which implies that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). To answer the research question, data were collected relating to challenges faced by public libraries on acquisition and storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. This implies that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. Table 2 , findings of the study revealed that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). Findings from the study based on the result in  Table 3  showed that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. Therefore, it was recommended that concerned policy makers should make policies that will allow public libraries to acquire store and preserve audiovisual materials on various indigenous knowledge for development to ensue; and for posterity. Moreover, governments and other stakeholders should ensure that the public library sector is adequately funded and manpower is adequately provided as well. The study found out that public library does not impact on Tiv management strategies of postharvest losses of cassava through storage of audiovisual materials on the subject matter. Importantly, if public library has acquired and stored audiovisual materials on Tiv management strategies for postharvest losses of cassava, cassava farmers would have accessed these information materials to improve on various ways of managing postharvest losses of cassava in Benue State of Nigeria. The study concludes that public library must make it a point of duty to start acquiring, storing and preserving information materials on indigenous knowledge, particularly in audiovisual format, since this is the only way the library will be useful to the host communities.
paper_298	The study investigated the awareness and use of library catalogue for accessing information resources by users of Federal University of Kashere (FUK) Library. Survey research design was used, registered users of the University Library between 2016-2019 numbering 920 formed the population of the study where a sample of 272 (30% sample) was used based on 95% confidence level and 5% confidence interval of Sample Size Calculator. The findings of the study revealed that majority of the respondents were aware of library catalogue existence; they were more informed about card catalogue usage than OPAC for retrieving information resources. Poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. The study recommended that the library management should organize a periodic user education, orientation and sensitization programmes for the undergraduate users to create awareness and enable them gain the needed skills to use the library catalogue maximally when searching for information resources. Guidelines on how to use catalogue and library in general should be prepared and issued to registered users. Regular shelf reading should be done so as to establish right contact between library users and library materials. The university library goals and objectives are to provide adequate and relevant information resources both in print and online for university community to support teaching, learning and research (these for undergraduates students may refer to class work, assignments, research/project work, term papers, seminar presentation by providing relevant information and services provision for effective and efficient achievement of academic pursuit). University library provides well stocked information resources and trained personnel to organize available information materials and assist faculty members and student users in the retrieval and use of these resources. The traditional goals and objectives of the library catalogue are to enable users to search a library's collection to find items pertaining to specific titles, authors, or subjects. Libraries were traditionally known to provide access to library information materials through card catalogues and book catalogue as the primary information locating tools. Following advancement in ICT and subsequent development of Online Public Access Catalogue (OPAC), the traditional concept of access to library resources which many scholars identified to be prone to numerous challenges has changed. Students' skills in search catalogue can be created through library tours, orientations, seminars, lectures, library staff, handbooks and use of library studies. KumarandVohra investigated the use of Online Public Access Catalogue by the users at Guru Nanak Dev University Library, Amritsar (Punjab) and discovered that majority of the respondents 68.7% were not aware regarding OPAC, 12.5% stated the reason to be lack of assistance from library staff and slow speed. 91% respondents used the title search approach and 83.04% used the author search approach, User also indicated that the information regarding the problem faced by the respondents while using the OPAC like 74.39% faced by the problem lack of proper guidance about OPAC followed by 67.47% lack of awareness, 36.33% satisfied with the OPAC and its services  [7] . The studies of Oghenekaro found that users exhibit patterns of library catalogue usage, that education, experience and sophistication of library users determine the pattern or level of library catalogue use  [1] . Consequently, AmkpaandIman emphasized that the success or failure of students to locate resources in the library depends on the skill acquired though the library instruction progamme  [8] . Aguolu andAguolu observed that students use catalogue mostly for educational purposes and have really helped in conducting and disseminating information resources in the library  [10] . It must contain different types of materials, very rich in nature, comprehensive in coverage with adequate bibliographical tools describing the location of each item, which is significant to the whole concept of library and librarianship. However, it is observed sometimes that the bibliographic tools that supposed to lead or guides user to the location of a particular item in the library are either found in adequate, misleading, totally not provided or somehow incomplete. This finding is in agreement with Clifford & Zaccus whose study on "users attitude towards the use of library catalogue in two selected University Libraries in South West Nigeria" revealed that number of the male respondents is higher than female respondents  [11] . Librarians in discharging their responsibilities should inform library users by communicating the availability of new technology and the way it operates to them. The awareness of respondents would depend on the way the library informs their users about the use and functions of library catalogue revealing the available resources and its location in the library. shows that majority of the respondents 178 (68%) were aware of the card catalogues as a access point / retrieval tool for searching for information resources in the library. 26 (10%) of the respondents were aware of the existence of OPAC as a retrieval tools in the library. This indicates that majority of the respondents were not aware of online public access catalogue in the library and they only use manual catalogue for information retrieval. This development is not healthy for university library in the 21 st century, where information resources should accessed from different locations within and outside the university using modern ICT facilities like OPAC. The finding of the study reveals that the creation of awareness on use of catalogue through library orientation, university website and notice board is significantly low. This implies that majority of respondents became aware of information retrieval tools through library staff and user education programmes these are more formal sources of awareness about library catalogue function and use. The result proves that the management of the university library provides awareness opportunity to users for retrieval and utilization of information resources, except that they need to put more emphasis during library orientation to add to existing ones. Table 5  indicated that majority of the respondents 158 (60%) used the library catalogue regularly when searching for materials in the library. 24 (9%) of the respondents never used the catalogue but they were aware of it existence in the library as indicated on table 2 above. The study found that majority of the respondents were aware of library catalogue in the library including OPAC  [12] . The reason for this could be attributed to the fact that users did not have the ICT skills to manipulate the OPAC and they solicit the help of library assistants or even friends to retrieve information materials in the library. Higher number of the respondents indicated poor computer skills hindering navigation when searching OPAC 96 (37%) and inability to locate materials on the shelves, indicated in the catalogue as being available 58 (22%) were the challenges affecting the use of catalogue as an information retrieval tools. This indicates that good of the respondents had difficulties using the library catalogue because the respondents lack sound ICT skills that could enable them use the OPAC. Again, the later challenge could be attributed to lack proper shelf reading by the library staff, which made users not to locate material indicated available in the library by the catalogue and not visible on its shelf. The difficult interface of OPAC poses a big challenge to undergraduates who are the target users of OPAC in any academic library. It is disturbing to discover from the study, that most of the respondents were aware of the card catalogues as access and retrieval tool for searching for information resources in the library. Unfortunately, most of the respondents were yet to be conscious of the existence of the OPAC, do not understand its operations and rarely use the service despite the huge investment of library resources in the production. Another source of frustration was reasonable number of the respondents indicated poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. This will enable them gain the needed skills to use the library catalogue maximally when searching for information resources. b. Librarians should organize orientation and sensitization programmes that will create awareness, and encourage the university community to effectively use OPAC to ensure proper use of library information resources. c. Guidelines on how to use catalogue and library in general should be prepared and issued to registered users. d. Regular shelf reading should be done so as to establish right contact between library users and library materials and avoid misplacement or wrong shelving of information resources.
paper_305	This is primarily due to difficulties in uncovering uncertainties in information provided by credit applicants and also due to lack of reliable automated techniques that would improve the efficiency of manual underwriting procedures. The implementation was based on the java netbeans development platform to create an interface that was used to train a model and its subsequent use in predicting credit decisions. The results obtained proved that such a mechanism can be applied to augment manual credit appraising processes, especially where large volumes of applications are to be processed within limited timeframes. Therefore the screening of the customer's financial history as well as the ability to remain faithful to new financial obligations is a very significant factor before any credit decision is taken and it is a major step in reducing credit risk. Despite the increase in consumer loans defaults and competition in the banking market, most of the Kenyan commercial banks are reluctant to use artificial intelligence technologies in their decision-making routines. Generally, bank loan officers rely on traditional methods to guide them in evaluating the worthiness of loan applications. A checklist of bank rules, conventional statistical methods and personal judgment are used to evaluate loan applications. Given the absence of objectivity, such judgment is biased, ambiguous and nonlinear and humans have limited capabilities to discover useful relationships or patterns from a large volume of historical data. The purpose of this study was to develop a loan decision system using the logistic regression Meta modeling algorithm -Logitboost around Java based open source software for the Kenya commercial banks. This is the first empirical research of its kind in our country that addresses in a systematic way the issue of using Meta classifiers in loan ap-plications. The general objectives of this study were to: 1) Implement the meta learning algorithm -LogitBoost to develop as system for evaluating credit applications to support loan decisions in Kenyan financial institutions 2) Outline some of the challenges of using the learning algorithm in the decision-making process for the banking industry in Kenya 3) Champion the applicability of Java as an open source software in business intelligence applications From time immemorial in the banking sector, banks have relied on the personal assessment of loan risks or on the traditional statistical methods to predict the default of loans instead of using a standardized evaluation tool. These traditional methods often require a great deal of subjective input from underwriters, making them un-reliable and often lack empirical and scientific backing. Further, the study considered a binary output from the classifier, hence dependent variable can only take on accept or reject values with an emphasis on the banking industry in Kenya; though the results can easily be generalized to institutions elsewhere. Loan appraisal decisions can easily extend beyond the "accept" or "reject" kind of classifications to include such other spectral values as "fairly good", "good" and so on. Although the classifier takes this into account through voting -in which those values that meet certain thresholds are promoted to either of the classification values, most of such incidences are minimal and can be handled through judgmental procedures by re-examining those peculiar cases and applying policies as laid out. The ongoing changes in the banking industry, in the form of new credit regulations, the need for innovative marketing strategies, the ever increasing competition and the constant changes in customer borrowing patterns; call for frequent adjustments to credit management in order to remain competitive. The traditional credit appraising techniques based on a hybrid mixture of manual and statistical techniques such as indices and reporting, credit bureau references, post screening, fact act, multiple credit accounts and initial credit line, the manual input are definitely inadequate in modern times. Automated techniques have progressively become popular in contemporary loan appraisal processes. One of the earliest automated procedures uses statistical tools which have fallen short of the inherent challenge for today's commercial banks is their desire to understand large amounts of information and reveal useful knowledge to improve decision-making. Today, lenders are making increased use of new and innovative techniques -the key being data mining and machine learning to evaluate loan applications for business and financial prospects  [2, 3] . Credit appraisal often amounts to making a decision whether to grant or to reject an application. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced  [4] [5] [6] . One such algorithm that effectively addresses these issues is the LogitBoost Meta classifier -based on the log of the odds ratio for the dependent variable  [7, 8] . Although the model was reported to perform relatively better than models developed using other approaches; as part of the limitations and recommendation, they suggested that such a model is usually a black box and more insight the model parameters was required to make it more effective. Further, they suggested an improvement to the model by introducing a graphical interface for the loans officer. There have been various other attempts to deal with the loan appraisal problem using various techniques  [10]  to varied degrees of success. The solution to the problem was an adaptation of ensemble machine learning strategies where a 'weak' classifier, commonly referred to as a base classifier was boosted through a series of adjustments through weighting and re-sampling to develop a better learner which was an additive aggregate of individual learners. The boosting method was developed around the Probably Approximately Correct (PAC) model that entails transforming 'weak learners' into 'strong learners'. The reported technique derives from the intuitive understanding that instead of putting all the effort on finding highly accurate base classifiers, it becomes sufficient or even desirable to use a set of weaker hypotheses. In majority voting, to predict the class of a new item, each base classifier got to vote for either the 'accept' or the 'reject' class. A accept classification for a loan decision meant pointed to a successful application while a reject classification pointed to the alternative. It can be proven (as discussed here-under), that under the assumption that all individual classifiers have the same prediction rate and that the distribution of the data correctly classified by each base classifier is independent and random, this is the best possible strategy. In our study, the variable k was bi-valued and took on either 'accept' or 'reject' values and K was set at 2. a) With K attributes , there are K different decision stumps to choose from b) At each stage of boosting i. given reweighted data from previous stage ii. The Java Development Kit (JDK) which is a Sun Microsystems product released under the GNU General Public License (GPL) was one of the packages used especially for the compilation of the source files. This strategy is similar to the use of two files as discussed earlier but relies on the learner to automatically partition a given data set into two given a split percentage It exhibits better statistical foundations than other performance measure techniques with diverse application in medicine and computing. Three options were investigated for training the algorithm namely: a) The use of single file both for training and testing the model through stratified cross validation. The model accuracy using this procedure was 86.86% making it a fairly reliable strategy b) The use of separate training and testing data sets returned an accuracy of 95% making it a relatively better strategy c) The use of a ratio to determine the size of the training and testing files from one data set returned an accuracy of 88.64% Therefore, it implies from these findings that the use of separate files for training and testing of the model returns the best model accuracy and hence should be adopted. The trained model was subjected to 20 instances of unclassified data which had been carefully selected from a portion of the training and through analysis returned 19 correctly classified instances resulting in a predictive accuracy of 95% A cost matrix can be fined as part of the training procedure that penalizes wrong classifications especially the true negatives for this study. Finally, as stated earlier in the introduction, it is not prudent to completely rely on an automated credit appraising as some cases might require subjective interpretation and personal judgment. As a conclusion, the reported work indeed confirmed that: 1) Machine learning procedures can be applied in financial modeling applications to augment manual underwriting techniques 2) These procedures can greatly improve the efficiency of such techniques because of their ability to handle large items of data generating very useful statistics 3) This work can be improved through the use of enhanced data set pre-processing procedures, the use of a cost matrix as well as parameter tuning to settle on the most effective set for various data mining requirements.
paper_310	Android application based smart phones are becoming each time more powerful and equipped with several accessories that are useful for Robots. We present a review of robots controlled by mobile phone via moving the robot upward, backward, left and right side by the android application such as Arduino, Bluetooth. We derived simple solutions to provide a framework for building robots with very low cost but with high computation and sensing capabilities provided by the smart phone that is used as a control device. Here we use mainly Arduino UNO (ATMEGA 328P), Bluetooth module (HC-05). The project aims in designing a Robot that can be operated using Android mobile phone. The controlling of the Robot is done wirelessly through Android smart phone using the Bluetooth feature present in it. Bluetooth module, DC motors are interfaced to the Microcontroller. In achieving the task the controller is loaded with a program written using Embedded 'C' language. Still there exists a requirement of a cost-effective automation system, which will be easy to implement. Our system aims to achieve the target to design a system that can provide following functionalities with a simple and easy-to-use interface: a) Develop an android application that will act as an remote of a robot. d) An android smart-phone and the technology of android is vast and can be used to interact with embedded system. f) Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. h) Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. The system consists of following parts: a) Arduino UNO (ATMEGA 328P) b) Bluetooth module (HC-05) c) Smart phone d) Motor driver (l293D) e) Arduino software (version 1.8.1) The basic building blocks of the project have been described below: The microcontroller is programmed with the help of the Embedded C programming. Bluetooth module will give the commands given by smart-phone to the microcontroller. Here we the Bluetooth RC Controller application (  Figure 3 ) as the operating remote of this system. The advantage of this project is that the application software designed for android phones is kept simple but attractive with all necessary built-in functions. The program is burnt in the microcontroller using burner software. The program is stored in the EEPROM of the microcontroller, which is present in the Arduino board. In android application when we press a button, a corresponding signal is sent through the Bluetooth to Bluetooth module (HC-05) which is connected with the Arduino board. There are two steps of the programming. The working principle of the circuit has been elaborated with the help of a block diagram, of the system interconnection as shown in  Figure 6 . As seen from the  Figure 6 . The Bluetooth module receives the signal sent from an android smart-phone, where the application software coded in C language is installed. The microcontroller, thereby, sends instructions, which when executed, helps in functioning of the motor driver. Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. Our proposed project consists of the following three sections: a) Input section b) Microcontroller section c) Output section In our android application base Bluetooth controlled robotic car, the user interacts with the system with a smart phone. When user sends any data to the Arduino board then the corresponding pin of Arduino goes to high state and switches the motor driver ic in the on mode. These commands help the microcontroller to interface with the Bluetooth module HC-05 and also with the motor driver IC L293D. Here the Bluetooth module act as a receiver which receives the instruction from the smart phone (remote or transmitter). Then the microcontroller decides the operation for the instruction which is coming from the smart phone. The instructions are sent by the smart phone. Until we send any instruction to the microcontroller the motors remain stop. When any input is given then the motors moves as per the preloaded functions in the microcontroller. The novelty lies in the fact that it is a cost-effective project with a simple and easy to use interface compared to existing ones.
paper_333	The diagnosis of diseases on the plant is a very important to provide large quantity and good qualitative agricultural products. Enset is an important food crops produced in Southern parts of the Ethiopia with great role in food security. There are several issues and diseases which try to decline the yield with quality. Particularly, diagnosis of potential diseases on Enset is based on traditional ways. This study presented a general process model to classify a given Enset leaf image as normal or infected. Bacterial Wilt and Fusarium Wilt disease and collected 430 Enset leaf images from Areka agricultural research center and some selected areas in SNNPR. The proposed model demonstrated with four different kernels, and the overall result indicates that the RBF Kernel achieves the highest accuracy as 94.04% and 92.44% for bacterial wilt and fusarium wilt respectively. Around 80% to 85% of people in Ethiopia are dependent on agriculture; among these more than 20% of them depend on Enset crop production in the country. The government committed 15% to 17% of expenditures to the sector; it covers 41% of gross domestic product (GDP), over 90% of export value and directly supports 80% of the populations' livelihoods  [1] . Enset crop is related to and resembles the banana plant which is an indigenous plant classified under the monocarpic genus Enset and family Musaceae. There are several issues and diseases which tries to decline the yield with quality. Particularly, diagnosis of potential diseases on Ethiopian banana is based on traditional ways and due to limited research attention given to Enset crop production. There is a potential need for technology supported alternative systems to support the manual identification of Enset crop diseases so as to optimize the accuracy for remedial action. If the Enset crop diseases can be minimized then definitely the production will significantly be increased to contribute to the country's economy towards a new boost. Therefore, the implementation of IT based solution in the sector will have a paramount importance to facilitate the activities such as economic, social and ecologic development in the country by increasing efficiency in both quality and quantity of Enset crops production, to sustain dependability of customer preferences and to preserve the ecology  [2] . In our country few researchers found the promising solutions to different plant diseases diagnosis such as maize, rose flower, coffee and others using computer vision and machine learning techniques  [3, 4, 5] . So far a number of fungal, nematode, viral and bacterial diseases were reported to cause damage at different degrees of intensity that was mainly explored by Quimio research finding since 1992. Among various diseases, Enset bacterial wilt and Fusarium Wilt is considered as the most dangers one that reduces Enset yield  [6, 7, 8] . Test Result For the experimentation a total of fifteen experimental setups which is color, texture morphological and a combination of each and a multiclass Kernel support vector machine model: Linear Kernel, polynomial Kernel, RBF kernel and Quadratic Kernel was used. If there is a shortage of experts in the area of those disease identification the system is going to be a replacement for the experts.
paper_389	Experiments are carried out to analyze the influence of the choice of conditional random field model parameters and the selection of Chinese character annotation sets on the experimental results. Furthermore, the condition of random field model can be used to add the advantages of arbitrary features, and some new features are added to the model. For example, Xue N divides Chinese characters into four categories according to their different positions in Chinese words, and then divides them into Chinese characters by using the maximum entropy model. Zhou J built a hybrid method of Chinese word segmentation around CRF model. Common Chinese character tagging method is based on the Chinese characters appear in the words of different locations marked different labels. For example, "O" can be used to represent individual Chinese characters, "B" means Chinese characters appear in the head, "I" means Chinese characters appear in the middle or the end of the word. We first set three candidate marks "O", "B" and "I" for each Chinese character and add a start node "BOS" to the head, An end node "EOS". According to the meaning of "O", "B" and "I", the rules are summarized as follows: 1, the sentence of the first Chinese character tag can not be I, the last character of the mark can not be B. Finally, according to the mark of each Chinese character, the mark result is "OOBIO", so the result is "这 / 是 / 武汉 /." As mentioned earlier, the CRF makes it easy to add any feature in the observed sequence to the model, so that not only the transfer and emission characteristics of the traditional HMM sequence model can be incorporated into the model, but also some other The feature information associated with the observation sequence or with the language itself is added to the model. The probability of the i-character of the position as the prefix: pY> 95% 昨狭抨第竞耽 3 Z The probability of the i-character as the suffix: pZ> 95% 丸役袄侣丸瞰 4 R The probability of the i-character individual word of the position: 85% <= pR <= 95% 吧枚刘磊躺却 5 U Position of the i-character as a prefix of the probability: formula_0 The probability of the i-character as the suffix: 85% <= pV<= 95% 貌型胁帘午岸 7 D The probability of the i-character position of the individual word: pD <= 5% 言辽改信申仪 8 E The probability of the i-character of the position as the prefix: pE <= 5% 业络王场姆杨 9 F The probability of the i-character as the suffix: pF<= 5% 增隐晋香浓伊 In order to deal with the long-distance information, this paper takes the context distance as 2. The feature template in  Table 2  is a classification of Chinese characters, the template well dealing with numbers, letters and punctuation marks such as error-prone not logged in the above table, the  Table 1 feature  template is a basic feature template, Use word-based. The probabilities of the position probabilities in  Table 3  are extracted from the training corpus and the probability of each position is calculated according to the following probability formula: P (individual word) = number of occurrences of the individual idiom of the word / total number of occurrences of the word * 100% (1) P (prefix) = number of occurrences of the word as a prefix / total number of occurrences of the word * 100% (2) P (suffix) = number of occurrences of the word as a suffix / total number of occurrences of the word * 100%  3 After many experiments and comparisons, the probability of more than 85% or probability of less than 5% of the word as a location probability feature, but also on the selected word for some filtering, the elimination of some unnecessary words, such as At the same time, select the probability of greater than or equal to 85% of the word is subdivided into the probability of 85% <p <95% and p> 95% of the two sets. In this paper, CRF automatic word segmentation experiments, the use of features include the following two: a single word characteristic: a position on the word characteristics. CRF word segmentation process of Chinese word segmentation shown in  Figure 2 : In order to construct the CRF model, we must first use a standardized process to convert the original corpus into a standard form. The standard corpus form used here dictates that each line in the corpus contains only one word, and the information associated with the word is followed by a tab stop followed by the word. Secondly, feature extraction is carried out to generate training corpus and test corpus which can be recognized by CRF model tool. The format is as follows: each line includes a word and some characters and markings related to the word, characters and characteristics, And between the feature and the tag are separated by tabs. In addition, automatic word segmentation system should also be easy to expand, maintainability and portability; to support different regions, different application areas of different application goals; vocabulary and processing functions, processing methods can be flexible combination of loading and unloading, thereby enhancing the system Processing precision and processing speed; also, to build a "information processing with the modern Chinese word segmentation standard" to match the common or common modern Chinese word segmentation. Table 4  below shows the results of the CRF word segmentation system on the Yangtze River Daily Test Set and the contribution of each feature template to the results. Moreover, since the word position probability feature is extracted completely from the training corpus, some of the participle criterion information of the corpus is extracted to a certain extent, so that when the test set is tested, Corpus. In order to compare the availability of the conditional random field model, we also established two word segmentation models: Hidden Markov Model (HMM) segmentation model and Maximum Entropy (MEM) segmentation model. (CRF) word segmentation model, the experiment uses the combination of "feature template one", "feature template two" and "feature template three" in the common daily closed test set Test, the performance comparison of the results shown in  Table 5 . This chapter first briefly introduces the CRF tools, experiment corpus and standard of experimental evaluation in Chinese word segmentation experiments. The experiment not only demonstrates the influence of the choice of conditional random field model parameters and Chinese character annotation set on the experimental results, but also verifies the validity of the new features and the feasibility of the new method. As the Yangtze River Daily corpus is from the newspaper news, for the news corpus in the special format, such as title, poetry, weather forecasting, etc., to our model training has a certain impact, so if the corpus in the handling of these disturbances, Our model should have better performance.
paper_391	Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. Globally, tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually. Thus, the aim of this study was to identify the best classifier, and to predict the pattern from the TT data set using the data mining algorithms technique. The data for this study were the Tetanus Toxoid data set from the Ethiopian Demographic and Health Survey (EDHS) 2011, and analyzed using the Knowledge discovery process of Selection, Processing, Transforming, mining, and interpretation. These results suggest that among the machine learning algorithm tested, multilayer perceptron classifier has the potential to significantly improve the conventional classification methods for use in EDHS data of Tetanus toxoid. Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. Thus, women receive doses of tetanus toxoid to protect their birth against neonatal tetanus  [1] . Infection is acquired through environmental exposure of any broken skin or dead tissue such as a wound or when the umbilical cord is cut to the spores of the bacteria. Globally tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually, accounting for up to 25% neonate's death in some African countries  [3] [4] [5] . As of December 2012, maternal and neonatal tetanus persist as a public health problem in 30 countries, mainly in Africa and Asia  [2] . In sub-Saharan Africa, up to an estimated 70,000 newborns die each year in the first four weeks of life due to neonatal tetanus  [5] . Ethiopia has one of the highest neonatal tetanus morbidity and mortality rate in the world due to low tetanus toxoid immunization coverage coupled with some 90% of deliveries taking place at home in unsanitary conditions. In Ethiopia in 1999 WHO has estimated about 17,875 neonatal tetanus cases and 13406 NNT deaths which made the country to contribute to 4.6% of the global NNT deaths  [3] . Ethiopia's Expanded Program on Immunization (EPI) started in 1980 and remains the single most important component of primary health care supported by the Ministry of Health. The vaccine to prevent Maternal Neonatal Tetanus (MNT) introduced as part of routine immunization programs in over 100 countries by the end of 2011. Vaccination coverage with at least two doses of tetanus toxoid vaccine estimated at 70% in 2011 and an estimated 82% of newborns protected against neonatal tetanus through immunization  [3] . However, until now maternal and neonatal tetanus persist as public health problems in 36 countries, mainly in Africa and Asia. The TT vaccination schedule in Ethiopia for childbearing women follows the schedule recommended by WHO for developing countries  [6] . Immunizing the mother prior to childbirth with TT protects both her and her newborn against tetanus and antenatal care is the main programmatic entry point for routine TT immunization. A pregnant woman should receive at least two doses while pregnant unless she already has immunity from previous TT vaccinations. Five doses of TT can ensure protection throughout the reproductive years and even longer. In this study, we have used several data mining techniques; classification, clustering, association and outlier detection techniques over the nominated Tetanus Toxoid immunization dataset of the EDHS 11. The main objective of this project was to identify the best classifier, and to predict the pattern from the TT data set using the data mining algorithms and tools for tetanus toxoid vaccination and to connect the technical field public health and medical field to serve the community. Knowledge discovery from data for prediction of the tetanus toxoid immunization among the women of childbearing age in Ethiopia following the standard process, guiding us in the analysis process, and exposing those aspects that could otherwise be neglected. The goal of interpreting and evaluating all the patterns discovered is to keep only those patterns that are interesting and useful to the user and discard the rest. The EDHS 2011 was conducted by the Central Statistical Agency (CSA) in collaboration with Measure DHS and ICF International under the auspices of the Ethiopian Ministry of Health (EMoH). For this particular study, the dataset was requested and accessed from DHS website https://dhsprogram.com after formal online registration and submission of project title and detail project description. Data values and attributes were modified, added and/or deleted, filtered, recorded, dropped the missing values, transformed and attributes are integrated in order to be used by the machine learning techniques in the analysis step for the study. The classification methods used in this study is to classify data according to their classes putting the data in a single group that belongs to a common class. Where each branch represents an outcome of the test, each internal node denotes a test on an attribute, and each leaf node holds a class label. This approach uses divide and conquers algorithm to split a root node into a subset of two partitions till leaf node that occurs in a tree. K-Nearest Neighbor Classifiers K-Nearest Neighbor is one of the simplest classifier which discovers the unidentified data point using the previously known data points, that is, nearest neighbor  [10] . When given an unknown tuple, a k-nearest-neighbor classifier searches the pattern space for the k training tuples that are closest to the unknown tuple. In K-fold cross-validation, the initial data are randomly partitioned into k mutually exclusive folds, D1, D2 … DK, each of approximately equal size. To discover acceptable classes using Simple K-Means based on the principle of maximizing the similarity between objects in the same class i.e., intra-class similarity and minimizing the similarity between objects of different classes i.e., inter-class similarity  [7] . First, it randomly selects k of the objects in D, each of them initially represents a cluster mean or simply center. This is to identify the frequency of the selected attribute occurring together with access to Tetanus toxoid vaccination, based on a threshold called support, identifies the frequent attribute sets. Another threshold is Confidence, which is the conditional probability than an attribute appears in a transaction using the Apriori algorithm. This shows that the results of the accuracy and performance of learning machines on the tetanus toxoid vaccination data set are hence reliable and can be used as good indicators of the ability of the classifier for detection. The only attribute of the algorithm we are interested in adjusting here is the "Num clusters field", which tells us how many clusters into five based on the instruction given. Briefly described as follows: Cluster 0-This group have 1534 (27%) instances of which consist of mothers who live in the rural, no access to radio, no access to TV, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 1-This group consists of 1004 (18%) instances of which mothers who live in the rural, no access to radio, access to TV yes, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, head of household male, tt-vaccinated yes. Cluster 2-This group consists of 2063 (37%) instances of which mothers who live in the rural, no access to radio, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated No. Cluster 3-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 4-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, orthodox in religion, Amhara in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education no education, women age 35-49, mother marital status married, hh_head Female, tt-vaccinated yes. As explained in the    The literacy status of the mother has high information gain (0.046) and followed by the literacy status of the mother (0.041) and the least information gained was head of household (0.00000147). In this study, the data mining tool and algorithm like (J48, k-nearest, and Bayes) is used for selecting the training and test data, for classification, and K-means approach for clustering, single-dimension association rule to identify the best association. These results suggest that among the machine learning algorithm tested, multilayer perceptron classifier has the potential to significantly improve the conventional classification methods for use in medical data.
paper_402	The BPNN and SFS were used interchangeably to identify the relevant features that contributed with the response variable. The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. It is concluded that the usage of ANN with SFS provided an improvement to the prediction model's accuracy, making it a viable tool for machine learning approaches in civil engineering case studies. Mainly, ANN was utilized to model the nonlinear behaviour of fatigue and creep of Reinforced Concrete (RC) members  [5] [6] [7] [8] . Recently, research interest has revolved around the development of ANN models to interpret the behaviour of structural materials such as steel, concrete, and composites  [9] [10] [11] [12] [13] [14] . The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete  [14] [15] [16] [17] . The evolution of UHPC has lead structural engineers to improve the compressive strength, ductility, and durability of heavy loaded reinforced concrete (RC) structures. Several researchers have been investigating the mechanical behaviour of UHPC and its applications over the last four decade, where it was founded that UHPC exhibits a compressive strength that would range from 150 to 810 MPa  [22, 23] . The underlying material constituents that enable such a superior mix are cement (up to 800 kg/m3), water/binder ratio that is lower than 0.20, high-range water-reducing (HRWR) admixture, very fine powders (crushed quartzite and silica fume), and steel fibers. Other researchers proposed different mixtures by adding fly ash and sand to reduce the amount of cement and silica fume, and acquire an optimum mix that is both economical and sustainable  [25, 26] . Therefore, researchers began conducting investigations on the utilization of machine learning techniques for the development of prediction models that could assist engineers and researchers to produce appropriate UHPC mixes. Despite the statistical advantages of ANN, it has been long regarded as a black box that evaluates functions using input covariates and yielding outputs. There are several machine learning techniques, in the literature, that assist researchers in identifying the underlying covariates impacting the prediction model. ANN) until the model's error function increases. There are two types of SFS classes -mainly filter method and wrapper method  [28] , where Zhou et al. Four sets of open human motion data and two types of machine learning algorithms were used. Moreover, Rodriguez-Galiano et al. Four types of machine learning algorithms were used as wrappers for the SFS. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. There are two types of ANN models: (1) feed forward; and (2) feed backward. The input neurons are responsible for containing the independent parameter presented by the user, the wires represent the randomly generated matrices called weights that manipulate the function's slope or steepness, the hidden neurons map the weights variables using an activation function, and the bias units control the output function's shift, either upward or downward. There are two types of search algorithms: sequential forward selection and sequential backward selection. The ANN numerical solver, Levenberg-Marquardt, was verified by testing different number of neurons using a basis like the normalized mean square error (NMSE) to measure the error. As a result, the model that used the selected features showed stronger agreement with the experimental results in contrast with that prior to the selection. The correlation plots between the predicted and experimental results for the ANN models, with and without selected features using SFS, are summarized in  Figure 4(a)  presents the percent deviation, where an arbitrary percent deviation was plotted above and below the perfect fit line with a deviation value of ±20%. The LSR model is a linear function and its form is shown in  (2) . Using Fly Ash quantities that range between 0-200 kg/m 3 and Silica Fume quantities that range between 40-160 kg/m 3 while fixing the quantity of cement at 1400 kg/m 3 and water at 175 kg/m 3 , several plots showing the variation of strength of UHPC were generated as shown in  Figure 5 . The SFS tool was used to select the relevant constituent that impacted have the most impact on the compressive strength of UHPC which are mainly Cement, Sillica Fume, Flyash, and Water. It can be concluded from this study that: 1) The use of ANN with SFS reduced the number of input parameters needed to accurately predict the compressive strength of UHPC mix for the prediction of compressive strength, making it less computationally expensive.
paper_418	Time series analyses are statistical methods used to assess trends in repeated measurements taken at equally spaced time intervals and their relationships with other trends or events, taking account of the temporal structure of such data. An important aspect of descriptive time series analysis is the choice of model for time series decomposition. The ultimate objective of this study is therefore, to compare the row, column and overall means and variances of the Buys-Ballot table for additive and mixed models. Table 1 shows that the column variances of Buys-Ballot table is constant for additive model but depends on slope and seasonal effects for mixed model. Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model. Also, when there is no trend (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. An important aspect of descriptive time series analysis is the choice of model for time series decomposition. The emphasis is to compare the row, column and overall means and variances of the Buys-Ballot table for additive and mixed model when trend-cycle component of time series is linear. Iwueze and Nwogu  [10]  observed that when the trend-cycle component is linear, the column variances of the Buys-Ballot table are constant for the additive model, but contain the seasonal component for the multiplicative model. The implication of this is that when the test for constant variance says the appropriate model for a study series is not the additive model; an analyst still faces the challenge of distinguishing between mixed model and the multiplicative model. For a series that has linear trend, the row, column and overall means and variances of the Buys-Ballot table for additive and mixed models are obtained by Iwueze and Nwogu  [14] , Nwogu el al  [15]  are given in  Table 1 . For additive and mixed models, 1 (a) The row means mimic the shape of the trending parameters and do not contain the seasonal effect for the additive model. 2 (a) The column means mimic the shape of the trending parameters and contain seasonal indices for additive model. (b) For mixed model, the column means mimic the shape of the trending curves of the original series and contain the seasonal indices. The row and overall variances contain both trending parameters and seasonal indices for additive and mixed models. The column variances of the Buys-Ballot table is constant for additive model, but a function of slope and seasonal indices for the mixed model. We assume that the length of periodic interval is s For additive model, using the expression in table 1, we obtain  formula_6 when there is no trend. That is when 0 b =  (Table 1)  For mixed model, we obtain using the expression in  Table  1  ( ) ( )  formula_8 when there is no trend. That is when ( 0 b = ), it is clear from  For mixed model, we obtain using the expression in  Table 1  . Table 3  that when there is no trend i.e. This paper has discussed the Buys-Ballot procedure for comparing the row, column and overall means and variances of the Buys-Ballot table for additive and mixed models in time series decomposition when trend-cycle component is linear. Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model. Also, when there is no trend ( ) 0 b = , the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models.
paper_428	The present article examines the influence of thermal radiation on two-dimensional incompressible magnetohydrodynamic (MHD) mixed convective heat transfer flow of Williamson fluid flowing past a porous wedge. In Non-Newtonian fluids, the most frequently encountered fluids are pseudoplastic fluids, and Navier-stokes equations alone are insufficient to describe the rheological properties of these fluids, therefore, to overcome this defect, several rheological model like Ellis model, Power law model, Carreaus model and Cross model are presented, but little attention has been compensated to the Williamson fluid model and estimated to explain the rheological properties of pseudoplastic fluids. Williamson analyzed the flow of pseudoplastic materials and presented model to described the behavior of pseudoplastic material and explain convenient importance of plastic flows, and also recognized that viscous flows are very varied from plastic flows  [1] . attained both numerical and analytical solutions of Williamson fluid transport through stretching surface subject to joule heating, and they observed that both methods have great argument with all parameters of flow  [7] . studied the peristaltic motion of non-Newtonian fluid through asymmetric channels along porous wall by means of various phase and amplitude, and also studied the manipulation of different wave structures on the fluid flow model  [8] . found numerically convergent solutions of two dimensional flows of non-Newtonian fluids along chemically reactive species  [11] . examined MHD motion of nanofluid owing to rotating disk with partial slip  [20] . Jabar addressed the influence of viscous dissipation and joule heating on MHD flow through a stratified sheet subjected to power Law Heat Flux having heat source  [22] . Reddy discussed unsteady MHD transport of rotating fluid past a permeable surface confined by infinite vertical permeable plate and concluded that by increasing rotating parameter the velocity field is also increased  [23] . Mixed convection is a coupled phantasm of two heat transfer mechanisms force convection and natural convection that act simultaneously to transfer heat in a fluid flow. analyzed mixed convection flow with irregular fluid properties through a vertical wavy plate  [33] . studied both forced and natural convection boundary layer transport by perpendicular surface in a stratified medium along connective boundary conditions  [34] . By utilizing Killer box technique numerical solutions of problem of mixed convection axisymmetric flow of air with variable physical properties was obtained by Ramarozara  [36] . Bau investigated the thermal convection in a saturated stratified medium bounded between two parallel eccentric cylinders with the help of a regular perturbation expansion along Daarcy-Rayleigh number; it was observed that the appropriate preference of eccentricity values can maximize the heat transfer inside annulus of various thermal insulators  [37] . investigated flow reversal of mixed convection in a three dimensional channel and concluded that an increase in Richardson number, natural convection dominates the flow and thermal field of combine convection  [39] . Kaya found nonsimilar solutions of steady laminar mixed convection heat transfer flow from a perpendicular cone in a porous medium with influence of radiation, conduction, interaction and having high porosity  [40] . studied unsteady combined convection flow in a cavity in presence of nanofluid  [41] . outlined mixed convection boundary layer flow influenced by thermo-diffusion  [42] . Chaudhary and Jain studied the impact of mass transfer, radiation and hall on MHD mixed convection flow of viscoelastic fluid in an infinite plate  [43] . Ferdows & Liu obtained the similarity solutions of mixed convection heat convey in parallel surface with internal heat production  [44] . Malleswaran & Sivasankaran carried an analysis for mixed convection flow and noticed that the average heat transfer decreases with an increase in Richardson number but in general heat transfer is better at force convection mode than free convection mode  [45] . studied boundary layer flow of fluid in a porous wedge subject to Newtonian heating along heat generation or absorption  [69] . Figure 3  displays the velocity profile for various values of the magnetic parameter M, the ratio of electromagnetic force to the viscous force that quantifies the intensity of applied magnetic field. The steady, incompressible two dimensional boundary layer flow of Williamson fluid past a porous wedge is analyzed numerically using the 5 th order Fehlberg technique.
paper_432	And found some results; The theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds, possibility study of Hamiltonian tubes when the simplistic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models. The Cotangent Bundle * is a smooth manifold itself, whose dimension is 2 . We can introduce the Legendre transformation we need some basic facts about the structure of the cotangent bundles * of a nd dimensional differentiable manifold . The Cotangent Space T 5 * (M) of a manifold at 0 ∈ is defined as the dual vector space to the tangent spaceT 5 M. A dual vector space is defined as follow ∶ given an − dimensional vector space G, with basis H , I=1, 2, 3,…, , the basis J = of the dual space G * is determined by the inner product. The Cotangent Bundle Τ * contains the following classes of Lagrangian submanifolds; The fibers of Τ * . Then exp is locally trivial fibrations with fiber the integers †. Let (H, |,`, 0) be a locally trivial fiber space whose total space and base space are path -connected and & a pathconnected topological space. For the mapping.∅: & → | to have a lift ‹ satisfying the condition ( ‚ ) =J ‚ , where ‚ ∈ &,J ‚ ∈ H, 0 (J ‚ ) =d 0 =( ‚ ), it is necessary that∅ (π n (&, ‚ )) ⊆0 (a n (H,J ‚ )) (3. Proof: If such a lift ‹ exists, then diagram is Commutative. A , principal bundles is a quintuple (0, , , R , where: 0 V →0 is a , right at action with the property of local triviality: Each point e ∈ has an open neighborhood for which there exists a , -diffeomorphism. It is a smooth manifold; in fact × °→ / is the fiber bundles associated with the principal bundles → / via the H on S. The left multiplication commutes with the twist and descends to a smooth on G × °, namely n`. A slice theorem (or tube theorem) is a theorem guaranteeing the existence of a tube under certain conditions. In this part we study the symplectic geometry of cotangent -lifted action induced by a smooth proper action of a Lie group on a smooth manifold. Symplectic manifolds have their origin in the geometric for Hamilton ' s and Lagrange's equations of classical mechanics, where symmetries is the main tool that can be used to simplify the equations of motion. This model is known as the Hamiltonian tubes; it the basis of almost all the local studies concerning Hamiltonian of Lie groups on symplectic manifolds. In the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models. The symplectic reduction of the cotangent bundles * Q has more structure than a symplectic manifold. Let act freely and properly by cotangent lifts on * Q, and let Jbe the momentum map of the G action (with respect to the canonical symplectic form on * Q. Leta ³ : → / is projection. Then ∅isc , invariant surjective submersion and descends to a symplectic homeomorphic. The map ∅ is a sort of pushforward, though a ³ is not injective. Note that ∅ is "injective mod '', meaning that ∅(± 1 ) =∅(± 2 ) if and only if ± > =n. The first result of the theory in cotangent bundles reduction, the theory developed for the problem with a single or bit type playing an important role in the solution to the general problem of a singular cotangent bundles reduction for base manifolds, Hamiltonian tubes when the symplectic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models and the first work studying symplectic normal forms in the specific case of cotangent bundles. Conclude that the theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds and found that the phase space is the cotangent bundle T * Q of a configuration space .
paper_444	Resource constrained project scheduling problem is to make a schedule for minimizing of the completion time or total cost subject to precedence rules and resource constraints. To meet the manger goals, it is assumed that the increased quantities of resource are uncertain variables and the finish time of each activity is a decision variable. The goals of the model are to minimize the completion time and the total cost which composed by the activity cost and the additional resource cost. Furthermore, the equivalent form of the above model is given and its equivalence is proved. The resource constrained project scheduling problem (RCPSP) takes into account the balance of makespan and total cost through resource allocation and reasonable activity schedules while the precedence constrains between the activities and resource constrains are all satisfied. However, the majority of the above studies focus on RCPSP in deterministic environment, and suppose that the resource availability is a real number. In real-word projects, there may be some uncertainty phenomena, for example, overdue materials, the decrease in the number of workers at Grain in Ear season, etc, which result in the resource availability may be changed. In this case, many scholars begin to consider the uncertain resource constrained project scheduling problem. Ying  [8]  proposed a schedule model of flexible work-hour constraint, in which the human resource was dealt with a new constraint to the classical RCPSP and the increased quantities of human resource were real-value variables. Xie  [9]  supposed that the increased quantities of resource were real-value variables, and built a multi-mode resource constrained project scheduling model for minimizing both the project makespan and cost. Lambrechts  [10]  established a stochastic project scheduling model in which the resource availability was a random variable in order to increase robustness. Chen  [11]  developed a project scheduling problem model under fuzzy resource constrained, of which the fuzzy duration time and fuzzy resource availability were represented by triangular fuzzy number. Liu  [13]  firstly established an uncertain project scheduling model, aiming to minimize the total cost under the constraint that the completion time does not exceed the deadline. Ji and Yao  [14]  recently considered the uncertainty of the duration times and the resources allocation times by assuming them are uncertain variables. Ma  [15]  considered resource constrained project scheduling problem with uncertain durations, and an uncertain excepted value model was built with the objective was to minimize the completion time. Up to now, we have not yet found uncertain resource availability constrained RCPSP in uncertain environment, which is not either randomness or fuzziness. In this paper, we consider uncertain resource availability project scheduling problem based on uncertainty theory, in which the increased quantities of resource are uncertain variables, and the finish time of each activity is a decision variable. Then, we build a multi-objective model which is under resource and precedence rule constrained to minimize the resource cost and the project completion time. In order to solve the problem of time-cost trade-off in project scheduling, we consider a project which is described as an activity-on-the-node network ( , ) , where = {1, 2, ⋯ , } is the set of activities and is the set of pairs of activities with precedence relations. A multi-objective programming model is built under the resource restricts and precedence rules of activities constrains in order to balance the completion time and the total cost of the project. For some time-intensive and heavy-duty projects, managers tend to be completed as quickly as possible and cost to be minimized. The uncertain resource constrained time-cost trade-off problem can be described as following optimization model: formula_0 In the above model, objective ○ 1 is to minimize the project total completion time; Objective ○ 2 is to minimize the expected project total cost which consists of the activity cost and the additional resource cost. Constraint ○ 3 declares that finish-start precedence relation among project activities. Constraint ○ 4 reflects that for any time # and each resource type , the demand for resources shall not exceed the total supply of resources with at least given confidence level < = . If the function Q(R A , R P , ⋯ , R ) ) is strictly increase with respect to R A , R P , ⋯ , R S and strictly decreaseing with respect to R STA , R STP , ⋯ , R ) , then formula_2 holds if and only if formula_3 To solve the model (1), according to the operational law of uncertain variables, we transformed it into its equivalent form, as shown below. formula_4 Proof: Because is an uncertain variable with regular uncertainty distribution Φ , and formula_5 by the definition of uncertain variable  [16] , we know that ? The manager tends to the demand of resource shall not exceed the total resource supply with at least given confidence level 0.9 for any time # and each resource type . By describing the increased quantities of resource as uncertain variables, an uncertain resource constrained project scheduling problem is discussed in this paper. Then an uncertain optimal model was built with objective of minimizing the completion time and the cost with resource constrained based on uncertainty theory.
paper_462	To deepen the reform of clinical medical personnel training in an all-round way with the cooperation of medicine and education is the strategic adjustment direction of clinical postgraduate education in China. Through improving the conditions of enrollment, innovating the training mode, adjusting the award marks and improving the system guarantee, it has effectively constructed the professional degree education of master of clinical medicine and standardized resident training. Since the implementation of the new training mode, the quality of postgraduate training has been significantly improved, the employment rate of graduates has been steadily improved, the influence of the school has been expanded, and the experience for relevant units to carry out the reform of postgraduate training mode of clinical medicine master's degree has also been provided for reference. Since 2009, the school has been focusing on reforming the training mode of clinical master degree postgraduates (hereinafter referred to as clinical master). In view of the main problems existing in clinical master education, the school has carried out a series of reforms and explorations on the organic link between clinical master education and standardized resident training (hereinafter referred to as standard training). The reform has opened up a new path and realized the organic link between professional degree education and vocational qualification certification. To solve these problems, the Academic Degree Committee of the State Council officially launched the pilot work of clinical medicine professional degree in 1998. The development of postgraduate education of clinical medicine degree in China is faced with the characteristics of short time, complex training objects, multiple administrative departments involved in medical degree education, and diversified training channels  [2] . In the long-term practice of training clinical master, Chongqing Medical University has gradually explored a new "5+3" training mode which combines clinical master education with regular training, and completed the educational practice of "Innovation and Practice of the Training Mode of Clinical Master Professional Degree in China"  [3] . It has effectively solved the problems of imperfect training mode of clinical master, lack of mature training mode of high-level clinicians, low overall training quality of clinical master, inability of clinical master to apply for the qualification of practicing doctor, difficulty of effective connection with regular training, difficulty of seamless connection between professional degree education and industry admission standards, and inadaptability of current management system and mechanism to professional degree postgraduates. The reform of training mode for clinical master of Chongqing Medical University is premised on defining training objectives, based on innovative training mode, with improving training quality as the core and linking up with professional qualification as the grasp  [4] . The system reform of training mode for clinical master of Medical University has effectively solved the problem of insufficient practical ability of clinical master, and has realized the degree education and professional qualification recognition of clinical medicine specialty. The school accurately grasps the law of professional degree postgraduate education, changes the concept of professional degree postgraduate education, and orientates the training purpose of clinical master as "training doctors who can really see a doctor", aiming to improve the clinical practice ability of postgraduates as the main objective, and is in line with the training goal, thus laying a solid foundation for the organic connection between the two. After completing the dissertation defense and meeting the award requirements, the graduate certificate and master's professional degree certificate can be obtained. "Four syndromes in one" has greatly saved the resources of education and training, and provided qualifications guarantee for the connection of clinical medical professional degree education and vocational qualification certification. In order to effectively improve the quality of clinical master training and strengthen the management of clinical rotation process, the school has set up postgraduate management offices in clinical colleges, implemented the system of professional degree tutorial group, and established three-level management systems of schools (graduate schools), departments (graduate management offices) and clinical departments (tutorial groups)  [6] . The Graduate Management Department of the Department organically integrates the training of clinical master's degree, the training of seven-year students' master's degree, and the application of resident doctors for master's degree related to the work of degree award, and arranges the clinical training and clinical competence assessment of clinical master's degree as a whole. The tutorial responsibility system is applied in postgraduate training, but after clinical master's enters the clinical rotation, many times are not in the Department where the tutor is located, and there is a problem that no one manages the professional degree students when they rotate in the clinical departments outside the department  [7] . Each clinical rotating Department of postgraduate clinical rotation appoints a teacher qualified as a professional master's tutor as rotating responsibility tutor to ensure that clinical master's is in clinical rotation. In order to consolidate the reform results of the training mode of clinical master, gradually standardize the management system and continuously improve the quality of training, the school continuously improves the rules and systems around the education of clinical master, covering the methods of re-examination admission, the quantitative assessment of the selection of tutors, the training program, the tutor group system, the curriculum, the professional course examination, the assessment of clinical ability, the regulation of publishing papers, the management of research funds and institutions  [8] . In terms of setting up, a complete system of management rules for clinical master's degree has been established, and the experience of reform has been standardized and institutionalized, thus forming a long-term mechanism for linking professional degree education with professional qualification certification. The staff of the Graduate Management Department of the school conducted a thorough investigation on the effect, problems and suggestions of the training mode of clinical master's degree in various clinical departments, and held dozens of meetings to solve common problems. Through the above measures, the problem of low enthusiasm of faculties and tutors in guiding graduate students with professional degrees has been solved, and the construction of professional degree tutors has been accelerated. In view of the fact that the subsidy of clinical master is much lower than that of regular trainees during the clinical rotation period, the school has made many investigations and repeated demonstrations to improve the treatment of Postgraduates during the clinical rotation period and to improve the enthusiasm of students in clinical training  [10] . Aiming at the characteristics of professional degree postgraduate education and adapting to the innovation of its training mode, the school has established a funding system different from academic degree postgraduates, and continuously improves the standards and coverage of grants. Postgraduates with licensed doctor qualification and independent bed management are awarded 500 yuan in schools and hospitals, 500-1000 yuan in departments and tutors, and 2000-3000 yuan in living allowance per month for each clinical master. Therefore, the school has improved the conditions for applying for clinical master's examination, so that it is consistent with the admission conditions of residents' standardized training: undergraduate majors should be clinical medicine, full-time national education series undergraduate graduates, to obtain bachelor's degree. Thirdly, according to the requirements of standardized resident training, and with the cooperation of training bases, starting from medical ethics, medical ethics, laws and regulations, professional ethics and basic clinical skills, we will offer lectures on medical law, applied psychology, humanistic literacy and doctor-patient communication to comprehensively improve the comprehensive quality of clinical master. System The secondary schools awarded professional degrees in clinical medicine in our university are all the standardized training bases for residents in Chongqing. The rotation requirement not only meets the requirements of the state for clinical master, but also closely combines with the regular training. For non-graduate students, we adopt the "fill-in" training method, that is, accurately record the clinical rotation time of clinical master, and add up the previous training time to meet the requirements of training time. the second stage of standardization training), which not only saves the time for graduate students to carry out clinical rotation, but also ensures the quality of training, and achieves seamless docking in the rotation cycle. The main reason for this phenomenon is that since the trial implementation of clinical medical professional degree, the state has not evaluated the quality of the pilot units and lacked an effective monitoring mechanism. The school has established a multi-level and whole-process clinical competence assessment system, which is suitable for clinical master's clinical competence assessment and regular training and graduation assessment. The Interim Measures for the Implementation of the Regulations on Academic Degrees of the People's Republic of China promulgated in 1981 stipulates that before the postgraduate's reply, the degree-granting unit shall employ one or two experts from the disciplines concerned with the thesis to review the thesis. The clinical master emphasizes on examining the clinical competence of postgraduates. Before graduation, clinical masters not only have to defend their dissertations, but also have to pass strict clinical competence assessment, which increases the workload of students. The school regularly carries out the training of tutors and managers, explaining in detail the strategic development direction of national postgraduate education and the related policies of clinical master's education in our school. The staff of the Graduate Management Department of the school conducted a thorough investigation on the effect, problems and suggestions of the reform of the training mode of clinical master in various clinical colleges and departments, and organized a meeting to solve some common problems. There are three difficult problems in the training process of master's degree postgraduates of clinical medicine specialty: first, the graduates of clinical medicine specialty with bachelor's degree must work in medical institutions for one year before they can apply for the qualification of practicing physician; second, the graduates with bachelor's degree can't obtain the qualification certificate of practicing physician and can't carry out clinical training; third, some of the graduates with bachelor's degree must work in medical institutions for one year. Qualification certificate students are not allowed to practise medicine in other places in accordance with the Law of Licensed Physicians, so it is difficult to carry out clinical training  [14] . The school has thoroughly reformed the training mode of clinical master, organically docked school education with post-graduation education and lifelong education, promoted the seamless docking of professional degree education with industry access standards, accelerated the internationalization process of higher medical personnel training mode, and formed a new systematic training mode for clinical doctors, which has great application value in the whole country. In view of the characteristics of professional degree postgraduate education, the school grasps the development trend of postgraduate education, and closely combines the admission criteria with the industry admission criteria in all aspects of enrollment, cultivation and award of posts, which ensures the complete docking of personnel training and qualifications, and provides mature experience for the seamless docking of professional degree education and Industry admission criteria in China. The school has continuously innovated the training mechanism and formed a "modular" curriculum system and a quantitative assessment system for clinical competence. Since 2009, the school began to explore the reform of the training mode of clinical master in an all-round way. It has basically realized the seamless connection between the degree education of clinical medicine specialty and the certification of professional qualification. The reform breaks through the restrictions of relevant industry policies on the training objectives and modes of clinical master, and provides innovative modes for brothers to learn from. In the 2011 "Forum on Reform and Development of Medical Education", the principal of the school presented the reform experience to the conference and won the unanimous praise of the broad masses of colleagues. The leaders of the Ministry of Education, the Health Planning Commission and more than 400 persons in charge of training units participated in the symposium, which greatly promoted the reform and development of clinical Master's training in China, and played a tremendous leading role in promoting the reform of the training model of clinical Master in the western region and even in the whole country. The article "Construction and Practice of Quality Assurance System for Master of Clinical Medicine Degree Postgraduates" published in "Degree and Graduate Education" won the second prize for Excellent Thesis of "Degree and Graduate Education", and it is also the only prize-winning thesis in the field of medical science. Since the implementation of the reform of the training mode of clinical master, the combination of degree education and vocational education in our school has become closer. Professor Jin Xianqing and Professor Xie Peng were appointed members of the National Medical Graduate Education Steering Committee in 1998 and 2010 respectively. The school assists in formulating the standardized training policy for general practitioners and residents in Chongqing, the construction of bases, the training and assessment system, and teacher training. The "5 + 3" reform of clinical medicine master's degree in Chongqing Medical University has established a "one goal", that is, to train "doctors who really can see a doctor", and established the basic principles of the reform of training mode of clinical medicine master's degree and the training of clinicians. It has effectively solved the problems of imperfect training mode of clinical master, lack of mature training mode of high-level clinicians, low overall training quality of clinical master, inability to apply for the qualification of licensed physician, difficulty in effectively linking up with regular training, and difficulty in seamlessly linking professional degree education with industry access standards. Chongqing Medical University combines many years of clinical master's education practice, highlights its own characteristics, gives full play to the advantages of running a school, and strives to solve the problem of linking professional degree education with vocational qualification certification. The unit pays attention to the degree education of clinical medicine specialty, strengthens the exchange study, improves the quality of medical higher education in our country, realizes the seamless connection between degree education of clinical medicine specialty and professional qualification certification, and explores the new mode of integrating medical education of our country with international practice.
paper_476	The results show that: (1) The cost and resources operational efficiency are closely related to the order splitting ratio under the same delivery frequency; (2) The delivery frequency has different effects on the operational efficiency of the resources in different links. (3) Through the proper splitting and loading of orders, staff scheduling, and area adjustment of delivery station, the optimal delivery frequency can be achieved under the balance of cost and resource operational efficiency. In order to reduce the operating costs of logistics enterprises on the basis of ensuring service levels, one should first properly allocate the amount of orders and splitting ratios to achieve an economic increase in the delivery frequency. As the volume of orders and consumers' demand of rapid delivery services increasing, express companies are required to increase the daily delivery frequency to cope with the pressure of delivery during peak period and meet the consumers' needs effectively. The order volume, the orders' splitting ratio, the operational efficiency of delivery resource and the delivery costs will affect the delivery frequency. System dynamics models not only offer a different Operational Efficiency: A Case Study of Jingdong Logistics perspective while whole system approach to transport planning, but also demonstrate to policy makers the importance of these feedbacks and lagged responses  [1] . At present, the researches on delivery frequency, resource operation efficiency and cost utilization were mainly focusing on the following two aspects: (1) Delivery efficiency improvement through the choice of delivery model, And (2) Delivery link optimization. That research explored three scenarios, which are joint delivery, autonomous delivery, and third-party delivery, which also pointed out that, in order to improve delivery efficiency, enterprises should adequately consider relevant factors such as own resources, competitors' delivery strategies, and urban transport policies before determining delivery methods. That research took Lyon in France as an example with the using of radar map to visually show CO2 emissions, risk values, delivery costs, traffic impact and delivery time of joint delivery under different scenarios. Wang Wei  [17]  analyzed the advantages and disadvantages of self-operated, outsourced, and federated modes in ecommerce delivery model, and constructed a system dynamics model for the combination selection of delivery modes by selecting six core factors, which can provide decision-making advice for managers to improve the delivery efficiency. Barla  [18]  used system dynamics to simulate the links of orders, production, and inventory for apparel delivery companies, and considered how to adjust inventory levels to increase the delivery systems' efficiency and economy when demand fluctuates significantly. Hongtao Yang and Jianbang Du  [19] [20] [21]  uses system dynamics to analyze the problems of supply chain and delivery efficiency from the perspectives of transportation routes, drivers and cooperation agreements, and its sensitivity analysis provides some reference to this paper for the corporate in this case to improve the delivery efficiency. Lin Wanting [22]  considers the phenomenon of unfilled orders, information leakage, and product damage which exists in crowdsourcing delivery, then built system dynamics model from five dimensions of reliability, responsiveness, assurance, empathy and security, which provide decision-making advice for enterprise delivery efficiency management. Next, it uses the simulation model to study the effect of changes in delivery frequency on delivery costs and resource operation efficiency under different quantity of delivery orders. Next, it analyzes the interaction between delivery frequency, cost, and resource efficiency, and builds dynamics simulation model to get the equilibrium of cost and resource operation efficiency under different delivery frequency. Based on field surveys of JDL delivery systems and interviews with operational personnel, on a route daily business JDL delivers orders within the region based on a fixed frequency. According to investigation results, the cost of storage and ferry in JDL only accounted for about 8% of the average cost in daily delivery. The variable costs occurred due to the volume of orders, which include operating cost of the sorting equipment, rental cost of the site, units fixed cost of transport vehicle, fuel costs, toll and so on. In addition, the number of working facilities and the operating time are affected by factors such as order quantity, delivery frequency, sorting equipment efficiency, and unit load of transport vehicles. The number of sorting staffs is influenced by factors such as the amount of cargo, the number of sorting equipment, the worker's efficiency, the sorting time requirements, the area of sorting venues, the number of logistics personnel in the yard, the delivery frequency and so on. This article considers the utilization efficiency for the facilities, personnel, and technology such as turnover rate of transportation vehicles, the utilization rate of delivery site, and the delivery personnel loading rate, etc., which are put into the sorting, transportation, and terminal delivery links. The increase in the delivery frequency will result in the following two changes: first, the increase in consumer satisfaction; second, the corresponding changes in the cost and resources operational efficiency. The first change may prompt the increase of customer orders and corporate income Therefore the company will increase the investment in delivery facilities and equipment, and thus increase the delivery frequency and service capabilities. As for the latter, given JDL's current batch-by-batch delivery mode, the increase in delivery frequency will reduce the fixed cost allocated to each delivery operation. At the same time, the factors such as sorting time requirements, proportion of per-order batch, efficiency of the delivery personnel, and unit fuel consumption of the vehicle, will have a direct impact on the delivery frequency. Among them, the positive loop represents that there is a mutually reinforcing relationship between the factors, while the negative loop indicates that there is a balanced relationship between factors (Such as: Delivery frequency --> + Consumer demand response ability --> + Impact of demand response capacity --> + Consumer satisfaction --> + Order quantity due to change in satisfaction --> + Total daily order quantity --> + Per batch delivery order quantity-->+ Sorting center cargo volume-->+ Number of on-site logistics personnel--> -Onsite logistics personnel operating time-->+On-site logistics personnel costs-->+ Sorting personnel costs-->+ Sorting costs-->+ Total cost --> Total profit --> + Delivery facility input --> + Delivery frequency). Since the current order quantity of each batch is mainly determined by the consumer's shopping habits, which cannot be arbitrarily changed, this paper splits the orders of the first batch (60% of the total orders). Therefore, the simulation calculates the utilization rate of the largest batch after splitting when considering the operational efficiency of resources. In scenarios 2 and 3, when the order quantity was lower than 3,500, the differences between the total delivery costs in Operational Efficiency: A Case Study of Jingdong Logistics different scenarios were very small; when the order quantity was higher than 3,500, the increased rate of cost in scenario 3 was higher for a while than that in scenario 2. The reason is that in scenario 3 the sorting center dispatched more vehicles in advance, which increased the cost of transportation. After the order volume reached 5,100, the total delivery cost in scenario 4 showed a downward trend and reached the minimum. Within the orders volume ruled in this paper, the delivery frequency and the cumulative delivery costs are positively correlated. The four scenarios showed a lower average unit cost when the order quantity was around 4,700 units indicating that this order quantity was a batch of economic orders. Due to the increase in the delivery frequency, the unit average time required to complete the sorting operation became shorter while the number of sorters and the salary of personnel increased. This is because that under the current total delivery volume, the order quantity after splitting in the scenario 4 was higher. The difference of average unit sorting costs among scenarios 1, 2 and 4 decreased as the order increased, which is because the change in the sorting costs under different order quantities was small and the difference in the delivery order quantity increased. Due to the splitting of order, the transportation costs of scenarios 2 and 4 remained unchanged when the order volume was small. When the volume of orders increased to a certain value, the transportation cost of scenario 1 became the highest among the three scenarios. In scenarios 2 and 4 the orders can be met by just increasing the frequency of existing vehicles, however, in scenario 1, new vehicles were needed to meet the transportation needs, which resulted in higher transportation costs. In the aspect of unit average transportation cost, similar trends were also emerged among the scenarios mentioned above. The main reason is that the increase in the delivery frequency improved the demand for the number of delivery personnel and caused the delivery time of delivery personnel to be shorter in a single batch. In terms of unit average terminal delivery cost, all three scenarios have shown a tendency of fluctuating decline, and some of the rises are due to the increase in the costs of delivery personnel and facilities. The difference between the vehicle utilization efficiency in scenarios 2 and 3 increased with order volume until it reached 3600 units. The vehicle utilization efficiency for the three sorting centers in scenario 4 increased with the order volume since only one transportation vehicle was required to be dispatched from each sorting center within the current order volume. This is because the order volume was so small in scenario 4 that in the unit batch delivery personnel was working inefficiently. In addition, scenario 4 can effectively ease the utilization of area and personnel in the delivery site. With the same delivery frequency, the orders splitting ratio influences delivery cost and resource utilization efficiency significantly. Recommendation I: one should properly allocate the amount of orders and splitting ratios to achieve an economic increase in the delivery frequency. With the same delivery frequency, different order splitting ratios affect delivery cost and resource operational efficiency. Therefore, JDL needs to consider the increase in delivery frequency, the increase in delivery costs, the overloading of resources, and the ratio of orders and splits. In addition, the average split of order quantity can effectively reduce the tension of delivery resources. When the delivery frequency is increased from 3 times per day to 4 times per day, scenario 3 should be adopted from the perspective of increasing consumer satisfaction; scenario 2 should be adopted when the area of delivery stations and the delivery personnel are tight. Since the average personnel utilization efficiencies of delivery both stations A and B are lower than 60%, the delivery personnel can be reasonably scheduled to improve the utilization efficiency and consumer satisfaction. In addition, the area utilization efficiency of delivery stations may exceed 100% under large orders. Therefore, according to the actual situation, it is necessary to increase the delivery frequency or expand the area of delivery sites.
paper_479	Handwriting is also a key element in document examination as it leaves a forensic document examiner with the task of determining who the writer of a particular document is and this is achieved through the likelihood ratio (L R) paradigm. Inability to model an individual's handwriting over time has made estimating a full likelihood ratio for comparative handwriting analysis impossible thereby employing nuisance parameters and subjectivity in computation of L R that is not full. This research employed back propagation neural network (BPNN) to model the writing pattern of individuals with input layer as the features of handwriting characters, two hidden layers of three neurons each, activation function sigmoid (s) and an output handwriting. With the help of handwriting model for individual writers, little or no assumptions and no nuisance parameters were employed in achieving full likelihood ratio for comparative handwriting analysis in forensic science. From the research carried out, it can be concluded that modeling an individual's handwriting is a crucial factor in achieving a full likelihood ratio, little/or no inconclusiveness in result reporting and a less degree of disagreements for handwriting identification in a forensic environment. The likelihood ratio paradigm has been studied as a means for quantifying the strength of evidence for a variety of forensic evidence types in handwriting and other types of forensic evidence such as earmark, speech, footprint, fingerprint, glass fragments and DNA  [1] [2] [3] [4] [5] [6] [7] [8] . It is straightforward for a DNA as compared to other areas of forensic evidence because estimation is done for the relative frequency of different DNA profile in relevant populations. For the handwriting evidence, previous literatures reported many factors and the inability of the forensic examiner to model the writing profile of a writer because it is often believed that an individual's writing profile is a latent characteristic that cannot be observed directly, and is not easily modeled because an individual's writing profile cannot have static characteristics and may change over time  [9] [10] [11]  which has resulted to adopt ad-hoc methods to compute likelihood  [9, 12, 13]  when full likelihood ratio is not achievable. The novelty of our proposed implementation relies on natural handwriting samples over a period of six months from known individuals to form the database to model the writing profile for each writer. The probability proportion has been used as a way for measuring the power of confirmation for various legal proof sorts. formula_0 Formally, posterior belief in favour of H p (for example the defendant is guilty) with fair facts are used to make a final decision on the evidence. The forensic examiner provides a summary of the evidence needed for belief based on the evidence and the fact that prior beliefs about H p and H d have been quantified. Likelihood ratio = Posterior ratio / prior ratio Likelihood ratio according to  [14]  are increasingly being adopted to convey expert evaluative opinions to courts. In the absence of appropriate databases, many of these likelihood ratios will include verbal rather than numerical estimates of the support offered by the analysis. Computational approaches to the handwriting facet of questioned document (QD) examination were developed with a view towards providing a scientific basis for handwriting evidence, formalizing human expert-based approaches and validating existing methodology. The problem of estimating a L R for handwriting has proven to be a non trival task due to the inability to model the writing pattern of an individual and due to the absence of a large database of handwriting and other factors. [17]  based their approach on distribution of measurements from comparing items but their approach was similar to that of marginal L R when full L R was not an option due to the presence of nuisance parameter. In statistics, a nuisance parameter is defined as: ''A parameter of a model where there is no scientific intrigue except for whose qualities are normally required (yet when all is said in done are obscure) to make deductions about those parameters which are of such intrigue [25].'' [19]  described a statistical model for the writer verification task to determine if two documents were written by the same writer, their model followed the scenario proposed by  [17]  thus suffers from the same weakness  [20]  presented one conceivable way to deal with gauge a probability proportion in near handwriting analysis was delineated. The oddity of the methodology depends on producing reproduced composing samples from an accumulation of composing samples acquired from a realized source to frame a database for evaluating the dissemination related with the numerator of a L R . In a related work, similar examination of probability proportion based proof appraisal strategies in both evaluative and analytical procedures was carried out using a sample collected from female and male author. While the utilization of probability proportions in the previous circumstance is currently rather entrenched spotlight on the insightful setting still remains rather past contemplations by and by. This paper features that investigative setting still remains rather past contemplations practically speaking; it is also attested that L R can be useful for analytical procedures bolstered through various simulations  [21] . Assessment of signature handwriting evidence through score-based likelihood ratio based on comparative measurement of relevant dynamic features was carried out by  [22, 9]  presented score-based approaches to calculating forensic probability ratios that have been established as becoming more progressively common in forensic literature. This method is genuinely direct for the score-based L R numerator, which involves creating a list of scores obtained by combining proof objects from the same source. Nonetheless, this method creates uncertainty for the generation of denominator databases -in general, how is the best way to produce a list of scores among two items from different source. The work discussed the reasons for the authors' opinion that interval quantifications for the value of evidence should not be used directly in the Bayesian decision making process to determine the support of the evidence for one of the two competing hypotheses. [24] In a related manner, statistical problem and pitfalls identifiable with forensic likelihood ratio were identified. The feature information extracted from student handwriting through LBP was entered in NN through input layer and the participation of each class of character variables is calculated by the hidden layer of the network using: formula_1 net j is ith type of factor, n is the number of factors in net i , & ' ! The algorithm can be broken down in the following four steps: Feed-forward computation Backpropagation to the output layer Backpropagation to the hidden layer Weight updates The algorithm is stopped when the value of the error function has become sufficiently small. Due to the complexity of modeling the handwriting of a writer and the absence of industrial size databases from which different handwriting can be described  [17, 9]  estimated a marginal L R when the full L R was not possible also in the presence of some parameters considered to be nuisance. The developed writing model for each writer is one of the criteria to eliminate the presence of nuisance parameters when estimating a full L R . An inconclusive state is declared if and only if L R value = 1 With the BPNN algorithm to model handwriting pattern for each writer and L R estimation described in  [10]  this paper was able to deal with the problem of inconclusiveness as there was no inconclusive results reported in the investigation, determine who the writer of a questioned document is as well as eliminate the inclusion of parameters considered nuisance in investigation because each writer's handwriting profile could be ascertained due to the BPNN training of the Handwriting categories. Several factors have to be put in place to estimate a full L R for forensic handwriting investigations. This research shows that modeling handwriting pattern for a writer is an important factor in achieving a full L R which will eliminate the use of nuisance parameters in the computation process, lower the disagreement rate with respect to the Hypothesis in support of the prosecutor and also produce no inconclusive result after estimation of L R .
paper_492	Based on known functions of neuroscience the neural network that performs serial parallel conversion and its inverse transformation is presented. The activity of the upper neural networks changes in response to the context structure inherent in the time series data and have both function of accepting and generating of general time series data. Eating behavior in animals in the early stages of evolution is also processing time series data, and it is possible to predict behavior although be limited short term by learning the contextual structure inherent in time series data. Although status of nervous system of the animal change according to the recognition by sensory organ and to the manipulation of the object by muscle in the vicinity of the animal itself, the evolved animals have in addition another nervous system so-called long-term memory or episodic memory being involved experience and prediction. By the workings of long-term memory, lot of information are exchanged between fellows, and lot of time series data are conserved by characters in human society. Furthermore retroactively, although there is a quantitative difference in each part between brain of ape who does not speak the language and our brain, but our brain is consisted of same material. In other words, the nervous system of animals called advanced higher animals is locally same as very primitive animal's nervous system. By hierarchical connection of the circuits, it is possible to accept and generate general time series data. As an example, a neural network that can accept and generate multi step time series data such as indispensable for eating behavior is presented. The nervous system related to short-term memory is activated in synchronization with environmental events, but the nervous system involved in long-term memory is highly layered to form an image corresponding to past and future events. Even bacteria like animals in the early stages of evolution must have some eating behavior such as moving relying on light and smell to search for food and determine whether they can be eaten. Whether the eating behavior is evaluated as intellectual behavior aside, it is possible to mimic this degree of behavior by electronic work at the junior high school level by combining sensors and logic ICs. The eating behavior of animals evolved from the aforementioned animals is composed of time series of actions such as extending arms toward the target, opening the palm of hand when approaching the target, closing the palm to grasp the target and bringing to the mouth. The recognition technology of the figure has evolved by the neural network which starts with the perceptron, but it is forced to judge by the relation of the part and the whole as in the example of kanji when the scale of the figure increases. It is desirable that the behavior of any neural network system can be expressed by combinations of the simple action parts of animals in the early stages of evolution. In this chapter, it is presented that arbitrary time series data can be divide into basic sequences as the logical basis of neural networks. Next, by providing a two-way function to the neural network both serial parallel conversion and vice versa on basic sequences is realized. Any time series data consisted finite type of element can be divided into multiple subsequences where the same element does not appear more than once. The neural circuit corresponding to each basic subsequence can be easily configured in the neural network, leading to the realization of processing by hardware of general time series data. For simplicity, assume time series data consisted of 10 type elements from a 0 to a 9 . The output of the element holding the activity when receiving the last c 4 of the time series data is the recognition result of the time series data c 0 c 1 c 2 c 3 c 4 . Therefore, the couplings between the elements are enhanced (Hebb rule) by repeating this conversion, as a result the elements involved in the conversion will be activated by only receiving the first element of the time series data. On the flow direction of data, this parallel to serial conversion is upside down with the serial to parallel conversion described above, but the basis of neural network operation is the same. In the spontaneous operation (here by voluntary muscle) performed reactions such as changes of weight feelings from muscle or joint are accompanied by. The essence of the neural network which performs the parallel serial conversion and the neural network which performs the serial parallel conversion is the same except that the conversion result comes out downward or upward. On other words, both operations of the serial parallel conversion and the reverse conversion are essentially the same on the point both operations are triggered by the first data and proceed waiting for input state change. Since general time series data is consisted of hierarchy of basic subsequences, basic units can process general time series data by identifying the outputs of lower layer basic units as the new time series data. Animal's behavior is considered a time series data consisted of the data that is couple of behavior data and received data. The example shown at the beginning of this chapter is also a time series data that is consisted of extending arms, opening the palm, closing the palm to grasp and so on. The time series data handled in the first 2 steps are consists of various stimuli of the internal and endocrine system, as well as data that captured by sensory organ from the environment in which the animal is placed. If the state of the unit becomes active (hungry), it has been shown to activate one after another the lower layer units that achieves eating behavior in the rest steps. After time sequence data learning, animals are possible to quickly start the operation that continues only by receiving the beginning of the time series data to be corresponding a slight sign. As neural circuits evolve, the movement of each element is captured and accumulated as a context, and it becomes possible to adapt to changes in the new environment. And in the nervous system, the eating behavior shown above will be nothing more than a brief occurrence that appears in a constantly continuing life. The reason why drawn a hierarchy further in the upper part of the  Figure 4 , is to suggest existence of the nervous system that processes intellectual judgment. In brief, images were advantageous even if an organism were not conscious of the images formed within it. The organism would not yet be capable of subjectivity and would be unable to inspect the images in its own mind, execution of a movement; the movement would be more precise in terms of its target and succeed rather than fail. Animals with some evolved sensory organs must recognize, for example, environmental changes from sunrise to sunset as repetitions of a time series data. Imitating is the basis of fellow empathy and group behavior, and will develop to ethical emotions such as mercy and encouragement  [4] . Human beings have adopted language as a means of exchanging information between peers, and began to exchange vast amounts of time series data. The object to be drawn is the change of all things, the joys and sorrows of livings, and their hope for the coming future. And you'll think of the shape of the cake and the action of lifting the chocolate plate and mouthing it. On your birthday day, you may want to see the cake in front of you, identify the cake from store A, which was the one that was the subject of a conversation with your family the day before, and worry about the difference from the previous day's expectations. Because the episode about the cake is remembered, communication is possible and feelings are transmitted. One is time series data based on visual information of the cake in front of you and muscle movements that manipulate chocolate plates, which is produced by the nervous system that animals have from the early stages of evolution. The other is generated by the nervous system that is called " image" by Damasio, is time series data based on the shape of cake produced from family conversations the day before. While the behavior of the former nervous system is reflection of the visual information and movement of the objects nearby, the latter nervous system not only no needs to be synchronized with the former, but also moves independently. However, if the nervous system of the episodic memory is activated by remembering the A shop and the chocolate plate while looking at the cake, the difference between reality and the expectation might become a problem. Stimulus from the sensory organs from the bottom becomes time series data and is transmitted to the upper part. The part shown in red is a part that is particularly activated, the lower red disk is the part activated by the visual data of the chocolate plate that placed in front, the upper red disk is the part that is activating by recalled episode about the chocolate plate. The connection between the two sides is enhanced (Hebb rule) as indicated by a bold line, and visual information is copied to the top to reinforce the episodic memory. When studying objects that are intertwined with portion and whole system such as the nervous system, the idea of category theory can be incorporated to develop the whole without focusing on the details of the object. [  formula_0 The composite of the coupling is considered to be a hierarchical connection of the objects, and the identity, which is considered a special morphism, can correspond long axons extending beyond hierarchies. And the time series data brought by other brain with highly hierarchically connected structure activities is concerning long-term memory. From an engineering point of view, the basic unit can be realized by a small microprocessor, and the neural network consists of multiple randomly connected units. It can be said that the essence of the logic of the operation exists not in the basic unit, but in the connecting situation among basic units selected from randomly initialized connection. There may be cases of errors in the accuracy of the operation compared to the circuit using the existing logic IC because there is a probabilistic part, but the bud of a new strategy might be hidden in the vicinity of the malfunction. From the viewpoint of neuroscience, even if there are no "parts" equivalent to the basic unit in the process of stimulation from the sensory organs propagating to the cerebral cortex, axons parallel to the propagation direction connect between the layers, and it can be said that it is a passage of serial parallel mutually converted information. On the other hand, it is considered that the axon perpendicular to the propagation direction is a connection such as causing self-oscillation related to the generation of serial data.
paper_507	Landslide is defined as a slow to rapid downward movement of instable rock and debris masses under the action of gravity. Landslides are one of the major natural hazards that account for hundreds of lives besides enormous damage to properties and blocking the communication links every year. Integrated them together using GIS and soft computing to create a database that will generate the output for the future use for prediction of susceptibility of landslide. The main aim of present study is to integrate the result of our study with spatial data, soil parameters, land inventory and used the output as a user friendly application using GIS which could predict the future susceptibility of region to landslide and% contribution of each factor for the same. In this study, layers are evaluated with the help of stability studies used to produce landslide susceptibility map by Artificial Neural Network (ANN). Finally, an overlay analysis will be carried out by evaluating the layers obtained according to their accepted coefficient in final model.. Efficiency of the application will be calculated by the help of previously acquired data of the study area at different places and then the reliability of the application will be judged. A frequently used definition of landslide is "movement of mass of rock, earth or debris down a slope" in the words of Cruden  [1] . They are the catastrophic phenomenon taken lives of many a hundred and destroyed the hard earned money, disrupting the communication facilities. As per the official figures of United Nations International strategy of disaster reduction (UN/ISDR) and Centre for Research on Epidemiology of Disasters CRED for the year 2006, landslide ranked 3 rd in terms of number of death among the top ten natural disasters  [2] . Uttarakhand is an appropriate choice for the study since the newly developed state has been fighting with the catastrophe and is making front paper headlines for landslide at Vishnuprayag, Baldora, Lambagharchatti, Jharkula, phatabyung, and Amiya landslides  [4] . These map divides the land into homogeneous areas or domain and their ranking according to degree of actual hazard potential caused by mass movement  (Guzzetti et. al, 1999 and varnes 1984)    [7, 8, 9] . In the present study with the help of satellite imageries like DEM from cartosat-1 and topographic maps from GSI (geographical survey of India) a susceptibility map is prepared. With the help of ANN model, we generated weightage for each factor and using this the hazard zonation map is produced  [10, 11] . Preparation of a landslide hazard zonation map that would divide Uttarkashi into different zones depending upon the factors. Uttarkashi falls under the physiographic division (s) Rohilkhand plains, Nepal Himalayas, Ganga, Yamuna daob, Siwalik range, Kumaun Himalaya, Dhaoladhar range. In the present paper landslide hazard zonation map has been prepared for the Rishikesh-Uttarkashi-Gaumukh-Gangotri. Therefore, the relation between landslide occurrence and the conditioning parameters used is crucially important for landslide susceptibility mapping. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area (Mohammad Onagh, 2012). Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. Four control points were selected at the corner of the concerned points, the geo-referencing of these coordinates was done by finding the coordinates from the Google Earth. Four control points are selected on the four corners of the map such that the points mark the spatial extent of the whole map as shown: formula_0 . Digitization for the various shape files has then being done by retrieving the concerned shape file and the map form which digitization has to be done. Both the shape file and the map being retrieved in Arc Map, the map is zoomed to a comfortable level such that all features on the map could be easily traced out on the screen itself to create new layers or themes. An artificial neural network is a "computational mechanism able to acquire, represent, and compute a mapping from one multivariate space of information to another, given a set of data representing that mapping". The purpose of an artificial neural network is to build a model of the data-generating process, so that the network can generalize and predict outputs from inputs that it has not previously seen  [11, 12] . In the present study we selected 107 points and all the six factors namely soil depth, soil type, rock type, land cover, slope and elevation and a excel database is created. The data is categorized into training, testing and validation, and all the six factors namely soil depth, soil type, rock and a excel database is created. Since ANN does not understand the 'landslide prone' and 'non-landslide prone' region we explain it by giving value '1' and '0' respectively. This study was concerned to the region of Uttarkashi due to the limitation of resources and time, we have been able to generate the results for a limited area Rishikesh Uttarkashi-Gangotri-Gaumukh route from latitude 78°19'55.14'' to 78°47'36.27" and longitude 30°32'30" to31°1'9.33". Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. Precipitation The next objective of our study was to present the weightage of various factors causing landslide. Other values for landslide susceptibility for the adjoining areas have been calculated using interpolation technique therefore the Rishikesh-Uttarkashi-Gangotri-Gaumukh route has been mapped for landslide Hazard. Using ArcGIS the Landslide susceptibility for whole of the map region can be seen. The study has to led the determination of factors on the basis of past studies and determination of weightage for the chosen six factors namely soil depth, soil texture, rock type, height, slope and land cover. With the further advancement in such type of study, we could interpret results for future from past records, if the site is inaccessible, or the test results are erroneous. Here we have used the already existing topographical maps, satellite imageries and field work integrating them together using GIS and ANN MODEL to create a database that has generated the output for the future use. The result of present study with spatial data, soil parameters, land inventory and presented by a landslide hazard zonation map and a user friendly application using GIS that could predict the future susceptibility of region to landslide and percentage contribution of each factor for the same. Largely this study emphasize on the lucid presentation of result for laymen.
