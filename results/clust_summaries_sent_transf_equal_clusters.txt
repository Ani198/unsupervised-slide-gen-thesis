paper_1	A cluster is a collection of data objects that are either similar to one another in the same group or dissimilar to objects in other groups. Some of them include summarization, compression and k-nearest neighbor which localizes search to one or a small number of clusters. Clustering is the concept that was used in this research to create groups from social media data which can be used for learning on electronic learning platforms. The system design methodology used was incremental prototyping. In incremental prototyping, the whole requirements are broken down into building blocks which are incremented each time a new component is integrated based on an overall design solution. The second task involved identifying the right kind of data to use for training the expected prototype as well as testing it. The third step involved using the data already preprocessed above to train the prototype. The relationship between the data items can be established using the k-nearest neighbor technique. They were used to confirm that the system indeed accurately did the classification given some data items. The illustration of the proposed prototype is given below. The results reported in this paper were obtained from a series of evaluations that were done on the classifier on different parameters including functionality, usability, accuracy, precision and recall. This is the section that captured the users view on the functioning of the prototype. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . A good number of learners indicated that they would continually use the system for the purposes of group formation and discussion. This is summarized in the chart below. In experimenting with the Naïve Bayes Classifier, we relied on the NLTK module which provides functions for calculating these measures for the classifier. A total of 200 tweets were extracted and used for this test which was summarized in a confusion matrix. The results below illustrate a summary of what was obtained when 200 tweets were used to test the Naïve Bayes Classifier. These measures can be improved if large amounts of data are used to train the classifier before being used to do actual classification. Through the study, it can be underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. They therefore come with a lot of challenges including time wastage. Through social media a better and more efficient way can be used to enable online learning generally and group formation specifically. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. The classifier then grouped the users into different categories based on various tweets that they posted on the task. The classifier was also able to assign other or new users groups also according to their tweets and the learning that the system had undergone. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. The study has proved that it can actually be used constructively in learning in various institutions. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. They therefore come with a lot of challenges including time wastage. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_2	This creates unwanted congestion during peak hours, loss of man-hours and eventually decline in productivity. In addition to this, the current traffic light systems encourage extortion by corrupt traffic officials as commuters often violate traffic rules because of the insufficient time allocated to their lanes or may want to avoid a long waiting period for their lanes to come up. This research is aimed at tackling the afore-mentioned problems by adopting a density based traffic control approach using Jakpa Junction, one of the busiest junctions in Delta State, Nigeria as a case study. The developed system uses a microcontroller of PIC89C51 microcontroller duly interfaced with sensors. The signal timing changes automatically based on the traffic density at the junction, thereby, avoiding unnecessary waiting time at the junction. The sensors used in this project were infra-red (IR) sensors and photodiodes which were placed in a Line of Sight configuration across the loads to detect the density of the traffic signal. The density of the vehicles is measured in three zones i.e., low, medium and high based on which timings were allotted accordingly. Traffic congestion is a common occurrence in many major cities across the world, especially in third world cities, and this has caused untold hardship to commuters in these cities in diverse ways  [1] . These include loss of man-hours, accident, missed opportunities, noise pollution, air-pollution, increased fuel consumption, increased tendency to violate traffic rules, and in some cases extortion by corrupt traffic control officials. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. There are other problems as well, like ambulances getting caught up by a red traffic signal and wasting valuable time  [2] . Sometimes higher traffic density at one side of the junction demands longer green time as compared to the standard allotted time. The suggested case study, Jakpa junction is a typical example of a traffic congested area. In order to overcome the aforementioned problem, this research adopted a density based approach in controlling vehicular traffic. The proposed system would use a microcontroller of PIC family duly interfaced with sensors, to change the junction timing automatically to accommodate movement of vehicles, thereby, avoiding unnecessary waiting time at the junction. The density of the vehicles is measured in three zones i.e., low, medium, high based on which timings were allotted accordingly. A traffic control model using signaling system in a discrete cross-road with NE-555 timer circuit was implemented by  [3] . The uniqueness of this model lies in the implementation of on-demand Pedestrian-Pass signaling, thereby transforming the design into dynamic controller. However, this system lacked inbuiltmechanism for controlling vehicular traffic based on density. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. Sometimes higher traffic density at one side of the junction demands longer green time as compared to standard allotted time  [4] . The image captured in the traffic signal is processed and converted into grayscale image then its threshold is calculated based on which the contour has been drawn in order to calculate the number of vehicles present in the image. After calculating the number of vehicles we will came to know in which side the density is high based on which signals will be allotted for a particular side. This is very important when the complexity of a task (problem) exceeds a certain threshold. Real world complex problems such as human controlled systems involve a certain degree of uncertainty, which cannot be handled by traditional binary set theory. The algorithm implementation was done using Mathworks, MATLAB software, and the results were simulated using a Simulink Tool to create traffic scenarios and comparisons between simple time-based algorithms and the developed system. For testing the adaptive traffic light controllers, a simulation system using Qt, C++ software integrated with MATLAB tools was developed. However, this method has no mechanisms for capturing traffic density and for providing a pass for emergency vehicles. This approach involves breaking down a system into smaller units to enable the designer get more insight into the system. This system was broken down into different units as listed below: (1) Power Supply Unit (2) Control Unit (3) Sensor Unit The complete circuitry is operated with TTL logic level of 0-5V. It comprises of a 0V to 12V transformer which is required to step down the 220V AC supply to 12V AC. This is converted by a bridge rectifier to a dc voltage. To isolate the output voltage of +5V from noise further filtering by a 220uF capacitor is done. The bridge rectifier consists of four single phase rectifier diodes connected together in a closed loop to form a circuit that is capable of converting AC voltage to DC voltage  [8] . The maximum output voltage of the bridge rectifier is known as the Peak Rectified Voltage, and is given as; formula_0 V be is the biasing voltage for the diode (i.e. The PIV of the diode is calculated using the relation formula_1 Where Vp = peak voltage of transformer Vrms = root mean square voltage The Vrms of the transformer is given as 12v formula_2 Recall PIV ≥ 2Vp Therefore, PIV of the selected diode is given as 2 x 16.9706 = 33.9412v For current, the maximum current of the transformer I (max) = 500mA The forward current of the diode I f is given as:   formula_3 As for the optocoupler, it is used to provide coupling while ensuring electrical isolation between its input and output  [12] . Another purpose of an optocoupler is to prevent rapidly changing voltages or high voltages on one side of a circuit from distorting transmissions or damaging components on the other side of the circuit. The Transformer steps down the 220 v AC supply to 12 v AC. This is rectified by the bridge rectifier, filtered by the capacitors to remove ripples and regulated by the voltage regulators to produce fixed value of 5 volts which is supplied to the system. The time delay in the traffic signal is set based on the density of vehicles on the roads. The IR sensors are used to sense the number of vehicles on the road. According to the IR count, microcontroller takes appropriate decisions as to which road is to be given the highest priority and the longest time delay for the corresponding traffic light. It is a widely used type of electronic prototyping board characterized by a 0.1-inch rectangular grid holes with parallel strips of copper cladding running in one direction all the way across one side of the board. The leads were then soldered to the copper tracks on the other side of the board to make the desired connections. The codes are as shown in the Appendix. Simulation was done via Proteus software. From the test carried out on the circuit, it was observed that the LEDs with the same color have equal timing, and that each pole of the four traffic light controlling poles, switches sequentially and repetitively until the circuit is disconnected from power. Jakpa Junction, in Effurun, Delta State, Nigeria is increasingly becoming chaotic by reason of the recurring traffic gridlock it experiences. The problem assumes a more worrisome dimension on Effurun Market days. On these days traffic rules are usually violated because of the complex traffic situation. One of such systems is the automatic signaling using IR sensors and Microcontroller. Although the aims and objectives of the project were achieved satisfactorily, it could be further improved upon. This new design would further reduce time delay, improve efficiency and reduce accidents by incorporating the following modifications: (1) The Airport -PTI road should have a two -way traffic light to pass the vehicles coming from Airport road to PTI road. There should also be a two -way traffic light between Effurun -Sapale road and Jakpa road to pass vehicles coming from Effurun -Sapale road to Jakpa road.
paper_3	The aim of this article is to present the general architecture trends of Web-based Adaptive Educational Hypermedia Systems (AEHSs) and to give a complete description of architecture of the AEHS MATHEMA. In the beginning, a related work on the architecture trends of Web-based AEHSs is presented. Then, a description of the aspects of the MATHEMA is done regarding both its pedagogical and technological part. Next, one-on-one unit is presented separately and their functions are generally described with respect to the adaptive Web technologies used. They build a personalized model of the learner and apply this model to adapt the content and/or the appearance of the hypermedia, according to the learner's specific characteristics, such as the cognitive goal, level of knowledge, pre-existing or prior knowledge, interests, preferences, stereotypes, cognitive preferences and cognitive or learning style  [4] . In the Web-based AEHSs, several adaptive and intelligent techniques have been applied to introduce adaptation, such as  [4] : (a) Curriculum Sequencing: It helps the learner to follow an optimal path through the learning material. (b) Adaptive Presentation: It adapts the content presented in each hypermedia node according to specific characteristics of the learner. The general purpose of the AEHS MATHEMA is to support learners through an interactive and constructivist educational environment in order to construct their knowledge in Physics and Technology, individually and/or collaboratively, and to overcome their misconceptions and/or learning difficulties AEHS MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and meta-adaptive navigation, interactive problem solving, and adaptive group formation. More specifically it supports: (1) adaptive presentation according to learner's learning style; (2) four adaptive navigation techniques which are direct guidance (uses a "next" button), link hiding (hide nonrelevant links), link annotation (use colors to indicate relevance), and link sorting (like in search engines); (3) a meta-adaptive navigation technique in order for the learner selects the more appropriate navigation technique that best suits him or her; (4) an interactive problem-solving method through personalized and collaborative problem-solving activities that exploits the teaching approaches of experimentation through simulations, guided inquiry and discovery, investigations and peer collaboration; and (5) a method of adaptive group formation for peer matching. The aim of this article is to highlight the main contributions of the architecture of the MATHEMA in the improvement of the functionality of AEHSs. The rest of the paper is organized as follows: In the section 2 related works for various models of architecture of AEHSs is presented. In the section 3 the architecture of the AEHS MATHEMA is presented in detail. For the AEHSs various models of architecture have been developed so far on which the designers of these systems are based on. The rules together form the adaptation model in AHAM. A key point in AHAM is that the adaptation rules are used to translate user actions into user model updates (as well as the presentation and adaptation of content). The rules are used by an adaptive engine in order to generate the presentation specifications. Today almost all AEHSs follow the general architecture of the AHAM reference model (but possibly with a different kind of rule or reasoning language and engine). The ADAPT project has identified high level design dimensions for AEHSs, which are: (1) context of use (CU), (2) content domain (DM), (3) instructional strategy (IS), (4) instructional view (IV), (5) learner model (LM), (6) adaptation model (AM), and (7) detection mechanism (DE). These dimensions form the axes on which both an AEH problem and its solution can be represented. Proper's architecture is a combined architecture of SCORM LMS and AEHS. The DM structure is exported by the manifest file and is stored into Java Object Files. Moreover domain independent data of UM is stored into the database while at the same time, data about user knowledge is stored into Java Object Files. The adaptive navigation techniques have been applied to it via Java servlets and Java Server Pages (JSP). All the runtime data about user actions and performance is stored into Java Object Files via JSP and Java servlets. The adaptation component consists of a Java servlet which automatically generates the individualized Webpage, each time an HTTP request is received by the server. WELSA does not store the course Web pages but instead generates them on the fly, following the structure indicated in the XML course and chapter files. Based on these preferences, the servlet applies the corresponding adaptation rules and generates the new HTML page. The Learner Model is the structure that contains the information on the learner's characteristics that allow the AEHS to adapt to these characteristics. The Presentation Generator requests a composition of the presentation to the JavaServer Pages. As JavaBeans are components of an application in the context of JavaServer Pages or servlets, they are suitable to implement the Presentation Generator and the Adaptation Decision Model. Additionally, the JavaBeans offer advantages of separating the programming logics from presentation, as for the Presentation Generator and the final page composition. The purpose of ULUL-ILM is to provide the AEHS that can recognize student' learning style automatically in real-time and then presents the learning content adaptively based on student learning style. The system then enables to present and recommends a variety of learning contents adaptively towards each of the student's learning style identified in the student model through the adaptation model. The system then analyzes the learning content on each of the learning material, and then comes up with the generated teaching strategies by means of the teaching strategy generator and fragment sorting. The adaptation model enables the system to adaptively presents the content, based on the student's learning style by combining the fragment sorting and adaptive annotation technique. The course player in ULUL-ILM enables the system to adaptively presents the content with various teaching strategies towards each of student's learning style. AEHS MATHEMA architecture is based on Web technologies similar to client/server distributed computing architecture of MIT and it is mainly supported by Apache Tomcat Server 5.5 and MySQL 6.0 relational database server. For supporting features, it uses the (JSP) pages, Java servlets, JavaBeans, JavaScripts, and Apache Struts technologies. This is an interface for Java that standardizes database connections, queries, and results of relational databases. Java servlet technology and JSP pages are server-side technologies that have dominated the server-side Java technology market; they have become the standard way to develop Web applications. A servlet is a Java class that implements the Java servlet API, a protocol to respond to HTTP requests. In the context of JSP pages, JavaBeans components contain business logic that returns data to a script on a JSP page, which in turn formats the data returned from the JavaBeans component for display by the browser. A JSP page uses a JavaBeans component by setting and getting the properties that it provides. The benefits of using JavaBeans components to augment JSP pages are the following  [10] : (1) Reusable components: Different applications will be able to reuse the components. The Apache Struts is a formalized framework for the architecture style design pattern Model View Controller (MVC). It is a set of pedagogical rules that combine the learner's model with the domain knowledge for adaptive performance. In the field of Adaptive Group Formation Module selecting of the most appropriate teaching strategy, the learner's learning style is taken into account during the learner's study. The domain knowledge of the AEHS MATHEMA is the basis of the system's adaptation. In the AEHS MATHEMA, the hierarchical representation of the domain model is adopted by the ELM-ART  [5] , as follows: The first level defines the cognitive objectives, the second the basic concepts and the third the pages with the corresponding educational material. The model that supports the AEHS MATHEMA is the overlay model. The characteristics that the system maintains in learner model for each learner are: name, gender, username, password, learning style, level of knowledge for each cognitive goal (general performance), level of knowledge for each basic meaning of the cognitive goal (quantitative and qualitative characterization), concepts that have been successfully evaluated for each cognitive goal, current page, current teaching method, current navigation technique, navigation history, meta-adaptive navigation status, initial Web experience and knowledge level (pre-existing knowledge) in the current cognitive goal, ways of presenting feedback messages, preference for assistance by Advisor in terms of its navigation, course sequence, and phases of the guided dialogues in interactive problem solving process in which each learner participates. The Student Model is transparent to the learner (open learner model) and is verifiable by the learner, that is, the learner can access his/her model and change the content of some characteristics. The learner during the study selects the cognitive goal he or she wishes to study, following his or her personal course in the educational content, exploits the system's assistance, navigating and studying, is informed about the information that the system maintains, who can change them by intervening and directing the system's adaptation in this way, in terms of curriculum sequencing, adaptive presentation and navigation and adaptive group formation, selects his or her learning style, selects the navigation technique, activates and deactivates the curriculum sequencing, selects the way that the feedback messages are presented, changes his or her knowledge level for each basic concept of the cognitive goal, and activates-deactivates the Advisor. The system, during the learner's study, monitors his or her interactions with the system and his or her assessment and accordingly updates the links to the course material provided and/or adapts learning material to learner according to his or her learning style. This is an innovation on the architecture of AEHSs. It is the second level meta-adaptation engine that is responsible for monitoring the cognitive improvement of the learner in the cognitive goal that he or she studies, after n successful evaluations of his or her knowledge in basic concepts, showing him or her advantages and disadvantages of the four navigation techniques that it supports. In AEHS MATHEMA, meta-adaptive navigation works as follows: The first time that the learner enters the system, he or she is asked to state his or her Web experience and level of knowledge in the cognitive goal he chose to study (preexisting knowledge). This module is responsible for transferring the training material to the presentation unit, for giving feedback, for creating guided dialogues and for evaluating the learner knowledge. It includes the content of the JSP pages, Java servlets responsible for feedback and creation of guided dialogs in problem-solving, Java servlets for creating evaluation questionnaires, and so on. In this design phase, the system does not follow a standard metadata description but the system supports data transfer objects. It is responsible for what will be presented to the learner as a result of the processing of the information that arrives from the adaptation engine, the meta-adaptation engine, adaptive group formation module, and application module. In this module there is the basic servlet, which deals with the presentation of the training material and other JSP pages and servlets, such as the page and the corresponding servlet to inform the learner about the characteristics of all learning styles, the servlet for the curriculum sequencing, the servlet for selecting and presenting the concept that the student selects to study according to his or her learning style, etc. More specifically it: (1) initializes and updates the Student Model; (2) collects and stores the data that the learner answers to open-ended questions, entries in tables, registers in notes, etc; (3) monitors the interactions between the learner and the system to get information about his or her choices in terms of the questionnaires, the phases of guided dialogs, the links, the options for meta-adaptive navigation, the visited pages, and more. Assessment of the pedagogical and technological aspects of the AEHS MATHEMA has been carried out. This article presents the trends of architecture of AEHSs so far and then an extensive description of the architecture of the AEHS MATHEMA. The implemented AEHSs so far use various techniques to implement their functions. Most of AEHSs presented in this paper are based on JSP, Java servlets, and Javabeans technologies to implement their functions, as implemented by AEHS MATHEMA. Furthermore, the presented AEHSs above have at least three main modules, such as Student Model, Domain Model, and Adaptation Model as the AEHS MATHEMA also has. The main contributions of the architecture of the MATHEMA in the improvement of functionality of AEHSs are the meta-adaptation technique, the adaptive group formation technique, the problem-solving technique, and the synchronous communication protocol that it supports.
paper_21	In this paper, we present the powerful scheme ZSISMP (Zimmermann Self Invertible Stabilizer Multiplier Permutation) to attack the hardness of the minimum distance search problem of BCH codes. This scheme consists in evaluating the minimum distance of the reduced dimension sub code fixed by a Self Invertible Stabilizer Multiplier Permutation by Zimmermann algorithm. The use of this efficient local search had yield to determine the error correcting capability of many BCH codes of length 1023 and 4095. In telecommunication and storage systems, the fundamental problem is the reproduction at one point exactly or approximately the selected data at another point. An efficient solution of this problem is the use of error correcting codes. BCH codes are a family of cyclic codes, which are used in many applications, due to their powerful algebraic decoding algorithms and their error-correcting capability. However, the determination of this metric is difficult in general as pointed out by Charpin in  [1]  and remains an open problem in coding theory. For these codes, only a lower bound is known and the minimum distance is known only for some lengths and special cases  [2-3-4-5-6-7-8] . In this paper, our work will focused on finding the minimum distance of large BCH codes. The remainder of this paper is organized as follows: The next section presents the main related works. The section 3 presents the proposed scheme ZSISMP. The section 4 presents the main results. The conclusion and possible future directions of this research are outlined in section 5. This section summarizes the most important ones. In  [9] , Augot, Charpin, and Sendrier presented an algebraic system constructed from Newton's identities. In  [10] , Augot and Sendrier found idempotent codewords of minimum weight for several primitive narrow-sense BCH codes. In  [11] , Canteaut and Chabaud have developed a new probabilistic algorithm, based on the heuristic proposed by Stern  [12] , for finding minimum-weight words in a linear code. The artificial intelligence Simulated Annealing presented in  [16] , Tabu Search  [17] , Hill-Climbing  [18] , Genetic Algorithm  [18] [19] , Ant Colony Optimization  [20] , Metropolis Algorithm  [21] , was shown to be useful to attack the difficulty of the minimum distance search problem for BCH Codes. This improvement has yield to a fast convergence of the Simulated Annealing by reducing the number of iterations, as well as obtaining good results in comparison with the previous works presented in  [17-18-19-21] . Unlike classical techniques based on exhaustive or partial enumeration of codewords, Berrou in  [23]  has presented an efficient approach based on the notion of Error Impulse response of a Soft-In decoder. This approach consists in adding to all-zero codeword a level of noise and considering the minimum distance as the smallest level of noise from which the Soft-In decoder fails in correction. The proposed method MIM-RSC, has allowed an efficient local search and therefore finding the true minimum distance of some BCH codes of length 1023 and 2047 as well as obtaining good results in comparison with the previous works presented in  [17-18-19-20-21-22 ]. The proposed scheme works as follows: Inputs: formula_0 Step 1: Find the sub code SC fixed by σ Step 2: Find the estimated minimum distance d of SC by using the Zimmermann algorithm. It is well known that the smallest primitive narrow-sense BCH code whose minimum distance is greater than its designed distance is BCH (127, 43, 29) and it is the only one for this length. The both last result have been proved in  [9] , by using the Newton's identities. In order to validate the proposed method, it is applied on all BCH codes of known minimum distance presented in table 1. the obtained results show that the minimum weight found by the proposed method is equal to the true value of the minimum distance of all BCH codes of length up to 255. Therefore, the proposed method is validated for BCH codes of lengths up to 255. These results demonstrate that the proposed scheme outperform greatly the famous Zimmermann algorithm. The table 3 presents a comparison between the proposed scheme and MIM-RSC method  [24] . The obtained results are given in the table 4 so that d f represent the minimum distance found by our scheme. In this paper, we have proposed a new efficient scheme to find the minimum distance for large BCH codes. The experimental results show that the proposed scheme outperforms several known powerful techniques. In the perspectives, we will apply this powerful scheme to construct good large cyclic codes, and adapt this scheme for other linear codes.
paper_31	This paper takes chronic diseases as the research object, and proposes a design of intelligent medical system for chronic diseases based on semantic matching by the adaptation of ZigBee technology in the front-end data acquisition. Through the ZigBee wireless sensor network, this system sends the physiological parameters collected by various medical sensors to the intelligent medical system, and innovatively proposes semantic matching algorithm to solve the queuing problem of data transmission, to ensure the accuracy of data transmission. This system employs the improved spatial vector model to process the data uploaded, and uses AES encryption algorithm in the process of data transmission to ensure the security of data transmission. With the means of the Internet, with the carrier of the mobile terminal, intelligent medical treatment founded on the units of family is a new trend. Therefore, it is of great significance to develop an intelligent data analysis platform to record, store, and share and handle various personal health signs in time through wireless transmission. For example, the General Hospital of the People's Liberation Army (PLAGH) communicated and discussed some diseases with foreign experts through satellites. With the great progress of mobile communication technology, Internet and WSN technology with low power consumption, some scholars have applied wireless transmission technology to medical detection system. In the meantime, some other scholars use 4G and the Internet transmission technology to send physiological parameters to the medical system through the Internet or 4G network, so as to realize medical monitoring. With the emergence of various new telecommunication technology, scholars kept providing health care using all sorts of communication technology. During this stage, information technology was connected with medical health care more closely. Haji Bagheri Fard present methods based on deep learning algorithms to achieve high classification accuracy  [8] . As an alternative for aiding healthcare systems, sensors and wearable devices are used for monitoring patient physiological data to help guide health services or the self-care of patients  [9] . (3) The problems of unitary monitoring data. The current WITMED system focuses only on data acquisition instead of processing uploaded data and returning the result. The growing trend of WITMED system shows that a variety of health signs data will be monitored, transmitted, processed and analyzed in real time, the system can process the uploaded data. Attracted by the broad prospect of smart medicine, BAT and other Internet enterprises have participated in the smart medical industry. Alibaba founded Alibaba health  [15] ; Tencent launched the first AI+ medical product Tencent Meiying in 2017; many large enterprises integrated medical resources through mergers and acquisitions to lay out the intelligent medical industry chain. The system processes the received information according to the relevant algorithm, and then feeds back the corresponding results to the designated user. The process of reading and transmitting data is a loop. The data acquisition structure charts After receiving the relevant data, the data analysis platform needs to analyze the corresponding data and feedback the processed results to relevant users. Data analysis roadmap is shown as follows: After collecting patients' physiological information from the node, various physiological sensors will continuously upload them to the coordinator. The general technique route of the algorithm is as follows: Propose a comprehensive sorting algorithm, on the basis of semantic matching and a first-come-first-served principle. Suppose there are N pieces of information to be sent according to the messages' arriving order, divide each message into many words, and mark each word as Nm, , . First, match the Word segmentation information by queued delivery with the taglib of the analysis system. If there are X words appearing in the taglib and N-X words not appearing in it, the matching degree of the information is (N-X) / N * 100%. Finally, the request is inserted into the message queuing sequence by the matching degree. The general algorithm of this part is as follows: Define priority function: f x, y ax by ε, with a basic definition as follows: x: semantic matching degree, 0≤x≤1 y: a request arriving at the serial position, 0<y≤1 a is defined as the weight of semantic matching degree, 0<a<1 b is defined as the weight of serial position of Request arrives, 0<b<1, a+b=1 ε is defined as the disturbance value, 0≤ε≤0.1, with the default as 0. Generally, a is close to µ= 1 2 , formula_0 Therefore, the process of queuing algorithm based on Semantic matching is as follows: With the hypothesis that a= 1 2 (to take one half of the sum) x= 5 10 (there are 10 participles in semantic information, of which 5 are matched), y 1 (Before a new message arrives, assumes that the request queue has 10 participles waiting, then: f x, y y a x y ε =0.294, therefore, the priority value of the message is 0.294. The patients' vital signs data will be transferred into the data analysis system by the sensor, and then the data will be analyzed by the big data system, various signs data will be compared with the characteristic values of related diseases and the results will be returned. This research proposes an improved space vector model algorithm to ensure whether the patient has a related disease by the similarity between the physical signs and the disease on the basis of calculation of the sign value, continuous sign value and the disease characteristic value. An improved SVM calculation method is as follows: Definition 1 (conception): Define a tuple F= (X, Y, Z), in which x is the collection of all objects is the object property set, Z (X*Y), Z is a subset of X*Y. Definition 3: The concept similarity is calculated as:  formula_1 The column mixed operation formula is shown as  Figure 6 : (4) key plus calculation Key plus calculation operates the 128 bit key is XOR bit by bit with the data in the state matrix. The process of key plus calculation can be regarded as the result of Bitwise XOR or byte level or bit level operation. In this study, a design scheme of intelligent medical system based on semantic matching is proposed, with the front-end data acquisition which adopts ZigBee technology, and a semantic matching algorithm is innovatively proposed to solve the queuing problem of data transmission to ensure the accuracy of data transmission. Through ZigBee wireless sensor network, the system sends the physical parameters collected by various medical sensors to the information control terminal, and the improved spatial vector model is employed to process the uploaded data in the process of data transmission by the intelligent data analysis platform, and the security of data transmission is ensured by adopting the AES encryption algorithm in the whole process. The intelligent system realizes data acquisition, data encryption and processing, and big data analysis. The system can gather and process big health data of chronic patients, and realizes the discovery, tracking and treatment of chronic diseases. The semantic matching algorithm and the space vector model algorithm proposed by the system can be widely used in the data acquisition of clinical big data to provide theoretical and technical support for artificial intelligence to assist disease risk prediction. The intelligent system realizes the data collection of the whole process of outpatient service, examination and treatment, which can provide data support for the precise treatment of diseases.
paper_38	A purposive sample of nine hundred and forty-three (943) first-year students constituted the population for the study were drawn from Computer Science, Mathematics and Physics undergraduate degree programmes from the Faculty of Science of the university who were admitted from the 2010/2011 to 2014/2015 academic sessions. The instruments for data collection were OL, UTME and first-year Cumulative Grade Point Average (CGPA) results, which were coded and analysed with the aid of Computational Statistical Package for Social Sciences (SPSS). The results revealed that with a weak correlation, OL is a good predictor on the CGPA, a dependent variable, for academic performance which holds true for students who are in the CGPA category of '1st class' and '2nd Class Lower' respectively. It concluded that the use of OL and UTME as instruments is not enough to select candidates for admission and therefore recommended that other instruments such as senior secondary school mock examinations need to be included as part of the entry requirements in the admission criteria. Education is an essential issue regarding the development of any country in the world. In Nigeria, the demand to acquire university education has been on the increase than ever before  [2]  due to the increase in the population of graduates from secondary schools  [3] . Each student is expected to have at least five credit passes in not more than two sittings in Mathematics, English Language and three other science-based subjects such as Chemistry, Biology, Physics and Geography at the Ordinary Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria Levels (OL) of either Senior School Certificate Examination (SSCE) which is conducted by West Africa Examination Council (WAEC) and National Examination Council (NECO). Despite all these, some students perform poorly in their studies during the first year while some perform very well. There have been various perspectives presented by researchers and psychologists about what is academic performance and its importance. Formula 1 is used for calculating the CGPA. The stakeholders agreed that the Pass Degree be abolished from the grading system and the lowest and highest CGPA scores are 0.00 and 4.00 respectively effective from the 2016/2017 academic session which implies that as long as the score is high, the better the academic performance of the students. As such, the CGPA is considered to be a good predictor indicator of a student's academic performance. Prediction and analysis of students' overall academic performance is a vital milestone in an educational environment that builds their future. The focus of this study is to predict full-time undergraduate students' first-year Cumulative Grade Point Average (CGPA), which is one of the variables for measuring the academic performance by using entry requirements, such as Ordinary Levels and UTME, for Faculty of Science in Kaduna State University (KASU), Kaduna -Nigeria. This poor performance has lead students spending extra years before they could graduate with a pass degree at best. Determine if any of the following entry qualification used for admission, OL results only, UTME scores only or Post-UTME scores (average of OL results and UTME scores) best predict the academic performance of students in the 100 level CGPA examinations; ii. Investigate the relationship between the students' performance of their entry qualification and the academic performance in the 100 level CGPA examinations. The following research questions directed the study: 1. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? It is a yardstick that is used to ascertain the competences of a student from which his abilities could be determined. The authors further explained that academic performance is usually used to determine "how well an individual assimilates, retains, recalls and communicates his knowledge of what is learnt and acquired". There are a lot of definitions of students' performance based on previous works of literature. Reference  [9]  stated that students' performance could be obtained by measuring the learning assessment and co-curriculum. Academic performance or sometimes known as an academic achievement is defined by  [10]  as "Knowledge attained or skill developed in the school subjects, usually designated by test scores or by marks assigned by teachers". A student's academic performance usually is measured in either examinations or continuous assessment tests, and this is expressed in various ways depending on what the scores should be used for. The numerous ways of reporting academic performance include raw scores, percentages, transformed scores, or even as categorical variables such as Excellent, Merit, Very Good, Pass, First Class, Distinction, A1, B2, C4, F9, and others. In Nigeria, students are admitted into universities using their scores in the UTME as well as Post-UTME (PUTME) subject to having at least five OL credit passes in relevant subjects obtained in not more than two examination sittings including the English Language. Several studies have criticised the use of UTME and PUTME as an imperfect instrument for predicting academic performance of students. However, the following review of literature examined the relationships between UTME and PUTME scores as a predictor for the academic performance of students have revealed contradictions in their findings. A sample of 214 students records was used for data collection. The author's findings in his study showed that there was a consistent decline in the number of students admitted using the PUTME which cannot do better than UTME in influencing students' academic performance as the outstanding and weak students formed the upper 12.5% and lower 12.5% while the remaining 75% consists of the average students. The records of students who graduated in the 2009/2010 and 2010/2011 academic sessions from seven faculties were obtained using the stratified random sampling technique. The authors tested their nine hypotheses using an independent samples t-test and two-way analysis of variance. Graduates with the best academic performance from the preliminary programmes were those admitted through the certificate programme. Due to this, the recommendation was that the stakeholders should review the use of UTME and PUTME results for university admissions. The study by  [13]  found a significant relationship between students' scores in three examinations, namely: UTME, PUTME and 100-Level Psychology course, Faculty of Agriculture, Federal University of Agriculture, Makurdi, and thus concluded that the UTME has predictive validity for performance in the university. In the same vein,  [14]  tested the predictive power of the Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria JAMB UTME in predicting students' performance in the university's semester examination by using a regression model. The article recommended that JAMB should embark on a more realistic review of the content of the UTME to enhance its predictive validity. A population of 1650 students admitted into the university during the 2011/2012 academic session from Faculties of Arts, Education, Science and Social and Management Science was used to obtain their UTME, PUTME scores along with their GPA for eight semesters. The author concluded from his findings that the use of PUTME is beneficial for selection of candidates for admission and also that candidates who had a high-performance level in the UTME have a positive effect on the academic performance in the university. The author further recommended the need for the PUTME exercise to be strengthened to have a fruitful admission of candidates. One major shortcoming of virtually all the studies reviewed is their scope in time and spread/coverage. This section discussed in detail the methodology employed such as research design, sample of study, instruments used, the procedure for the collection of data and data analysis. He explained that there are two types of ex-post facto research designs namely the correlational and the casual comparative. The design adopted in the study is the correlational ex-post facto, which is used to measure the degree of association between two or more variables or sets of scores. The Predictive Correlational Ex-Post Facto design was identified to be the most appropriate for the study since the results (CGPA, UTME and OL) of students in the Faculty of Science were used in reaching conclusions about the whole prediction of academic performance. The Faculty of Science consists of nine undergraduate B. Sc. The population of the study was limited to consist of all students admitted into three Faculty of Science undergraduate degree programmes of Kaduna State University for five academic sessions from 2010/2011 to 2014/2015 using the OL, UTME and CGPA results. The sample distribution is as shown in  Table 1 . The instrument used to derive the data for this study were: the JAMB UTME scores from 2010 to 2014 and OL grades in the five relevant subjects which were the pre-admission criteria and first-year CGPA results obtained from the semester examinations from 2010/2011 to 2014/2015 academic sessions. The UTME was wholly multiple-choice objective questions conducted via Computer-Based Tests (CBTs) by JAMB. The semester examinations were mostly essay type questions. However, the OL results obtained from the database was for students admitted during 2010/2011 to 2014/2015 academic sessions. The coding for the CGPA is also shown in  Table 4 . It was used in this research study. Since the focus of the study is to determine the predictive validity of OL results and CGPA scores (OL-CGPA), UTME and CGPA scores (UTME-CGPA), and PUTME and CGPA scores (PUTME-CGPA), the statistics employed on the extracted data were Multinominal Logistic Regression (MLR) and Pearson Product Moment Correlation (PPMC) coefficient. PPMC is used to determine the degree of relationship between two sets of variables and compute the strength of association between the variables  [19] . There are three types of linear relationship that may exist between these two variables namely positive linear correlation, negative linear correlation and no correlation. In other words, MLR is used to predict a nominal dependent variable given one or more independent variables. MLR can have interactions between nominal and continuous independent variables to predict the dependent variable. The results of the data analysis are presented in tables according to the research questions that guided the study in this section. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? Table 5  shows the summary of correlations coefficient between OL results and CGPA scores (OL-CGPA), UTME and CGPA scores (UTME-CGPA) and PUTME and CGPA scores (PUTME-CGPA) aimed at all the academic sessions for Computer Science, Mathematics and Physics degree programmes. For the Computer Science programme, as shown in  Table 5 , the correlation coefficient indicated a low negative correlation in UTME-CGPA (-0.092) and PUTME-CGPA (-0.001) and a low positive correlation for OL-CGPA (0.072). The indication shows that the nine correlation coefficients for this research question are very low out of which five have a low negative relationship. Likelihood Ratio Tests is a statistical test of the goodness-offit between two models. Table 7  shows the results of the Parameter estimates, which is also called coefficients, for the Multinomial Logistic Regression (MLR) for each degree programme. A Multinomial Logistic Regression (MLR) was performed to model the relationship between the predictors and membership in the six groups, i.e., CGPA categories (Fail, Pass, 3 rd Class, 2 nd Class Lower, 2 nd Class Upper and 1 st Class) for each degree programme. Each of the five equations for every degree programme in  Table 7  includes the intercept and the slope for the predictors. For Computer Science, Mathematics and Physics programmes, the first equation intercept is the log of the ratio of the likelihood of a student having a 'pass' degree to the likelihood of that student having a 'Fail' degree. Among the classification of degrees, each of the five subgroups for each programme, that is Pass, 3 rd Class, 2 nd Class Lower, 2 nd Class Upper and 1 st Class, are contrasted with the baseline group of 'fail' degree. For the Computer Science programme, the slopes (B) of OL in all the CGPA categories are positive. These showed that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. This result shows that the relative strength of UTME on the former is higher than those with a CGPA category of 'Fail' and otherwise for the latter. Finally, the slope (B) of PUTME in all the CGPA categories is negative, which shows that the relative strength of those with a CGPA category of 'Fail' is higher than the other categories. For Mathematics and Physics students the relative strength of OL, UTME and PUTME on CGPA performance are statistically insignificant except for the slope (B) of OL in the CGPA category of '1 st Class' for Mathematics students, which statistically significant. Table 8  shows the summary of correlations coefficient between OL-CGPA, UTME-CGPA and PUTME-CGPA aimed at all the degree programmes for the academic sessions ranging from 2010/2011 to 2014/2015. The summary of  Table 8  shows the correlations of OL and CGPA scores, UTME and CGPA scores and PUTME and CGPA scores for all the programmes used for the analysis. In 2012/2013, UTME-CGPA (-0.363) and PUTME-CGPA (-0.123) have low negative relationship whereas OL-CGPA (0.111) has a low positive relationship. The OL-CGPA (-0.071), UTME-CGPA (-0.090), and PUTME-CGPA (-0.040) in the 2014/2015 session signifies a low negative correlation. However, the entire results revealed that all the 15 correlation coefficients are very low, with seven of the result showing a low negative correlation relationship. How well do OL results, UTME scores and Post-UTME Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria scores of students predict their first-year CGPA across the academic session, 2010/2011 to 2014/2015? Table 9  is the likelihood ratio test results for all academic sessions ranging from 2010/2011 to 2014/2015. As shown in the Likelihood Ratio Test results in  Table 9 , the likelihood ratio Chi-Square of 37.446, 19.938, 46.141, 14.349 and 11.167 for 2010/2011, 2011/2012, 2012/2013, 2013/2014 and 2014/2015 academic sessions which has the following as significant values of 0.001, 0.174,.000, 0.499 and 0.741 tells us that the model for students admitted during the 2010/2011 and 2012/2013 academic sessions predicts the CGPA, which is the dependent variable while the other academic sessions does not. For all the academic sessions as shown on the results, the first equation intercept is the log of the ratio of the possibility of a student having a 'Pass' degree to the possibility of that student having a 'fail' degree. Among the classification of degrees, each of the CGPA categories is contrasted with the baseline group of 'Fail' degree. For the students admitted during the 2010/2011 academic session, the slopes (B) of OL in all the CGPA categories are positive. These show that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. Furthermore, the slopes (B) of UTME in the CGPA categories of the '3 rd Class' is negatively signifying that the relative strength of UTME is lower than those with a CGPA category of 'Fail' and the rest are positive which signifies otherwise. This indicates that the relative strength of UTME is higher than those with a CGPA category of 'Fail' and the rest of the CGPA categories are negative which indicates otherwise. Based on the analysis and results using MLR and PPMC for each programme and each academic session, it is evident that OL, UTME or PUTME could not individually significantly predict the academic performance of students in Faculty of Science. Also, there is a need to do a further study by including some more variables, such as age and senior secondary school mock examinations results, as criteria to significantly predict the academic performance of students successfully.
paper_57	This paper attempts to overcome stagnation problem of Ant Colony Optimization (ACO) algorithms. Stagnation is undesirable state which occurs at a later phases of the search process. Excessive pheromone values attract more ants and make further exploration hardly possible. Crossover and mutation operations have been adapted for use with ant generated strings which still have to provide feasible solutions. Extensive simulation tests were made in order to determine influence of genetic operation on algorithm performance. Exact algorithms for instance Dijkstra or Bellman-Ford appear to be slow and inefficient on large scale graphs. One of the well-known graph search algorithm that utilizes a heuristic is A* search  [1]  or ACO algorithm. Ant colony optimization represents an efficient tool for optimization and design of graph oriented problems. It is a multi-agent meta-heuristic approach and was first purposed by M. Dorigo et al. [2]  as Ant system (AS) algorithm. The aim is to find the shortest path. On their way back they use the same way from which abundant loops has been removed, but the amount of pheromone (1) ∆τ k ij (t) they produced is inversely proportional to the tour length L k (t). formula_0 formula_1 T k (t) is the tour generated by ant k, Q is a constant and tuple (i, j) denotes beginning and termination node of an arc. Evaporation rate is a user adjusted parameter and affects pheromone durability; i.e. At the beginning when no pheromone values are available heuristic values η ij takes dominance. Later the ant uses probability selection rule to choose the next arc according to formula_3 where p k ij (t) is probability the ant k chooses the arc (i, j) from the neighborhood N k i of node i except the node visited previously. The more pheromone is located on particular arc, the more attractive it appears. Heuristic values η ij affect probability only at the beginning when pheromone values are low. Disadvantages of ACO algorithms are (i) many user parameters and (ii) the selection pressure. Let's mention ant colony system (ACS) with pseudo-random proportional rule  [3]  in which random uniformly distributed variable q ∈ (0,1) is compared with a tunable parameter formula_5 else the probability selection rule (3) is applied; random selection applied to AS rank  [4]  where random selection rate r is the probability of random selection and represents a user parameter which adjusts the balance between exploration and exploitation; prevention of quick convergence (i) and stagnation avoidance (ii) mechanisms applied to AS  [5] . The stagnation avoidance mechanism (ii) is based on the comparison of a randomly generated quantity q ∈ (0,1) with probability p k ij (t) of selected arc. This occurs in later stages of the search process, where pheromone values tend to be high, and thus the chance of further exploration is low. Genetic algorithms (GA) were proposed by  Holland (1975) . The original GA is known as simple genetic algorithm (SGA). The four main components of GA are representation (i), mutation (ii), crossover (iii) and selection (vi) mechanism. Each component is adapted in order to provide feasible solution for ACO algorithm. In ACO algorithms representation (i) of genotype space is sequence of nodes: formula_6 where gene n is graph node and L is path length. In each genome each gene is changed with the equal probability. The simplest form is one point mutation on  Fig. In ACO adaptation the first and the last node is excluded from mutation. If more such nodes occur, random selection is applied. In ACO algorithm crossover position is represented by a common node of parental strings except the first and the last node  (Fig. If more of such nodes exist, random selection is applied. Crossover operation makes sense only if both child strings differ from their parents. In GA many selection (vi) mechanisms are available, like roulette-wheel selection, tournament selection, stochastic universal sampling or reward-based selection  [7] . At the end of each cycle t, when all the ants finish their tours T k (t), genetic operations are applied on the T k (t) strings which represents the list of nodes. At first mutation is applied. If mutation is not feasible, another node is chosen. If mutation fails on all nodes of the tour, another tour is chosen. After all mutation operations are performed, crossover operations are applied. If crossover operation is not feasible, another second string is selected. If no tour has common node with the first selected tour, another first tour is selected and the random selection process is repeated. Since genetic operations may produce strings with loops, in ACO framework prior and immediately after each genetic operation a loop removal procedure is performed. For this purpose ACO GO algorithm has embedded user feedback which represents a ratio between accomplished and required genetic operations. The above described ACO GO algorithm has been tested on a random generated graph. Common ACO parameter values were set in accordance with  [8]  and are listed in the  Table 1 . The value for the number of cycles represents three macro cycles of ACO MC  [9]  for the same graph and parameters. Test graph is a symmetrical multi-graph with 80 nodes and 300 arcs  (Fig. Node coordinates x,y fall in range <0,1> and arc's values c ij represent the arc lengths. The task is to find the shortest path between start node n s = 1 and end node n e = 80. For each setting 500 trials were performed. Simulation results were divided into three groups according to number of crossover pairs and are listed in the  Table 2 . The results received with GO are better almost in any case. It can be seen that the higher number of mutation operations, the better the performance is (Tables 2). The outcome with different mutation distribution is asymmetric. Results received without crossover operation have higher values along with the Mutation paths axis  (Fig. with no crossover pair certain amount of mutation operations should be spread out among more paths, but with 2 crossover pairs concentration of mutation operation on less paths tends to perform better. Genetic operations where nearly always feasible; ratio accomplished / required mutation operations is 100% and for the crossover operation over 99%. In order to determine the effect of crossover operation crossover rate was let to grow up to 100%  (Table 3) . 8) ; the highest output was gained for 60% of crossover rate. Beyond 60% threshold ants foraging behavior is suppressed by crossover overload. As the crossover rate increases, ratio accomplished / required operation decreases  (Fig. The mean value of the cycle when the best value was found is 109.081 with standard deviation 2.617. Limit of crossover is 60% of crossover rate. The higher the crossover rate, the lower the accomplished / required ratio. This can be explained by the nature of the mutation operation which creates new paths whilst crossover operation can only combine already existing solutions. The higher amount of mutation operations the higher the performance gain is. Without crossover operation distributed mutation operation has better performance, but with two crossover pairs concentrated mutation operation on less paths tends to perform better. The results are promising; GO improves ACO algorithm performance more than twice. Further research and more experiments are needed to determine the distribution and optimal amount of mutation operation with respect to the number of ants and length of the paths.
paper_78	Fuzzy C-means clustering is a soft technique and has some advantages in ecological studies. Glycyrrhiza uralensis is an endangered medicinal plant species and mainly distributed in North China. A hundred plots were classified into 12 vegetation formations by fuzzy C-means clustering. These formations were main communities dominated by Glycyrrhiza uralensis in North China. Each formation has its own composition, structure, distribution range and environment, and all of them should be protected effectively. The results suggest that fuzzy C-means clustering is an useful technique for classification of plant community. Fuzzy C-means clustering is the only soft method in the clustering family and should have some advantages  [5] . Ecological communities, such as Glycyrrhiza uralensis communities, have great variations in their species composition respond more or less independently to environmental variables. Because of this, methods based on fuzzy mathematics might be more appropriate in ecological analysis  [6, 7] . Medicinal plant is important natural resource and significant for people health in many countries and regions, such as China, Japan, Korea, India and so forth  [8, 9] . Licorice (Glycyrrhiza uralensis) is one of the most popular Chinese herbal medicines and a significant resource plant species. Beside medicine, it is also widely used in food, tobacco, chemical industries  [10, 11] . Like licorice, most medicinal plants are harvested in the wild and the extended use has led to some medicinal species endangered and tending to be extinct, and the protection of such medicinal species has been urgent  [12, 13] . Further more, conservation of medicinal plant species is important in term of biodiversity conservation  [11, 14] . This study aims to identify Glycyrrhiza uralensis communities and analyze their characteristics on composition, structure and environment in North China. Fuzzy C-means clustering is a soft classification technique  [5, 7]  in which a plot can belong to more than one cluster, and associated with each plot is a set of membership levels. Given a finite set of data matrix (species × plots), the method calculates a list of C cluster centres  formula_1 formula_2 Where i = 1, 2, …, N = the number of plots; j = 1, 2, …, C = the number of clusters; U = {U ij } = the matrix of membership values, U ij is the membership of plot i in cluster j; V is a matrix of cluster centers; m is fuzzifier which determines the level of cluster fuzziness (1 ≤ m < ∞). A large m results in smaller memberships u ij and hence, fuzzier clusters. In the absence of experimentation or domain knowledge, m is commonly set to 2. (dA ij ) 2 is the distance index: formula_3 X i is the vector of attribute measurements in plots, usually a vector of ordination scores; V j is the centre of cluster j, if A is a unit matrix, then formula_4 Based on the algorithm above, the procedure of fuzzy C-means clustering is as follows: (1) Selecting a common ordination method, we use Detrended Correspondence Analysis (DCA), and do ordination analysis for species data. The first DCA axis was used as the basic data X i in clustering. (3) Assigning the primary membership values, U 0 . formula_5 formula_7 Based on the new membership values U, we go back to the fourth step and calculated the next turn V j , (dA ij )  2  and U ij iteratively, and until the membership values become approximately stable. 7 Classified plots into clusters based on the final U. We can use U to identify the relationships among plots and communities directly. Twenty plots of 5 m × 5 m were established randomly at each site which is over 40 ha in area. The coverage, mean height, individual number for shrub and herb species was measured in each plot. The coverage of species was estimated by eyes, and the heights were measured using a tape ruler. In total, 191 plant species were recorded in 100 plots. Elevation, slope and aspect for each plot were also measured and recorded. The elevation was measured by a GPS, slope and aspect measured by a compass meter. The importance value was calculated by the formulas  [7] :  formula_8 Fuzzy C-means clustering is a nonhierarchical method which provides C clusters and their plot compositions  (Table  1) . The name and characteristics in species composition, structure and environment of each community are described below. Its disturbance intensity is medium and heavy. The common species are Ziziphus jujuba var. It is distributed from 300 to 500 m in hills with slope 20 -30° in sunny，semi-sunny and semi-shady slope and sandy chestnut soil. Its disturbance intensity is medium and heavy. The average cover of Glycyrrhiza uralensis in this community is 35% with a density of 5700 ha -1 . The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 60%. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 59500 ha -1 . The common species are Salicornia Bigelivii, Carex duriuscula, Stipa sareptana, Artemisias phaerocephala, Alopecurus pratensis, Saposhnikovia divariicata, and Carex pediformis. Glycyrrhiza uralensis + Carex pediformis +Stipa sareptana. Its disturbance intensity is heavy. The common species are Cleistogenes squarrosa, Caragana pygmaea, Hordeum brevisublatum, Ephedra sinica, Achnatherum sibiricum, Artemisia frigida, Viola tianschanica, Carex duriuscula, and Alopecurus pratensis. Glycyrrhiza uralensis + Astragalinae triloa + Stipa sareptana. Its disturbance intensity is heavy. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 65%. It is distributed from 400 to 800 m in hills with slope 20 -35° in sunny and semi-sunny slope and sandy soil. Fuzzy C-means clustering successfully classified 100 plots into 12 communities dominated by Glycyrrhiza uralensis. Theoretically, fuzzy C-means clustering is the only one soft clustering method and can deal with much imprecise and incomplete information and has advantages in solving non-linear problems and in studying complex system  [2, 17] . Vegetation systems are very complex with interactions of species, communities, environmental variables, and so on  [18] . Therefore, fuzzy C-means clustering should describe ecological phenomena and rules better  [5, 6] . Glycyrrhiza uralensis communities recognizing by fuzzy C-means clustering varied greatly in species composition, structure and distribution area  [20] . These further confirm that fuzzy C-means clustering is an effective technique in vegetation analysis  [28, 29] .
paper_96	With the rapid development of China's transportation, the frequency of traffic accidents is also high. This not only restricted the development of China's transportation greatly, but also threatened to people's safety seriously. In particular, the accidents caused by the blind due to there are more frequent reproduction, so their traffic safety has become a big issue to solve urgently. Such warning system of obstacle avoidance of embedded electronic guide dog can effectively eliminate the impact of the traffic environment and the subjective factors of the blind, warning in advance for the travelling blind in time, effectively avoiding obstacles such as vehicles, to reduce traffic accidents caused by the their blindness. This humane technology innovation is the specific embodiment of environmental science and technology aesthetic theory in the field of scientific and technological innovation. It has a positive and promoting role to the development of transportation and blind-man welfare in China. I think that if electronic guide dogs guide them to avoid obstacles such as vehicles, the blind people are able to avoid such accidents, aren't they? The design of obstacle-avoiding early warning system of embedded electronic guide dog is an inheritance and innovation, based on the design of traffic information collecting system, which is generally used in vehicle anti-collision at present. Its operating system is thought from the perspectives of practical application, development tool, instantaneity, technical services, and price, etc. Moreover, the obstacle-avoiding early warning system of embedded electronic guide dog also selects USB protocol to transfer data, saving the collected data in the hardware after being managed. The overall design plan of obstacle-avoiding early warning system of embedded electronic guide dog mainly is the velocity and distance measurement module, the power circuit module, the LED screen show module, USBcommunication circuit module, and clock circuit module. The techniques of information collection are ultrasonic wave, laser, infrared ray, machine vision and interactive method. Information processing is mainly to analyze the collected information, usually using ARM, MCU and DSP microprocessor, etc. Therefore, in the design process of the obstacle-avoiding early warning system of embedded electronic guide dog, after the comprehensive and comparative analysis of the anti-collision warning techniques, we have compared the optimization principles of the anti-collision warning techniques, and finally adopt the approach of millimeter wave radar sensor + ARM microprocessor. The design of obstacle-avoiding early warning system of embedded electronic guide dog also adopts the generally used embedded processor. The embedded system will further compile all of its procedure codes, including the operating system code, driver's code and application code, into a whole paragraph of executable code and in inserted them into the hardware. Besides, its other main features are universal Linux API, core file less than 512KB, core+file system less than 900KB, and complete TCP/IP. In addition, the design of obstacle-avoiding early warning system of embedded electronic guide dog also makes a full use of USB. USB is a new kind of computer peripheral communication interface standard, which abandons the defects of the traditional computer series/parallel, with advantages of the reliability of data transmission, hot plug (plug and play), convenient extension, and low cost. The warning system of embedded electronic guide dog avoiding obstacles studied in the project can effectively eliminate the influence of traffic environment and blind subjective factors, send the pre-trip alarm for the blind in time, effectively avoid obstacles such vehicles, to reduce traffic accidents caused due to the blindness. This humanized technology innovation is the embodiment of environmental science and technology aesthetics theory in the field of science and technology innovation practice. This has not only a positive role in promoting the development of Chinese transport and the blind welfare, but also important practical significance and practical value for us to build a harmonious society and a beautiful China.
paper_134	Data mining, also referred to as knowledge extraction from databases, is one of the most important analytical methods for identifying the relationships between the various elements of the information collected in order to discover the useful knowledge and support of strategic decision-making and sustainable development systems in various industries. The most jobbery ways during olive oil production consist of mixing other oils such as maize, sunflower, Canola and corn into the olive oil. A set of 16 samples of olive oil, sunflower, canola and corn oil which mixed with different ratio of Authentication, were used for calibration and evaluation of developed system. Data mining takes advantage of the progress made in artificial intelligence and statistics. Both of these areas work in model identification and data classification issues and will, in effect, be directly used in data mining, and both groups are active in identifying and using neural networks and decision trees  [2] . These methods are very damaging, costly and time-consuming. Nowadays, many non-destructive methods have been investigated, which have the ability to identify various components of the quality and purity of a substance at a widespread level. Among the non-destructive methods, the dielectric method has advantages due to the relationship between the steady-state dielectric with some qualitative materials, the cost less than other methods, high speed and high efficiency in the construction of systems Different recognition of quality has attracted the attention of many researchers  [5] . If the two metal plates are placed opposite and insulated, they form a capacitor. Typically, the conductor plates of the capacitor are made of aluminum, zinc and silver, and among them, a dielectric can be placed in the air or other material. The association of these properties with many quantitative and qualitative components of the product, high speed, low cost and high efficiency of its use as a new and non-destructive technology has been considered by many researchers to measure a wide range of qualitative and quantitative characteristics of agricultural products  [8] . The latest statistics have shown that every Iranian consumes 100 grams of olive oil per year. One of the main reasons for this low consumption is the high price of this oil. This high price, in addition to having an impact on consumption, has been motivated to enter the market for profits. This high price, in addition to having an impact on consumption, has been motivated to enter the market for profits  [10] . Knowing the original oil is also not an easy task that anyone can handle. The second pertain to data mining algorithms; third part related to samples and used methods in the article. Also, processing of data and results were presented in fourth section  [11] . (2010) conducted a research on olive oil for detecting adulteration using a dielectric spectrum. They also used the partial least squares model (PLS) to detect oil falsification. From the result of this study it can be seen that the dielectric spectrum can be used to detect fake oils with different types of oils with a percentage of their mixture below 5%  [12] . The relationship between the rates of fruit dielectric constant in a frequency of 1 kHz to 10 MHz was investigated. The results of the experiment showed that the dielectric constant is strongly affected by the size and volume of the fruit and also decreases with the increase in the fruit juice, which is clearer in a frequency of 1 MHz. The results of the prediction were satisfactory and the correlation coefficient between the actual yield and the expected value of the fruit was 0.85  [13] . They used 4-volt sin voltage with a frequency of 120 MHz and a rectangular parallel capacitor, which obtained capacitive capacitors (DCs) and voltages (DVs). The results showed a significant difference between DC / DV ratio during storage period. Using this parameter and egg mass, they extracted regression models and reported that the application of this method to the egg production line and its grading based on quality properties requires more research  [14] . (2015) reported a research about egg grading using image and sensor processing. The correlation coefficients and the mean square error of the statistical indexes were 0.98 and 0.0006, respectively. 70% of the data was for network training, 15% for validation and 15% for the network testing. In order to select the best network, the number of hidden layer neurons was changed from 2 to 50  [15] . In this study, a recent study has been carried out to identify the authentication of olive oil. In this article the experiment was done by olive, sunflower, Canola and corn oil. Then different classification algorithms by MATLAB software and various techniques such as support vector regression were done and finally output dates were processed. The samples were blended with sunflower canola and corn oil, and they made up a total of 9 classes with varying percentages (from 60 to 100% in 5% increments), each of which was shed separately, according to the desired amount, of 100 centimeters inside the sensor. The device used consists of the Arduino board, ICL8083 and AD8302. Due to the high flow of data, the ch340g chip on the Arduino board is used to measure dielectric parameters as well as a device that can detect the purity of olive oil. The Arduino board is used on the Uno device, which uses the Atmega2560 microcontroller. The microcontroller is an electronic application chip that can increase the speed and efficiency of the circuit versus reducing the volume and cost of the circuit. Arduino has a variety of ports, controls, side panels, and screen connectivity, while it may be smaller, less expensive, and possibly faster than a microcomputer system. The AD8302 chipset has been used to measure the domain and fuzzy detection, as well as to measure the dielectric parameters. The AD8302 phase detection chip provides a simple method for measuring the input ratio of the domain and the phase difference of the two signals simultaneously (22). Support Vector Machine Regression (SVR) aims at finding a linear hyper plane, which fits the multidimensional input vectors to output values. One of the new ideas in identifying and categorizing patterns is a backup vector machine or SVM. In particular, SVM does not have the problem of local optimization in its training, it builds the categorizer with maximum generalization, determines its structure and topology optimally, and functions nonlinear differentiation easily with low computing using It forms the concept of an inner product in the Hilbert space. In this study, coarse function was used to regression test data. Using this technique, the amount of RMSE for the samples were equal to 3.56, 2.45, 3.05 and the amount R 2 was equal to 0.92, 96 and 94, respectively. Also, in regard to the amount of RMSE, result has shown 4.81, 2.54, and 4.38 for Olive-Sunflower, Olive-Canola and Olive-corn, respectively. After providing adulterated samples and pouring them into a capacitive sensor, output data was analyzed by MATLAB software. In this article two factors (gain voltage and phase shift voltage) were measured. It is noticed that The AD8302 measures the magnitude ratio, defined here as gain, and phase difference between two signals. In this study, Quadratic function was used to regression test data. Figure 2  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn Using this technique, the amount of RMSE for the samples were equal to 3.65, 2.46, 3.31 and the amount R 2 was equal to 0.92, 96 and 93, respectively. Comparison of the testing stage of these techniques showed coarse tree was the best prediction. Interestingly, olive-Canola oil samples predicted with high accuracy in all techniques. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud.
paper_139	Much effort made towards in investigates mechanism with identical detection algorithms, still the retrieved web documents with outmodedlink. In this proposed system, we are successfully identifying and minimize the redundant information and like link in web documents. Retrieving relevant information from web without redundancy is more challenge task nowadays where in web mining communities  [3, 4] . Utilizing customary data recovery  [5]  and information mining systems it get to the known and obscure data from the Web content. A query is searched in a web search tool to recover some significant and required data for the client, either the search query is known or unintelligible to the client, it generally to reply with relevant data rather than redundant, however we can't guarantee that the reply for the query about the significance and redundancy. Once the input query is requested, the search engine generate the document with multiple web pages along with the links, the user will be unaware about the content of the web pages, the extracted web documents contain multiple web pages either be redundant or not. The Document retrieved must follow some constraints which have less time & space requirements, based upon the criteria the extracted web document must be preprocessed, for preprocessing & information selection, need to apply some techniques such as stop word removal, Stemming of word, phrasing, normalization of tokens. Step2: Pick the minimum degree vertex 'v' in the set-up and include in the set U. Step3:While U doesn't include all vertices Step3A: Include the entire isolated vertex which is adjacent to the vertex 'v' to U. Step3B: Find the adjacent vertex 'u' to 'v' which is not in U and has maximumdegree. Update 'u' to U. Step3C: Update the value of degree for all adjacent vertices of 'u'. Apply the proposed KTMIN-JAK-MAXAM ALGORITHMto G 1 By step 1: deg(All Nodes of G 1 ) = 3 By step 2: Mark the node A as visited and put it onto the set U. By step 3: There is no isolated vertex in the given graph G 1 By step 4: 4.1 Investigate any unvisited adjacent node from A. 4.3 Now travel around the node 2, the unvisited adjacent node is from 2 as 1 and 4. 4.4 Discover the node 1, the unvisited adjacent node is from 1 as only 3. Now the set U consists of the nodes A, B, 2, 1, 3. Case II: Connected Complete Network G 4 in figure 8  Here notice that, all complete connected networks G, after applying the proposed KTMIN-JAK-MAXAM ALGORITHM to G, we get linear path of length (n-1). While getting more redundant web pages for a single search query, it's more difficult to recognize the redundant links.
paper_145	Overweight and obesity were more among urban residents compared to rural residents and they were thirty two percent more exposed to overweight and obesity. According to factor weights it was noted that the important factors for variation in the level of obesity were mainly gender variation followed by occupation, education and type of work. The prevalence of overweight and obesity has increased rapidly over the last decades especially in developed countries  [1] [2] [3] . Obesity is generally associated to a significantly higher risk of arterial hypertension, diabetes mellitus (DM), hepatic steatosis, hyperdyslipidemia and renal failure  [5, 6] . The aim of this paper was to identify the socioeconomic factors responsible for obesity and overweight among some rural and urban people of Bangladesh. The important factors for obesity and overweight were identified by factor analysis, where largest factor weight indicated the most important variables  [14, 15]  responsible for obesity. The study was based on data collected from both urban and rural people of Bangladesh. To study the variability of socioeconomic variables for diabetic and nondiabetic people, some respondents were also investigated as a control group. Thus, finally, the analysis was performed using the data of 635 diabetic patients and 265 non-diabetic people. The data were collected through a pre-designed and pretested questionnaire during the months of May and June, 2015 by some undergraduate and post graduate students of American International University-Bangladesh, most of whom were doctors and nurses, of the department of Public Health and they were associated with public health services. Data have also been collected from parents/guardians of 200 randomly selected students of different disciplines of the university, on the assumption that the respondents would be of normal group of people. But during investigation some of them were found as diabetic patients. However, from the filled-in questionnaires 356 were found in completed form and the information of these 356 respondents were included in the analysis. The latter information were provided by the diabetic patients. The information regarding blood sugar level and blood pressure level were also noted down according to the latest measurement by doctors/diagnostic centers. The variables included for factor analysis were residence of the respondents, their age, gender, marital status, religion, level of education, occupation, type of work, monthly income and smoking habit. Besides the factor analysis, the association of different socioeconomic variables with level of obesity were investigated. Significant association was decided by chi-square test with p-value< 0.05 and odd ratio was calculated for respondents who were overweight and obese compared to normal group. It was observed from the analysis that among 900 respondents 7.6 percent were underweight [  Table 1 ] and 19.1 percent of them were from rural area. Maximum (43.1) of the respondents were overweight and 20.9 percent of them were rural people. Obesity was observed among 15.3 percent people and overweight respondents were 34.0 percent. In each level of obesity the majority were from urban area. However, urban people were more exposed to overweight and obesity by thirty two percent compared to rural people [O. R.= 1.32]. They were in more risk of overweight and obesity by 51 percent compared to males [O. R.= 1.51] Obesity and severe obesity were observed almost similar among Muslims and Non-Muslims [  Table 3 ]. But the O. R.= 1.07 indicated that both the religious groups were similarly exposed to overweight and obesity. Among the investigated respondents 92.6 percent were currently married and 43.1 percent of them were normal  [Table  4 ]. Similar normal group was noted among the other group of respondents. Majority (52.9%) of the respondents were of age 50 years and above and 48.5 percent of them were normal [  Table 5 ]. The percentages of normal groups among the respondents of ages 25 -40 and 40 -50 were 35.5 and 36.6, respectively. Levels of obesity was significantly associated with levels of ages [P (χ2 ≥ 18.34) = 0.008]. But if classification of respondents was done into two groups, one group of ages <40 years and another group of ages 40 years and above, both the groups were almost similarly exposed to overweight and obesity [O. R.= 0.81]. The proportions of different levels of obesity according to professional variations were significant [P (χ2 ≥ 46.472) = 0.000]. But, all other professional groups of respondents had the similar risk of obesity compared to servicemen [O. R. = 1.04]. Significant association was noted between the level of obesity and the level of income [P(χ2 ≥ 64.994) = 0.00]. But overweight and obese respondents were 62 percent more exposed to diabetes compared to other groups of respondents [O. R.=1.62] In one study  [20] , it was reported that smoking was one of the factor to increase the level of obesity. The association between smoking habit and level of obesity was significant [P(χ2 ≥ 20.189) = 0.0.002]. The association between smoking habit and level of obesity was significant [P(χ2 ≥ 20.189) = 0.0.002] and smokers were 21 percent more exposed to overweight and obesity compared to non-smokers [O. R. = 1.21]. The analysis helps to identify the important variables to explain the variation in the data set  [15, 21] . The variables which were included in the analysis were sufficient to explain the variation as KMO = 0.633, χ2 = 256.371, p-value = 0.000. From the analytical results it was seen that all the p-values were less than 0.05 which indicated that the inclusion of variables for factor analysis were justified. From the results of the communality it could be concluded that the variable marital status was more important followed by gender and education. From the results of the communality of the variables it could be concluded that 85.6 percent variation of the variable marital status would be explained by the extracted factors  [22] . These three variables were more important for the variation in the level of obesity. From the factor analysis it was noted that the coefficients of the variables gender was highest followed by occupation, education and type of work. These coefficients were observed from the first component. This information were noted from the characteristic roots of the correlation matrix of the variables, where the roots were 2.573, 1.616, 1.086, 1.053, and 1.003. The second component showed that the most important variables to explain the variation in the data of obesity were residence followed by religion and income. The analysis presented here was done from the data collected from 635 diabetic patients and 265 control group of respondents. The prevalence of overweight and obesity were significantly associated with age, religion, education, occupation, marital status, income and smoking habit. Factor analysis also indicated that some of the socioeconomic variables were responsible for increased rates of overweight and obesity. The percentages of these four groups of respondents were 7.6, 43.1.34.0 and 15.3, respectively. The obesity is one of the risk factor of prevalence of non-communicable diseases  [NCD]  and it enhances the arterial hypertension, diabetes renal failure etc. Among the respondents 84.1 percent were of the age 40 years and above and among them 42.8 percent were overweight and obese. The factor analysis showed that sex, occupation, education and type of work were more important to explain the variation in the level of obesity.
paper_212	Mathematical modeling of disease has been an indispensable tool in accounting for disease transmission dynamics as well as disease spread. The models developed however faced considerable challenges ranging from inaccurate representation of natural data for deterministic models, to methods of forecasting such as statistical extrapolation which assumes that current conditions will prevail which is not always the case. Despite the spread of HIV/AIDS having been explored widely, not much literature is available on the Gillespie Algorithm based SIR model. We conclude that the simulated model reflects reality. On June 5, 1981 a mysterious disease was recognized among MSM in the USA. In the same year, it was identified that it attacks the immune system of the host, incapacitating them to heal subsequently leading to death. It was characterized by its etiological agent HIV in 1983,  [1] . A serological test, was then made available. In 1984, Robert Gallo discovered that HIV was responsible for AIDS. This virus, being highly transmittable is analyzed and capturing how it is transmitted is crucial in attempting to model the disease. In 1984, several HIV and AIDS cases were documented in Kenya. The following year, 26 new cases of HIV were recorded from sex workers and the NAC was established. The NACC was established under Section 3 of the State Corporations Act Cap 446 through the National AIDS Control Council Order, 1999 published vide Legal Notice No. Among other countries in the world, Kenya is among the twenty two that account for 90% of expectant women living with HIV. This accounts for 4% of new pediatric infections worldwide. Among the expectant women there are 13,000 new HIV infections among children. The number of those that died account for 7% of the global total. Sex workers have the highest percentage of HIV prevalence at 29.3% according to the Kenya HIV estimates report-National Aids Control Council. This study employs mathematical modeling tools in the transmission probability of HIV and analyses done on how the cumulative number of infected individuals responds as well as the AIDS death probability and how the cumulative cases of removed individuals responds to this probability. The following reviews consider models developed for HIV/AIDS data that either differed too greatly with other model estimates or still fail even with developments on the model. In 2010 several authors came up with a model to predict HIV transmission in China in 2002  [3] . In this model, there were no forms of intervention. The transmission parameter was held constant for all stages of HIV. This model approximated that there would be 6000000 cases of HIV and 400000 cases of AIDS in China if there were no forms of interventions implemented. In 2007 the government of China alongside UNAIDS made an estimate of 700000 cases of HIV and 85000 cases of AIDS in China at the time, which is much lower that the estimates made by Liu  [4] . The number of HIV infections in 2010 was predicted to approximately 1000000. The group most affected would be the 31-40 years group. The group comprising of the largest individuals predicted to be living with HIV/AIDS was approximately 650000. In several concentrated epidemics, HIV prevalence estimates do not match reported cases and mortality estimates do not match reported deaths, even after adjusting. Even with the 2013 updates of Spectrum where adjustments were made in the parameter values empirically to improve the fit to program data, the estimates given by Spectrum still differed with data available. The parameter depended on the varying population size N. This meant that both the population size varied as well as the transmission/contact rate. Despite the fact that a lot of research has been done on modeling disease trajectory, not much literature is available on the use of Gillespie based SIR models to simulate the trajectory of a disease in the population. In their works they build a disease spread prediction model based on the SIR model and applied parameter values to a stochastic model based on Gillespie's algorithm. This is applied to data and the conclusion was that the model well explains the process of the spread of the disease in the population  [10] . Infection-transmission deterministic models are based on the characteristics of population growth, disease occurrence, and spread within a population. By contrast, in a deterministic process, there is no randomness  [12] . This paper will contribute and build on to the existing literature on modeling disease dynamics in the population with the model tested on HIV/AIDS data 1985-2018 to investigate if the simulated values would reflect results that are close to reality. This paper will help bridge the gap between conceptual epidemiological models and its simulated version by providing a developed version of an SIR model that solves one inherent problem that deterministic models do not reflect the natural data. The Classic SIR model The Kermack-McKendrick theory illustrates individuals grouped as susceptible and removeds only  [13] . The transmission and infection rates were considered to be variant. They set the transmission and infection rates as invariant for all ages and this allowed the inclusion of an infectives class. This transformed the theory to the basic SIR model such that when demographics were included becomes (4) (5)  (6)  where N denotes the total host population. denotes the birth rate and death rate denotes the infection rate denotes the recovery rate t denotes time point Model development The Gillespie algorithm was used to simulate a statistically correct trajectory given initial SIR conditions. The model explored how altering transmission dynamics affected the model as a whole. The reliability of the simulated values would set the precedent for the valued to be predicted based on the model is also explored. The stochastic SIR model. The assumption is that the population is finite and is sub-divided into categories of finite discrete compartments. , , This Gillespie's algorithm based stochastic SIR model generates a statistically correct trajectory from the initial vector as , , where i=s, i, r S+I+R=N i denotes the population size of the state at time t denotes a function characterized by two quantities as a state change vector and a propensity function. Assuming that the resulting state is . A propensity function is the probability of one event occurring in the time interval ! Continuous-time Markov chains are the basic tool for building discrete population epidemic models. The Markov property lets us specify a model by giving the transition probabilities-defined as rates-on a small interval between the compartments. Considering the fact that the propensity functions require to be in probability form, we explore this assumption further by defining and interpreting it. This implies the probabilities are individual therefore discrete. Continuoustime Markov chains are the basic tool for building discrete population epidemic models. The Markov property lets us specify a model by giving the transition probabilities-defined as rates-on a small interval between the compartments. The transition probabilities assigned are defined on an open interval (t, t + ), such that the probability an individual moves from the susceptible compartment to the infectives compartment is  [ 1 ] . The wait times between events can either assume an exponentially distributed wait time or the rate of flow between compartments can assume any of the following distributions depending on the results. In order to assess how the simulated data performs against natural data, a modified chi-square test was used. The data was obtained from NACC for HIV/AIDS cases. Considering the hypotheses, 4 = If the mean and variance of the simulated and natural data are equal, the simulated mean does not fit the data. 4 5 = If the mean and variance of the simulated and natural data are not equal, the simulated data fits the natural data. A modified chi-square test for simulation models was used to see how well the simulated data fit the natural data  [18] . A stochastic SIR model was simulated with a mean step size of 0.006336446. 537 tau steps were made in the model. Variables in the model were S = 3507162, I = 45820, R = 4597, parameters in the model are crude birth rate of 0.06, non-AIDS death rate of 0.025, transition rate of 0.1 and AIDS death rate of 0.48. Curves produced are illustrated below. where C represents the variance of D The calculated value found was 64.958. The critical value was 47.4. Since the calculated value is greater than the critical value, the decision rule is to reject the null hypothesis. After simulating, values were produced by the algorithm for each time step. Recommendation for research would be to explore other variations of the SIR model such as SI, SEIR under Gillespie's algorithm. The SIR model as well as Gillespie algorithm could continue to be applied other areas such as viral marketing and behavioural science as has already been done successfully.
paper_214	The paper presents the usage of databases that store business data into a warehouse star model that permits to create queries using SQL language and business intelligence tools. This kind of model allow to the decision maker to create complex reports and graphs based on the columns from the dimension tables and measures from fact tables, that can be the base for creating alternatives and scenarious acording to the economical indicators. The business environments require analyses on large amount of data, big data and necessitate advanced tools to query through numerous criterias and also to create different realistic scenarious that allow choosing one option, so the business manager can use the right tool to gain economic advantage. The optimal solution or satisfactory is obtained using either algorithms or formulas within optimization models, either by experiencing various possible alternatives in a process simulation. Optimizing the use of a certain class of models for finding the best solutions. Among the methods most commonly used are decision analysis and mathematical programming  [1, 2] . It is the only method that can be applied to unstructured problems. Simulation becomes a technical coordination of procedures using the computer. The solution offered is one spot that has no counterpart in the real system. always taking into account the unique factors of a problem specific results can not be transferred to other issues; results are difficult to interpret, being dependent on random factors; no matter how powerful your computer is, the optimal solution is difficult to obtain a model that has many equations and a significant number of parameters. These limitations have led to the use of simulation only when the interactions between components of the system are complex, when factors random have a significant and requires a large number of observations on the behavior data, the problem can not be solved by an algorithm or experiments direct. The simulation can be conventionally divided into the following steps: the problem and research purpose; model development and data collection system; model verification and validation; describing experiments on the computer; simulation execution and achieving results; analyze the simulation results. Quantitative mathematical models are embedded in base models, managed by subsystem a management model that requires separate users from the physical aspects of data processing and storage that extract, create, delete and modify models. Decision making process, conducted with the help of tools, methods and techniques, conduct to the scenarios constructed according to a definite objective. The activity of the coordinations of inputs is done in most cases with specialized software systems that create analytical databases or modeling languages. To be more precise, a specific problem highlighted in a model is called one of the most used tools in the decision making simulation. The next logical of optimization and forecasting, simulation assists with the running complex patterns, resulting variables whose analysis highlights the value adopated lead to a decision. The outputs from the process of decision making, represented by analytical indicators reflecting the performance of the system analyzed variables results the evaluation criteria or implementation plans of the decisions  [2, 3] . Evaluation of search results depends on the method of presenting results and depends on the facilities of component dialog with users that provide inputs. The decident system uses a dialog interface with the key users of the company, enabling connectivity and communication between networks with different topologies and areas. After analyzing the results achieved and the objectives of their reporting, signaled differences and after identify problems it was reveal the need to take action. Solving the result of communication between all stakeholders, sharing the general manager responsibilities both at decisionmaking levels and the corresponding subproblems defined. For example, after the first phase, the scope may relate to excessive spending decisions of a functional department, inventories too high or adoption of a draft research and evaluation on the introduction of computers  [3, 4] . In the model design phase defines a model for decision shall be tested and validated under real system. Modeling takes expression of reality by means of abstract entities possessing quantitative and qualitative attributes. From model design and solution choice there is a strict demarcation, certain activities may be conducted during both phases, and return of election phase in phase. The assessment in turn depends on the search method. For complex problems, solving is carried progressing from one situation to another, until a final statement, which is the solution. It receives relevant and substantiated elements on activity in the real system and builds models for solving future decision making on the basis of current assessments.Of the foregoing that a decision support system provides a filtering of information provided to decision makers and indicates certain restrictions. The database is built to meet the information requirements of the system and is an interrelated database operated by one or more users, one or more applications. In current systems, the company's intranet, are increasingly present data accessible through web browsers and multimedia items such as maps, images, sounds. The data source, internal or external, data is extracted and managed by a management database. The management of the database depends on the organization of data. In most cases there is SGBS transactional relational data system and a management database for multidimensional data warehouses created. The data dictionary is a catalog of all data from the database. The most common data dictionary used in the first phase of decision making is data mining to identify their problems and opportunities. The subsystem management model consists of the following components: base models, the management models, dictionary and processor execution models and integration patterns Base models contain the set of models that make it possible to analyze the facts and the choice of options in terms required by the user. It is the component that differentiates interactive decision support systems to other systems. Database management system allows creating new models models using programming languages, update and modify existing models, establish interdependencies between models. In building a data warehouse is based on the analysis of data. Conceptual models are multidimensional and designed to organize data necessary decision-making process on issues. The star is the type of aggregation criteria when codes are explained in type tables list. Using data from lists, star type structure enables higher levels of aggregation on the initial size  [4, 6] . The databases permits flexibility in creating queries by choosing columns from dimension tables and measures from the fact table and so the decision maker may choose what economical indicators is useful to analyse and also permits to create different types of graphs such as columns, pies, scatter points or webs. It is very important to store data into a warehouse model and then to create future strategies that can help a company to expand or to grow the profit in a realistic way based on complex analyses and alternatives.
paper_216	Amongst the non-parametric approaches used to improve the accuracy of the model in this study is the Wavelet-based pricing model. The MSE and RMSE of Wavelet model is 0.208546 and 0.456669 respectively which is much lower than that of Black-Scholes model and therefore in conclusion, Wavelet model outperforms the other model. The Black Scholes model has been developed under some assumptions such as the risk-free rate and volatility of the returns are known and constant, the returns of the underlying security are normally distributed, markets are efficient (market movements cannot be predicted). In the real market, the asset returns follow a leptokurtic distribution which is in contrast to the Black-Scholes model where returns are assumed to be log-normally distributed. Variables used in this model are observable, for example the time to expiration, exercise price and the closing price except the volatility which is not directly observable. According to the literature, this model is the latest theoretical contribution to the option pricing and it is better at capturing the volatility smiles which is as a result of the Black-Scholes' assumption on volatility constant. The implied risk neutral moment-generating functions (MGF) is estimated by the wavelet method. This method only requires a reasonable amount of data (different strikes) and is very efficient, unlike other non-parametric methods which requires large amount of data. This research therefore compared the performance of Wavelet based pricing model and Black-Scholes model in the valuation of a European option. Their formula can be used to obtain the following parameters; the spot price, the exercise price, interest rate, time to maturity and volatility. This model has been discussed extensively in order to improve its pricing biases and to impose more practical assumptions. The approximation of the implied risk-neutral MGF of the underlying security (asset) returns was focused by developing a non -parametric option pricing model called Wavelet model  [4] . In comparison between the risk-neutral MGF and the implied risk-neutral PDF, the risk-neutral MGF has a number of advantages even though between them, there is a one to one relationship. The following are some examples of these advantages; when there is presence of jumps in the underlying process MGF is more tractable, the option's obtained MGF is a continuous function, statistical moments of the asset distributions can be obtained using MGF and the risk neutral MGF can be used to estimate the out of sample options that has maturity dates which are different. The second use of wavelet method is that, they can be used to de-noise raw data. Also, wavelets can be used to improve analysis of volatility since they are a preprocessing denoising tool  [10] . The de-noising ability of wavelets was also recognized in  [7] . Evidence have been provided to support that wavelet based local linear approximation  [11] . Another example is the application of the wavelets in nonparametrically estimation of a diffusion function  [15] . formula_0 From the Black-Scholes formula above, we see that the strike price (K), risk free interest rate (r), time to maturity (T), the volatility (") and the current price of the stock (S0) are required. Based on some general assumptions like independent and identically distribution (iid) for asset returns and that the moment generating function is defined well, the wavelet based pricing model can be expressed as follows; formula_1 Where, t is the current time, is the underlying asset price at time t, # $ is the time t price for a European call option written on asset, K is the strike price and T is the future maturity date. The underlying asset dynamics and investor expectation in option prices is captured by the MGF Θ !$ + of the logarithmic returns /0 7 . When T -t = 1, Θ (v) is the risk-neutral MGF for the rate of return per unit of time. This needs to be approximated by wavelets. In order to approximate the implied MGF using the wavelet method, one has to choose a particular wavelet from a large family of wavelets. From the wavelet literature, researches have shown that there is no best wavelet for a particular application. And therefore, we choose a wavelet that can achieve a reasonable accuracy with wavelet terms being very minimal. To estimate the risk neutral MGF of the return per unit time, we use the Franklin hat function f (t) expressed as; The data sets consist of simulated stock prices, the exercise price, the time to matureness, interest rate and volatility. The focus on this study was on pricing of a European Call option using two models, one parametric model (BS Model) and one non-parametric models (Wavelet Pricing Model). In order to estimate the risk neutral Moment Generating Function of the underlying security from option prices, we applied the methodology of wavelet. Therefore, one of the most significant input of this study is that the wavelet -based pricing model is an alternative model for pricing options and other derivatives on the same underlying asset with varying times to maturity and different strike values. The superiority of the wavelet method comes from the ability of the wavelets to estimate the risk neutral MGF. This is evident from the values of the RMSE and MSE, whereby MSE of wavelet model is lower than that of Black-Scholes model.
paper_219	There is also a need to assess the quality of data sharing across the enterprise network. When discussing the theoretical, it is easy to dismiss the challenges of limited resources to implement new policies such as schedule, budget, manpower, etc. Successful policy implementation amongst other factors requires system analysis methods that assist acquisition agencies in targeting limited developmental resources to areas of greatest impact to the overall mission objectives. This paper introduces such a method in support of achieving maximum data quality for military enterprise networks: a quantitative mechanism by which the value of different net-ready implementation options can be evaluated and graded. Importantly, a model for valuing enterprise data quality is introduced to bridge the gap between measure of technical performance and operational benefit. Section 2 of this paper provides a brief overview of the origins of net-centric and net-ready policies. The attributes in the new model represents data quality of the enterprise system within three contexts: net-centric measures, traditional network quality of service, and cyber-security. Admiral Jay L. Johnson, Chief of Naval Operations, stated in 1997 that the military is undergoing "a fundamental shift from what we call platform-centric warfare to what we call network-centric warfare"  [1] . In the years following, the Chairman of the Joint Chiefs of Staff Instruction (CJSCI) 6212.01 was released that set-forth the procedures for development and certification of a Net-Ready (NR) Key Performance Parameter (KPP); the NR KPP process later being subsumed into the Joint Capabilities Integration and Development System (JCIDS) process  [3]    [7] . Making data accessible requires providing authorized users the ability to view, transfer, download, or otherwise consume a data asset once discovered  [6] . Making data understandable requires alignment of terminology, data protocols, data formats, and data meaning between produced and consumer  [6] . Untrusted data can introduce error, uncertainty, and delay into the military decision process  [8] . To satisfy the attribute of entered and managed on the network, the IT connections to external networks required to perform net-centric operational tasks must be identified  [3] . Each net-centric information exchange defined MOPs that are measurable and each information exchange must also identify how the four criteria for net-centric data sharing (visible, accessible, understandable, and trusted) are satisfied for authorized consumers across the enterprise  [3] . Evaluation of the data sharing enterprise requires a holistic view that considers the net-centric attributes of the data simultaneously with the quality of service for the data network. The combined consideration for each of these areas yields a newly defined model for the data sharing enterprise comprised of three equally important attributes: data relevance, quality of data at source, and quality of service for the enterprise network. The most commonly used subjective rating in standards and in conjunction with QoE is the mean opinion score (MOS) where score of 1 is bad, 2 is poor, 3 is fair, 4 is good, and 5 is excellent. Where an accurate model exists for QoE as a function of that attribute's objective measures then it's used instead of sampling users' opinions. For most data the paper assumes that a model is inadequate and sampling the users' opinion is required. Usually the MOS is formed from arithmetic mean of a population of user opinions. But the arithmetic mean assumes that all user groups are of equal size which could lead to biased estimates of the MOS. The survey is not just restricted to a complete enterprise system but can be performed in the early design phases of prototypes and help analyze operational performance of an enterprise attribute as a function of its objective measures, i.e. A novel application to networks of a common survey practice is proposed to use the Horvitz-Thompson (HT) estimator  [10]  for determining the value of the total score of the population of size N. HT is commonly used because of its versatility as an unbiased estimator of the total for a sampling/sample with or without replacement. The Horvitz Thompson estimator for stratified sampling for the total Y is formula_1 where each independent random sample i, is denoted by y i , the probability of inclusion in the sample is denoted by p i . Where it is not easy to design a stratified sampling plan a stratification post collection of n samples without replacement can be constructed. The sample is organized into their various strata with the number of elements in each total number of strata k, in the total population N. The HT estimator for stratification becomes formula_2 and estimate of the stratified mean is formula_3 where m k = ∑ i in s k y i /n is the arithmetic mean of strata k. With each attribute having its MOS value using the stratified estimate in (3). But cost and time to use panels of experts to assess MOS has resulted in seeking alternative approaches. Full reference PESQ is formed by taking the speech codec output and comparing it with the original signal inputted into the codec  [13] . To produce a subjective rating MOS, or QoE, the ITU-T P.862.1  [13]  is used to map raw PESQ to the final rating. There are three levels of reference used in determining the models for estimating the subjecting ratings: full reference (undistorted service is available for comparison with distorted outcome), partial (or reduced) reference, and no reference. In  [15]  equations are formulated for quality in terms of interpretability and MOS; also with or without references. But just for completeness a brief explanation of QoS is given here. data at the source as well as objective measurements of that data on the network that can be used to correlate with the end user's QoE. Examples include understanding relationships between objective measures of QoS like jitter, throughput, and latency to be able to control the QoE. It is expected that models of QoE can be accurately built as a function of objective measures from network, applications, environment, and terminals. These models of QoE based on QoS objective measures are often referred to as "QoE\QoS" correlators  [21]  and  [12] . For both models  [21]  showed QoE had an exponential model in terms of their QoS objective metric(s). A simplistic rejection of non-relevant data is insufficient given the negative effect that excess amounts of data can have on human decision makers, even when the data in the network is restricted to only relevant data  [23] . The use of the QoE estimate is proposed to provide a subjective rating of the overall relevance of data shared on the enterprise. One way to analyze these properties and their effect on QoE is to form each stratum of (3) in such a way that we have homogenous groups of consumers with similar intrinsic DR properties. Tagged data relevance is a function of the quality of the taxonomy, the process performance, and the degree of understanding the producer has of the potential consumers. The measure of discovered data relevance is an indication of how well the enterprise system enables a consumer to differentiate the data product offerings accessible via the network according to the level of relevance and value to the mission objectives. To support a high level of discovered relevance, The taxonomy available to the consumer must be sufficient to explicitly discriminate the desired data features from the undesired. A poorly performing taxonomy would be one that prompts extraneous definition of detail or that includes terminology with such subtle variation as to lack the mutual exclusivity necessary to select between one term or the other. The processing element of discovered relevance is a measure of how well the search methods of the system match the descriptive words of the consumer (i.e. Just as the performance of tagged relevance is improved by an understanding of the consumer by the producer, discovered relevance benefits from awareness that the consumer has of the full range of data offerings of the enterprise and how they may be described. This is why an overall QoE value for data relevance is important as it indicates the relative success of the discovery process across multiple groups of users at finding data with intrinsic relevancy suitable to meet their needs. We explain here our method to evaluate the overall quality of data of the enterprise based on a minimax decision criterion and its connection with minimax game theory. In game theory, the minimax value of a player is the smallest value that other players can force without knowing the player's actions. Formal minimax definition  [24]  is formula_4 where a(i) denotes the actions of the i-th player of n players, a(-i) is actions of all other players except the i-th, and v i is the value function of player i. Consider a simple example in  Table 1  to illustrate the minimax in a game theory context. Rows in  Table 1  represent actions of player one and columns in  Table I  represent actions of player two. Going through each action of Player one with knowledge of player two actions L, R we have maximum payoff for player one for U of 5, for D of 5, for N of 4, making a minimax pay-off action of player one of N. Player two takes each action along columns for action L maximum payoff is 4 and 3 for action R, resulting in minimax action for Player two of R with payoff of 3. Thus, the minimax strategy is Player one move of N and Player two move of R with a payoff vector (4, 3). The minimax strategy in game theory inspired the decision theory approach of Abraham Wald's minimax model  [25] . The minimax model of Wald produces a decision with an outcome (payoff value) which is the worst chosen amongst the best outcomes of all decisions. The formal definition of Wald minimax model is  And for QoS the objective measures could be latency, packet loss, jitter, and sequence of packets. percentage of spatial and temporal coverage between consumer desired and actual data, and percentage of ontology alignment user and producer. formula_5 Thus, for each d in D, starting with d=QDS, a state vector s max (d) is found that produces max s in S(d) f d (s) where f d (s) is a discrete function formed by a predictive model for QoE or in the absence of a model formed from direct user sampled MOS value calculated using (3), i.e. 3 , as formula_6 In  Fig. This paper introduced a framework to determine data quality on enterprise networks for DoD net-centric and net-ready initiatives. The framework's data quality model consists of three attributes: quality of data at the source, data relevance, and network QoS. The paper described the data quality using minimax decisions based on users' survey ratings for a given enterprise configuration. The implemented strategy provides a tool for decision makers to prioritize data and manage their resources without comprising any part of the data sharing system. The paper notes the definition of the relationships between attribute's objective measures and the final quality of experience. Thus, support is indicated for further research in the development of objective measures using the definition of data relevance elements presented and to determine models for predicting QoE ratings as a function of these objective measures.
paper_241	So the use of transformers protection in electric power systems is very crucial and critical as they are required for efficient transportation of electricity to consumers for long term. This paper is concerned with the differential current protection scheme of transformer using a differential relay technique with arduino Uno microcontroller as a differential relay responsible for comparing the differential current values and sends trip signal to the relay (acting as circuit breaker) to open the circuit when there is fault in the protected zone as a result of imbalance in the differential current values. The demand of electricity in our modern day as led to advancement in electrical power systems, which is reflected in the development of all the power system device generators, different sizes of transformers, transmission lines and the protection equipment. Due to advancement in technology and daily use of electrical devices by industries, organizations and individuals, there is an increase in electricity demand which most likely results systems overload, reducing its efficiency and can cause damage to the transformer  [1] . Typically, Power systems are built to allow continuous generation, transmission and consumption of energy. The system is capable of sustaining a variety of environmental and operating impacts that resemble normal operating conditions which may include lightning striking the transmission lines, excessive loading, deterioration or breakdown of the equipment insulation resulting to power systems experiencing occasional faults such as phase to phase or phase to ground faults, over current, over voltage or even temperature based faults. The Differential current protection scheme is based on the principle that the input power to the transformer under normal condition is equal to the output power and is concerned with having a differential or balanced current between the primary and secondary side which would be digitally displayed  [2] . At all times, the Arduino senses the condition of the transformer. [4] Ochieng designed a microcontroller based power transformer protection system which uses a current sensor as the interfacing instrument between the current transformer and the pic16f690 microcontroller. This paper is concern with the implementation of digital differential current protection of a single phase transformer using Arduino Uno microcontroller as a mechanism of differential relay. The fundamental idea of the differential current protection scheme is to provide protection to the transformer if any fault occurs in the protected zone that will cause an imbalance in the differential currents. When there is an occurrence of internal fault an imbalanced or non-zero differential current will flow through the operating coil of the differential relay which would be substituted with a microcontroller that sends a trip signal to the relays needed to open the circuit. The magnetizing inrush current is a phenomenon that occurs during brief initial state of energization of the transformer even when the secondary side has no load connected to it and has its current a lot higher than the rated current  [6] [7] [8] . The power transformer to be protected is connected to the main AC supply and delivered power (P=IV) to the output terminal. The power transformer primary and secondary currents are measured by the current transformers, which are to be protected by the Arduino and the circuit breaker. The current transformers are each connected to the primary and secondary side of the step-up transformer and are used to reduce the currents to a measureable value that can be sensed by the current sensors. A bridge rectifier is used to convert the alternating current from the secondary side of the step-down transformer to direct current for use by the microcontroller. The differential protection of a transformer is implemented using Arduino Uno microcontroller as a decision making device that sends a trip signal to the relay (acting as circuit breaker) whenever there is faults (internal or external faults).
paper_251	In this paper we propose a concept of multi agent based batch scheduling and monitoring in cloud computing environment, where the number of agent are more than or equal to two with reducing the complexity of accessing and responding time. Heterogeneity in the computational requirement, dynamic choice and infrequent usages types of resources of the users in modern era has main challenge for service provider (application developer and hardware manufacturer). Various devices (Computing) and application has been developed and developing to fulfill the common users need. Hence demand of users is heterogeneous in nature so that varieties of application (hardware & software) have been developed to achieve the highest user satisfaction. Hence the utility types of computing paradigm will play an import role. Modern era is reflection of human creative thinking and application of optimize solution for the problems mapped and simulated into the machines using technological skills and advancement on it. Hence agent based solution has been proposed to meet the requirement of the modern cloud computing with pace of dynamic provision to insure shrink in shrink out (elasticity) of the cloud service provider to achieve highest scalability and reliability in extent of the maximum availability of the service to the requisites. Integration of the agent in the propose system provides the cost effective and reliable with dynamic pace, solution for efficient scheduling (elasticity of the resource and services) and proposer monitoring of the cloud computing systems. Codenvy -To develop an / are application i.e. Cloudbees -To deploy and test our SaaS application onto the cloud, propose system needs a platform i.e. For this Cloudbees service provider has been integrated onto the developed SaaS application. Propose system has surveyed and identified the problem domain that must be addressed in context of the cloud computing and consequently present the idea of agent integration. Our main research work is to enhance the agent based model for SaaS delivery in the cloud as depicted in the  [1]  and  [2] . To Evaluate and delivered the cloud computing services (SaaS) using agent (for better and fast delivery) using public cloud such as "New Relic and cloud bees". Deploying a web services under SaaS paradigm and evaluate the effectiveness of the web application in the cloud environment with the help of agent. Cloud bees service integrating a SaaS deployment on it and delivering through agent. Model for Proposed Work Our proposed work is to schedule and monitor cloud SaaS application onto the cloud and evaluate the performance of the same using proposed agent based. Resources: Algorithm for Scheduling (influenced from Aneka) developed onto the Cloudbees Proposed Algorithm Step1: Register user login Step2: Sign in the user login Step3: Select multi monitoring agent in cloud environment (for each subc_request with QoS constraints) Step 4: Mapping of resources between cloud client, cloud agent and cloud service providers. Following public cloud service providers or tools (platform) and data set has been used for evaluating the effectiveness of the proposed monitoring and provision approach. For evaluating the performance of the proposed agent system, the obtained results have been compared with  [2]  in which author has proposed "MABOCCF" the realization of the federation of different cloud (cloud interoperability) using agent. Author has choose two matrices to evaluate the performance of the it's proposed MABOCCF techniqueaverage user satisfaction and another one is average utilization ratio which has been derived from following fundamental (base) matrices 1. Number of tasks submitted at instant i (Ni) 2. Scalability Author has compared the outcome of their experiment with Non -MABOCCF (NMABOCCF) technique. All the matrices of the performance checking has been same meaning as our proposed system generated like 1. Response time is same as to average utilization ratio in addition to CPU usages. Average user satisfaction is same as to availability and scalability of the proposed system. Table 3  shows the user satisfaction of the deployed SaaS obtained results and has been compared with existing malty agent based method  Rather than performing analytical calculation, proposed system has been compared with the author's MABOCCF and NMABOCCF method. Hence proposed system performs better than  [2]  and any other methods as shown in figure 3. for their granting/releasing) and their exact monitoring in the cloud in context of public cloud computing service provider such as Cloudbees. Analytical analysis is to collect statistics to check the required number of resources needs or used and provides dynamic indication to better elasticity achievement. Proposed agent based solution for guaranteed better elasticity and their efficient monitoring of the resources in the cloud which helps to gather analytical statistics of the resources currently held and will be used such a memory, number of instances and CPU. For evaluation of the propose agent based method data set (jsp) has been developed using jsp web pages and deployed onto the cloud evaluating the elasticity and its monitoring. Proposed agent based methods obtained result has been found satisfactory and performs better than existing available solution. Following few areas has been chosen as future work as derivative of the proposed multi agent based solution where the current work can be taken further.
paper_272	Moreover the switching process associated with power electronics offers many modern benefits as a static switching system when joints together by using a snubber circuits for damping chopping current and transient over voltages or other facilities  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] . In addition the inside electroplates some characteristics can be described as followings: Non-sustained disruptive discharge voltages In general, the above characteristics are not involving associated with only vacuum switchgear itself a lone, but when connects the switchgear with power distribution transforms or synchronous motors, there are also many negative interactive processes for instance hysteresis currents, eddy currents and high voltage capacitive inside each transformer (Parasites electrostatics charged) feedback to the interrupter process. A more exact name would be metal vapor arc inside vacuum electroplate. J=2*10 6 (A/m 2 ) Joule heating as thermionic emission Where Ø is the thermionic work function. K is Boltzmann's constant (1.37x10-23 Joules/K) The constant A is approximately 6x10 5 for most metals According to the Harris model and classification who was classified "the positive ions initiated from ionization process in an Anode plate and generates joule heating to a cathode plate surface during switching process"; i. Ionization zone ii. The total above processing time was measured by Harris approximately (20-250µs)  [15] . Current chopping refers to the prospective over voltage events which can result with certain types of inductive load (power transformer) due to the premature suppression of the power frequency current before normal current zero in the vacuum interrupter. Virtual current chopping involves the load current in the other two phases being forced to zero by superimposed highfrequency re ignition coupled current. Restrike overvoltage due to the multiple re-ignition of circuit breaker when a switching interrupter process is initiated before current zero. Since the high frequency characteristics of the power system equipment are depend on stray capacitances that mentioned above and inductances also and they in turn depend on physical dimensions, equipment layout, materials, cable lengths, all these can be represented as follows  [17] ; The figure represents TRV & Re-ignitions computed in the system when switching off power transformer load that the transient voltage approximately (350µs-450 µs) and shows very high frequency sub transients both going in the negative direction of the voltage -Left side. Normally our analysis for the switching process basically on parallel RLC circuit as following; L= Indicative load of stator winding coils C= parallel parasite `s capacitors -see the introduction R= evaluating resistance that can be damping oscillating. Where η =0.5, the sine function changes from a circular to a hyperbolic function. By calculating the inductor current in parallel RLC circuit under conditions of a subsidence transient, but have a far wider application. However to gain familiarity with these curves, consider a specific example where the inductor current is required in a circuit in which the components have the following values: R=10 5 Ω. L =5 Henrys, C = 2X10 -8 F. These values are typical of unloaded transformer, where R represents the equivalent loss resistance. Suppose Vo =13.8√2 KV So. Zo = √L/C = 5X104 ohms η. The curve labelled η=2 in  figure 5  gives the shape of the current we are looking for. The ionization electroplates zone will be evaluated between (20us -250 us) respectively. The other processing steps as followings: a) Chopping current b) Restrikes voltages c) Prestrikes voltages d) Multi reignitions e) Voltage escalation f) Post-arc currents g) NSDD (non-sustained disruptive discharges) It is often misunderstood how all these phenomena are created and what are their real causes.
paper_294	The cultural heritage of any people can be redressed through the preserved cultural values tangibly stored and preserved in media and forms retrievable and usable. Audiovisual heritage are quite essential that the public library system must not afford to elude its storage not only for users but also for posterity. Cassava has played an important role as a staple crop in the feeding of the Tiv people. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. This study therefore, looks at the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. The study sampled 377 out of the population of 20,000. The snowball sampling technique was used in selecting the subjects for the study. The study found that the Tiv people had various strategies they adopted in managing postharvest losses of cassava. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. It was therefore, recommended that concerned policy makers should make policies that will allow public libraries to acquire store and preserve audiovisual materials on various indigenous knowledge for development to ensue; and for posterity; and that governments and other stakeholders should ensure that the public library sector is adequately funded and manpower is adequately provided as well. The cultural heritage of any people can be redressed through the preserved cultural values tangibly stored and preserved in media and forms retrievable and usable. The photographs people take, videos they shoot, speeches and music they record, capture in bits, every moment of their life, culture, event and times that inevitably speak volumes of their history. Abrigo and Abrigo (2010)  assert that, these media are jealously guarded and relayed, shown and played back for the younger generations. It is a legacy that people would want to impart to their grandchildren, so that the next generation would have the opportunity to understand their heritage. Audiovisual heritage are quite essential that the public library system must not afford to elude its storage not only for users but also for posterity. Cassava has played an important role as a staple crop in the feeding of the Tiv people. The Tiv had different management strategies of postharvest losses of cassava (Akom or Alogo) that includes dried cassava chips (kpor), imyorun or imorun (gari) processing, fermented cassava (akpu) and production of base (mtuhem) among others, which were potent managing postharvest losses of cassava. Public libraries are supposed to be conscious of ensuring that these postharvest losses management strategies are captured, acquired stored in audiovisual format. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. Apparently, if this is allowed to continue, the consequences cannot be foretold in the near future. The study intends to investigate the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. Specifically, the study sought to: 1. What are the Tiv management strategies for postharvest losses of cassava? What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? Public library is a library that is established and managed with public funds. Public libraries are established wholly or partly from public funds. They are not restricted to any class of persons in the community but freely available to all. They are the venue through which the overall information resources are made freely available to all (Edoka, 2000; Assoh, 2011; Shidi, Aju & Ashaver, 2014). Apparently, in order to achieve this mission, public libraries preserve a variety of information materials including print materials such as books, monographs, serials and periodicals; and non-print materials such as audio or audiovisual materials such as cassettes, microfiche and films among others. Public libraries anywhere they are established, is for the purpose of development. Their collection forms the body (theory) of knowledge from which the young members of the society are taught from. Quite so, public library's primary role in the society is to collect information sources of diverse kinds among others. This is their most crucial function of all. This is in line with Agber and Mngutyô (2013); Agber, Ugbagir, Mngutyo and Amaakaven (2014) who reiterated that for developing countries such as Nigeria, audio and audio-visual media are appropriate information formats for capturing information for the people. Benue State like other African Regions has farmers who cannot read nor write but who need information to develop their agricultural enclaves. The term cassava is most likely derived from the Arawak word for bread, casavi or cazabi, and the term manioc from the Tupi word maniot, which French explorers converted to manioc  (Lebot, 2009) . Agber (2007)  reported that a variety of cassava species are found in the Benue Basin including Aiv-kpenga, Pavnya (pronounced Panya) and Imande; and that Panya was discovered by a Tiv hunter called Adaga from Gaav Megaclan of the Tiv in about 1794 according to oral history. The crop therefore represents a household food bank that can be drawn on when adverse climatic conditions limit the availability of other foods. Therefore, the Tiv people develop different ways of processing it into durable forms soon after it is harvested, which forms part of management strategies for postharvest losses. Postharvest priorities across the globe have evolved considerably over the past four decades, from being exclusively technical in their outlook, to being more responsive to consumer demand. Consumer-driven trends which have contributed to this shift include rising incomes in urban areas, changing dietary habits, more women in the work-place, reduced time for meal preparation and growing demand for safety, quality and convenience  (Rolle, 2006) . Postharvest encompasses the conditions and situations surrounding the state of the food after separation from the medium and site of immediate growth or production of that food. Harris and Lindblad (1978)  asserted that postharvest begins when the process of collecting or separating food of edible quality from its site of immediate production has been completed. Apparently, postharvest losses therefore mean any change in the availability, edibility, wholesomeness or quality of the food that prevents it from being consumed by people. Food losses may be direct or indirect. A direct loss is disappearance of food by spillage, or consumption by insects, rodents, and birds. Peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips) 2. The public library owes the people a duty to go into the hinterlands of the Tiv nation in Benue State to collect audiovisual materials on these postharvest losses management strategies. The study adopted a survey design, which is the type of design that enables the researcher to collect data from a group of people through questionnaire, interview or observation techniques for the purpose of analysis and subsequent interpretation. There are 7 public library branches in the Tiv speaking local government areas of Benue State. These are the Benue state Library Board Headquarters in Makurdi and its branches in Gboko, Vandeikya, Katsina-Ala, Gungur, Aliade and Adikpo with about 33 workers  (Mngutyô & Amaakaven, 2013) . Significantly, the researchers got a hypothetical population of 993 library users and 18,974 cassava farmers making the total population of 20,000. The snowball sampling was adopted in selecting the subjects. They used the Fish Bowl Technique by writing Yes and No for the respondents to choose and those who chose Yes were finally given questionnaire to respond to it. The researchers did this until they arrive at the sample size of 680. The instrument used for data collection was Questionnaire constructed by the researchers. Section A of the questionnaire contained respondents' bio-data, which included sex and occupation. These research assistants were asked to administer and retrieve the questionnaire through personal contact to avoid delays associated with mailing and multiple filling. Data were analyzed using mean and standard deviations. Importantly, on each research question, data were collected on related items in the instrument. The collected data were analyzed using mean and standard deviation. Demographically, data were collected from 680 Library Staffs, Cassava Farmers and Library Users, out of which 348 were male representing 51.2% while 332 representing 48.8% were female. Apparently, 28 representing 4.1% were library staffs, 376 representing 55.3% were cassava farmers and 276 representing 40.6% were library users. What are the Tiv management strategies for postharvest losses of cassava? The collected data were analyzed and presented in  Table 1 . From  Table 1 , it can be seen that the mean rating scores of all the items are above 2.50. This means that peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips), peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour), peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom, slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base), peeling the cassava, fermenting, sieving and squeezing it into lumps known as Akpu, peeling the cassava, grating, squeezing (demoisturizing) it, mixing with salt and other ingredients and frying with palm or groundnuts oil known as Kweesi or Akweesa Akom and peeling the cassava, grating, squeezing and sieving the starch and preparing it for food as Kamu Akom are the management strategies for postharvest losses of cassava. In order to answer the research question, data were collected relating to the research question, analyzed and presented in  Table 2 . Table 2  shows the mean rating scores of all the items are below 2.50, which implies that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? The collected data were analyzed and presented in  Table 3 . Table 3 , it can be seen that the mean rating scores of all the items are above 2.50. This implies that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. Based on the results in  Table 1 , the findings from the study revealed that processing cassava into Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom, Mtuhem (Base), Akpu, Kweesi or Akweesa Akom and Kamu Akom are the Tiv management strategies for postharvest losses of cassava. Table 2 , findings of the study revealed that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). Findings from the study based on the result in  Table 3  showed that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. The research discovered that the Tiv people had various strategies they adopted in managing postharvest losses of cassava. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. Therefore, it was recommended that concerned policy makers should make policies that will allow public libraries to acquire store and preserve audiovisual materials on various indigenous knowledge for development to ensue; and for posterity.
paper_298	The study investigated the awareness and use of library catalogue for accessing information resources by users of Federal University of Kashere (FUK) Library. Survey research design was used, registered users of the University Library between 2016-2019 numbering 920 formed the population of the study where a sample of 272 (30% sample) was used based on 95% confidence level and 5% confidence interval of Sample Size Calculator. The instrument used for data collection was questionnaire. Data generated was analyzed using simple percentage and descriptive statistics. The findings of the study revealed that majority of the respondents were aware of library catalogue existence; they were more informed about card catalogue usage than OPAC for retrieving information resources. Poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. The study recommended that the library management should organize a periodic user education, orientation and sensitization programmes for the undergraduate users to create awareness and enable them gain the needed skills to use the library catalogue maximally when searching for information resources. Guidelines on how to use catalogue and library in general should be prepared and issued to registered users. Regular shelf reading should be done so as to establish right contact between library users and library materials. The university library goals and objectives are to provide adequate and relevant information resources both in print and online for university community to support teaching, learning and research (these for undergraduates students may refer to class work, assignments, research/project work, term papers, seminar presentation by providing relevant information and services provision for effective and efficient achievement of academic pursuit). University library provides well stocked information resources and trained personnel to organize available information materials and assist faculty members and student users in the retrieval and use of these resources. The traditional goals and objectives of the library catalogue are to enable users to search a library's collection to find items pertaining to specific titles, authors, or subjects. Library catalogue is considered as an interface of information retrieval system which assists information searchers to access resources of libraries using several access points. Like the card catalogues, sheaf catalogues, books, machine readable catalogues (MARCs) and online public access catalogues (OPACs). Following advancement in ICT and subsequent development of Online Public Access Catalogue (OPAC), the traditional concept of access to library resources which many scholars identified to be prone to numerous challenges has changed. OPAC has brought a changing relationship between libraries and their users in the changing ICT environment. Library OPACs first emerged in the late 1970s and early 1980s and have gone through several cycles of change and development all geared towards improvement. The library OPAC as an automated catalogue system was developed as a tool to locate those information resources that had been acquired, organized and preserved by the library itself. In this environment, the user is both the primary searcher of the system and the user of the information resources  [2] . Online public access catalogue (OPAC) is the most modern form of library catalogue, whereby bibliographic records of all the documents collection are stored in the computer memory or server. Again, awareness of the library catalogue is the ability of the students to have communication and consciousness of its essence, its retrieval technique as well as their relevance to the information user. KumarandVohra investigated the use of Online Public Access Catalogue by the users at Guru Nanak Dev University Library, Amritsar (Punjab) and discovered that majority of the respondents 68.7% were not aware regarding OPAC, 12.5% stated the reason to be lack of assistance from library staff and slow speed. The purpose of using OPAC majority of the respondents 63.2% stated that they use OPAC to know the availability of required document. The study suggests that the users should be made familiar with the use and operation of the OPAC by providing special training  [6] . Kumar revealed that 26.99% respondents use the OPAC fortnightly and 22.84% weekly. 61.59% use the OPAC to locate a document on shelves and 58.48% to know the particular book is available on the shelves or not, 37.71% to know the bibliographical details, 31.14%. 91% respondents used the title search approach and 83.04% used the author search approach, User also indicated that the information regarding the problem faced by the respondents while using the OPAC like 74.39% faced by the problem lack of proper guidance about OPAC followed by 67.47% lack of awareness, 36.33% satisfied with the OPAC and its services  [7] . If a user lacks skills to use a library catalogue, the user may not be able to make effective use of the library resources. The studies of Oghenekaro found that users exhibit patterns of library catalogue usage, that education, experience and sophistication of library users determine the pattern or level of library catalogue use  [1] . The author attributed the reason for the poor usage to lack of user education programme  [9] . In essence, students use the catalogue to enable them conduct research in the library. While this is a welcome development, it is important to occasionally assess the effectiveness of the library catalogue especially from the users' point of view. However, it is observed sometimes that the bibliographic tools that supposed to lead or guides user to the location of a particular item in the library are either found in adequate, misleading, totally not provided or somehow incomplete. The study is designed to achieve the following objectives: a. To find out methods employ by students to consult library catalogue to search for information resources. The study is designed to answer the following research questions: a The survey research method was adopted for the study, and questionnaire was the instruments use for collecting data. The stratification sampling technique was employed to sample the entire registered population of undergraduate library users from each level (100-500) in Federal University of Kashere respectively. The completed and returned questionnaires were analyzed using descriptive statistics, percentage and frequency distribution. This section presents questionnaire collected on the basis of gender of the research respondents. The gender breakdown is presented on  Table 1 . The question on the demographic information of the respondents was on gender presented in  Table 1 . It shows that majority of the respondents 188 (69%) were male, followed by females 84 (31%). This shows that majority of library users in the Federal University of Kashere Library were male. Table 2  revealed that majority of the respondents 262 (96.3%) were aware of the existence of the library catalogue in the university library. The awareness of respondents would depend on the way the library informs their users about the use and functions of library catalogue revealing the available resources and its location in the library. 26 (10%) of the respondents were aware of the existence of OPAC as a retrieval tools in the library. Their responses were presented in the  Table 4  below. Table 5  reveals that 106 (40%) got their awareness through library staff, followed by those that knew it through the user education programme (GST) 88 (16%). The result proves that the management of the university library provides awareness opportunity to users for retrieval and utilization of information resources, except that they need to put more emphasis during library orientation to add to existing ones. Never  Total  F  %  F  %  F  %  100  20  40  28  56  2  4  50  200  14  32  20  45  10  23  44  300  24  44  20  37  10  19  54  400  42  75  12  21  2  4  56  500  58  100  0  0  0  0  58  Total  158  60  80  31  24  9  262  Key: F=Frequency, %= Percentage. Table 5  indicated that majority of the respondents 158 (60%) used the library catalogue regularly when searching for materials in the library. 24 (9%) of the respondents never used the catalogue but they were aware of it existence in the library as indicated on table 2 above. This shows that these few users preferred browsing the shelves to search for information resources. This finding is contrary to that of Oghenekaro whose study found low usage of the library catalogue attributed the low frequency of use to lack of user education programmes  [1] . Total  F  %  F  %  F  %  F  %  F  %  F  %  100  10  20  0  0  6  12  28  56  6  12  0  0  50  200  10  23  6  14  10  23  16  36  2  5  0  0  44  300  10  19  6  11  14  26  12  22  6  11  6  11  54  400  14  25  0  0  36  64  6  11  0  0  0  0  56  500  2  3  0  0  54  93  2  3  0  0  0  0  58  Total  46  18  12  5  120  46  64  24  14  5  6  2  262  The finding indicated poor use of OPAC 12 (5%) at the university library. This finding is in agreement with Fabunmi & Asubiojo whose study on OPAC use by students of Obafemi Awolowo University, Ile-Ife found out that though many students were aware of the OPAC, few actually made use of it. The study found that majority of the respondents were aware of library catalogue in the library including OPAC  [12] . The finding in this case was surprising considering that majority of the respondents had indicated that they do not use the OPAC. It could be concluded that majority of the respondents used the library for research work rather than for leisure and recreational purposes. The results are presented in  Table 8 . Table 8  focused on the challenges associated with catalogue use for information retrieval. This indicates that good of the respondents had difficulties using the library catalogue because the respondents lack sound ICT skills that could enable them use the OPAC. Again, the later challenge could be attributed to lack proper shelf reading by the library staff, which made users not to locate material indicated available in the library by the catalogue and not visible on its shelf. Again with that of [Ogunniyi & Efosa whose study concluded that the problem of catalogue use is associated with lack of knowledge on how to use the library catalogue  [11] . The study revealed that majority of the university library users were male. It is disturbing to discover from the study, that most of the respondents were aware of the card catalogues as access and retrieval tool for searching for information resources in the library. Therefore, proper orientation and awareness campaign should be done to address this ugly situation. It could be concluded that majority of the respondents used the library for research work rather than for leisure and recreational purposes. Another source of frustration was reasonable number of the respondents indicated poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. Considering the important roles library catalogue plays in effective and efficiency use of library information resources, the research recommends the following to improve the use of library catalogue for information access and retrieval: a. Because most challenges associated with the use of the OPAC often have to do with lack of ICT skills on how to make use of it. c. Guidelines on how to use catalogue and library in general should be prepared and issued to registered users. d. Regular shelf reading should be done so as to establish right contact between library users and library materials and avoid misplacement or wrong shelving of information resources.
paper_305	The implementation was based on the java netbeans development platform to create an interface that was used to train a model and its subsequent use in predicting credit decisions. The results obtained proved that such a mechanism can be applied to augment manual credit appraising processes, especially where large volumes of applications are to be processed within limited timeframes. Therefore the screening of the customer's financial history as well as the ability to remain faithful to new financial obligations is a very significant factor before any credit decision is taken and it is a major step in reducing credit risk. We begin by highlighting the relevant base literature upon which the experiment was setup and the subsequent experimental setup and finally the results obtained with corresponding analysis and conclusion. After some experience, these officers develop their own experiential knowledge or intuition to judge the worthiness of a loan decision. Further, the complexity of loan decision tools and variation between applications is an opportunity for the use of a machine learning tool to provide learning capability that does not exist in other technologies. Ensemble meta modeling techniques, are empirically some of the best machines learning tools applicable to financial risk analysis. The purpose of this study was to develop a loan decision system using the logistic regression Meta modeling algorithm -Logitboost around Java based open source software for the Kenya commercial banks. The general objectives of this study were to: 1) Implement the meta learning algorithm -LogitBoost to develop as system for evaluating credit applications to support loan decisions in Kenyan financial institutions 2) Outline some of the challenges of using the learning algorithm in the decision-making process for the banking industry in Kenya 3) Champion the applicability of Java as an open source software in business intelligence applications From time immemorial in the banking sector, banks have relied on the personal assessment of loan risks or on the traditional statistical methods to predict the default of loans instead of using a standardized evaluation tool. Further, the study considered a binary output from the classifier, hence dependent variable can only take on accept or reject values with an emphasis on the banking industry in Kenya; though the results can easily be generalized to institutions elsewhere. Although the classifier takes this into account through voting -in which those values that meet certain thresholds are promoted to either of the classification values, most of such incidences are minimal and can be handled through judgmental procedures by re-examining those peculiar cases and applying policies as laid out. Further, the classifier labels every classification instance with a level of confidence value. The study has left such analysis to oversight procedures especially where the confidence level of the classifier does not meet a certain threshold. This calls for the use of more efficient and effective loan screening tools and procedures. One of the earliest automated procedures uses statistical tools which have fallen short of the inherent challenge for today's commercial banks is their desire to understand large amounts of information and reveal useful knowledge to improve decision-making. Today, lenders are making increased use of new and innovative techniques -the key being data mining and machine learning to evaluate loan applications for business and financial prospects  [2, 3] . Further, ensemble learning algorithms -those that combine a number of base algorithms, through empirical reports typically lead to better results. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced  [4] [5] [6] . One such algorithm that effectively addresses these issues is the LogitBoost Meta classifier -based on the log of the odds ratio for the dependent variable  [7, 8] . Although the model was reported to perform relatively better than models developed using other approaches; as part of the limitations and recommendation, they suggested that such a model is usually a black box and more insight the model parameters was required to make it more effective. The solution to the problem was an adaptation of ensemble machine learning strategies where a 'weak' classifier, commonly referred to as a base classifier was boosted through a series of adjustments through weighting and re-sampling to develop a better learner which was an additive aggregate of individual learners. The reported technique derives from the intuitive understanding that instead of putting all the effort on finding highly accurate base classifiers, it becomes sufficient or even desirable to use a set of weaker hypotheses. Resulting classifier is a simple threshold on a single feature a) Outputs a +1 if the attribute is above a certain threshold b) Outputs a -1 if the attribute is below the threshold In majority voting, to predict the class of a new item, each base classifier got to vote for either the 'accept' or the 'reject' class. It can be proven (as discussed here-under), that under the assumption that all individual classifiers have the same prediction rate and that the distribution of the data correctly classified by each base classifier is independent and random, this is the best possible strategy. The implementation detailed lay in the use of a logistic regression that models the posterior class probabilities Pr (G = k|X = x) for the K classes. In our study, the variable k was bi-valued and took on either 'accept' or 'reject' values and K was set at 2. The model was specified in terms of K −1 log-odds that separate each class from the base class K. a) With K attributes , there are K different decision stumps to choose from b) At each stage of boosting i. given reweighted data from previous stage ii. Classify and obtain performance metrics. Select the next partition as testing and use the rest as training data. v. Classify until each partition has been used as the test set. This strategy relies on two separate files, one for training and the other for testing. This strategy is similar to the use of two files as discussed earlier but relies on the learner to automatically partition a given data set into two given a split percentage Class =Accept: The number of correctly classified instances is 12 and that of instances classified as belong to the class is 13. This gives a precision value of 12/13=0.92 Class =Reject: The number of correctly classified instances is 7 and that of instances classified as belong to the class is 7. This gives a recall value of 12/12=1 Class =Reject: The number of correctly classified instances is 7 and the number of instances belonging to the class is 8. The ROC graph is a plot of two measures: Sensitivity: The probability of true classifications given true instances i.e. Three options were investigated for training the algorithm namely: a) The use of single file both for training and testing the model through stratified cross validation. This is a strategy where the training file was portioned into complementary data sets called the training set and the validation set. The model accuracy using this procedure was 86.86% making it a fairly reliable strategy b) The use of separate training and testing data sets returned an accuracy of 95% making it a relatively better strategy c) The use of a ratio to determine the size of the training and testing files from one data set returned an accuracy of 88.64% Therefore, it implies from these findings that the use of separate files for training and testing of the model returns the best model accuracy and hence should be adopted. Finally, as stated earlier in the introduction, it is not prudent to completely rely on an automated credit appraising as some cases might require subjective interpretation and personal judgment. As a conclusion, the reported work indeed confirmed that: 1) Machine learning procedures can be applied in financial modeling applications to augment manual underwriting techniques 2) These procedures can greatly improve the efficiency of such techniques because of their ability to handle large items of data generating very useful statistics 3) This work can be improved through the use of enhanced data set pre-processing procedures, the use of a cost matrix as well as parameter tuning to settle on the most effective set for various data mining requirements.
paper_310	We are now living in the 21 st century. This project describes how to control a robot using mobile through Bluetooth communication, some features about Bluetooth technology, components of the mobile and robot. We present a review of robots controlled by mobile phone via moving the robot upward, backward, left and right side by the android application such as Arduino, Bluetooth. We derived simple solutions to provide a framework for building robots with very low cost but with high computation and sensing capabilities provided by the smart phone that is used as a control device. This paper represents android application based Bluetooth controlled robotic car. We interface the Bluetooth module with the system so that we can easily control the system by smart phone application. Here in the project the Android smart phone is used as a remote control for operating the Robot. The controller acts accordingly on the DC motors of the Robot. In achieving the task the controller is loaded with a program written using Embedded 'C' language. Still there exists a requirement of a cost-effective automation system, which will be easy to implement. An example of such a costeffective project has been proposed here. The design of the system is kept as simple as possible. Few things like cost-effectiveness and simplicity in design, lowprofile structure etc. have been kept in mind before designing the project. Our system aims to achieve the target to design a system that can provide following functionalities with a simple and easy-to-use interface: a) Develop an android application that will act as an remote of a robot. g) The Bluetooth module is connected with the Arduino UNO board for the connection with the user. The microcontroller is programmed with the help of the Embedded C programming. Bluetooth module will give the commands given by smart-phone to the microcontroller. It sends the data to microcontroller through Bluetooth module. The novelty lies in the simplicity of the design and functioning. Arduino software (  Figure 5 ) is used to put the instruction of whole functions of this system to the microcontroller. The program is stored in the EEPROM of the microcontroller, which is present in the Arduino board. By this software we put the data and instruction for forward, backward, left, right operation of this system. Motor driver switches accordingly the data bit, if the data bit is low then the corresponding pin of the motor driver doesn't work else highbit then thecorrespondingpinof the motor driver is on. There are two steps of the programming. Second loop part where the program runs continuously. The working principle is kept as simple as possible. The working principle of the circuit has been elaborated with the help of a block diagram, of the system interconnection as shown in  Figure 6 . The microcontroller, thereby, sends instructions, which when executed, helps in functioning of the motor driver. The Bluetooth module is connected with the Arduino UNO board for the connection with the user. When user sends any data to the Arduino board then the corresponding pin of Arduino goes to high state and switches the motor driver ic in the on mode. The circuit diagram of this project is shown below: Here at first we construct the circuit as shown in  Figure 7 . Here the Bluetooth module act as a receiver which receives the instruction from the smart phone (remote or transmitter). Then the microcontroller decides the operation for the instruction which is coming from the smart phone. The functions of the given instructions are operated by the microcontroller. When any input is given then the motors moves as per the preloaded functions in the microcontroller. The novelty lies in the fact that it is a cost-effective project with a simple and easy to use interface compared to existing ones.
paper_333	The diagnosis of diseases on the plant is a very important to provide large quantity and good qualitative agricultural products. Enset is an important food crops produced in Southern parts of the Ethiopia with great role in food security. There are several issues and diseases which try to decline the yield with quality. The aim of this study is to design a model for Enset diseases diagnosis using Image processing and Multiclass SVM techniques. The strategy of K-fold stratified cross validation was used to enhance generalization of the model. Food security is a challenge in many developing countries like Ethiopia. It is also the sector that is given an overriding focus in the government's plan for growth of the economy as a whole. This is commonly known as false banana, the Ethiopian banana or the Abyssinian banana. Ethiopia is one of the grand producer of Enset in African continent countries. There are several issues and diseases which tries to decline the yield with quality. Particularly, diagnosis of potential diseases on Ethiopian banana is based on traditional ways and due to limited research attention given to Enset crop production. There is a potential need for technology supported alternative systems to support the manual identification of Enset crop diseases so as to optimize the accuracy for remedial action. If the Enset crop diseases can be minimized then definitely the production will significantly be increased to contribute to the country's economy towards a new boost. In our country few researchers found the promising solutions to different plant diseases diagnosis such as maize, rose flower, coffee and others using computer vision and machine learning techniques  [3, 4, 5] . The remaining part of this paper is organized as follows. In Section III, we present the architecture of proposed system. Experimental results are reported in Section IV. Section V presents the conclusion and discusses of future works. In this work, disease identification was done by using characteristics of Enset plant. The feature of normal and diseased Enset image features was extracted to train kernel support vector machine. Figure 2  shows the architecture of the proposed system A. Dataset Partitioning The images of the dataset are collected from different parts of southern Ethiopia. Those images are collected by using a digital camera and some of them are collected from secondary sources like research centers which collects pictures for visual inspection of Enset disease. As it is mentioned in the above section, for the experiment two diseases and one normal of Enset were identified. From all those disease category a total of 460 Enset leaf images are collected from which 368 is used for training and 92 images are used for testing. Test Result For the experimentation a total of fifteen experimental setups which is color, texture morphological and a combination of each and a multiclass Kernel support vector machine model: Linear Kernel, polynomial Kernel, RBF kernel and Quadratic Kernel was used. In order to measure the performance of the classifiers K-fold (in this case k is 10) cross validation method was used. The detail classification result is shown in  figure 4 . From the figure it is shown that the RBF kernel classifier has classified the given enset image in its correct class category by 94.04% and 92.44% respectively. We designed a system which can detect Enset disease automatically and this will help the farmers to detect the diseases in its early stage and to take relevant action.
paper_389	This paper systematically describes the definition, model structure, parameter estimation and corpus selection of the conditional random field model, and applies the conditional random field to the Chinese word segmentation and the Chinese word segmentation method. Word probability, the paper explores the probability characteristic of word location. Experiments on the corpus show that the introduction of the word position probability feature has improved the accuracy, recall and the value of Fl. The so-called Chinese word segmentation, is the process of word segmentation as each Chinese character classification process, by marking each character in the sentence to segmentation. Zhou J built a hybrid method of Chinese word segmentation around CRF model. For example, "O" can be used to represent individual Chinese characters, "B" means Chinese characters appear in the head, "I" means Chinese characters appear in the middle or the end of the word. So the word segmentation problem is transformed into a pure sequence data labeling problem, you can use a lot of sequence tag algorithm for word segmentation. Figure 1  is an example of the use of Chinese characters marked word segmentation. Finally, according to the mark of each Chinese character, the mark result is "OOBIO", so the result is "这 / 是 / 武汉 /." As mentioned earlier, the CRF makes it easy to add any feature in the observed sequence to the model, so that not only the transfer and emission characteristics of the traditional HMM sequence model can be incorporated into the model, but also some other The feature information associated with the observation sequence or with the language itself is added to the model. In this paper, CRF automatic word segmentation experiments, the use of features include the following two: a single word characteristic: a position on the word characteristics. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. CRF word segmentation system is a word segmentation system based on conditional random field, and the Chinese word segmentation method is adopted. Secondly, feature extraction is carried out to generate training corpus and test corpus which can be recognized by CRF model tool. Then the training corpus is trained to generate a CRF model, and some training parameters such as iteration number are added in the training process. Using the CRF model generated by the training, the test corpus is tested and a prediction result is obtained. The performance of a word segmentation system mainly depends on two aspects: segmentation precision and word segmentation speed. Word segmentation accuracy, also known as word segmentation accuracy, is the core performance index system. The performance of Chinese automatic word segmentation is evaluated by the following three indexes: correct rate (P), recall rate (R) and F value. Where, P refers to the accuracy of word segmentation; R refers to the word recall rate; F value refers to the P and R integrated value. CRF The word segmentation system is a word segmentation system based on the conditional random field, and uses the feature template one, the feature template two and the feature template three. Moreover, since the word position probability feature is extracted completely from the training corpus, some of the participle criterion information of the corpus is extracted to a certain extent, so that when the test set is tested, Corpus. This chapter first briefly introduces the CRF tools, experiment corpus and standard of experimental evaluation in Chinese word segmentation experiments. The experiment not only demonstrates the influence of the choice of conditional random field model parameters and Chinese character annotation set on the experimental results, but also verifies the validity of the new features and the feasibility of the new method. In the experiment, only some feature information is used, and most of the features are extracted from the training corpus, we have achieved good results. As the Yangtze River Daily corpus is from the newspaper news, for the news corpus in the special format, such as title, poetry, weather forecasting, etc., to our model training has a certain impact, so if the corpus in the handling of these disturbances, Our model should have better performance.
paper_391	Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. Globally, tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The data for this study were the Tetanus Toxoid data set from the Ethiopian Demographic and Health Survey (EDHS) 2011, and analyzed using the Knowledge discovery process of Selection, Processing, Transforming, mining, and interpretation. The WEKA 3.6.1 tool was used for classification, clustering, association and attribute selection. The accuracy rate of the classifiers on training data is relatively higher than on test data and the multilayer perceptron is the best classifier in our data set on Tetanus toxoid. In the cross-validation with 10 folds, correctly classified best are by naïve Bayesian 63.30% and the least accurate were by k-nearest neighbor 60.52%. Single data instance test using Naïve Bayesian was done by creating test 1, test 2, test 3, and test 4 data test instance, three of them are correctly predicted but one of them incorrectly classified. The maximum confidence attained in the general association is 0.98. But, in the class attribute, it is 0.72. The literacy status of the mother has high information gain with the value 0.046. As a conclusion, the best algorithm based on the TT vaccination data is multilayer perceptron classifier with an accuracy of 67.28% and the total time taken to build the model is at 0.01 seconds. Multilayer perceptron classifier has the lowest average error at 32.72% compared to others. These results suggest that among the machine learning algorithm tested, multilayer perceptron classifier has the potential to significantly improve the conventional classification methods for use in EDHS data of Tetanus toxoid. Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. And the major cause of early infant deaths in many developing countries is often due to failure to observe hygienic procedures during delivery. Infection is acquired through environmental exposure of any broken skin or dead tissue such as a wound or when the umbilical cord is cut to the spores of the bacteria. Globally tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually, accounting for up to 25% neonate's death in some African countries  [3] [4] [5] . As of December 2012, maternal and neonatal tetanus persist as a public health problem in 30 countries, mainly in Africa and Asia  [2] . However, until now maternal and neonatal tetanus persist as public health problems in 36 countries, mainly in Africa and Asia. The TT vaccination schedule in Ethiopia for childbearing women follows the schedule recommended by WHO for developing countries  [6] . Five doses of TT can ensure protection throughout the reproductive years and even longer. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. It is a young and fast-growing field also known as knowledge discovery from data (KDD) and used for discovering interesting patterns from data in various applications  [7] . The health care industry is one of the world's largest and fastest growing industries having a huge amount of healthcare data. This health care data include relevant information about Client, their treatment, and resource management data. The information is rich and massive. Data mining techniques are more effective that has used in healthcare research. This problem caused the loss of time and effort in the healthcare system environment and spending lots of efforts and costs without having evidence-based information for planning and intervention. The standards used are a percentage of accuracy and error rate of every classification techniques used. This process must have a model to control its execution steps. Knowledge discovery from data for prediction of the tetanus toxoid immunization among the women of childbearing age in Ethiopia following the standard process, guiding us in the analysis process, and exposing those aspects that could otherwise be neglected. The  Figure 1 (Adapted from  [7] ) shows, the basic phases of the knowledge discovery from data, we have undergone. Preprocessing solves issues about noise, incomplete and inconsistent data. The next phase is the transformation of the preprocessed data into a suitable form for performing the desired data mining task. In the data mining phase, a procedure is run that executes the desired task and generates a set of patterns. However, not all of the patterns are useful. The methodology of this study was the practical research method applied on the Tetanus Toxoid data of the Ethiopian DHS 2011. The data used in this investigation are the TT immunization data. It has a dimension of 7033 rows and 12 columns. Only 80% of the overall data is used for training and the rest 20% was used for testing the accuracy of the classification of the selected classification methods. csv" file formats and stored as an ". Classification is one of the data mining techniques and it is used to group the instances which belong to same class  [8] . Classification also extracts models describing important data classes. The classification methods used in this study is to classify data according to their classes putting the data in a single group that belongs to a common class. The approaches are; (a). And the topmost node in a tree is the root node. This method of classifiers is based on learning by analogy, by comparing a given test tuple with training tuples that are similar to it. The training tuples are described by m attributes. In this way, all the training tuples are stored in an m-dimensional pattern space. When given an unknown tuple, a k-nearest-neighbor classifier searches the pattern space for the k training tuples that are closest to the unknown tuple. These k training tuples are the k "nearest neighbors" of the unknown tuple  [7, 10] . Bayes Classification Methods Bayes classifiers are statistical classifiers based on Bayes' theorem, which is probabilistic learning method. It is made to simplify the computations involved and, in this sense, is considered "Naïve"  [7] . Multilayer preceptor Multilayer preceptor is a simple two-layer neural network classifier with no hidden layers. We will consider the case of where the class tuples are more or less evenly distributed, as well as the case where classes are unbalanced. Confusion matrix; is used in this study, include accuracy, sensitivity, specificity, and precision. Understanding them will make it easy to grasp the meaning of the various measures. To discover acceptable classes using Simple K-Means based on the principle of maximizing the similarity between objects in the same class i.e., intra-class similarity and minimizing the similarity between objects of different classes i.e., inter-class similarity  [7] . "How does the k-means ( ) algorithm work?" The k-means algorithm defines the centroid of a cluster as the mean value of the points within the cluster. First, it randomly selects k of the objects in D, each of them initially represents a cluster mean or simply center. The k-means algorithm then iteratively boost the within-cluster variation. For each cluster, it computes the new center using the objects assigned to the cluster in the previous iteration. All the objects are then re-assigned using the updated center as the new cluster centers. The association rules that contain a single predicate are referred to as single-dimensional association rules. Another threshold is Confidence, which is the conditional probability than an attribute appears in a transaction using the Apriori algorithm. The InfoGainAttributeEval with Ranker T used with respect to the class attribute and visualization was done in the attribute selection. (Table 1)  As can be seen in  Figure 2 , the accuracy rate of the classifiers on training data is relatively high. A multilayer perceptron is the best classifier in our data set. Using evaluation with cross-validation (10 folds) correctly classified best are by naïve Bayesian 63.30% and the least accurate were by K-nearest neighbor 60.52%. (Table 3)  Simple K-Means preferred the method of clustering for this project we have adjusted the attributes of our cluster algorithm by clicking Simple K-Means. The only attribute of the algorithm we are interested in adjusting here is the "Num clusters field", which tells us how many clusters into five based on the instruction given. Cluster 3-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 4-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, orthodox in religion, Amhara in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education no education, women age 35-49, mother marital status married, hh_head Female, tt-vaccinated yes. As explained in the    The literacy status of the mother has high information gain (0.046) and followed by the literacy status of the mother (0.041) and the least information gained was head of household (0.00000147). In this study, the data mining tool and algorithm like (J48, k-nearest, and Bayes) is used for selecting the training and test data, for classification, and K-means approach for clustering, single-dimension association rule to identify the best association. csv" file and features were described using WEKA performance. As a conclusion, the best algorithm based on the TT vaccination data is multilayer perceptron classifier with an accuracy of 67.28% and the total time taken to build the model is at 0.01 seconds. Multilayer perceptron classifier has the lowest average error at 32.72% compared to others. These results suggest that among the machine learning algorithm tested, multilayer perceptron classifier has the potential to significantly improve the conventional classification methods for use in medical data.
paper_402	The BPNN and SFS were used interchangeably to identify the relevant features that contributed with the response variable. As a result, the BPNN with the selected features was able to interpret more accurate results (r 2 = 0.991) than the model with all the features (r 2 = 0.816). The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. Several types of machine learning algorithms such as Artificial Neural Network (ANN) have been used in different fields for the development of models that predict response parameters (experimental dataset) using certain independent input parameters. However, an experiment could have a large number of independent parameters most of which are redundant and have negligible effects on the response parameters. The need for soft computing tools and models for the prediction of behavioural properties of engineering components, systems and materials is continuously rising. ANN emerged as one of soft computing paradigms that have been successfully applied in several engineering fields  [1] . Recently, research interest has revolved around the development of ANN models to interpret the behaviour of structural materials such as steel, concrete, and composites  [9] [10] [11] [12] [13] [14] . The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete  [14] [15] [16] [17] . In this study ANN is employed with other machine learning techniques to identify the parameters that capture the compressive strength of UHPC using data collected from the literature. However, most of the aforementioned mixtures result in exhausting a large amount of resources and performing tests on many batches, while barely predicting the strength of UHPC  [19] . [19]  used the back-propagation neural network (BPNN) and statistical mixture design (SMD) in predicting the required performance of UHPC. Despite the statistical advantages of ANN, it has been long regarded as a black box that evaluates functions using input covariates and yielding outputs. This will reduce the amount of parameters in the model, which will improve the computation complexity of the ANN model and simplify the derivation strategies of a mathematical model used to predict the compressive strength of UHPC. There are several machine learning techniques, in the literature, that assist researchers in identifying the underlying covariates impacting the prediction model. Sequential feature selection (SFS) is a machine learning tool that sequentially selects features and inputs them into a fitting model (i.e. This technique makes use of ANN's complex computation and allows the SFS tool to select and remove the influential and redundant parameters, respectively. [29]  used the Markov Blanket with a wrapper method to select the most relevant features of human motion recognition. Four sets of open human motion data and two types of machine learning algorithms were used. Moreover, Rodriguez-Galiano et al. [30]  used SFS when tackling ground water quality problems, where 20 datasets of parameters were extracted from a GIS database. Four types of machine learning algorithms were used as wrappers for the SFS. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. The input neurons are responsible for containing the independent parameter presented by the user, the wires represent the randomly generated matrices called weights that manipulate the function's slope or steepness, the hidden neurons map the weights variables using an activation function, and the bias units control the output function's shift, either upward or downward. Equation  1 shows the linear combination of mapping weights from each input neuron, via wires, to the hidden neurons. SFS is composed of two components: the objective function, which is the criteria the algorithm follows when selecting the features (i.e. the NMSE), and the search algorithm, which are the methods of how the machine add/removes the features from the subset. There are two types of search algorithms: sequential forward selection and sequential backward selection. In this study, the previously verified ANN model was used as the objective function and the forward selection was used in selecting the relevant features. The ANN numerical solver, Levenberg-Marquardt, was verified by testing different number of neurons using a basis like the normalized mean square error (NMSE) to measure the error. Hence, for each neuron tested, ten NMSE values will be stored in a column vector, where each column vector will be averaged and plotted against its corresponding number of neuron(s). Figure 2  shows the plot of all the scenarios with the minimum point circled at 11 neurons. Therefore, 11 neurons is, approximately, the number of neurons that is sufficient enough for BPNN to facilitate an accurate ANN model for the collected dataset. The SFS algorithm was run 200 times to capture all possible combinations of independent features when using ANN. Table 2  tabulates the percentage of features that were used during the 200 trials. Based on the results of these trials, the most abundant combination during the SFS analysis, within a 20% threshold, was selected as the important parameters that contribute mostly in the model. In this study, four variables (Cement, Sillica Fume, Flyash, and Water) were selected as the most relevant features for the prediction model. The selected features, using SFS, were analyzed by the previous BPNN model. As a result, the model that used the selected features showed stronger agreement with the experimental results in contrast with that prior to the selection. Table 3  shows the statistical measurements calculated for both cases. It was observed that the r 2 and NMSE before and after selection yielded 81.6% and 99.1%, respectively, and 0.0594 and 0.026, respectively. The correlation plots between the predicted and experimental results for the ANN models, with and without selected features using SFS, are summarized in  Figure 4(a)  presents the percent deviation, where an arbitrary percent deviation was plotted above and below the perfect fit line with a deviation value of ±20%. As a result, the ANN model with the relevant features was capable of predicting 89.6% of its values within the aforementioned boundaries, as opposed to the ANN model with all the features which predicted 58.7% of its values within the boundaries. Table 4  shows the coefficient values, with their corresponding symbols, for each UHPC constituent with the statistical measurements of the LSR model. Since the developed LSR model is capable of accurately predicting the experimentally measured compressive strength, a parametric study was conducted, using this model, to study the effect of Fly Ash and Silica Fume on the compressive strength of UHPC. Using Fly Ash quantities that range between 0-200 kg/m 3 and Silica Fume quantities that range between 40-160 kg/m 3 while fixing the quantity of cement at 1400 kg/m 3 and water at 175 kg/m 3 , several plots showing the variation of strength of UHPC were generated as shown in  Figure 5 . This study was conducted to detect the correlation between the material constituents of UHPC and its compressive strength. BPNN was used and three major steps were executed: (1) verification of ANN; (2) application of both SFS; and NID, and (3) analysis of selected features using ANN and LSR. 2) The use of ANN with selected input parameters improved the accuracy of prediction of compressive strength of UHPC and reduced the computational effort. The correlation coefficient (r 2 ) before and after the use of SFS improved from 81.6% to 99.1% while the NMSE improved from 0.0594 to 0.026, respectively. 3) The ANN model with the selected relevant input parameters also showed a lower deviation (89.6 %) than the ANN model with all the features (58.7%). 4) LSR was implemented using the selected input parameters to develop an analytical model that can be used to accurately predict the compressive strength of UHPC.
paper_418	Table 1 shows that the column variances of Buys-Ballot table is constant for additive model but depends on slope and seasonal effects for mixed model. Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model. Also, when there is no trend (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The decomposition models are Additive model: formula_0 Multiplicative model: formula_1 Mixed model: formula_2 where t M is the trend-cycle component, t S is the seasonal component and t e is the error. In particular, this means that the fluctuations overlapping the trend-cycle are not dependent on the series level. They do not depend on the level of the trend  [3] . On the other hand, the appropriate model is multiplicative when the seasonal standard deviations show appreciable increase/decrease relative to any increase /decrease in the seasonal means. Here again, no statistical test was provided for the choice. The emphasis is to compare the row, column and overall means and variances of the Buys-Ballot table for additive and mixed model when trend-cycle component of time series is linear. In such cases it is appropriate to use a multiplicative model. Gupta  [8]  observed that, the additive model assumes that all the four components of the time series operate independently of each other so that none of these components has any effect on the remaining three. According to him, this series are not independent of which other. Oladugba et al  [9]  gave brief description of additive and multiplicative seasonality. 2 (a) The column means mimic the shape of the trending parameters and contain seasonal indices for additive model. The column variances of the Buys-Ballot table is constant for additive model, but a function of slope and seasonal indices for the mixed model. These properties of row, column and overall averages and variances of the Buys-Ballot table are what could be used for estimation and assessment of trend parameters, estimation and assessment of seasonal indices and choice of the appropriate for time series decomposition. That is when 0 b =  (Table 1)  For mixed model, we obtain using the expression in  Table  1  ( ) ( )  formula_8 when there is no trend. That is when ( 0 b = ), it is clear from  For mixed model, we obtain using the expression in  Table 1  . formula_10 When there is no trend ( 0 b = ) we obtain from  (Table 1)  . (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The real life example is based on monthly data on number of registered road traffic accidents in Owerri, Imo State, Nigeria for the period of 2009 to 2018 shown in  Table A1 . The actual and transformed series are given in figures 3.1 and 3.2. Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model. Also, when there is no trend ( ) 0 b = , the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models.
paper_428	The present article examines the influence of thermal radiation on two-dimensional incompressible magnetohydrodynamic (MHD) mixed convective heat transfer flow of Williamson fluid flowing past a porous wedge. An adequate similarity transformation is adopted to reduce the fundamental boundary layer partial differential equations of Williamson fluid model in to a set of non-linear ordinary differential equations. The solutions of the resulting nonlinear system are obtained numerically using the fifth order numerical scheme the Runge-Kutta-Fehlberg method. Williamson analyzed the flow of pseudoplastic materials and presented model to described the behavior of pseudoplastic material and explain convenient importance of plastic flows, and also recognized that viscous flows are very varied from plastic flows  [1] . studied the peristaltic motion of non-Newtonian fluid through asymmetric channels along porous wall by means of various phase and amplitude, and also studied the manipulation of different wave structures on the fluid flow model  [8] . By considering the approximation of long wave length and small Reynolds number the peristaltic pumping of Williamson fluid in a planar channel was investigated by Vasudev et al. found numerically convergent solutions of two dimensional flows of non-Newtonian fluids along chemically reactive species  [11] . Jabar addressed the influence of viscous dissipation and joule heating on MHD flow through a stratified sheet subjected to power Law Heat Flux having heat source  [22] . studied both forced and natural convection boundary layer transport by perpendicular surface in a stratified medium along connective boundary conditions  [34] . Bau investigated the thermal convection in a saturated stratified medium bounded between two parallel eccentric cylinders with the help of a regular perturbation expansion along Daarcy-Rayleigh number; it was observed that the appropriate preference of eccentricity values can maximize the heat transfer inside annulus of various thermal insulators  [37] . Ferdows & Liu obtained the similarity solutions of mixed convection heat convey in parallel surface with internal heat production  [44] . Malleswaran & Sivasankaran carried an analysis for mixed convection flow and noticed that the average heat transfer decreases with an increase in Richardson number but in general heat transfer is better at force convection mode than free convection mode  [45] . studied boundary layer flow of fluid in a porous wedge subject to Newtonian heating along heat generation or absorption  [69] . Deka   is wedge angle parameter. The nondimensional form of the given system of partial differential equations is obtained by introducing the following stream function and the similarity variables  [76] . E 0 = E 6 , E′ 0 = 0, F 0 = 1 (9) E′ ∞ = 1, F ∞ = 0 Where E 6 is injection/suction parameter. The system of ordinary differential equations 7 and 8 subject to the boundary conditions 9 and 10 is first reduced to a system of first order ordinary differential equations using the substitutions E K =`, $ K = a, F K = b. This With the boundary conditions E 0 E 6 , $ 0 0, F 0 1, $ ∞ 1, F ∞ 0 (13) The resulting system in Eq. (11) (12) (13)  is solved numerically with the help of 5 th order Runge-Kutta-Fehlberg method. The transformed governing equations  (11) (12)  subjected to boundary conditions  (13)  are solved numerically by employing the fifth order Runge-Kutta-Fehlberg method. Figures (1-2)  illustrate the influence of wedge angle parameter < with on velocity and temperature profile. Figure 6  drafts the non-dimensional velocity E′ for different values of suction parameter E 6 . Figures 7-8  illustrate the behavior of thermal radiation and Prandtl number on fluid flow region with M= E 6 = O = < =1. Because the prandtl number is the relation of momentum diffusivity to thermal diffusivity, when it increases then it decreases the thermal boundary layer thickness and temperature but increases thermal capacity of fluid.
paper_432	And found some results; The theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds, possibility study of Hamiltonian tubes when the simplistic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models. The Cotangent Bundles * of manifold d provides the basic model of a symplectic manifold. The total space of a cotangent bundles naturally has the structure of a symplectic manifold. Let be dimensional differentiable manifold of class ∞ and * the cotangent bundles over . Then the Cotangent Bundle has a dual space * . We can introduce the Legendre transformation we need some basic facts about the structure of the cotangent bundles * of a nd dimensional differentiable manifold . 1, for any 0 ∈ , the tangent space at 0, denoted , is the space of linear derivations on 1 2, , that vanish on 3 2, , . Thus,T 5 M can be identified with (1 2, , /3 2, , )* the space formula_0 , is called the cotangent space at 0; it is isomorphic to the dual * , of M . The Cotangent Space T 5 * (M) of a manifold at 0 ∈ is defined as the dual vector space to the tangent spaceT 5 M. A dual vector space is defined as follow ∶ given an − dimensional vector space G, with basis H , I=1, 2, 3,…, , the basis J = of the dual space G * is determined by the inner product. Let ∈ and let I; T { * Q → Τ * be the natural inclusion mapping. b) Let exp ∶ → 3 > be given by exp(% =J •" … ∈ 3 > . c) Recall that the n-dimensional real projective space 0 is defined by 0 =3 /o, where ~ , for ∈ 3 ⊂ Š> . For the mapping.∅: & → | to have a lift ‹ satisfying the condition ( ‚ ) =J ‚ , where ‚ ∈ &,J ‚ ∈ H, 0 (J ‚ ) =d 0 =( ‚ ), it is necessary that∅ (π n (&, ‚ )) ⊆0 (a n (H,J ‚ )) (3. A , principal bundles is a quintuple (0, , , R , where: 0 V →0 is a , right at action with the property of local triviality: Each point e ∈ has an open neighborhood for which there exists a , -diffeomorphism. šJ% ± ∈ , with isotropy subgroup = ² . A tube for the action at ± is a −equivariant diffeomorphism from some twisted product × ° to onopen neighborhood of ±in , that maps [J, 0] H to ±. The space o may be embedded in × ° as {[J, ] : ∈ 3}; the image of the latter by the tube is called a slice theorem. In this part we study the symplectic geometry of cotangent -lifted action induced by a smooth proper action of a Lie group on a smooth manifold. The first work studying symplectic normal forms in the specific case of cotangent bundles. The symplectic reduction of the cotangent bundles * Q has more structure than a symplectic manifold. Let act freely and properly by cotangent lifts on * Q, and let Jbe the momentum map of the G action (with respect to the canonical symplectic form on * Q. Leta ³ : → / is projection. Then ∅isc , invariant surjective submersion and descends to a symplectic homeomorphic. The first result of the theory in cotangent bundles reduction, the theory developed for the problem with a single or bit type playing an important role in the solution to the general problem of a singular cotangent bundles reduction for base manifolds, Hamiltonian tubes when the symplectic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models and the first work studying symplectic normal forms in the specific case of cotangent bundles. Conclude that the theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds and found that the phase space is the cotangent bundle T * Q of a configuration space .
paper_444	Resource constrained project scheduling problem is to make a schedule for minimizing of the completion time or total cost subject to precedence rules and resource constraints. Traditional resource constrained project scheduling problem research takes into account achieve management goal in certain environment. In this paper, for better described the uncertain resource constrained project scheduling problem, we firstly consider the uncertain resource availability project scheduling problem based on uncertainty theory. To meet the manger goals, it is assumed that the increased quantities of resource are uncertain variables and the finish time of each activity is a decision variable. One of the constraints is the finish-start precedence relationship among the project activities. The resource constrained project scheduling problem (RCPSP) takes into account the balance of makespan and total cost through resource allocation and reasonable activity schedules while the precedence constrains between the activities and resource constrains are all satisfied. Xie  [9]  supposed that the increased quantities of resource were real-value variables, and built a multi-mode resource constrained project scheduling model for minimizing both the project makespan and cost. In this paper, we consider uncertain resource availability project scheduling problem based on uncertainty theory, in which the increased quantities of resource are uncertain variables, and the finish time of each activity is a decision variable. Then, we build a multi-objective model which is under resource and precedence rule constrained to minimize the resource cost and the project completion time. For solving the above model, the equivalent form of the model is provided and the proof is given. In order to solve the problem of time-cost trade-off in project scheduling, we consider a project which is described as an activity-on-the-node network ( , ) , where = {1, 2, ⋯ , } is the set of activities and is the set of pairs of activities with precedence relations. A multi-objective programming model is built under the resource restricts and precedence rules of activities constrains in order to balance the completion time and the total cost of the project. (2)  Suppose that the increased quantities of resource are independent uncertain variables with regular uncertainty distributions. The uncertain resource constrained time-cost trade-off problem can be described as following optimization model: formula_0 In the above model, objective ○ 1 is to minimize the project total completion time; Objective ○ 2 is to minimize the expected project total cost which consists of the activity cost and the additional resource cost. Constraint ○ 6 shows the range of decision variables. In order to transform the model into deterministic form, we introduce the following several theorems. [16]  Let G be an uncertain variable with regular uncertainty distribution H. If the excepted value exists, then +IGJ = K Φ MA (<) A = <. (2) For instance, let G be a linear uncertain variable, it has inverse uncertainty distribution H MA = (1 − <)N + <O. Then excepted value of G is formula_1 Theorem 2. [16]  Let G A , G P , ⋯ , G ) be independent uncertain variables with regular uncertainty distributions H A , H P , ⋯ , H ) , respectively. If the function Q(R A , R P , ⋯ , R ) ) is strictly increase with respect to R A , R P , ⋯ , R S and strictly decreaseing with respect to R STA , R STP , ⋯ , R ) , then formula_2 holds if and only if formula_3 To solve the model (1), according to the operational law of uncertain variables, we transformed it into its equivalent form, as shown below. formula_4 Proof: Because is an uncertain variable with regular uncertainty distribution Φ , and formula_5 by the definition of uncertain variable  [16] , we know that ? is formula_6 By Theorem 1, we know that the excepted value of ? By Theorem 2, we have ∑ ∈8 9 − − Φ MA (1 − <) ≤ 0，  (12)  i.e. formula_10 Therefore, the model (1) is equivalent to the model (2). In the real-life project, due to the influence of uncertain environment, managers should consider the trade-off between the completion time and cost. By describing the increased quantities of resource as uncertain variables, an uncertain resource constrained project scheduling problem is discussed in this paper. Then an uncertain optimal model was built with objective of minimizing the completion time and the cost with resource constrained based on uncertainty theory. To solve this model, we transformed the uncertain model into equivalent form and proved it. Finally, we used genetic algorithm to search quasi-optimal solution of the model and gave a numerical example to illustrate the validity of the model.
paper_462	To deepen the reform of clinical medical personnel training in an all-round way with the cooperation of medicine and education is the strategic adjustment direction of clinical postgraduate education in China. Since 1998, the postgraduate education for master's degree in clinical medicine has been piloted in China. Through improving the conditions of enrollment, innovating the training mode, adjusting the award marks and improving the system guarantee, it has effectively constructed the professional degree education of master of clinical medicine and standardized resident training. Chongqing Medical University was founded in 1956. It was founded by the former Shanghai First Medical College (now Shanghai Medical College of Fudan University) and moved to Chongqing. In 1998, it became the first batch of doctoral and master's degree in clinical medicine in China. Pilot units for professional degrees. Since 2009, the school has been focusing on reforming the training mode of clinical master degree postgraduates (hereinafter referred to as clinical master). For a long time, the medical postgraduates trained in our country are seriously lacking in practical operation ability, resulting in the embarrassing situation that "medical doctor will not see a doctor". The development of postgraduate education of clinical medicine degree in China is faced with the characteristics of short time, complex training objects, multiple administrative departments involved in medical degree education, and diversified training channels  [2] . It has effectively solved the problems of imperfect training mode of clinical master, lack of mature training mode of high-level clinicians, low overall training quality of clinical master, inability of clinical master to apply for the qualification of practicing doctor, difficulty of effective connection with regular training, difficulty of seamless connection between professional degree education and industry admission standards, and inadaptability of current management system and mechanism to professional degree postgraduates. To vigorously develop professional degree postgraduate education and improve the quality of training high-level applied talents is the focus of the current degree and postgraduate education work. First, after one year's clinical training in the tertiary and first-class affiliated hospitals of the school, the clinical master who has not obtained the license of a licensed doctor is allowed to apply for the qualification examination of a licensed doctor, which solves the contradiction that the clinical master cannot obtain the license of a licensed doctor during the period of study  [5] . Thirdly, we allow our clinical master not to take the entrance examination, but to be directly incorporated into the standardized resident training system, so that they can participate in the standardized resident training. Under the above policy guarantee, the clinical master trained by the school has the dual status of postgraduate and regular trainee. After completing the relevant training content and passing the examination, the qualified certificate of licensed physician qualification and resident standardized training can be obtained. After completing the dissertation defense and meeting the award requirements, the graduate certificate and master's professional degree certificate can be obtained. To ensure the quality of clinical master's training, the school has set up a guidance group composed of the first tutor and rotating responsibility tutor. Effective management during the transition period. The staff of the Graduate Management Department of the school conducted a thorough investigation on the effect, problems and suggestions of the training mode of clinical master's degree in various clinical departments, and held dozens of meetings to solve common problems. In order to speed up the development of professional degree postgraduate education, the school has formulated an enrollment policy conducive to the development of professional degree  [9] . At the same time, the workload of guiding professional degree postgraduates is directly linked to the promotion of their professional titles. Through the above measures, the problem of low enthusiasm of faculties and tutors in guiding graduate students with professional degrees has been solved, and the construction of professional degree tutors has been accelerated. Aiming at the characteristics of professional degree postgraduate education and adapting to the innovation of its training mode, the school has established a funding system different from academic degree postgraduates, and continuously improves the standards and coverage of grants. Excluding very few phenomena, the school grants cover 100% of the total, and the living allowance for clinical master's degree has also been greatly increased. Therefore, the school has improved the conditions for applying for clinical master's examination, so that it is consistent with the admission conditions of residents' standardized training: undergraduate majors should be clinical medicine, full-time national education series undergraduate graduates, to obtain bachelor's degree. After the admission of clinical master, the school will submit the list to Chongqing Municipal Health Bureau, and register it to Chongqing Municipal Health Bureau resident standardized training registration system without test. The reformed curriculum system not only meets the training objectives of clinical master, but also meets the requirements of theoretical knowledge for trainees. It is closely related to clinical practice. The curriculum involves many independent modules, such as clinical research methods, clinical diagnostics, internal medicine, surgery and so on  [11] . Students are brought into the "two levels, two stages" training after they enter school. According to whether the students have obtained the certificate of licensed physician, they are divided into two levels, and different awarding standards are formulated. Requirements for the first stage of training. Students who have obtained the certificate of resident standardization training are directly transferred to specialist training (i.e. How to objectively and effectively assess the clinical competence of postgraduates is the key to ensure the quality of clinical medicine degree award. The main reason for this phenomenon is that since the trial implementation of clinical medical professional degree, the state has not evaluated the quality of the pilot units and lacked an effective monitoring mechanism. There are mainly "pre-job training", "centralized departure assessment", "stage assessment" and "annual assessment"  [13] . The Interim Measures for the Implementation of the Regulations on Academic Degrees of the People's Republic of China promulgated in 1981 stipulates that before the postgraduate's reply, the degree-granting unit shall employ one or two experts from the disciplines concerned with the thesis to review the thesis. The clinical master emphasizes on examining the clinical competence of postgraduates. If the reviewers are hired, some experts will evaluate the papers according to the requirements of scientific degree papers. The staff of the Graduate Management Department of the school conducted a thorough investigation on the effect, problems and suggestions of the reform of the training mode of clinical master in various clinical colleges and departments, and organized a meeting to solve some common problems. Thirdly, we should formulate an enrollment system conducive to the development of professional degrees. The number of professional degree postgraduates enrolled by tutors in clinical departments is clearly required. All tutors working in clinical departments must enroll professional degree postgraduates. There are three difficult problems in the training process of master's degree postgraduates of clinical medicine specialty: first, the graduates of clinical medicine specialty with bachelor's degree must work in medical institutions for one year before they can apply for the qualification of practicing physician; second, the graduates with bachelor's degree can't obtain the qualification certificate of practicing physician and can't carry out clinical training; third, some of the graduates with bachelor's degree must work in medical institutions for one year. By promoting the combination of clinical master training and regular training, the problem of clinical master qualification certification has been solved, and a new "5 + 3" training mode has been constructed. The school has thoroughly reformed the training mode of clinical master, organically docked school education with post-graduation education and lifelong education, promoted the seamless docking of professional degree education with industry access standards, accelerated the internationalization process of higher medical personnel training mode, and formed a new systematic training mode for clinical doctors, which has great application value in the whole country. In view of the characteristics of professional degree postgraduate education, the school grasps the development trend of postgraduate education, and closely combines the admission criteria with the industry admission criteria in all aspects of enrollment, cultivation and award of posts, which ensures the complete docking of personnel training and qualifications, and provides mature experience for the seamless docking of professional degree education and Industry admission criteria in China. It provides a mature reference model for the docking of professional degree postgraduate education and industry access standards in other fields in China. It fully guarantees the quality of clinical master training and brings up a large number of high-level applied medical talents for Chongqing and even the whole central and Western regions, which has produced remarkable economic and social effects. Since 2009, the school began to explore the reform of the training mode of clinical master in an all-round way. It has overcome many obstacles and steadfastly promoted the reform. Over the past five years, the reform has achieved fruitful results and basically achieved the expected goals. The main results are as follows: The reform breaks through the restrictions of relevant industry policies on the training objectives and modes of clinical master, and provides innovative modes for brothers to learn from. Dozens of brothers such as Fudan University learn from our experience. In the past five years, a total of 2063 clinical masters enrolled in the school have been integrated into the training. The employment rate of graduates has been guaranteed to be 100% for a long time. They have trained a large number of high-level applied medical talents for Chongqing and the whole central and Western regions, promoted the development of health undertakings in the central and Western regions, and produced remarkable economic and social benefits  [16] . At present, the school has completed more than 10 research reports and published more than 30 academic papers on the sub-project "Construction and Practice of Quality Assurance System for Medical Degree Postgraduates" of the Ministry of Education Innovation Project. Our school has 10 affiliated hospitals of Grade A and 28 key national clinical specialties. Since the reform of training mode, the enrollment of clinical masters in our university has been increasing rapidly. In the past five years, a total of 2063 clinical masters have been enrolled in the standardized resident training system. The "5 + 3" reform of clinical medicine master's degree in Chongqing Medical University has established a "one goal", that is, to train "doctors who really can see a doctor", and established the basic principles of the reform of training mode of clinical medicine master's degree and the training of clinicians. The establishment of "two sets of systems", namely curriculum system and assessment system, has solved the problem of insufficient clinical practice ability of master of clinical medicine and improved the overall quality of training. The implementation of "three combinations", namely, the combination of enrollment and enrollment, training and training, degree award and physician access. The "four certificates association" should be implemented, namely "licensed physician qualification certificate", "standard training qualification certificate", "diploma certificate" and "degree certificate"  [17] . We should build "five guarantees", innovate the management system and mechanism, and ensure that the reform is in place. It has effectively solved the problems of imperfect training mode of clinical master, lack of mature training mode of high-level clinicians, low overall training quality of clinical master, inability to apply for the qualification of licensed physician, difficulty in effectively linking up with regular training, and difficulty in seamlessly linking professional degree education with industry access standards. Through a series of reforms, it has achieved relatively ideal results and accumulated rich experience in reform. It hopes that through continuous exploration and reform, it can play a role in attracting more brothers.
paper_476	The results show that: (1) The cost and resources operational efficiency are closely related to the order splitting ratio under the same delivery frequency; (2) The delivery frequency has different effects on the operational efficiency of the resources in different links. (3) Through the proper splitting and loading of orders, staff scheduling, and area adjustment of delivery station, the optimal delivery frequency can be achieved under the balance of cost and resource operational efficiency. In order to reduce the operating costs of logistics enterprises on the basis of ensuring service levels, one should first properly allocate the amount of orders and splitting ratios to achieve an economic increase in the delivery frequency. As the volume of orders and consumers' demand of rapid delivery services increasing, express companies are required to increase the daily delivery frequency to cope with the pressure of delivery during peak period and meet the consumers' needs effectively. The order volume, the orders' splitting ratio, the operational efficiency of delivery resource and the delivery costs will affect the delivery frequency. System dynamics models not only offer a different Operational Efficiency: A Case Study of Jingdong Logistics perspective while whole system approach to transport planning, but also demonstrate to policy makers the importance of these feedbacks and lagged responses  [1] . At present, the researches on delivery frequency, resource operation efficiency and cost utilization were mainly focusing on the following two aspects: (1) Delivery efficiency improvement through the choice of delivery model, And (2) Delivery link optimization. Wang Wei  [17]  analyzed the advantages and disadvantages of self-operated, outsourced, and federated modes in ecommerce delivery model, and constructed a system dynamics model for the combination selection of delivery modes by selecting six core factors, which can provide decision-making advice for managers to improve the delivery efficiency. Hongtao Yang and Jianbang Du  [19] [20] [21]  uses system dynamics to analyze the problems of supply chain and delivery efficiency from the perspectives of transportation routes, drivers and cooperation agreements, and its sensitivity analysis provides some reference to this paper for the corporate in this case to improve the delivery efficiency. Lin Wanting [22]  considers the phenomenon of unfilled orders, information leakage, and product damage which exists in crowdsourcing delivery, then built system dynamics model from five dimensions of reliability, responsiveness, assurance, empathy and security, which provide decision-making advice for enterprise delivery efficiency management. Then, it establishes a simulation model for the operation of the delivery system on the base of system dynamics. Next, it uses the simulation model to study the effect of changes in delivery frequency on delivery costs and resource operation efficiency under different quantity of delivery orders. Secondly, it constructs subsystems of cost and resource operation efficiency for the delivery activities in the boundary. Next, it analyzes the interaction between delivery frequency, cost, and resource efficiency, and builds dynamics simulation model to get the equilibrium of cost and resource operation efficiency under different delivery frequency. The relationship of the workflows in the delivery system is shown in  Figure 1 . As shown in  Figure 1 , JDL delivery process nodes in the area include: warehouses, three sorting centers and two delivery stations; resources include: storage facilities, sorting facilities, various transport vehicles and delivery workers. The delivery cost mainly includes the equipment usage costs and labor costs in the three stages of sorting, transportation and terminal delivery. The variable costs occurred due to the volume of orders, which include operating cost of the sorting equipment, rental cost of the site, units fixed cost of transport vehicle, fuel costs, toll and so on. Labor costs are composed of fixed wages and performance wages for employees in the three links mentioned above. The on-site logistics personnel is the person who puts the sorted goods into a mail packages and places them on a pallet and then sends it to the transportation vehicles. This article considers the utilization efficiency for the facilities, personnel, and technology such as turnover rate of transportation vehicles, the utilization rate of delivery site, and the delivery personnel loading rate, etc., which are put into the sorting, transportation, and terminal delivery links. The utilization rate of transportation vehicles is calculated by dividing the actual traffic volume at each sorting center by the vehicle capacity. The personnel load rate in the process is calculated by dividing the actual workload by the authorized workload. The delivery frequency refers to the number of times of terminal delivery by the company in unit time (in days). The increase in the delivery frequency will result in the following two changes: first, the increase in consumer satisfaction; second, the corresponding changes in the cost and resources operational efficiency. At the same time, the factors such as sorting time requirements, proportion of per-order batch, efficiency of the delivery personnel, and unit fuel consumption of the vehicle, will have a direct impact on the delivery frequency. After that, the validity of the model was verified though the error analysis by comparing the simulation data and actual operational data, which includes the average cost of sorting, transportation and terminal delivery. Different delivery frequencies have different splitting ways of total order quantity per day. The delivery frequency in scenarios 2 and 3 was increased by 9.5% over that in scenario 1. In scenarios 2 and 3, when the order quantity was lower than 3,500, the differences between the total delivery costs in Operational Efficiency: A Case Study of Jingdong Logistics different scenarios were very small; when the order quantity was higher than 3,500, the increased rate of cost in scenario 3 was higher for a while than that in scenario 2. The reason is that in scenario 3 the sorting center dispatched more vehicles in advance, which increased the cost of transportation. After the order volume reached 5,100, the total delivery cost in scenario 4 showed a downward trend and reached the minimum. Within the orders volume ruled in this paper, the delivery frequency and the cumulative delivery costs are positively correlated. The four scenarios showed a lower average unit cost when the order quantity was around 4,700 units indicating that this order quantity was a batch of economic orders. In  Figures 10 and 11 , it can be seen that the sorting cost of scenario 2 increased by an average of 157 yuan per day compared to scenario 1, which was due to the increase of delivery Higher frequency led to an increase in the total working time of sorting equipment and equipment costs. This is because that under the current total delivery volume, the order quantity after splitting in the scenario 4 was higher. The difference of average unit sorting costs among scenarios 1, 2 and 4 decreased as the order increased, which is because the change in the sorting costs under different order quantities was small and the difference in the delivery order quantity increased. Due to the splitting of order, the transportation costs of scenarios 2 and 4 remained unchanged when the order volume was small. In scenarios 2 and 4 the orders can be met by just increasing the frequency of existing vehicles, however, in scenario 1, new vehicles were needed to meet the transportation needs, which resulted in higher transportation costs. The main reason is that the increase in the delivery frequency improved the demand for the number of delivery personnel and caused the delivery time of delivery personnel to be shorter in a single batch. The vehicles utilizations of the three sorting centers in scenario 1 showed the first drop point when the order volume was 2530, 2567, and 2350, and the second drop point occurred at 5000, 5200, and 4800. The vehicle utilization efficiency for the three sorting centers in scenario 4 increased with the order volume since only one transportation vehicle was required to be dispatched from each sorting center within the current order volume. As a result, the company had to increase the number of delivery personnel and the area of shipments. The results of this study indicate that under the impact of order volume there is no fixed relationship between delivery frequency with delivery cost and resource operational efficiency. With the same delivery frequency, the orders splitting ratio influences delivery cost and resource utilization efficiency significantly. The change of delivery frequency has different effects on the resource operational efficiency in different stages of the delivery process. With the same delivery frequency, different order splitting ratios affect delivery cost and resource operational efficiency. Therefore, JDL needs to consider the increase in delivery frequency, the increase in delivery costs, the overloading of resources, and the ratio of orders and splits. For example, when the JDL order volume fluctuates between 3086-3154, one should adopt scenario 2 or scenario 3; when the order volume is greater than 5100, scenario 4 should be adopted. In the same delivery link the demand for delivery resources can be different due to different delivery operational capacities. Therefore, JDL needs to consider different constraints when making delivery frequency decisions. In this case, if scenario 2, 3, or 4 is adopted, one can increase the total number of order shipments by 2,400, 1,400, and 4,450 units respectively without increasing the number of transport vehicles. When the delivery frequency is increased from 3 times per day to 4 times per day, scenario 3 should be adopted from the perspective of increasing consumer satisfaction; scenario 2 should be adopted when the area of delivery stations and the delivery personnel are tight. The delivery frequency has different effects on the resource operational efficiency in different delivery stages. Therefore, JDL should combine the simulation results with the actual situation before making the delivery frequency decision.
paper_479	Handwriting is also a key element in document examination as it leaves a forensic document examiner with the task of determining who the writer of a particular document is and this is achieved through the likelihood ratio (L R) paradigm. This research employed back propagation neural network (BPNN) to model the writing pattern of individuals with input layer as the features of handwriting characters, two hidden layers of three neurons each, activation function sigmoid (s) and an output handwriting. With the help of handwriting model for individual writers, little or no assumptions and no nuisance parameters were employed in achieving full likelihood ratio for comparative handwriting analysis in forensic science. From the research carried out, it can be concluded that modeling an individual's handwriting is a crucial factor in achieving a full likelihood ratio, little/or no inconclusiveness in result reporting and a less degree of disagreements for handwriting identification in a forensic environment. The likelihood ratio paradigm has been studied as a means for quantifying the strength of evidence for a variety of forensic evidence types in handwriting and other types of forensic evidence such as earmark, speech, footprint, fingerprint, glass fragments and DNA  [1] [2] [3] [4] [5] [6] [7] [8] . For the handwriting evidence, previous literatures reported many factors and the inability of the forensic examiner to model the writing profile of a writer because it is often believed that an individual's writing profile is a latent characteristic that cannot be observed directly, and is not easily modeled because an individual's writing profile cannot have static characteristics and may change over time  [9] [10] [11]  which has resulted to adopt ad-hoc methods to compute likelihood  [9, 12, 13]  when full likelihood ratio is not achievable. The purpose of this paper is to illustrate how neural network approach to comparative handwriting analysis can greatly affect the outcome of a forensic investigation and make full likelihood ratio achievable. The task of verification, which is to determine whether two writing samples compared side-by-side, originates from the same person, was the principal problem addressed. A statistical model for writer verification was developed; it allows computing the likelihood ratio based on a variety of different feature types. [17]  based their approach on distribution of measurements from comparing items but their approach was similar to that of marginal L R when full L R was not an option due to the presence of nuisance parameter. [19]  described a statistical model for the writer verification task to determine if two documents were written by the same writer, their model followed the scenario proposed by  [17]  thus suffers from the same weakness  [20]  presented one conceivable way to deal with gauge a probability proportion in near handwriting analysis was delineated. The oddity of the methodology depends on producing reproduced composing samples from an accumulation of composing samples acquired from a realized source to frame a database for evaluating the dissemination related with the numerator of a L R . In a related work, similar examination of probability proportion based proof appraisal strategies in both evaluative and analytical procedures was carried out using a sample collected from female and male author. Assessment of signature handwriting evidence through score-based likelihood ratio based on comparative measurement of relevant dynamic features was carried out by  [22, 9]  presented score-based approaches to calculating forensic probability ratios that have been established as becoming more progressively common in forensic literature. [23]  Asserted that recent analytical developments paired with modern statistical computational tools have led to the proliferation of adhoc techniques for quantifying the probative value of forensic evidence. The handwritings were preprocessed using the Otsu method after which they were segmented into different words using the Sobel edge detection algorithm. Features were extracted via local binary pattern from each clustered characters, while the back-propagation neural network was used to learn the writing pattern for each writer and these writing pattern were then stored in the database. The target will help to know which handwriting is original and disguised. BPNN is designed to process information on how the human brain processes information by gathering its knowledge by finding trends and associations in data and learning from experience. Various things come together to form a BPNN, such as hundreds of single units, artificial neurons or processing elements (PE), connected to coefficients (weights), which represent a neural structure and are organized in layers. The transition of the neuron functions determines the behavior of the neural networks, the laws of learning and the structure itself. The activation signal is passed through the transfer feature to produce a single neuron output. In addition, that each individual's writing pattern was modeled using BPNN with the algorithm below, as BPNN can combine and incorporate both literature-based and experimental data to solve problems. Each character variable has a weight W i which shows its contribution in the training process. The feature information extracted from student handwriting through LBP was entered in NN through input layer and the participation of each class of character variables is calculated by the hidden layer of the network using: formula_1 net j is ith type of factor, n is the number of factors in net i , & ' ! This study used a network concealed with n input points, k 1 and k 2 hidden, and m output units. The weight from input points i and two hidden unit j is ! " Weight from second hidden unit i and output unit j is ! ) Weight of additional edge for each unit is bias− θ, where input unit and output vector from the hidden layer are expanded with a 1 component as seen in  Figure 2 . After randomly selecting the weights of the network, the backpropagation algorithm is used to measure the necessary corrections. Step 1: Feed-Forward Computation The vectors & " , & ( and & ) are computed and stored, evaluated derivative also stored. Step 2: Backpropagation to the output layer This research looked for the first partial derivatives 12 1 ! ) This research work was able to model the handwriting for individual in the presence of large-scale database using the back-propagation neural network (BPNN). An inconclusive state is declared if and only if L R value = 1 With the BPNN algorithm to model handwriting pattern for each writer and L R estimation described in  [10]  this paper was able to deal with the problem of inconclusiveness as there was no inconclusive results reported in the investigation, determine who the writer of a questioned document is as well as eliminate the inclusion of parameters considered nuisance in investigation because each writer's handwriting profile could be ascertained due to the BPNN training of the Handwriting categories. Table 1  shows the results of our Estimated Likelihood Ratio (ELR) for a writer against any other author in a collection of appropriate databases where the upper confidence interval (CiU) and the lower confidence interval (CiL) are indicated. This research shows that modeling handwriting pattern for a writer is an important factor in achieving a full L R which will eliminate the use of nuisance parameters in the computation process, lower the disagreement rate with respect to the Hypothesis in support of the prosecutor and also produce no inconclusive result after estimation of L R .
paper_492	Based on known functions of neuroscience the neural network that performs serial parallel conversion and its inverse transformation is presented. The activity of the upper neural networks changes in response to the context structure inherent in the time series data and have both function of accepting and generating of general time series data. Eating behavior in animals in the early stages of evolution is also processing time series data, and it is possible to predict behavior although be limited short term by learning the contextual structure inherent in time series data. Although status of nervous system of the animal change according to the recognition by sensory organ and to the manipulation of the object by muscle in the vicinity of the animal itself, the evolved animals have in addition another nervous system so-called long-term memory or episodic memory being involved experience and prediction. By hierarchical connection of the circuits, it is possible to accept and generate general time series data. The nervous system related to short-term memory is activated in synchronization with environmental events, but the nervous system involved in long-term memory is highly layered to form an image corresponding to past and future events. Whether the eating behavior is evaluated as intellectual behavior aside, it is possible to mimic this degree of behavior by electronic work at the junior high school level by combining sensors and logic ICs. In the following, rather than the engineering usefulness, to extend the function of the circuit according the evolution of the neural system of the animal. The eating behavior of animals evolved from the aforementioned animals is composed of time series of actions such as extending arms toward the target, opening the palm of hand when approaching the target, closing the palm to grasp the target and bringing to the mouth. The recognition technology of the figure has evolved by the neural network which starts with the perceptron, but it is forced to judge by the relation of the part and the whole as in the example of kanji when the scale of the figure increases. In other words, a trade-off is made between the recognition power of the part in the complex figure recognition and the processing power of the data sequence of the recognition result, and by learning the animal might got the most efficient processing method. It is desirable that the behavior of any neural network system can be expressed by combinations of the simple action parts of animals in the early stages of evolution. In this chapter, it is presented that arbitrary time series data can be divide into basic sequences as the logical basis of neural networks. Finally, it shows the hierarchically connected neural network that can process for general time series data. The neural circuit corresponding to each basic subsequence can be easily configured in the neural network, leading to the realization of processing by hardware of general time series data. Because the elements activated by c 1 is randomly connected to input, not all elements activated by c 0 are additionally activated. Other portion will return to the initial state because no activation factors (may be activated by another time series data). The number and their position of the activated elements is the conversion output corresponding to the serial input  [1] . In the neural network shown in  Figure 2 , the elements are activated one after another by the time series data (in this case, the basic subsequence) from below, and the result is output upward. Its output is the result of the serial parallel conversion, it is the result of AND logic of the output of the activated elements. Therefore, the couplings between the elements are enhanced (Hebb rule) by repeating this conversion, as a result the elements involved in the conversion will be activated by only receiving the first element of the time series data. This operation is a generation of (learned) time series data. On the flow direction of data, this parallel to serial conversion is upside down with the serial to parallel conversion described above, but the basis of neural network operation is the same. When the state transition diagram of the  Figure 3  is seen as a serial parallel conversion, the bottom is inputs connected to such sensory organs. After the first data reception, the connected elements are activated as described above. The essence of the neural network which performs the parallel serial conversion and the neural network which performs the serial parallel conversion is the same except that the conversion result comes out downward or upward. On other words, both operations of the serial parallel conversion and the reverse conversion are essentially the same on the point both operations are triggered by the first data and proceed waiting for input state change. For each basic subsequence a neural network that accepts and generates the basic subsequence is considered. Since general time series data is consisted of hierarchy of basic subsequences, basic units can process general time series data by identifying the outputs of lower layer basic units as the new time series data. Animal's behavior is considered a time series data consisted of the data that is couple of behavior data and received data. The time series data handled in the first 2 steps are consists of various stimuli of the internal and endocrine system, as well as data that captured by sensory organ from the environment in which the animal is placed. After time sequence data learning, animals are possible to quickly start the operation that continues only by receiving the beginning of the time series data to be corresponding a slight sign. In brief, images were advantageous even if an organism were not conscious of the images formed within it. Animals in the early stages of evolution will spend most of their living time obtaining food and avoiding danger. Animals with some evolved sensory organs must recognize, for example, environmental changes from sunrise to sunset as repetitions of a time series data. One of the factors that made the language possible to dramatically expand the range of expression is that it can be used as expression of object even if it is not near. For example, a family might have conversation like this toward you who is about to have a birthday the next day. "Tomorrow I'd like to buy a birthday cake at Store A to celebrate you with everyone." "Let's ask store A to put a chocolate plate on the cake with your name on it." And you'll think of the shape of the cake and the action of lifting the chocolate plate and mouthing it. On your birthday day, you may want to see the cake in front of you, identify the cake from store A, which was the one that was the subject of a conversation with your family the day before, and worry about the difference from the previous day's expectations. Because the episode about the cake is remembered, communication is possible and feelings are transmitted. One is time series data based on visual information of the cake in front of you and muscle movements that manipulate chocolate plates, which is produced by the nervous system that animals have from the early stages of evolution. The other is generated by the nervous system that is called " image" by Damasio, is time series data based on the shape of cake produced from family conversations the day before. While the behavior of the former nervous system is reflection of the visual information and movement of the objects nearby, the latter nervous system not only no needs to be synchronized with the former, but also moves independently. Like talking about your childhood while eating cake. However, if the nervous system of the episodic memory is activated by remembering the A shop and the chocolate plate while looking at the cake, the difference between reality and the expectation might become a problem. Stimulus from the sensory organs from the bottom becomes time series data and is transmitted to the upper part. The part shown in red is a part that is particularly activated, the lower red disk is the part activated by the visual data of the chocolate plate that placed in front, the upper red disk is the part that is activating by recalled episode about the chocolate plate. As a result, the shape of the chocolate in the episodic memory is identified as the thing seen in front. When studying objects that are intertwined with portion and whole system such as the nervous system, the idea of category theory can be incorporated to develop the whole without focusing on the details of the object. In neuroscience, the morphism corresponds to synapses and axons, and is responsible for the transmission of information between objects. Thus, the behavior of both categories can be migrated to each other. In this paper, short-term memory and long-term memory are both regarded as time series data brought about by activity in the brain. It is a short-term memory to be involved in the recognition and manipulation of the thing in the vicinity of the animal itself. From among the random connections, the necessary connections for the desired operation are selected and enhanced, and the target function is realized. It can be said that the essence of the logic of the operation exists not in the basic unit, but in the connecting situation among basic units selected from randomly initialized connection. Therefore, even if the circuit is partially damaged, it is possible to supplement by learning the function of the peripheral circuit is lost. From the viewpoint of neuroscience, even if there are no "parts" equivalent to the basic unit in the process of stimulation from the sensory organs propagating to the cerebral cortex, axons parallel to the propagation direction connect between the layers, and it can be said that it is a passage of serial parallel mutually converted information. Constructing neural networks based on known functions of neural circuits, including the Hebb rule, and considering the correspondence between the movements of biological neural circuits will lead to new discoveries in both fields.
paper_507	Landslide is defined as a slow to rapid downward movement of instable rock and debris masses under the action of gravity. The area chosen in the present study is Uttarkashi district of Uttarakhand, suffering from frequent landslides every year. In present study we used the already existing topographical maps, satellite imageries and field work. The main aim of present study is to integrate the result of our study with spatial data, soil parameters, land inventory and used the output as a user friendly application using GIS which could predict the future susceptibility of region to landslide and% contribution of each factor for the same. Efficiency of the application will be calculated by the help of previously acquired data of the study area at different places and then the reliability of the application will be judged. A frequently used definition of landslide is "movement of mass of rock, earth or debris down a slope" in the words of Cruden  [1] . They are the catastrophic phenomenon taken lives of many a hundred and destroyed the hard earned money, disrupting the communication facilities. The active areas include Himalayan region of India and the process of development of this region thus slows down. Uttarakhand is an appropriate choice for the study since the newly developed state has been fighting with the catastrophe and is making front paper headlines for landslide at Vishnuprayag, Baldora, Lambagharchatti, Jharkula, phatabyung, and Amiya landslides  [4] . The recent landslide of 2012 took a heavy toll on life and property, many people lost their lives, and thousands of tourist were stranded due to disrupted communication services  [5] . The phenomenon can be easily classified and described by two nouns. The result obtained i.e. vector maps could be used for accessing the features for a particular location. Identification of factors which affects to the landslide. Preparation of a landslide hazard zonation map that would divide Uttarkashi into different zones depending upon the factors. There are 793 villages with area drained by major river (s), Yamuna, Ganga. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area (Mohammad Onagh, 2012). slope, aspect, lithology, rainfall, land cover etc. were generated using ERDAS and ARCGIS v. 9.3. Four control points were selected at the corner of the concerned points, the geo-referencing of these coordinates was done by finding the coordinates from the Google Earth. DEM (Digital elevation model) was obtained from BHUVAN. The chosen data are related with the various factors causing landslide at a place. in the form of images and have to be converted into the Vector Form for use in the susceptibility analysis. Four control points are selected on the four corners of the map such that the points mark the spatial extent of the whole map as shown: formula_0 . An artificial neural network is a "computational mechanism able to acquire, represent, and compute a mapping from one multivariate space of information to another, given a set of data representing that mapping". The ANN is a black box model is a multi-layered neural network, which consists of an input layer, hidden layers, and an output layer. An artificial neural network "learns" by adjusting the weights between the neurons in response to the errors between the actual output values and the target output values. A neural network consists of a number of interconnected nodes. Each node is a simple processing element that responds to the weighted inputs it receives from other nodes. In the present study we selected 107 points and all the six factors namely soil depth, soil type, rock type, land cover, slope and elevation and a excel database is created. The dataset is categorized into 60% training and 40% validation. In the study we used multiple layers with structure of 6 X 20 X 1 that is 6 input and 1 output neuron. ANNs can be grouped into two major forward and feedback (recurrent) networks. In the former network, no loops are formed by the network connections, while one or more loops may exist in the back propagation neural network with feed forward approach. The data is categorized into training, testing and validation, and all the six factors namely soil depth, soil type, rock and a excel database is created. The dataset is categorized into 60% training and 40% validation. In these points we categorized them into two categories landslide prone and non-landslide prone. Since ANN does not understand the 'landslide prone' and 'non-landslide prone' region we explain it by giving value '1' and '0' respectively. As we have seen neural network can compute the output for a given input. However, this is possible only if we know the coefficients called weights. Here, "feed-forward" denotes that the interconnections between the layers propagate forward to the next layer. To calculate weights for different factors we have to train the network, thus interrelationship between the nodes of different factors are given. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer (20), and between the hidden layer (20) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). In the training process we change the weights in that way in which the network output and the true values get closer and closer to each other. For a new dataset the weights are unknown. Most of the training datasets met the 0.01 RMSE goal. For easy interpretation, the average values were calculated, and these values were divided by the average of the weights of the some factor that had a minimum value. This study was concerned to the region of Uttarkashi due to the limitation of resources and time, we have been able to generate the results for a limited area Rishikesh Uttarkashi-Gangotri-Gaumukh route from latitude 78°19'55.14'' to 78°47'36.27" and longitude 30°32'30" to31°1'9.33". It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area  [13] . slope, aspect, lithology, rainfall, land cover etc. were generated using ERDAS and ARCGIS v. 9.3. We have used only six factors to limit the bulk of data. An artificial neural network technique was used. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer  (15) , and between the hidden layer (15) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). Other values for landslide susceptibility for the adjoining areas have been calculated using interpolation technique therefore the Rishikesh-Uttarkashi-Gangotri-Gaumukh route has been mapped for landslide Hazard. With the further advancement in such type of study, we could interpret results for future from past records, if the site is inaccessible, or the test results are erroneous. Here we have used the already existing topographical maps, satellite imageries and field work integrating them together using GIS and ANN MODEL to create a database that has generated the output for the future use. The result of present study with spatial data, soil parameters, land inventory and presented by a landslide hazard zonation map and a user friendly application using GIS that could predict the future susceptibility of region to landslide and percentage contribution of each factor for the same.
paper_1	A cluster is a collection of data objects that are either similar to one another in the same group or dissimilar to objects in other groups. Some of them include summarization, compression and k-nearest neighbor which localizes search to one or a small number of clusters. Clustering is the concept that was used in this research to create groups from social media data which can be used for learning on electronic learning platforms. The system design methodology used was incremental prototyping. In incremental prototyping, the whole requirements are broken down into building blocks which are incremented each time a new component is integrated based on an overall design solution. The second task involved identifying the right kind of data to use for training the expected prototype as well as testing it. The third step involved using the data already preprocessed above to train the prototype. The relationship between the data items can be established using the k-nearest neighbor technique. They were used to confirm that the system indeed accurately did the classification given some data items. The illustration of the proposed prototype is given below. The results reported in this paper were obtained from a series of evaluations that were done on the classifier on different parameters including functionality, usability, accuracy, precision and recall. This is the section that captured the users view on the functioning of the prototype. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . A good number of learners indicated that they would continually use the system for the purposes of group formation and discussion. This is summarized in the chart below. In experimenting with the Naïve Bayes Classifier, we relied on the NLTK module which provides functions for calculating these measures for the classifier. A total of 200 tweets were extracted and used for this test which was summarized in a confusion matrix. The results below illustrate a summary of what was obtained when 200 tweets were used to test the Naïve Bayes Classifier. These measures can be improved if large amounts of data are used to train the classifier before being used to do actual classification. Through the study, it can be underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. They therefore come with a lot of challenges including time wastage. Through social media a better and more efficient way can be used to enable online learning generally and group formation specifically. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. The classifier then grouped the users into different categories based on various tweets that they posted on the task. The classifier was also able to assign other or new users groups also according to their tweets and the learning that the system had undergone. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. The study has proved that it can actually be used constructively in learning in various institutions. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. They therefore come with a lot of challenges including time wastage. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_2	This creates unwanted congestion during peak hours, loss of man-hours and eventually decline in productivity. In addition to this, the current traffic light systems encourage extortion by corrupt traffic officials as commuters often violate traffic rules because of the insufficient time allocated to their lanes or may want to avoid a long waiting period for their lanes to come up. This research is aimed at tackling the afore-mentioned problems by adopting a density based traffic control approach using Jakpa Junction, one of the busiest junctions in Delta State, Nigeria as a case study. The developed system uses a microcontroller of PIC89C51 microcontroller duly interfaced with sensors. The signal timing changes automatically based on the traffic density at the junction, thereby, avoiding unnecessary waiting time at the junction. The sensors used in this project were infra-red (IR) sensors and photodiodes which were placed in a Line of Sight configuration across the loads to detect the density of the traffic signal. The density of the vehicles is measured in three zones i.e., low, medium and high based on which timings were allotted accordingly. Traffic congestion is a common occurrence in many major cities across the world, especially in third world cities, and this has caused untold hardship to commuters in these cities in diverse ways  [1] . These include loss of man-hours, accident, missed opportunities, noise pollution, air-pollution, increased fuel consumption, increased tendency to violate traffic rules, and in some cases extortion by corrupt traffic control officials. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. There are other problems as well, like ambulances getting caught up by a red traffic signal and wasting valuable time  [2] . Sometimes higher traffic density at one side of the junction demands longer green time as compared to the standard allotted time. The suggested case study, Jakpa junction is a typical example of a traffic congested area. In order to overcome the aforementioned problem, this research adopted a density based approach in controlling vehicular traffic. The proposed system would use a microcontroller of PIC family duly interfaced with sensors, to change the junction timing automatically to accommodate movement of vehicles, thereby, avoiding unnecessary waiting time at the junction. The density of the vehicles is measured in three zones i.e., low, medium, high based on which timings were allotted accordingly. A traffic control model using signaling system in a discrete cross-road with NE-555 timer circuit was implemented by  [3] . The uniqueness of this model lies in the implementation of on-demand Pedestrian-Pass signaling, thereby transforming the design into dynamic controller. However, this system lacked inbuiltmechanism for controlling vehicular traffic based on density. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. Sometimes higher traffic density at one side of the junction demands longer green time as compared to standard allotted time  [4] . The image captured in the traffic signal is processed and converted into grayscale image then its threshold is calculated based on which the contour has been drawn in order to calculate the number of vehicles present in the image. After calculating the number of vehicles we will came to know in which side the density is high based on which signals will be allotted for a particular side. This is very important when the complexity of a task (problem) exceeds a certain threshold. Real world complex problems such as human controlled systems involve a certain degree of uncertainty, which cannot be handled by traditional binary set theory. The algorithm implementation was done using Mathworks, MATLAB software, and the results were simulated using a Simulink Tool to create traffic scenarios and comparisons between simple time-based algorithms and the developed system. For testing the adaptive traffic light controllers, a simulation system using Qt, C++ software integrated with MATLAB tools was developed. However, this method has no mechanisms for capturing traffic density and for providing a pass for emergency vehicles. This approach involves breaking down a system into smaller units to enable the designer get more insight into the system. This system was broken down into different units as listed below: (1) Power Supply Unit (2) Control Unit (3) Sensor Unit The complete circuitry is operated with TTL logic level of 0-5V. It comprises of a 0V to 12V transformer which is required to step down the 220V AC supply to 12V AC. This is converted by a bridge rectifier to a dc voltage. To isolate the output voltage of +5V from noise further filtering by a 220uF capacitor is done. The bridge rectifier consists of four single phase rectifier diodes connected together in a closed loop to form a circuit that is capable of converting AC voltage to DC voltage  [8] . The maximum output voltage of the bridge rectifier is known as the Peak Rectified Voltage, and is given as; formula_0 V be is the biasing voltage for the diode (i.e. The PIV of the diode is calculated using the relation formula_1 Where Vp = peak voltage of transformer Vrms = root mean square voltage The Vrms of the transformer is given as 12v formula_2 Recall PIV ≥ 2Vp Therefore, PIV of the selected diode is given as 2 x 16.9706 = 33.9412v For current, the maximum current of the transformer I (max) = 500mA The forward current of the diode I f is given as:   formula_3 As for the optocoupler, it is used to provide coupling while ensuring electrical isolation between its input and output  [12] . Another purpose of an optocoupler is to prevent rapidly changing voltages or high voltages on one side of a circuit from distorting transmissions or damaging components on the other side of the circuit. The Transformer steps down the 220 v AC supply to 12 v AC. This is rectified by the bridge rectifier, filtered by the capacitors to remove ripples and regulated by the voltage regulators to produce fixed value of 5 volts which is supplied to the system. The time delay in the traffic signal is set based on the density of vehicles on the roads. The IR sensors are used to sense the number of vehicles on the road. According to the IR count, microcontroller takes appropriate decisions as to which road is to be given the highest priority and the longest time delay for the corresponding traffic light. It is a widely used type of electronic prototyping board characterized by a 0.1-inch rectangular grid holes with parallel strips of copper cladding running in one direction all the way across one side of the board. The leads were then soldered to the copper tracks on the other side of the board to make the desired connections. The codes are as shown in the Appendix. Simulation was done via Proteus software. From the test carried out on the circuit, it was observed that the LEDs with the same color have equal timing, and that each pole of the four traffic light controlling poles, switches sequentially and repetitively until the circuit is disconnected from power. Jakpa Junction, in Effurun, Delta State, Nigeria is increasingly becoming chaotic by reason of the recurring traffic gridlock it experiences. The problem assumes a more worrisome dimension on Effurun Market days. On these days traffic rules are usually violated because of the complex traffic situation. One of such systems is the automatic signaling using IR sensors and Microcontroller. Although the aims and objectives of the project were achieved satisfactorily, it could be further improved upon. This new design would further reduce time delay, improve efficiency and reduce accidents by incorporating the following modifications: (1) The Airport -PTI road should have a two -way traffic light to pass the vehicles coming from Airport road to PTI road. There should also be a two -way traffic light between Effurun -Sapale road and Jakpa road to pass vehicles coming from Effurun -Sapale road to Jakpa road.
paper_3	The aim of this article is to present the general architecture trends of Web-based Adaptive Educational Hypermedia Systems (AEHSs) and to give a complete description of architecture of the AEHS MATHEMA. In the beginning, a related work on the architecture trends of Web-based AEHSs is presented. Then, a description of the aspects of the MATHEMA is done regarding both its pedagogical and technological part. Next, one-on-one unit is presented separately and their functions are generally described with respect to the adaptive Web technologies used. They build a personalized model of the learner and apply this model to adapt the content and/or the appearance of the hypermedia, according to the learner's specific characteristics, such as the cognitive goal, level of knowledge, pre-existing or prior knowledge, interests, preferences, stereotypes, cognitive preferences and cognitive or learning style  [4] . In the Web-based AEHSs, several adaptive and intelligent techniques have been applied to introduce adaptation, such as  [4] : (a) Curriculum Sequencing: It helps the learner to follow an optimal path through the learning material. (b) Adaptive Presentation: It adapts the content presented in each hypermedia node according to specific characteristics of the learner. The general purpose of the AEHS MATHEMA is to support learners through an interactive and constructivist educational environment in order to construct their knowledge in Physics and Technology, individually and/or collaboratively, and to overcome their misconceptions and/or learning difficulties AEHS MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and meta-adaptive navigation, interactive problem solving, and adaptive group formation. More specifically it supports: (1) adaptive presentation according to learner's learning style; (2) four adaptive navigation techniques which are direct guidance (uses a "next" button), link hiding (hide nonrelevant links), link annotation (use colors to indicate relevance), and link sorting (like in search engines); (3) a meta-adaptive navigation technique in order for the learner selects the more appropriate navigation technique that best suits him or her; (4) an interactive problem-solving method through personalized and collaborative problem-solving activities that exploits the teaching approaches of experimentation through simulations, guided inquiry and discovery, investigations and peer collaboration; and (5) a method of adaptive group formation for peer matching. The aim of this article is to highlight the main contributions of the architecture of the MATHEMA in the improvement of the functionality of AEHSs. The rest of the paper is organized as follows: In the section 2 related works for various models of architecture of AEHSs is presented. In the section 3 the architecture of the AEHS MATHEMA is presented in detail. For the AEHSs various models of architecture have been developed so far on which the designers of these systems are based on. The rules together form the adaptation model in AHAM. A key point in AHAM is that the adaptation rules are used to translate user actions into user model updates (as well as the presentation and adaptation of content). The rules are used by an adaptive engine in order to generate the presentation specifications. Today almost all AEHSs follow the general architecture of the AHAM reference model (but possibly with a different kind of rule or reasoning language and engine). The ADAPT project has identified high level design dimensions for AEHSs, which are: (1) context of use (CU), (2) content domain (DM), (3) instructional strategy (IS), (4) instructional view (IV), (5) learner model (LM), (6) adaptation model (AM), and (7) detection mechanism (DE). These dimensions form the axes on which both an AEH problem and its solution can be represented. Proper's architecture is a combined architecture of SCORM LMS and AEHS. The DM structure is exported by the manifest file and is stored into Java Object Files. Moreover domain independent data of UM is stored into the database while at the same time, data about user knowledge is stored into Java Object Files. The adaptive navigation techniques have been applied to it via Java servlets and Java Server Pages (JSP). All the runtime data about user actions and performance is stored into Java Object Files via JSP and Java servlets. The adaptation component consists of a Java servlet which automatically generates the individualized Webpage, each time an HTTP request is received by the server. WELSA does not store the course Web pages but instead generates them on the fly, following the structure indicated in the XML course and chapter files. Based on these preferences, the servlet applies the corresponding adaptation rules and generates the new HTML page. The Learner Model is the structure that contains the information on the learner's characteristics that allow the AEHS to adapt to these characteristics. The Presentation Generator requests a composition of the presentation to the JavaServer Pages. As JavaBeans are components of an application in the context of JavaServer Pages or servlets, they are suitable to implement the Presentation Generator and the Adaptation Decision Model. Additionally, the JavaBeans offer advantages of separating the programming logics from presentation, as for the Presentation Generator and the final page composition. The purpose of ULUL-ILM is to provide the AEHS that can recognize student' learning style automatically in real-time and then presents the learning content adaptively based on student learning style. The system then enables to present and recommends a variety of learning contents adaptively towards each of the student's learning style identified in the student model through the adaptation model. The system then analyzes the learning content on each of the learning material, and then comes up with the generated teaching strategies by means of the teaching strategy generator and fragment sorting. The adaptation model enables the system to adaptively presents the content, based on the student's learning style by combining the fragment sorting and adaptive annotation technique. The course player in ULUL-ILM enables the system to adaptively presents the content with various teaching strategies towards each of student's learning style. AEHS MATHEMA architecture is based on Web technologies similar to client/server distributed computing architecture of MIT and it is mainly supported by Apache Tomcat Server 5.5 and MySQL 6.0 relational database server. For supporting features, it uses the (JSP) pages, Java servlets, JavaBeans, JavaScripts, and Apache Struts technologies. This is an interface for Java that standardizes database connections, queries, and results of relational databases. Java servlet technology and JSP pages are server-side technologies that have dominated the server-side Java technology market; they have become the standard way to develop Web applications. A servlet is a Java class that implements the Java servlet API, a protocol to respond to HTTP requests. In the context of JSP pages, JavaBeans components contain business logic that returns data to a script on a JSP page, which in turn formats the data returned from the JavaBeans component for display by the browser. A JSP page uses a JavaBeans component by setting and getting the properties that it provides. The benefits of using JavaBeans components to augment JSP pages are the following  [10] : (1) Reusable components: Different applications will be able to reuse the components. The Apache Struts is a formalized framework for the architecture style design pattern Model View Controller (MVC). It is a set of pedagogical rules that combine the learner's model with the domain knowledge for adaptive performance. In the field of Adaptive Group Formation Module selecting of the most appropriate teaching strategy, the learner's learning style is taken into account during the learner's study. The domain knowledge of the AEHS MATHEMA is the basis of the system's adaptation. In the AEHS MATHEMA, the hierarchical representation of the domain model is adopted by the ELM-ART  [5] , as follows: The first level defines the cognitive objectives, the second the basic concepts and the third the pages with the corresponding educational material. The model that supports the AEHS MATHEMA is the overlay model. The characteristics that the system maintains in learner model for each learner are: name, gender, username, password, learning style, level of knowledge for each cognitive goal (general performance), level of knowledge for each basic meaning of the cognitive goal (quantitative and qualitative characterization), concepts that have been successfully evaluated for each cognitive goal, current page, current teaching method, current navigation technique, navigation history, meta-adaptive navigation status, initial Web experience and knowledge level (pre-existing knowledge) in the current cognitive goal, ways of presenting feedback messages, preference for assistance by Advisor in terms of its navigation, course sequence, and phases of the guided dialogues in interactive problem solving process in which each learner participates. The Student Model is transparent to the learner (open learner model) and is verifiable by the learner, that is, the learner can access his/her model and change the content of some characteristics. The learner during the study selects the cognitive goal he or she wishes to study, following his or her personal course in the educational content, exploits the system's assistance, navigating and studying, is informed about the information that the system maintains, who can change them by intervening and directing the system's adaptation in this way, in terms of curriculum sequencing, adaptive presentation and navigation and adaptive group formation, selects his or her learning style, selects the navigation technique, activates and deactivates the curriculum sequencing, selects the way that the feedback messages are presented, changes his or her knowledge level for each basic concept of the cognitive goal, and activates-deactivates the Advisor. The system, during the learner's study, monitors his or her interactions with the system and his or her assessment and accordingly updates the links to the course material provided and/or adapts learning material to learner according to his or her learning style. This is an innovation on the architecture of AEHSs. It is the second level meta-adaptation engine that is responsible for monitoring the cognitive improvement of the learner in the cognitive goal that he or she studies, after n successful evaluations of his or her knowledge in basic concepts, showing him or her advantages and disadvantages of the four navigation techniques that it supports. In AEHS MATHEMA, meta-adaptive navigation works as follows: The first time that the learner enters the system, he or she is asked to state his or her Web experience and level of knowledge in the cognitive goal he chose to study (preexisting knowledge). This module is responsible for transferring the training material to the presentation unit, for giving feedback, for creating guided dialogues and for evaluating the learner knowledge. It includes the content of the JSP pages, Java servlets responsible for feedback and creation of guided dialogs in problem-solving, Java servlets for creating evaluation questionnaires, and so on. In this design phase, the system does not follow a standard metadata description but the system supports data transfer objects. It is responsible for what will be presented to the learner as a result of the processing of the information that arrives from the adaptation engine, the meta-adaptation engine, adaptive group formation module, and application module. In this module there is the basic servlet, which deals with the presentation of the training material and other JSP pages and servlets, such as the page and the corresponding servlet to inform the learner about the characteristics of all learning styles, the servlet for the curriculum sequencing, the servlet for selecting and presenting the concept that the student selects to study according to his or her learning style, etc. More specifically it: (1) initializes and updates the Student Model; (2) collects and stores the data that the learner answers to open-ended questions, entries in tables, registers in notes, etc; (3) monitors the interactions between the learner and the system to get information about his or her choices in terms of the questionnaires, the phases of guided dialogs, the links, the options for meta-adaptive navigation, the visited pages, and more. Assessment of the pedagogical and technological aspects of the AEHS MATHEMA has been carried out. This article presents the trends of architecture of AEHSs so far and then an extensive description of the architecture of the AEHS MATHEMA. The implemented AEHSs so far use various techniques to implement their functions. Most of AEHSs presented in this paper are based on JSP, Java servlets, and Javabeans technologies to implement their functions, as implemented by AEHS MATHEMA. Furthermore, the presented AEHSs above have at least three main modules, such as Student Model, Domain Model, and Adaptation Model as the AEHS MATHEMA also has. The main contributions of the architecture of the MATHEMA in the improvement of functionality of AEHSs are the meta-adaptation technique, the adaptive group formation technique, the problem-solving technique, and the synchronous communication protocol that it supports.
paper_21	In this paper, we present the powerful scheme ZSISMP (Zimmermann Self Invertible Stabilizer Multiplier Permutation) to attack the hardness of the minimum distance search problem of BCH codes. This scheme consists in evaluating the minimum distance of the reduced dimension sub code fixed by a Self Invertible Stabilizer Multiplier Permutation by Zimmermann algorithm. The use of this efficient local search had yield to determine the error correcting capability of many BCH codes of length 1023 and 4095. In telecommunication and storage systems, the fundamental problem is the reproduction at one point exactly or approximately the selected data at another point. An efficient solution of this problem is the use of error correcting codes. BCH codes are a family of cyclic codes, which are used in many applications, due to their powerful algebraic decoding algorithms and their error-correcting capability. However, the determination of this metric is difficult in general as pointed out by Charpin in  [1]  and remains an open problem in coding theory. For these codes, only a lower bound is known and the minimum distance is known only for some lengths and special cases  [2-3-4-5-6-7-8] . In this paper, our work will focused on finding the minimum distance of large BCH codes. The remainder of this paper is organized as follows: The next section presents the main related works. The section 3 presents the proposed scheme ZSISMP. The section 4 presents the main results. The conclusion and possible future directions of this research are outlined in section 5. This section summarizes the most important ones. In  [9] , Augot, Charpin, and Sendrier presented an algebraic system constructed from Newton's identities. In  [10] , Augot and Sendrier found idempotent codewords of minimum weight for several primitive narrow-sense BCH codes. In  [11] , Canteaut and Chabaud have developed a new probabilistic algorithm, based on the heuristic proposed by Stern  [12] , for finding minimum-weight words in a linear code. The artificial intelligence Simulated Annealing presented in  [16] , Tabu Search  [17] , Hill-Climbing  [18] , Genetic Algorithm  [18] [19] , Ant Colony Optimization  [20] , Metropolis Algorithm  [21] , was shown to be useful to attack the difficulty of the minimum distance search problem for BCH Codes. This improvement has yield to a fast convergence of the Simulated Annealing by reducing the number of iterations, as well as obtaining good results in comparison with the previous works presented in  [17-18-19-21] . Unlike classical techniques based on exhaustive or partial enumeration of codewords, Berrou in  [23]  has presented an efficient approach based on the notion of Error Impulse response of a Soft-In decoder. This approach consists in adding to all-zero codeword a level of noise and considering the minimum distance as the smallest level of noise from which the Soft-In decoder fails in correction. The proposed method MIM-RSC, has allowed an efficient local search and therefore finding the true minimum distance of some BCH codes of length 1023 and 2047 as well as obtaining good results in comparison with the previous works presented in  [17-18-19-20-21-22 ]. The proposed scheme works as follows: Inputs: formula_0 Step 1: Find the sub code SC fixed by σ Step 2: Find the estimated minimum distance d of SC by using the Zimmermann algorithm. It is well known that the smallest primitive narrow-sense BCH code whose minimum distance is greater than its designed distance is BCH (127, 43, 29) and it is the only one for this length. The both last result have been proved in  [9] , by using the Newton's identities. In order to validate the proposed method, it is applied on all BCH codes of known minimum distance presented in table 1. the obtained results show that the minimum weight found by the proposed method is equal to the true value of the minimum distance of all BCH codes of length up to 255. Therefore, the proposed method is validated for BCH codes of lengths up to 255. These results demonstrate that the proposed scheme outperform greatly the famous Zimmermann algorithm. The table 3 presents a comparison between the proposed scheme and MIM-RSC method  [24] . The obtained results are given in the table 4 so that d f represent the minimum distance found by our scheme. In this paper, we have proposed a new efficient scheme to find the minimum distance for large BCH codes. The experimental results show that the proposed scheme outperforms several known powerful techniques. In the perspectives, we will apply this powerful scheme to construct good large cyclic codes, and adapt this scheme for other linear codes.
paper_31	This paper takes chronic diseases as the research object, and proposes a design of intelligent medical system for chronic diseases based on semantic matching by the adaptation of ZigBee technology in the front-end data acquisition. Through the ZigBee wireless sensor network, this system sends the physiological parameters collected by various medical sensors to the intelligent medical system, and innovatively proposes semantic matching algorithm to solve the queuing problem of data transmission, to ensure the accuracy of data transmission. This system employs the improved spatial vector model to process the data uploaded, and uses AES encryption algorithm in the process of data transmission to ensure the security of data transmission. With the means of the Internet, with the carrier of the mobile terminal, intelligent medical treatment founded on the units of family is a new trend. Therefore, it is of great significance to develop an intelligent data analysis platform to record, store, and share and handle various personal health signs in time through wireless transmission. For example, the General Hospital of the People's Liberation Army (PLAGH) communicated and discussed some diseases with foreign experts through satellites. With the great progress of mobile communication technology, Internet and WSN technology with low power consumption, some scholars have applied wireless transmission technology to medical detection system. In the meantime, some other scholars use 4G and the Internet transmission technology to send physiological parameters to the medical system through the Internet or 4G network, so as to realize medical monitoring. With the emergence of various new telecommunication technology, scholars kept providing health care using all sorts of communication technology. During this stage, information technology was connected with medical health care more closely. Haji Bagheri Fard present methods based on deep learning algorithms to achieve high classification accuracy  [8] . As an alternative for aiding healthcare systems, sensors and wearable devices are used for monitoring patient physiological data to help guide health services or the self-care of patients  [9] . (3) The problems of unitary monitoring data. The current WITMED system focuses only on data acquisition instead of processing uploaded data and returning the result. The growing trend of WITMED system shows that a variety of health signs data will be monitored, transmitted, processed and analyzed in real time, the system can process the uploaded data. Attracted by the broad prospect of smart medicine, BAT and other Internet enterprises have participated in the smart medical industry. Alibaba founded Alibaba health  [15] ; Tencent launched the first AI+ medical product Tencent Meiying in 2017; many large enterprises integrated medical resources through mergers and acquisitions to lay out the intelligent medical industry chain. The system processes the received information according to the relevant algorithm, and then feeds back the corresponding results to the designated user. The process of reading and transmitting data is a loop. The data acquisition structure charts After receiving the relevant data, the data analysis platform needs to analyze the corresponding data and feedback the processed results to relevant users. Data analysis roadmap is shown as follows: After collecting patients' physiological information from the node, various physiological sensors will continuously upload them to the coordinator. The general technique route of the algorithm is as follows: Propose a comprehensive sorting algorithm, on the basis of semantic matching and a first-come-first-served principle. Suppose there are N pieces of information to be sent according to the messages' arriving order, divide each message into many words, and mark each word as Nm, , . First, match the Word segmentation information by queued delivery with the taglib of the analysis system. If there are X words appearing in the taglib and N-X words not appearing in it, the matching degree of the information is (N-X) / N * 100%. Finally, the request is inserted into the message queuing sequence by the matching degree. The general algorithm of this part is as follows: Define priority function: f x, y ax by ε, with a basic definition as follows: x: semantic matching degree, 0≤x≤1 y: a request arriving at the serial position, 0<y≤1 a is defined as the weight of semantic matching degree, 0<a<1 b is defined as the weight of serial position of Request arrives, 0<b<1, a+b=1 ε is defined as the disturbance value, 0≤ε≤0.1, with the default as 0. Generally, a is close to µ= 1 2 , formula_0 Therefore, the process of queuing algorithm based on Semantic matching is as follows: With the hypothesis that a= 1 2 (to take one half of the sum) x= 5 10 (there are 10 participles in semantic information, of which 5 are matched), y 1 (Before a new message arrives, assumes that the request queue has 10 participles waiting, then: f x, y y a x y ε =0.294, therefore, the priority value of the message is 0.294. The patients' vital signs data will be transferred into the data analysis system by the sensor, and then the data will be analyzed by the big data system, various signs data will be compared with the characteristic values of related diseases and the results will be returned. This research proposes an improved space vector model algorithm to ensure whether the patient has a related disease by the similarity between the physical signs and the disease on the basis of calculation of the sign value, continuous sign value and the disease characteristic value. An improved SVM calculation method is as follows: Definition 1 (conception): Define a tuple F= (X, Y, Z), in which x is the collection of all objects is the object property set, Z (X*Y), Z is a subset of X*Y. Definition 3: The concept similarity is calculated as:  formula_1 The column mixed operation formula is shown as  Figure 6 : (4) key plus calculation Key plus calculation operates the 128 bit key is XOR bit by bit with the data in the state matrix. The process of key plus calculation can be regarded as the result of Bitwise XOR or byte level or bit level operation. In this study, a design scheme of intelligent medical system based on semantic matching is proposed, with the front-end data acquisition which adopts ZigBee technology, and a semantic matching algorithm is innovatively proposed to solve the queuing problem of data transmission to ensure the accuracy of data transmission. Through ZigBee wireless sensor network, the system sends the physical parameters collected by various medical sensors to the information control terminal, and the improved spatial vector model is employed to process the uploaded data in the process of data transmission by the intelligent data analysis platform, and the security of data transmission is ensured by adopting the AES encryption algorithm in the whole process. The intelligent system realizes data acquisition, data encryption and processing, and big data analysis. The system can gather and process big health data of chronic patients, and realizes the discovery, tracking and treatment of chronic diseases. The semantic matching algorithm and the space vector model algorithm proposed by the system can be widely used in the data acquisition of clinical big data to provide theoretical and technical support for artificial intelligence to assist disease risk prediction. The intelligent system realizes the data collection of the whole process of outpatient service, examination and treatment, which can provide data support for the precise treatment of diseases.
paper_38	A purposive sample of nine hundred and forty-three (943) first-year students constituted the population for the study were drawn from Computer Science, Mathematics and Physics undergraduate degree programmes from the Faculty of Science of the university who were admitted from the 2010/2011 to 2014/2015 academic sessions. The instruments for data collection were OL, UTME and first-year Cumulative Grade Point Average (CGPA) results, which were coded and analysed with the aid of Computational Statistical Package for Social Sciences (SPSS). The results revealed that with a weak correlation, OL is a good predictor on the CGPA, a dependent variable, for academic performance which holds true for students who are in the CGPA category of '1st class' and '2nd Class Lower' respectively. It concluded that the use of OL and UTME as instruments is not enough to select candidates for admission and therefore recommended that other instruments such as senior secondary school mock examinations need to be included as part of the entry requirements in the admission criteria. Education is an essential issue regarding the development of any country in the world. In Nigeria, the demand to acquire university education has been on the increase than ever before  [2]  due to the increase in the population of graduates from secondary schools  [3] . Each student is expected to have at least five credit passes in not more than two sittings in Mathematics, English Language and three other science-based subjects such as Chemistry, Biology, Physics and Geography at the Ordinary Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria Levels (OL) of either Senior School Certificate Examination (SSCE) which is conducted by West Africa Examination Council (WAEC) and National Examination Council (NECO). Despite all these, some students perform poorly in their studies during the first year while some perform very well. There have been various perspectives presented by researchers and psychologists about what is academic performance and its importance. Formula 1 is used for calculating the CGPA. The stakeholders agreed that the Pass Degree be abolished from the grading system and the lowest and highest CGPA scores are 0.00 and 4.00 respectively effective from the 2016/2017 academic session which implies that as long as the score is high, the better the academic performance of the students. As such, the CGPA is considered to be a good predictor indicator of a student's academic performance. Prediction and analysis of students' overall academic performance is a vital milestone in an educational environment that builds their future. The focus of this study is to predict full-time undergraduate students' first-year Cumulative Grade Point Average (CGPA), which is one of the variables for measuring the academic performance by using entry requirements, such as Ordinary Levels and UTME, for Faculty of Science in Kaduna State University (KASU), Kaduna -Nigeria. This poor performance has lead students spending extra years before they could graduate with a pass degree at best. Determine if any of the following entry qualification used for admission, OL results only, UTME scores only or Post-UTME scores (average of OL results and UTME scores) best predict the academic performance of students in the 100 level CGPA examinations; ii. Investigate the relationship between the students' performance of their entry qualification and the academic performance in the 100 level CGPA examinations. The following research questions directed the study: 1. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? It is a yardstick that is used to ascertain the competences of a student from which his abilities could be determined. The authors further explained that academic performance is usually used to determine "how well an individual assimilates, retains, recalls and communicates his knowledge of what is learnt and acquired". There are a lot of definitions of students' performance based on previous works of literature. Reference  [9]  stated that students' performance could be obtained by measuring the learning assessment and co-curriculum. Academic performance or sometimes known as an academic achievement is defined by  [10]  as "Knowledge attained or skill developed in the school subjects, usually designated by test scores or by marks assigned by teachers". A student's academic performance usually is measured in either examinations or continuous assessment tests, and this is expressed in various ways depending on what the scores should be used for. The numerous ways of reporting academic performance include raw scores, percentages, transformed scores, or even as categorical variables such as Excellent, Merit, Very Good, Pass, First Class, Distinction, A1, B2, C4, F9, and others. In Nigeria, students are admitted into universities using their scores in the UTME as well as Post-UTME (PUTME) subject to having at least five OL credit passes in relevant subjects obtained in not more than two examination sittings including the English Language. Several studies have criticised the use of UTME and PUTME as an imperfect instrument for predicting academic performance of students. However, the following review of literature examined the relationships between UTME and PUTME scores as a predictor for the academic performance of students have revealed contradictions in their findings. A sample of 214 students records was used for data collection. The author's findings in his study showed that there was a consistent decline in the number of students admitted using the PUTME which cannot do better than UTME in influencing students' academic performance as the outstanding and weak students formed the upper 12.5% and lower 12.5% while the remaining 75% consists of the average students. The records of students who graduated in the 2009/2010 and 2010/2011 academic sessions from seven faculties were obtained using the stratified random sampling technique. The authors tested their nine hypotheses using an independent samples t-test and two-way analysis of variance. Graduates with the best academic performance from the preliminary programmes were those admitted through the certificate programme. Due to this, the recommendation was that the stakeholders should review the use of UTME and PUTME results for university admissions. The study by  [13]  found a significant relationship between students' scores in three examinations, namely: UTME, PUTME and 100-Level Psychology course, Faculty of Agriculture, Federal University of Agriculture, Makurdi, and thus concluded that the UTME has predictive validity for performance in the university. In the same vein,  [14]  tested the predictive power of the Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria JAMB UTME in predicting students' performance in the university's semester examination by using a regression model. The article recommended that JAMB should embark on a more realistic review of the content of the UTME to enhance its predictive validity. A population of 1650 students admitted into the university during the 2011/2012 academic session from Faculties of Arts, Education, Science and Social and Management Science was used to obtain their UTME, PUTME scores along with their GPA for eight semesters. The author concluded from his findings that the use of PUTME is beneficial for selection of candidates for admission and also that candidates who had a high-performance level in the UTME have a positive effect on the academic performance in the university. The author further recommended the need for the PUTME exercise to be strengthened to have a fruitful admission of candidates. One major shortcoming of virtually all the studies reviewed is their scope in time and spread/coverage. This section discussed in detail the methodology employed such as research design, sample of study, instruments used, the procedure for the collection of data and data analysis. He explained that there are two types of ex-post facto research designs namely the correlational and the casual comparative. The design adopted in the study is the correlational ex-post facto, which is used to measure the degree of association between two or more variables or sets of scores. The Predictive Correlational Ex-Post Facto design was identified to be the most appropriate for the study since the results (CGPA, UTME and OL) of students in the Faculty of Science were used in reaching conclusions about the whole prediction of academic performance. The Faculty of Science consists of nine undergraduate B. Sc. The population of the study was limited to consist of all students admitted into three Faculty of Science undergraduate degree programmes of Kaduna State University for five academic sessions from 2010/2011 to 2014/2015 using the OL, UTME and CGPA results. The sample distribution is as shown in  Table 1 . The instrument used to derive the data for this study were: the JAMB UTME scores from 2010 to 2014 and OL grades in the five relevant subjects which were the pre-admission criteria and first-year CGPA results obtained from the semester examinations from 2010/2011 to 2014/2015 academic sessions. The UTME was wholly multiple-choice objective questions conducted via Computer-Based Tests (CBTs) by JAMB. The semester examinations were mostly essay type questions. However, the OL results obtained from the database was for students admitted during 2010/2011 to 2014/2015 academic sessions. The coding for the CGPA is also shown in  Table 4 . It was used in this research study. Since the focus of the study is to determine the predictive validity of OL results and CGPA scores (OL-CGPA), UTME and CGPA scores (UTME-CGPA), and PUTME and CGPA scores (PUTME-CGPA), the statistics employed on the extracted data were Multinominal Logistic Regression (MLR) and Pearson Product Moment Correlation (PPMC) coefficient. PPMC is used to determine the degree of relationship between two sets of variables and compute the strength of association between the variables  [19] . There are three types of linear relationship that may exist between these two variables namely positive linear correlation, negative linear correlation and no correlation. In other words, MLR is used to predict a nominal dependent variable given one or more independent variables. MLR can have interactions between nominal and continuous independent variables to predict the dependent variable. The results of the data analysis are presented in tables according to the research questions that guided the study in this section. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? Table 5  shows the summary of correlations coefficient between OL results and CGPA scores (OL-CGPA), UTME and CGPA scores (UTME-CGPA) and PUTME and CGPA scores (PUTME-CGPA) aimed at all the academic sessions for Computer Science, Mathematics and Physics degree programmes. For the Computer Science programme, as shown in  Table 5 , the correlation coefficient indicated a low negative correlation in UTME-CGPA (-0.092) and PUTME-CGPA (-0.001) and a low positive correlation for OL-CGPA (0.072). The indication shows that the nine correlation coefficients for this research question are very low out of which five have a low negative relationship. Likelihood Ratio Tests is a statistical test of the goodness-offit between two models. Table 7  shows the results of the Parameter estimates, which is also called coefficients, for the Multinomial Logistic Regression (MLR) for each degree programme. A Multinomial Logistic Regression (MLR) was performed to model the relationship between the predictors and membership in the six groups, i.e., CGPA categories (Fail, Pass, 3 rd Class, 2 nd Class Lower, 2 nd Class Upper and 1 st Class) for each degree programme. Each of the five equations for every degree programme in  Table 7  includes the intercept and the slope for the predictors. For Computer Science, Mathematics and Physics programmes, the first equation intercept is the log of the ratio of the likelihood of a student having a 'pass' degree to the likelihood of that student having a 'Fail' degree. Among the classification of degrees, each of the five subgroups for each programme, that is Pass, 3 rd Class, 2 nd Class Lower, 2 nd Class Upper and 1 st Class, are contrasted with the baseline group of 'fail' degree. For the Computer Science programme, the slopes (B) of OL in all the CGPA categories are positive. These showed that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. This result shows that the relative strength of UTME on the former is higher than those with a CGPA category of 'Fail' and otherwise for the latter. Finally, the slope (B) of PUTME in all the CGPA categories is negative, which shows that the relative strength of those with a CGPA category of 'Fail' is higher than the other categories. For Mathematics and Physics students the relative strength of OL, UTME and PUTME on CGPA performance are statistically insignificant except for the slope (B) of OL in the CGPA category of '1 st Class' for Mathematics students, which statistically significant. Table 8  shows the summary of correlations coefficient between OL-CGPA, UTME-CGPA and PUTME-CGPA aimed at all the degree programmes for the academic sessions ranging from 2010/2011 to 2014/2015. The summary of  Table 8  shows the correlations of OL and CGPA scores, UTME and CGPA scores and PUTME and CGPA scores for all the programmes used for the analysis. In 2012/2013, UTME-CGPA (-0.363) and PUTME-CGPA (-0.123) have low negative relationship whereas OL-CGPA (0.111) has a low positive relationship. The OL-CGPA (-0.071), UTME-CGPA (-0.090), and PUTME-CGPA (-0.040) in the 2014/2015 session signifies a low negative correlation. However, the entire results revealed that all the 15 correlation coefficients are very low, with seven of the result showing a low negative correlation relationship. How well do OL results, UTME scores and Post-UTME Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria scores of students predict their first-year CGPA across the academic session, 2010/2011 to 2014/2015? Table 9  is the likelihood ratio test results for all academic sessions ranging from 2010/2011 to 2014/2015. As shown in the Likelihood Ratio Test results in  Table 9 , the likelihood ratio Chi-Square of 37.446, 19.938, 46.141, 14.349 and 11.167 for 2010/2011, 2011/2012, 2012/2013, 2013/2014 and 2014/2015 academic sessions which has the following as significant values of 0.001, 0.174,.000, 0.499 and 0.741 tells us that the model for students admitted during the 2010/2011 and 2012/2013 academic sessions predicts the CGPA, which is the dependent variable while the other academic sessions does not. For all the academic sessions as shown on the results, the first equation intercept is the log of the ratio of the possibility of a student having a 'Pass' degree to the possibility of that student having a 'fail' degree. Among the classification of degrees, each of the CGPA categories is contrasted with the baseline group of 'Fail' degree. For the students admitted during the 2010/2011 academic session, the slopes (B) of OL in all the CGPA categories are positive. These show that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. Furthermore, the slopes (B) of UTME in the CGPA categories of the '3 rd Class' is negatively signifying that the relative strength of UTME is lower than those with a CGPA category of 'Fail' and the rest are positive which signifies otherwise. This indicates that the relative strength of UTME is higher than those with a CGPA category of 'Fail' and the rest of the CGPA categories are negative which indicates otherwise. Based on the analysis and results using MLR and PPMC for each programme and each academic session, it is evident that OL, UTME or PUTME could not individually significantly predict the academic performance of students in Faculty of Science. Also, there is a need to do a further study by including some more variables, such as age and senior secondary school mock examinations results, as criteria to significantly predict the academic performance of students successfully.
paper_57	This paper attempts to overcome stagnation problem of Ant Colony Optimization (ACO) algorithms. Stagnation is undesirable state which occurs at a later phases of the search process. Excessive pheromone values attract more ants and make further exploration hardly possible. Crossover and mutation operations have been adapted for use with ant generated strings which still have to provide feasible solutions. Extensive simulation tests were made in order to determine influence of genetic operation on algorithm performance. Exact algorithms for instance Dijkstra or Bellman-Ford appear to be slow and inefficient on large scale graphs. One of the well-known graph search algorithm that utilizes a heuristic is A* search  [1]  or ACO algorithm. Ant colony optimization represents an efficient tool for optimization and design of graph oriented problems. It is a multi-agent meta-heuristic approach and was first purposed by M. Dorigo et al. [2]  as Ant system (AS) algorithm. The aim is to find the shortest path. On their way back they use the same way from which abundant loops has been removed, but the amount of pheromone (1) ∆τ k ij (t) they produced is inversely proportional to the tour length L k (t). formula_0 formula_1 T k (t) is the tour generated by ant k, Q is a constant and tuple (i, j) denotes beginning and termination node of an arc. Evaporation rate is a user adjusted parameter and affects pheromone durability; i.e. At the beginning when no pheromone values are available heuristic values η ij takes dominance. Later the ant uses probability selection rule to choose the next arc according to formula_3 where p k ij (t) is probability the ant k chooses the arc (i, j) from the neighborhood N k i of node i except the node visited previously. The more pheromone is located on particular arc, the more attractive it appears. Heuristic values η ij affect probability only at the beginning when pheromone values are low. Disadvantages of ACO algorithms are (i) many user parameters and (ii) the selection pressure. Let's mention ant colony system (ACS) with pseudo-random proportional rule  [3]  in which random uniformly distributed variable q ∈ (0,1) is compared with a tunable parameter formula_5 else the probability selection rule (3) is applied; random selection applied to AS rank  [4]  where random selection rate r is the probability of random selection and represents a user parameter which adjusts the balance between exploration and exploitation; prevention of quick convergence (i) and stagnation avoidance (ii) mechanisms applied to AS  [5] . The stagnation avoidance mechanism (ii) is based on the comparison of a randomly generated quantity q ∈ (0,1) with probability p k ij (t) of selected arc. This occurs in later stages of the search process, where pheromone values tend to be high, and thus the chance of further exploration is low. Genetic algorithms (GA) were proposed by  Holland (1975) . The original GA is known as simple genetic algorithm (SGA). The four main components of GA are representation (i), mutation (ii), crossover (iii) and selection (vi) mechanism. Each component is adapted in order to provide feasible solution for ACO algorithm. In ACO algorithms representation (i) of genotype space is sequence of nodes: formula_6 where gene n is graph node and L is path length. In each genome each gene is changed with the equal probability. The simplest form is one point mutation on  Fig. In ACO adaptation the first and the last node is excluded from mutation. If more such nodes occur, random selection is applied. In ACO algorithm crossover position is represented by a common node of parental strings except the first and the last node  (Fig. If more of such nodes exist, random selection is applied. Crossover operation makes sense only if both child strings differ from their parents. In GA many selection (vi) mechanisms are available, like roulette-wheel selection, tournament selection, stochastic universal sampling or reward-based selection  [7] . At the end of each cycle t, when all the ants finish their tours T k (t), genetic operations are applied on the T k (t) strings which represents the list of nodes. At first mutation is applied. If mutation is not feasible, another node is chosen. If mutation fails on all nodes of the tour, another tour is chosen. After all mutation operations are performed, crossover operations are applied. If crossover operation is not feasible, another second string is selected. If no tour has common node with the first selected tour, another first tour is selected and the random selection process is repeated. Since genetic operations may produce strings with loops, in ACO framework prior and immediately after each genetic operation a loop removal procedure is performed. For this purpose ACO GO algorithm has embedded user feedback which represents a ratio between accomplished and required genetic operations. The above described ACO GO algorithm has been tested on a random generated graph. Common ACO parameter values were set in accordance with  [8]  and are listed in the  Table 1 . The value for the number of cycles represents three macro cycles of ACO MC  [9]  for the same graph and parameters. Test graph is a symmetrical multi-graph with 80 nodes and 300 arcs  (Fig. Node coordinates x,y fall in range <0,1> and arc's values c ij represent the arc lengths. The task is to find the shortest path between start node n s = 1 and end node n e = 80. For each setting 500 trials were performed. Simulation results were divided into three groups according to number of crossover pairs and are listed in the  Table 2 . The results received with GO are better almost in any case. It can be seen that the higher number of mutation operations, the better the performance is (Tables 2). The outcome with different mutation distribution is asymmetric. Results received without crossover operation have higher values along with the Mutation paths axis  (Fig. with no crossover pair certain amount of mutation operations should be spread out among more paths, but with 2 crossover pairs concentration of mutation operation on less paths tends to perform better. Genetic operations where nearly always feasible; ratio accomplished / required mutation operations is 100% and for the crossover operation over 99%. In order to determine the effect of crossover operation crossover rate was let to grow up to 100%  (Table 3) . 8) ; the highest output was gained for 60% of crossover rate. Beyond 60% threshold ants foraging behavior is suppressed by crossover overload. As the crossover rate increases, ratio accomplished / required operation decreases  (Fig. The mean value of the cycle when the best value was found is 109.081 with standard deviation 2.617. Limit of crossover is 60% of crossover rate. The higher the crossover rate, the lower the accomplished / required ratio. This can be explained by the nature of the mutation operation which creates new paths whilst crossover operation can only combine already existing solutions. The higher amount of mutation operations the higher the performance gain is. Without crossover operation distributed mutation operation has better performance, but with two crossover pairs concentrated mutation operation on less paths tends to perform better. The results are promising; GO improves ACO algorithm performance more than twice. Further research and more experiments are needed to determine the distribution and optimal amount of mutation operation with respect to the number of ants and length of the paths.
paper_78	Fuzzy C-means clustering is a soft technique and has some advantages in ecological studies. Glycyrrhiza uralensis is an endangered medicinal plant species and mainly distributed in North China. A hundred plots were classified into 12 vegetation formations by fuzzy C-means clustering. These formations were main communities dominated by Glycyrrhiza uralensis in North China. Each formation has its own composition, structure, distribution range and environment, and all of them should be protected effectively. The results suggest that fuzzy C-means clustering is an useful technique for classification of plant community. Fuzzy C-means clustering is the only soft method in the clustering family and should have some advantages  [5] . Ecological communities, such as Glycyrrhiza uralensis communities, have great variations in their species composition respond more or less independently to environmental variables. Because of this, methods based on fuzzy mathematics might be more appropriate in ecological analysis  [6, 7] . Medicinal plant is important natural resource and significant for people health in many countries and regions, such as China, Japan, Korea, India and so forth  [8, 9] . Licorice (Glycyrrhiza uralensis) is one of the most popular Chinese herbal medicines and a significant resource plant species. Beside medicine, it is also widely used in food, tobacco, chemical industries  [10, 11] . Like licorice, most medicinal plants are harvested in the wild and the extended use has led to some medicinal species endangered and tending to be extinct, and the protection of such medicinal species has been urgent  [12, 13] . Further more, conservation of medicinal plant species is important in term of biodiversity conservation  [11, 14] . This study aims to identify Glycyrrhiza uralensis communities and analyze their characteristics on composition, structure and environment in North China. Fuzzy C-means clustering is a soft classification technique  [5, 7]  in which a plot can belong to more than one cluster, and associated with each plot is a set of membership levels. Given a finite set of data matrix (species × plots), the method calculates a list of C cluster centres  formula_1 formula_2 Where i = 1, 2, …, N = the number of plots; j = 1, 2, …, C = the number of clusters; U = {U ij } = the matrix of membership values, U ij is the membership of plot i in cluster j; V is a matrix of cluster centers; m is fuzzifier which determines the level of cluster fuzziness (1 ≤ m < ∞). A large m results in smaller memberships u ij and hence, fuzzier clusters. In the absence of experimentation or domain knowledge, m is commonly set to 2. (dA ij ) 2 is the distance index: formula_3 X i is the vector of attribute measurements in plots, usually a vector of ordination scores; V j is the centre of cluster j, if A is a unit matrix, then formula_4 Based on the algorithm above, the procedure of fuzzy C-means clustering is as follows: (1) Selecting a common ordination method, we use Detrended Correspondence Analysis (DCA), and do ordination analysis for species data. The first DCA axis was used as the basic data X i in clustering. (3) Assigning the primary membership values, U 0 . formula_5 formula_7 Based on the new membership values U, we go back to the fourth step and calculated the next turn V j , (dA ij )  2  and U ij iteratively, and until the membership values become approximately stable. 7 Classified plots into clusters based on the final U. We can use U to identify the relationships among plots and communities directly. Twenty plots of 5 m × 5 m were established randomly at each site which is over 40 ha in area. The coverage, mean height, individual number for shrub and herb species was measured in each plot. The coverage of species was estimated by eyes, and the heights were measured using a tape ruler. In total, 191 plant species were recorded in 100 plots. Elevation, slope and aspect for each plot were also measured and recorded. The elevation was measured by a GPS, slope and aspect measured by a compass meter. The importance value was calculated by the formulas  [7] :  formula_8 Fuzzy C-means clustering is a nonhierarchical method which provides C clusters and their plot compositions  (Table  1) . The name and characteristics in species composition, structure and environment of each community are described below. Its disturbance intensity is medium and heavy. The common species are Ziziphus jujuba var. It is distributed from 300 to 500 m in hills with slope 20 -30° in sunny，semi-sunny and semi-shady slope and sandy chestnut soil. Its disturbance intensity is medium and heavy. The average cover of Glycyrrhiza uralensis in this community is 35% with a density of 5700 ha -1 . The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 60%. The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 59500 ha -1 . The common species are Salicornia Bigelivii, Carex duriuscula, Stipa sareptana, Artemisias phaerocephala, Alopecurus pratensis, Saposhnikovia divariicata, and Carex pediformis. Glycyrrhiza uralensis + Carex pediformis +Stipa sareptana. Its disturbance intensity is heavy. The common species are Cleistogenes squarrosa, Caragana pygmaea, Hordeum brevisublatum, Ephedra sinica, Achnatherum sibiricum, Artemisia frigida, Viola tianschanica, Carex duriuscula, and Alopecurus pratensis. Glycyrrhiza uralensis + Astragalinae triloa + Stipa sareptana. Its disturbance intensity is heavy. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 65%. It is distributed from 400 to 800 m in hills with slope 20 -35° in sunny and semi-sunny slope and sandy soil. Fuzzy C-means clustering successfully classified 100 plots into 12 communities dominated by Glycyrrhiza uralensis. Theoretically, fuzzy C-means clustering is the only one soft clustering method and can deal with much imprecise and incomplete information and has advantages in solving non-linear problems and in studying complex system  [2, 17] . Vegetation systems are very complex with interactions of species, communities, environmental variables, and so on  [18] . Therefore, fuzzy C-means clustering should describe ecological phenomena and rules better  [5, 6] . Glycyrrhiza uralensis communities recognizing by fuzzy C-means clustering varied greatly in species composition, structure and distribution area  [20] . These further confirm that fuzzy C-means clustering is an effective technique in vegetation analysis  [28, 29] .
paper_96	With the rapid development of China's transportation, the frequency of traffic accidents is also high. This not only restricted the development of China's transportation greatly, but also threatened to people's safety seriously. In particular, the accidents caused by the blind due to there are more frequent reproduction, so their traffic safety has become a big issue to solve urgently. Such warning system of obstacle avoidance of embedded electronic guide dog can effectively eliminate the impact of the traffic environment and the subjective factors of the blind, warning in advance for the travelling blind in time, effectively avoiding obstacles such as vehicles, to reduce traffic accidents caused by the their blindness. This humane technology innovation is the specific embodiment of environmental science and technology aesthetic theory in the field of scientific and technological innovation. It has a positive and promoting role to the development of transportation and blind-man welfare in China. I think that if electronic guide dogs guide them to avoid obstacles such as vehicles, the blind people are able to avoid such accidents, aren't they? The design of obstacle-avoiding early warning system of embedded electronic guide dog is an inheritance and innovation, based on the design of traffic information collecting system, which is generally used in vehicle anti-collision at present. Its operating system is thought from the perspectives of practical application, development tool, instantaneity, technical services, and price, etc. Moreover, the obstacle-avoiding early warning system of embedded electronic guide dog also selects USB protocol to transfer data, saving the collected data in the hardware after being managed. The overall design plan of obstacle-avoiding early warning system of embedded electronic guide dog mainly is the velocity and distance measurement module, the power circuit module, the LED screen show module, USBcommunication circuit module, and clock circuit module. The techniques of information collection are ultrasonic wave, laser, infrared ray, machine vision and interactive method. Information processing is mainly to analyze the collected information, usually using ARM, MCU and DSP microprocessor, etc. Therefore, in the design process of the obstacle-avoiding early warning system of embedded electronic guide dog, after the comprehensive and comparative analysis of the anti-collision warning techniques, we have compared the optimization principles of the anti-collision warning techniques, and finally adopt the approach of millimeter wave radar sensor + ARM microprocessor. The design of obstacle-avoiding early warning system of embedded electronic guide dog also adopts the generally used embedded processor. The embedded system will further compile all of its procedure codes, including the operating system code, driver's code and application code, into a whole paragraph of executable code and in inserted them into the hardware. Besides, its other main features are universal Linux API, core file less than 512KB, core+file system less than 900KB, and complete TCP/IP. In addition, the design of obstacle-avoiding early warning system of embedded electronic guide dog also makes a full use of USB. USB is a new kind of computer peripheral communication interface standard, which abandons the defects of the traditional computer series/parallel, with advantages of the reliability of data transmission, hot plug (plug and play), convenient extension, and low cost. The warning system of embedded electronic guide dog avoiding obstacles studied in the project can effectively eliminate the influence of traffic environment and blind subjective factors, send the pre-trip alarm for the blind in time, effectively avoid obstacles such vehicles, to reduce traffic accidents caused due to the blindness. This humanized technology innovation is the embodiment of environmental science and technology aesthetics theory in the field of science and technology innovation practice. This has not only a positive role in promoting the development of Chinese transport and the blind welfare, but also important practical significance and practical value for us to build a harmonious society and a beautiful China.
paper_134	Data mining, also referred to as knowledge extraction from databases, is one of the most important analytical methods for identifying the relationships between the various elements of the information collected in order to discover the useful knowledge and support of strategic decision-making and sustainable development systems in various industries. The most jobbery ways during olive oil production consist of mixing other oils such as maize, sunflower, Canola and corn into the olive oil. A set of 16 samples of olive oil, sunflower, canola and corn oil which mixed with different ratio of Authentication, were used for calibration and evaluation of developed system. Data mining takes advantage of the progress made in artificial intelligence and statistics. Both of these areas work in model identification and data classification issues and will, in effect, be directly used in data mining, and both groups are active in identifying and using neural networks and decision trees  [2] . These methods are very damaging, costly and time-consuming. Nowadays, many non-destructive methods have been investigated, which have the ability to identify various components of the quality and purity of a substance at a widespread level. Among the non-destructive methods, the dielectric method has advantages due to the relationship between the steady-state dielectric with some qualitative materials, the cost less than other methods, high speed and high efficiency in the construction of systems Different recognition of quality has attracted the attention of many researchers  [5] . If the two metal plates are placed opposite and insulated, they form a capacitor. Typically, the conductor plates of the capacitor are made of aluminum, zinc and silver, and among them, a dielectric can be placed in the air or other material. The association of these properties with many quantitative and qualitative components of the product, high speed, low cost and high efficiency of its use as a new and non-destructive technology has been considered by many researchers to measure a wide range of qualitative and quantitative characteristics of agricultural products  [8] . The latest statistics have shown that every Iranian consumes 100 grams of olive oil per year. One of the main reasons for this low consumption is the high price of this oil. This high price, in addition to having an impact on consumption, has been motivated to enter the market for profits. This high price, in addition to having an impact on consumption, has been motivated to enter the market for profits  [10] . Knowing the original oil is also not an easy task that anyone can handle. The second pertain to data mining algorithms; third part related to samples and used methods in the article. Also, processing of data and results were presented in fourth section  [11] . (2010) conducted a research on olive oil for detecting adulteration using a dielectric spectrum. They also used the partial least squares model (PLS) to detect oil falsification. From the result of this study it can be seen that the dielectric spectrum can be used to detect fake oils with different types of oils with a percentage of their mixture below 5%  [12] . The relationship between the rates of fruit dielectric constant in a frequency of 1 kHz to 10 MHz was investigated. The results of the experiment showed that the dielectric constant is strongly affected by the size and volume of the fruit and also decreases with the increase in the fruit juice, which is clearer in a frequency of 1 MHz. The results of the prediction were satisfactory and the correlation coefficient between the actual yield and the expected value of the fruit was 0.85  [13] . They used 4-volt sin voltage with a frequency of 120 MHz and a rectangular parallel capacitor, which obtained capacitive capacitors (DCs) and voltages (DVs). The results showed a significant difference between DC / DV ratio during storage period. Using this parameter and egg mass, they extracted regression models and reported that the application of this method to the egg production line and its grading based on quality properties requires more research  [14] . (2015) reported a research about egg grading using image and sensor processing. The correlation coefficients and the mean square error of the statistical indexes were 0.98 and 0.0006, respectively. 70% of the data was for network training, 15% for validation and 15% for the network testing. In order to select the best network, the number of hidden layer neurons was changed from 2 to 50  [15] . In this study, a recent study has been carried out to identify the authentication of olive oil. In this article the experiment was done by olive, sunflower, Canola and corn oil. Then different classification algorithms by MATLAB software and various techniques such as support vector regression were done and finally output dates were processed. The samples were blended with sunflower canola and corn oil, and they made up a total of 9 classes with varying percentages (from 60 to 100% in 5% increments), each of which was shed separately, according to the desired amount, of 100 centimeters inside the sensor. The device used consists of the Arduino board, ICL8083 and AD8302. Due to the high flow of data, the ch340g chip on the Arduino board is used to measure dielectric parameters as well as a device that can detect the purity of olive oil. The Arduino board is used on the Uno device, which uses the Atmega2560 microcontroller. The microcontroller is an electronic application chip that can increase the speed and efficiency of the circuit versus reducing the volume and cost of the circuit. Arduino has a variety of ports, controls, side panels, and screen connectivity, while it may be smaller, less expensive, and possibly faster than a microcomputer system. The AD8302 chipset has been used to measure the domain and fuzzy detection, as well as to measure the dielectric parameters. The AD8302 phase detection chip provides a simple method for measuring the input ratio of the domain and the phase difference of the two signals simultaneously (22). Support Vector Machine Regression (SVR) aims at finding a linear hyper plane, which fits the multidimensional input vectors to output values. One of the new ideas in identifying and categorizing patterns is a backup vector machine or SVM. In particular, SVM does not have the problem of local optimization in its training, it builds the categorizer with maximum generalization, determines its structure and topology optimally, and functions nonlinear differentiation easily with low computing using It forms the concept of an inner product in the Hilbert space. In this study, coarse function was used to regression test data. Using this technique, the amount of RMSE for the samples were equal to 3.56, 2.45, 3.05 and the amount R 2 was equal to 0.92, 96 and 94, respectively. Also, in regard to the amount of RMSE, result has shown 4.81, 2.54, and 4.38 for Olive-Sunflower, Olive-Canola and Olive-corn, respectively. After providing adulterated samples and pouring them into a capacitive sensor, output data was analyzed by MATLAB software. In this article two factors (gain voltage and phase shift voltage) were measured. It is noticed that The AD8302 measures the magnitude ratio, defined here as gain, and phase difference between two signals. In this study, Quadratic function was used to regression test data. Figure 2  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn Using this technique, the amount of RMSE for the samples were equal to 3.65, 2.46, 3.31 and the amount R 2 was equal to 0.92, 96 and 93, respectively. Comparison of the testing stage of these techniques showed coarse tree was the best prediction. Interestingly, olive-Canola oil samples predicted with high accuracy in all techniques. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud.
paper_139	Much effort made towards in investigates mechanism with identical detection algorithms, still the retrieved web documents with outmodedlink. In this proposed system, we are successfully identifying and minimize the redundant information and like link in web documents. Retrieving relevant information from web without redundancy is more challenge task nowadays where in web mining communities  [3, 4] . Utilizing customary data recovery  [5]  and information mining systems it get to the known and obscure data from the Web content. A query is searched in a web search tool to recover some significant and required data for the client, either the search query is known or unintelligible to the client, it generally to reply with relevant data rather than redundant, however we can't guarantee that the reply for the query about the significance and redundancy. Once the input query is requested, the search engine generate the document with multiple web pages along with the links, the user will be unaware about the content of the web pages, the extracted web documents contain multiple web pages either be redundant or not. The Document retrieved must follow some constraints which have less time & space requirements, based upon the criteria the extracted web document must be preprocessed, for preprocessing & information selection, need to apply some techniques such as stop word removal, Stemming of word, phrasing, normalization of tokens. Step2: Pick the minimum degree vertex 'v' in the set-up and include in the set U. Step3:While U doesn't include all vertices Step3A: Include the entire isolated vertex which is adjacent to the vertex 'v' to U. Step3B: Find the adjacent vertex 'u' to 'v' which is not in U and has maximumdegree. Update 'u' to U. Step3C: Update the value of degree for all adjacent vertices of 'u'. Apply the proposed KTMIN-JAK-MAXAM ALGORITHMto G 1 By step 1: deg(All Nodes of G 1 ) = 3 By step 2: Mark the node A as visited and put it onto the set U. By step 3: There is no isolated vertex in the given graph G 1 By step 4: 4.1 Investigate any unvisited adjacent node from A. 4.3 Now travel around the node 2, the unvisited adjacent node is from 2 as 1 and 4. 4.4 Discover the node 1, the unvisited adjacent node is from 1 as only 3. Now the set U consists of the nodes A, B, 2, 1, 3. Case II: Connected Complete Network G 4 in figure 8  Here notice that, all complete connected networks G, after applying the proposed KTMIN-JAK-MAXAM ALGORITHM to G, we get linear path of length (n-1). While getting more redundant web pages for a single search query, it's more difficult to recognize the redundant links.
paper_145	Overweight and obesity were more among urban residents compared to rural residents and they were thirty two percent more exposed to overweight and obesity. According to factor weights it was noted that the important factors for variation in the level of obesity were mainly gender variation followed by occupation, education and type of work. The prevalence of overweight and obesity has increased rapidly over the last decades especially in developed countries  [1] [2] [3] . Obesity is generally associated to a significantly higher risk of arterial hypertension, diabetes mellitus (DM), hepatic steatosis, hyperdyslipidemia and renal failure  [5, 6] . The aim of this paper was to identify the socioeconomic factors responsible for obesity and overweight among some rural and urban people of Bangladesh. The important factors for obesity and overweight were identified by factor analysis, where largest factor weight indicated the most important variables  [14, 15]  responsible for obesity. The study was based on data collected from both urban and rural people of Bangladesh. To study the variability of socioeconomic variables for diabetic and nondiabetic people, some respondents were also investigated as a control group. Thus, finally, the analysis was performed using the data of 635 diabetic patients and 265 non-diabetic people. The data were collected through a pre-designed and pretested questionnaire during the months of May and June, 2015 by some undergraduate and post graduate students of American International University-Bangladesh, most of whom were doctors and nurses, of the department of Public Health and they were associated with public health services. Data have also been collected from parents/guardians of 200 randomly selected students of different disciplines of the university, on the assumption that the respondents would be of normal group of people. But during investigation some of them were found as diabetic patients. However, from the filled-in questionnaires 356 were found in completed form and the information of these 356 respondents were included in the analysis. The latter information were provided by the diabetic patients. The information regarding blood sugar level and blood pressure level were also noted down according to the latest measurement by doctors/diagnostic centers. The variables included for factor analysis were residence of the respondents, their age, gender, marital status, religion, level of education, occupation, type of work, monthly income and smoking habit. Besides the factor analysis, the association of different socioeconomic variables with level of obesity were investigated. Significant association was decided by chi-square test with p-value< 0.05 and odd ratio was calculated for respondents who were overweight and obese compared to normal group. It was observed from the analysis that among 900 respondents 7.6 percent were underweight [  Table 1 ] and 19.1 percent of them were from rural area. Maximum (43.1) of the respondents were overweight and 20.9 percent of them were rural people. Obesity was observed among 15.3 percent people and overweight respondents were 34.0 percent. In each level of obesity the majority were from urban area. However, urban people were more exposed to overweight and obesity by thirty two percent compared to rural people [O. R.= 1.32]. They were in more risk of overweight and obesity by 51 percent compared to males [O. R.= 1.51] Obesity and severe obesity were observed almost similar among Muslims and Non-Muslims [  Table 3 ]. But the O. R.= 1.07 indicated that both the religious groups were similarly exposed to overweight and obesity. Among the investigated respondents 92.6 percent were currently married and 43.1 percent of them were normal  [Table  4 ]. Similar normal group was noted among the other group of respondents. Majority (52.9%) of the respondents were of age 50 years and above and 48.5 percent of them were normal [  Table 5 ]. The percentages of normal groups among the respondents of ages 25 -40 and 40 -50 were 35.5 and 36.6, respectively. Levels of obesity was significantly associated with levels of ages [P (χ2 ≥ 18.34) = 0.008]. But if classification of respondents was done into two groups, one group of ages <40 years and another group of ages 40 years and above, both the groups were almost similarly exposed to overweight and obesity [O. R.= 0.81]. The proportions of different levels of obesity according to professional variations were significant [P (χ2 ≥ 46.472) = 0.000]. But, all other professional groups of respondents had the similar risk of obesity compared to servicemen [O. R. = 1.04]. Significant association was noted between the level of obesity and the level of income [P(χ2 ≥ 64.994) = 0.00]. But overweight and obese respondents were 62 percent more exposed to diabetes compared to other groups of respondents [O. R.=1.62] In one study  [20] , it was reported that smoking was one of the factor to increase the level of obesity. The association between smoking habit and level of obesity was significant [P(χ2 ≥ 20.189) = 0.0.002]. The association between smoking habit and level of obesity was significant [P(χ2 ≥ 20.189) = 0.0.002] and smokers were 21 percent more exposed to overweight and obesity compared to non-smokers [O. R. = 1.21]. The analysis helps to identify the important variables to explain the variation in the data set  [15, 21] . The variables which were included in the analysis were sufficient to explain the variation as KMO = 0.633, χ2 = 256.371, p-value = 0.000. From the analytical results it was seen that all the p-values were less than 0.05 which indicated that the inclusion of variables for factor analysis were justified. From the results of the communality it could be concluded that the variable marital status was more important followed by gender and education. From the results of the communality of the variables it could be concluded that 85.6 percent variation of the variable marital status would be explained by the extracted factors  [22] . These three variables were more important for the variation in the level of obesity. From the factor analysis it was noted that the coefficients of the variables gender was highest followed by occupation, education and type of work. These coefficients were observed from the first component. This information were noted from the characteristic roots of the correlation matrix of the variables, where the roots were 2.573, 1.616, 1.086, 1.053, and 1.003. The second component showed that the most important variables to explain the variation in the data of obesity were residence followed by religion and income. The analysis presented here was done from the data collected from 635 diabetic patients and 265 control group of respondents. The prevalence of overweight and obesity were significantly associated with age, religion, education, occupation, marital status, income and smoking habit. Factor analysis also indicated that some of the socioeconomic variables were responsible for increased rates of overweight and obesity. The percentages of these four groups of respondents were 7.6, 43.1.34.0 and 15.3, respectively. The obesity is one of the risk factor of prevalence of non-communicable diseases  [NCD]  and it enhances the arterial hypertension, diabetes renal failure etc. Among the respondents 84.1 percent were of the age 40 years and above and among them 42.8 percent were overweight and obese. The factor analysis showed that sex, occupation, education and type of work were more important to explain the variation in the level of obesity.
paper_212	Mathematical modeling of disease has been an indispensable tool in accounting for disease transmission dynamics as well as disease spread. The models developed however faced considerable challenges ranging from inaccurate representation of natural data for deterministic models, to methods of forecasting such as statistical extrapolation which assumes that current conditions will prevail which is not always the case. Despite the spread of HIV/AIDS having been explored widely, not much literature is available on the Gillespie Algorithm based SIR model. We conclude that the simulated model reflects reality. On June 5, 1981 a mysterious disease was recognized among MSM in the USA. In the same year, it was identified that it attacks the immune system of the host, incapacitating them to heal subsequently leading to death. It was characterized by its etiological agent HIV in 1983,  [1] . A serological test, was then made available. In 1984, Robert Gallo discovered that HIV was responsible for AIDS. This virus, being highly transmittable is analyzed and capturing how it is transmitted is crucial in attempting to model the disease. In 1984, several HIV and AIDS cases were documented in Kenya. The following year, 26 new cases of HIV were recorded from sex workers and the NAC was established. The NACC was established under Section 3 of the State Corporations Act Cap 446 through the National AIDS Control Council Order, 1999 published vide Legal Notice No. Among other countries in the world, Kenya is among the twenty two that account for 90% of expectant women living with HIV. This accounts for 4% of new pediatric infections worldwide. Among the expectant women there are 13,000 new HIV infections among children. The number of those that died account for 7% of the global total. Sex workers have the highest percentage of HIV prevalence at 29.3% according to the Kenya HIV estimates report-National Aids Control Council. This study employs mathematical modeling tools in the transmission probability of HIV and analyses done on how the cumulative number of infected individuals responds as well as the AIDS death probability and how the cumulative cases of removed individuals responds to this probability. The following reviews consider models developed for HIV/AIDS data that either differed too greatly with other model estimates or still fail even with developments on the model. In 2010 several authors came up with a model to predict HIV transmission in China in 2002  [3] . In this model, there were no forms of intervention. The transmission parameter was held constant for all stages of HIV. This model approximated that there would be 6000000 cases of HIV and 400000 cases of AIDS in China if there were no forms of interventions implemented. In 2007 the government of China alongside UNAIDS made an estimate of 700000 cases of HIV and 85000 cases of AIDS in China at the time, which is much lower that the estimates made by Liu  [4] . The number of HIV infections in 2010 was predicted to approximately 1000000. The group most affected would be the 31-40 years group. The group comprising of the largest individuals predicted to be living with HIV/AIDS was approximately 650000. In several concentrated epidemics, HIV prevalence estimates do not match reported cases and mortality estimates do not match reported deaths, even after adjusting. Even with the 2013 updates of Spectrum where adjustments were made in the parameter values empirically to improve the fit to program data, the estimates given by Spectrum still differed with data available. The parameter depended on the varying population size N. This meant that both the population size varied as well as the transmission/contact rate. Despite the fact that a lot of research has been done on modeling disease trajectory, not much literature is available on the use of Gillespie based SIR models to simulate the trajectory of a disease in the population. In their works they build a disease spread prediction model based on the SIR model and applied parameter values to a stochastic model based on Gillespie's algorithm. This is applied to data and the conclusion was that the model well explains the process of the spread of the disease in the population  [10] . Infection-transmission deterministic models are based on the characteristics of population growth, disease occurrence, and spread within a population. By contrast, in a deterministic process, there is no randomness  [12] . This paper will contribute and build on to the existing literature on modeling disease dynamics in the population with the model tested on HIV/AIDS data 1985-2018 to investigate if the simulated values would reflect results that are close to reality. This paper will help bridge the gap between conceptual epidemiological models and its simulated version by providing a developed version of an SIR model that solves one inherent problem that deterministic models do not reflect the natural data. The Classic SIR model The Kermack-McKendrick theory illustrates individuals grouped as susceptible and removeds only  [13] . The transmission and infection rates were considered to be variant. They set the transmission and infection rates as invariant for all ages and this allowed the inclusion of an infectives class. This transformed the theory to the basic SIR model such that when demographics were included becomes (4) (5)  (6)  where N denotes the total host population. denotes the birth rate and death rate denotes the infection rate denotes the recovery rate t denotes time point Model development The Gillespie algorithm was used to simulate a statistically correct trajectory given initial SIR conditions. The model explored how altering transmission dynamics affected the model as a whole. The reliability of the simulated values would set the precedent for the valued to be predicted based on the model is also explored. The stochastic SIR model. The assumption is that the population is finite and is sub-divided into categories of finite discrete compartments. , , This Gillespie's algorithm based stochastic SIR model generates a statistically correct trajectory from the initial vector as , , where i=s, i, r S+I+R=N i denotes the population size of the state at time t denotes a function characterized by two quantities as a state change vector and a propensity function. Assuming that the resulting state is . A propensity function is the probability of one event occurring in the time interval ! Continuous-time Markov chains are the basic tool for building discrete population epidemic models. The Markov property lets us specify a model by giving the transition probabilities-defined as rates-on a small interval between the compartments. Considering the fact that the propensity functions require to be in probability form, we explore this assumption further by defining and interpreting it. This implies the probabilities are individual therefore discrete. Continuoustime Markov chains are the basic tool for building discrete population epidemic models. The Markov property lets us specify a model by giving the transition probabilities-defined as rates-on a small interval between the compartments. The transition probabilities assigned are defined on an open interval (t, t + ), such that the probability an individual moves from the susceptible compartment to the infectives compartment is  [ 1 ] . The wait times between events can either assume an exponentially distributed wait time or the rate of flow between compartments can assume any of the following distributions depending on the results. In order to assess how the simulated data performs against natural data, a modified chi-square test was used. The data was obtained from NACC for HIV/AIDS cases. Considering the hypotheses, 4 = If the mean and variance of the simulated and natural data are equal, the simulated mean does not fit the data. 4 5 = If the mean and variance of the simulated and natural data are not equal, the simulated data fits the natural data. A modified chi-square test for simulation models was used to see how well the simulated data fit the natural data  [18] . A stochastic SIR model was simulated with a mean step size of 0.006336446. 537 tau steps were made in the model. Variables in the model were S = 3507162, I = 45820, R = 4597, parameters in the model are crude birth rate of 0.06, non-AIDS death rate of 0.025, transition rate of 0.1 and AIDS death rate of 0.48. Curves produced are illustrated below. where C represents the variance of D The calculated value found was 64.958. The critical value was 47.4. Since the calculated value is greater than the critical value, the decision rule is to reject the null hypothesis. After simulating, values were produced by the algorithm for each time step. Recommendation for research would be to explore other variations of the SIR model such as SI, SEIR under Gillespie's algorithm. The SIR model as well as Gillespie algorithm could continue to be applied other areas such as viral marketing and behavioural science as has already been done successfully.
paper_214	The paper presents the usage of databases that store business data into a warehouse star model that permits to create queries using SQL language and business intelligence tools. This kind of model allow to the decision maker to create complex reports and graphs based on the columns from the dimension tables and measures from fact tables, that can be the base for creating alternatives and scenarious acording to the economical indicators. The business environments require analyses on large amount of data, big data and necessitate advanced tools to query through numerous criterias and also to create different realistic scenarious that allow choosing one option, so the business manager can use the right tool to gain economic advantage. The optimal solution or satisfactory is obtained using either algorithms or formulas within optimization models, either by experiencing various possible alternatives in a process simulation. Optimizing the use of a certain class of models for finding the best solutions. Among the methods most commonly used are decision analysis and mathematical programming  [1, 2] . It is the only method that can be applied to unstructured problems. Simulation becomes a technical coordination of procedures using the computer. The solution offered is one spot that has no counterpart in the real system. always taking into account the unique factors of a problem specific results can not be transferred to other issues; results are difficult to interpret, being dependent on random factors; no matter how powerful your computer is, the optimal solution is difficult to obtain a model that has many equations and a significant number of parameters. These limitations have led to the use of simulation only when the interactions between components of the system are complex, when factors random have a significant and requires a large number of observations on the behavior data, the problem can not be solved by an algorithm or experiments direct. The simulation can be conventionally divided into the following steps: the problem and research purpose; model development and data collection system; model verification and validation; describing experiments on the computer; simulation execution and achieving results; analyze the simulation results. Quantitative mathematical models are embedded in base models, managed by subsystem a management model that requires separate users from the physical aspects of data processing and storage that extract, create, delete and modify models. Decision making process, conducted with the help of tools, methods and techniques, conduct to the scenarios constructed according to a definite objective. The activity of the coordinations of inputs is done in most cases with specialized software systems that create analytical databases or modeling languages. To be more precise, a specific problem highlighted in a model is called one of the most used tools in the decision making simulation. The next logical of optimization and forecasting, simulation assists with the running complex patterns, resulting variables whose analysis highlights the value adopated lead to a decision. The outputs from the process of decision making, represented by analytical indicators reflecting the performance of the system analyzed variables results the evaluation criteria or implementation plans of the decisions  [2, 3] . Evaluation of search results depends on the method of presenting results and depends on the facilities of component dialog with users that provide inputs. The decident system uses a dialog interface with the key users of the company, enabling connectivity and communication between networks with different topologies and areas. After analyzing the results achieved and the objectives of their reporting, signaled differences and after identify problems it was reveal the need to take action. Solving the result of communication between all stakeholders, sharing the general manager responsibilities both at decisionmaking levels and the corresponding subproblems defined. For example, after the first phase, the scope may relate to excessive spending decisions of a functional department, inventories too high or adoption of a draft research and evaluation on the introduction of computers  [3, 4] . In the model design phase defines a model for decision shall be tested and validated under real system. Modeling takes expression of reality by means of abstract entities possessing quantitative and qualitative attributes. From model design and solution choice there is a strict demarcation, certain activities may be conducted during both phases, and return of election phase in phase. The assessment in turn depends on the search method. For complex problems, solving is carried progressing from one situation to another, until a final statement, which is the solution. It receives relevant and substantiated elements on activity in the real system and builds models for solving future decision making on the basis of current assessments.Of the foregoing that a decision support system provides a filtering of information provided to decision makers and indicates certain restrictions. The database is built to meet the information requirements of the system and is an interrelated database operated by one or more users, one or more applications. In current systems, the company's intranet, are increasingly present data accessible through web browsers and multimedia items such as maps, images, sounds. The data source, internal or external, data is extracted and managed by a management database. The management of the database depends on the organization of data. In most cases there is SGBS transactional relational data system and a management database for multidimensional data warehouses created. The data dictionary is a catalog of all data from the database. The most common data dictionary used in the first phase of decision making is data mining to identify their problems and opportunities. The subsystem management model consists of the following components: base models, the management models, dictionary and processor execution models and integration patterns Base models contain the set of models that make it possible to analyze the facts and the choice of options in terms required by the user. It is the component that differentiates interactive decision support systems to other systems. Database management system allows creating new models models using programming languages, update and modify existing models, establish interdependencies between models. In building a data warehouse is based on the analysis of data. Conceptual models are multidimensional and designed to organize data necessary decision-making process on issues. The star is the type of aggregation criteria when codes are explained in type tables list. Using data from lists, star type structure enables higher levels of aggregation on the initial size  [4, 6] . The databases permits flexibility in creating queries by choosing columns from dimension tables and measures from the fact table and so the decision maker may choose what economical indicators is useful to analyse and also permits to create different types of graphs such as columns, pies, scatter points or webs. It is very important to store data into a warehouse model and then to create future strategies that can help a company to expand or to grow the profit in a realistic way based on complex analyses and alternatives.
paper_216	Amongst the non-parametric approaches used to improve the accuracy of the model in this study is the Wavelet-based pricing model. The MSE and RMSE of Wavelet model is 0.208546 and 0.456669 respectively which is much lower than that of Black-Scholes model and therefore in conclusion, Wavelet model outperforms the other model. The Black Scholes model has been developed under some assumptions such as the risk-free rate and volatility of the returns are known and constant, the returns of the underlying security are normally distributed, markets are efficient (market movements cannot be predicted). In the real market, the asset returns follow a leptokurtic distribution which is in contrast to the Black-Scholes model where returns are assumed to be log-normally distributed. Variables used in this model are observable, for example the time to expiration, exercise price and the closing price except the volatility which is not directly observable. According to the literature, this model is the latest theoretical contribution to the option pricing and it is better at capturing the volatility smiles which is as a result of the Black-Scholes' assumption on volatility constant. The implied risk neutral moment-generating functions (MGF) is estimated by the wavelet method. This method only requires a reasonable amount of data (different strikes) and is very efficient, unlike other non-parametric methods which requires large amount of data. This research therefore compared the performance of Wavelet based pricing model and Black-Scholes model in the valuation of a European option. Their formula can be used to obtain the following parameters; the spot price, the exercise price, interest rate, time to maturity and volatility. This model has been discussed extensively in order to improve its pricing biases and to impose more practical assumptions. The approximation of the implied risk-neutral MGF of the underlying security (asset) returns was focused by developing a non -parametric option pricing model called Wavelet model  [4] . In comparison between the risk-neutral MGF and the implied risk-neutral PDF, the risk-neutral MGF has a number of advantages even though between them, there is a one to one relationship. The following are some examples of these advantages; when there is presence of jumps in the underlying process MGF is more tractable, the option's obtained MGF is a continuous function, statistical moments of the asset distributions can be obtained using MGF and the risk neutral MGF can be used to estimate the out of sample options that has maturity dates which are different. The second use of wavelet method is that, they can be used to de-noise raw data. Also, wavelets can be used to improve analysis of volatility since they are a preprocessing denoising tool  [10] . The de-noising ability of wavelets was also recognized in  [7] . Evidence have been provided to support that wavelet based local linear approximation  [11] . Another example is the application of the wavelets in nonparametrically estimation of a diffusion function  [15] . formula_0 From the Black-Scholes formula above, we see that the strike price (K), risk free interest rate (r), time to maturity (T), the volatility (") and the current price of the stock (S0) are required. Based on some general assumptions like independent and identically distribution (iid) for asset returns and that the moment generating function is defined well, the wavelet based pricing model can be expressed as follows; formula_1 Where, t is the current time, is the underlying asset price at time t, # $ is the time t price for a European call option written on asset, K is the strike price and T is the future maturity date. The underlying asset dynamics and investor expectation in option prices is captured by the MGF Θ !$ + of the logarithmic returns /0 7 . When T -t = 1, Θ (v) is the risk-neutral MGF for the rate of return per unit of time. This needs to be approximated by wavelets. In order to approximate the implied MGF using the wavelet method, one has to choose a particular wavelet from a large family of wavelets. From the wavelet literature, researches have shown that there is no best wavelet for a particular application. And therefore, we choose a wavelet that can achieve a reasonable accuracy with wavelet terms being very minimal. To estimate the risk neutral MGF of the return per unit time, we use the Franklin hat function f (t) expressed as; The data sets consist of simulated stock prices, the exercise price, the time to matureness, interest rate and volatility. The focus on this study was on pricing of a European Call option using two models, one parametric model (BS Model) and one non-parametric models (Wavelet Pricing Model). In order to estimate the risk neutral Moment Generating Function of the underlying security from option prices, we applied the methodology of wavelet. Therefore, one of the most significant input of this study is that the wavelet -based pricing model is an alternative model for pricing options and other derivatives on the same underlying asset with varying times to maturity and different strike values. The superiority of the wavelet method comes from the ability of the wavelets to estimate the risk neutral MGF. This is evident from the values of the RMSE and MSE, whereby MSE of wavelet model is lower than that of Black-Scholes model.
paper_219	There is also a need to assess the quality of data sharing across the enterprise network. When discussing the theoretical, it is easy to dismiss the challenges of limited resources to implement new policies such as schedule, budget, manpower, etc. Successful policy implementation amongst other factors requires system analysis methods that assist acquisition agencies in targeting limited developmental resources to areas of greatest impact to the overall mission objectives. This paper introduces such a method in support of achieving maximum data quality for military enterprise networks: a quantitative mechanism by which the value of different net-ready implementation options can be evaluated and graded. Importantly, a model for valuing enterprise data quality is introduced to bridge the gap between measure of technical performance and operational benefit. Section 2 of this paper provides a brief overview of the origins of net-centric and net-ready policies. The attributes in the new model represents data quality of the enterprise system within three contexts: net-centric measures, traditional network quality of service, and cyber-security. Admiral Jay L. Johnson, Chief of Naval Operations, stated in 1997 that the military is undergoing "a fundamental shift from what we call platform-centric warfare to what we call network-centric warfare"  [1] . In the years following, the Chairman of the Joint Chiefs of Staff Instruction (CJSCI) 6212.01 was released that set-forth the procedures for development and certification of a Net-Ready (NR) Key Performance Parameter (KPP); the NR KPP process later being subsumed into the Joint Capabilities Integration and Development System (JCIDS) process  [3]    [7] . Making data accessible requires providing authorized users the ability to view, transfer, download, or otherwise consume a data asset once discovered  [6] . Making data understandable requires alignment of terminology, data protocols, data formats, and data meaning between produced and consumer  [6] . Untrusted data can introduce error, uncertainty, and delay into the military decision process  [8] . To satisfy the attribute of entered and managed on the network, the IT connections to external networks required to perform net-centric operational tasks must be identified  [3] . Each net-centric information exchange defined MOPs that are measurable and each information exchange must also identify how the four criteria for net-centric data sharing (visible, accessible, understandable, and trusted) are satisfied for authorized consumers across the enterprise  [3] . Evaluation of the data sharing enterprise requires a holistic view that considers the net-centric attributes of the data simultaneously with the quality of service for the data network. The combined consideration for each of these areas yields a newly defined model for the data sharing enterprise comprised of three equally important attributes: data relevance, quality of data at source, and quality of service for the enterprise network. The most commonly used subjective rating in standards and in conjunction with QoE is the mean opinion score (MOS) where score of 1 is bad, 2 is poor, 3 is fair, 4 is good, and 5 is excellent. Where an accurate model exists for QoE as a function of that attribute's objective measures then it's used instead of sampling users' opinions. For most data the paper assumes that a model is inadequate and sampling the users' opinion is required. Usually the MOS is formed from arithmetic mean of a population of user opinions. But the arithmetic mean assumes that all user groups are of equal size which could lead to biased estimates of the MOS. The survey is not just restricted to a complete enterprise system but can be performed in the early design phases of prototypes and help analyze operational performance of an enterprise attribute as a function of its objective measures, i.e. A novel application to networks of a common survey practice is proposed to use the Horvitz-Thompson (HT) estimator  [10]  for determining the value of the total score of the population of size N. HT is commonly used because of its versatility as an unbiased estimator of the total for a sampling/sample with or without replacement. The Horvitz Thompson estimator for stratified sampling for the total Y is formula_1 where each independent random sample i, is denoted by y i , the probability of inclusion in the sample is denoted by p i . Where it is not easy to design a stratified sampling plan a stratification post collection of n samples without replacement can be constructed. The sample is organized into their various strata with the number of elements in each total number of strata k, in the total population N. The HT estimator for stratification becomes formula_2 and estimate of the stratified mean is formula_3 where m k = ∑ i in s k y i /n is the arithmetic mean of strata k. With each attribute having its MOS value using the stratified estimate in (3). But cost and time to use panels of experts to assess MOS has resulted in seeking alternative approaches. Full reference PESQ is formed by taking the speech codec output and comparing it with the original signal inputted into the codec  [13] . To produce a subjective rating MOS, or QoE, the ITU-T P.862.1  [13]  is used to map raw PESQ to the final rating. There are three levels of reference used in determining the models for estimating the subjecting ratings: full reference (undistorted service is available for comparison with distorted outcome), partial (or reduced) reference, and no reference. In  [15]  equations are formulated for quality in terms of interpretability and MOS; also with or without references. But just for completeness a brief explanation of QoS is given here. data at the source as well as objective measurements of that data on the network that can be used to correlate with the end user's QoE. Examples include understanding relationships between objective measures of QoS like jitter, throughput, and latency to be able to control the QoE. It is expected that models of QoE can be accurately built as a function of objective measures from network, applications, environment, and terminals. These models of QoE based on QoS objective measures are often referred to as "QoE\QoS" correlators  [21]  and  [12] . For both models  [21]  showed QoE had an exponential model in terms of their QoS objective metric(s). A simplistic rejection of non-relevant data is insufficient given the negative effect that excess amounts of data can have on human decision makers, even when the data in the network is restricted to only relevant data  [23] . The use of the QoE estimate is proposed to provide a subjective rating of the overall relevance of data shared on the enterprise. One way to analyze these properties and their effect on QoE is to form each stratum of (3) in such a way that we have homogenous groups of consumers with similar intrinsic DR properties. Tagged data relevance is a function of the quality of the taxonomy, the process performance, and the degree of understanding the producer has of the potential consumers. The measure of discovered data relevance is an indication of how well the enterprise system enables a consumer to differentiate the data product offerings accessible via the network according to the level of relevance and value to the mission objectives. To support a high level of discovered relevance, The taxonomy available to the consumer must be sufficient to explicitly discriminate the desired data features from the undesired. A poorly performing taxonomy would be one that prompts extraneous definition of detail or that includes terminology with such subtle variation as to lack the mutual exclusivity necessary to select between one term or the other. The processing element of discovered relevance is a measure of how well the search methods of the system match the descriptive words of the consumer (i.e. Just as the performance of tagged relevance is improved by an understanding of the consumer by the producer, discovered relevance benefits from awareness that the consumer has of the full range of data offerings of the enterprise and how they may be described. This is why an overall QoE value for data relevance is important as it indicates the relative success of the discovery process across multiple groups of users at finding data with intrinsic relevancy suitable to meet their needs. We explain here our method to evaluate the overall quality of data of the enterprise based on a minimax decision criterion and its connection with minimax game theory. In game theory, the minimax value of a player is the smallest value that other players can force without knowing the player's actions. Formal minimax definition  [24]  is formula_4 where a(i) denotes the actions of the i-th player of n players, a(-i) is actions of all other players except the i-th, and v i is the value function of player i. Consider a simple example in  Table 1  to illustrate the minimax in a game theory context. Rows in  Table 1  represent actions of player one and columns in  Table I  represent actions of player two. Going through each action of Player one with knowledge of player two actions L, R we have maximum payoff for player one for U of 5, for D of 5, for N of 4, making a minimax pay-off action of player one of N. Player two takes each action along columns for action L maximum payoff is 4 and 3 for action R, resulting in minimax action for Player two of R with payoff of 3. Thus, the minimax strategy is Player one move of N and Player two move of R with a payoff vector (4, 3). The minimax strategy in game theory inspired the decision theory approach of Abraham Wald's minimax model  [25] . The minimax model of Wald produces a decision with an outcome (payoff value) which is the worst chosen amongst the best outcomes of all decisions. The formal definition of Wald minimax model is  And for QoS the objective measures could be latency, packet loss, jitter, and sequence of packets. percentage of spatial and temporal coverage between consumer desired and actual data, and percentage of ontology alignment user and producer. formula_5 Thus, for each d in D, starting with d=QDS, a state vector s max (d) is found that produces max s in S(d) f d (s) where f d (s) is a discrete function formed by a predictive model for QoE or in the absence of a model formed from direct user sampled MOS value calculated using (3), i.e. 3 , as formula_6 In  Fig. This paper introduced a framework to determine data quality on enterprise networks for DoD net-centric and net-ready initiatives. The framework's data quality model consists of three attributes: quality of data at the source, data relevance, and network QoS. The paper described the data quality using minimax decisions based on users' survey ratings for a given enterprise configuration. The implemented strategy provides a tool for decision makers to prioritize data and manage their resources without comprising any part of the data sharing system. The paper notes the definition of the relationships between attribute's objective measures and the final quality of experience. Thus, support is indicated for further research in the development of objective measures using the definition of data relevance elements presented and to determine models for predicting QoE ratings as a function of these objective measures.
paper_241	So the use of transformers protection in electric power systems is very crucial and critical as they are required for efficient transportation of electricity to consumers for long term. This paper is concerned with the differential current protection scheme of transformer using a differential relay technique with arduino Uno microcontroller as a differential relay responsible for comparing the differential current values and sends trip signal to the relay (acting as circuit breaker) to open the circuit when there is fault in the protected zone as a result of imbalance in the differential current values. The demand of electricity in our modern day as led to advancement in electrical power systems, which is reflected in the development of all the power system device generators, different sizes of transformers, transmission lines and the protection equipment. Due to advancement in technology and daily use of electrical devices by industries, organizations and individuals, there is an increase in electricity demand which most likely results systems overload, reducing its efficiency and can cause damage to the transformer  [1] . Typically, Power systems are built to allow continuous generation, transmission and consumption of energy. The system is capable of sustaining a variety of environmental and operating impacts that resemble normal operating conditions which may include lightning striking the transmission lines, excessive loading, deterioration or breakdown of the equipment insulation resulting to power systems experiencing occasional faults such as phase to phase or phase to ground faults, over current, over voltage or even temperature based faults. The Differential current protection scheme is based on the principle that the input power to the transformer under normal condition is equal to the output power and is concerned with having a differential or balanced current between the primary and secondary side which would be digitally displayed  [2] . At all times, the Arduino senses the condition of the transformer. [4] Ochieng designed a microcontroller based power transformer protection system which uses a current sensor as the interfacing instrument between the current transformer and the pic16f690 microcontroller. This paper is concern with the implementation of digital differential current protection of a single phase transformer using Arduino Uno microcontroller as a mechanism of differential relay. The fundamental idea of the differential current protection scheme is to provide protection to the transformer if any fault occurs in the protected zone that will cause an imbalance in the differential currents. When there is an occurrence of internal fault an imbalanced or non-zero differential current will flow through the operating coil of the differential relay which would be substituted with a microcontroller that sends a trip signal to the relays needed to open the circuit. The magnetizing inrush current is a phenomenon that occurs during brief initial state of energization of the transformer even when the secondary side has no load connected to it and has its current a lot higher than the rated current  [6] [7] [8] . The power transformer to be protected is connected to the main AC supply and delivered power (P=IV) to the output terminal. The power transformer primary and secondary currents are measured by the current transformers, which are to be protected by the Arduino and the circuit breaker. The current transformers are each connected to the primary and secondary side of the step-up transformer and are used to reduce the currents to a measureable value that can be sensed by the current sensors. A bridge rectifier is used to convert the alternating current from the secondary side of the step-down transformer to direct current for use by the microcontroller. The differential protection of a transformer is implemented using Arduino Uno microcontroller as a decision making device that sends a trip signal to the relay (acting as circuit breaker) whenever there is faults (internal or external faults).
paper_251	In this paper we propose a concept of multi agent based batch scheduling and monitoring in cloud computing environment, where the number of agent are more than or equal to two with reducing the complexity of accessing and responding time. Heterogeneity in the computational requirement, dynamic choice and infrequent usages types of resources of the users in modern era has main challenge for service provider (application developer and hardware manufacturer). Various devices (Computing) and application has been developed and developing to fulfill the common users need. Hence demand of users is heterogeneous in nature so that varieties of application (hardware & software) have been developed to achieve the highest user satisfaction. Hence the utility types of computing paradigm will play an import role. Modern era is reflection of human creative thinking and application of optimize solution for the problems mapped and simulated into the machines using technological skills and advancement on it. Hence agent based solution has been proposed to meet the requirement of the modern cloud computing with pace of dynamic provision to insure shrink in shrink out (elasticity) of the cloud service provider to achieve highest scalability and reliability in extent of the maximum availability of the service to the requisites. Integration of the agent in the propose system provides the cost effective and reliable with dynamic pace, solution for efficient scheduling (elasticity of the resource and services) and proposer monitoring of the cloud computing systems. Codenvy -To develop an / are application i.e. Cloudbees -To deploy and test our SaaS application onto the cloud, propose system needs a platform i.e. For this Cloudbees service provider has been integrated onto the developed SaaS application. Propose system has surveyed and identified the problem domain that must be addressed in context of the cloud computing and consequently present the idea of agent integration. Our main research work is to enhance the agent based model for SaaS delivery in the cloud as depicted in the  [1]  and  [2] . To Evaluate and delivered the cloud computing services (SaaS) using agent (for better and fast delivery) using public cloud such as "New Relic and cloud bees". Deploying a web services under SaaS paradigm and evaluate the effectiveness of the web application in the cloud environment with the help of agent. Cloud bees service integrating a SaaS deployment on it and delivering through agent. Model for Proposed Work Our proposed work is to schedule and monitor cloud SaaS application onto the cloud and evaluate the performance of the same using proposed agent based. Resources: Algorithm for Scheduling (influenced from Aneka) developed onto the Cloudbees Proposed Algorithm Step1: Register user login Step2: Sign in the user login Step3: Select multi monitoring agent in cloud environment (for each subc_request with QoS constraints) Step 4: Mapping of resources between cloud client, cloud agent and cloud service providers. Following public cloud service providers or tools (platform) and data set has been used for evaluating the effectiveness of the proposed monitoring and provision approach. For evaluating the performance of the proposed agent system, the obtained results have been compared with  [2]  in which author has proposed "MABOCCF" the realization of the federation of different cloud (cloud interoperability) using agent. Author has choose two matrices to evaluate the performance of the it's proposed MABOCCF techniqueaverage user satisfaction and another one is average utilization ratio which has been derived from following fundamental (base) matrices 1. Number of tasks submitted at instant i (Ni) 2. Scalability Author has compared the outcome of their experiment with Non -MABOCCF (NMABOCCF) technique. All the matrices of the performance checking has been same meaning as our proposed system generated like 1. Response time is same as to average utilization ratio in addition to CPU usages. Average user satisfaction is same as to availability and scalability of the proposed system. Table 3  shows the user satisfaction of the deployed SaaS obtained results and has been compared with existing malty agent based method  Rather than performing analytical calculation, proposed system has been compared with the author's MABOCCF and NMABOCCF method. Hence proposed system performs better than  [2]  and any other methods as shown in figure 3. for their granting/releasing) and their exact monitoring in the cloud in context of public cloud computing service provider such as Cloudbees. Analytical analysis is to collect statistics to check the required number of resources needs or used and provides dynamic indication to better elasticity achievement. Proposed agent based solution for guaranteed better elasticity and their efficient monitoring of the resources in the cloud which helps to gather analytical statistics of the resources currently held and will be used such a memory, number of instances and CPU. For evaluation of the propose agent based method data set (jsp) has been developed using jsp web pages and deployed onto the cloud evaluating the elasticity and its monitoring. Proposed agent based methods obtained result has been found satisfactory and performs better than existing available solution. Following few areas has been chosen as future work as derivative of the proposed multi agent based solution where the current work can be taken further.
paper_272	Moreover the switching process associated with power electronics offers many modern benefits as a static switching system when joints together by using a snubber circuits for damping chopping current and transient over voltages or other facilities  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] . In addition the inside electroplates some characteristics can be described as followings: Non-sustained disruptive discharge voltages In general, the above characteristics are not involving associated with only vacuum switchgear itself a lone, but when connects the switchgear with power distribution transforms or synchronous motors, there are also many negative interactive processes for instance hysteresis currents, eddy currents and high voltage capacitive inside each transformer (Parasites electrostatics charged) feedback to the interrupter process. A more exact name would be metal vapor arc inside vacuum electroplate. J=2*10 6 (A/m 2 ) Joule heating as thermionic emission Where Ø is the thermionic work function. K is Boltzmann's constant (1.37x10-23 Joules/K) The constant A is approximately 6x10 5 for most metals According to the Harris model and classification who was classified "the positive ions initiated from ionization process in an Anode plate and generates joule heating to a cathode plate surface during switching process"; i. Ionization zone ii. The total above processing time was measured by Harris approximately (20-250µs)  [15] . Current chopping refers to the prospective over voltage events which can result with certain types of inductive load (power transformer) due to the premature suppression of the power frequency current before normal current zero in the vacuum interrupter. Virtual current chopping involves the load current in the other two phases being forced to zero by superimposed highfrequency re ignition coupled current. Restrike overvoltage due to the multiple re-ignition of circuit breaker when a switching interrupter process is initiated before current zero. Since the high frequency characteristics of the power system equipment are depend on stray capacitances that mentioned above and inductances also and they in turn depend on physical dimensions, equipment layout, materials, cable lengths, all these can be represented as follows  [17] ; The figure represents TRV & Re-ignitions computed in the system when switching off power transformer load that the transient voltage approximately (350µs-450 µs) and shows very high frequency sub transients both going in the negative direction of the voltage -Left side. Normally our analysis for the switching process basically on parallel RLC circuit as following; L= Indicative load of stator winding coils C= parallel parasite `s capacitors -see the introduction R= evaluating resistance that can be damping oscillating. Where η =0.5, the sine function changes from a circular to a hyperbolic function. By calculating the inductor current in parallel RLC circuit under conditions of a subsidence transient, but have a far wider application. However to gain familiarity with these curves, consider a specific example where the inductor current is required in a circuit in which the components have the following values: R=10 5 Ω. L =5 Henrys, C = 2X10 -8 F. These values are typical of unloaded transformer, where R represents the equivalent loss resistance. Suppose Vo =13.8√2 KV So. Zo = √L/C = 5X104 ohms η. The curve labelled η=2 in  figure 5  gives the shape of the current we are looking for. The ionization electroplates zone will be evaluated between (20us -250 us) respectively. The other processing steps as followings: a) Chopping current b) Restrikes voltages c) Prestrikes voltages d) Multi reignitions e) Voltage escalation f) Post-arc currents g) NSDD (non-sustained disruptive discharges) It is often misunderstood how all these phenomena are created and what are their real causes.
paper_294	The cultural heritage of any people can be redressed through the preserved cultural values tangibly stored and preserved in media and forms retrievable and usable. Audiovisual heritage are quite essential that the public library system must not afford to elude its storage not only for users but also for posterity. Cassava has played an important role as a staple crop in the feeding of the Tiv people. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. This study therefore, looks at the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. The study sampled 377 out of the population of 20,000. The snowball sampling technique was used in selecting the subjects for the study. The study found that the Tiv people had various strategies they adopted in managing postharvest losses of cassava. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. It was therefore, recommended that concerned policy makers should make policies that will allow public libraries to acquire store and preserve audiovisual materials on various indigenous knowledge for development to ensue; and for posterity; and that governments and other stakeholders should ensure that the public library sector is adequately funded and manpower is adequately provided as well. The cultural heritage of any people can be redressed through the preserved cultural values tangibly stored and preserved in media and forms retrievable and usable. The photographs people take, videos they shoot, speeches and music they record, capture in bits, every moment of their life, culture, event and times that inevitably speak volumes of their history. Abrigo and Abrigo (2010)  assert that, these media are jealously guarded and relayed, shown and played back for the younger generations. It is a legacy that people would want to impart to their grandchildren, so that the next generation would have the opportunity to understand their heritage. Audiovisual heritage are quite essential that the public library system must not afford to elude its storage not only for users but also for posterity. Cassava has played an important role as a staple crop in the feeding of the Tiv people. The Tiv had different management strategies of postharvest losses of cassava (Akom or Alogo) that includes dried cassava chips (kpor), imyorun or imorun (gari) processing, fermented cassava (akpu) and production of base (mtuhem) among others, which were potent managing postharvest losses of cassava. Public libraries are supposed to be conscious of ensuring that these postharvest losses management strategies are captured, acquired stored in audiovisual format. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. Apparently, if this is allowed to continue, the consequences cannot be foretold in the near future. The study intends to investigate the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. Specifically, the study sought to: 1. What are the Tiv management strategies for postharvest losses of cassava? What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? Public library is a library that is established and managed with public funds. Public libraries are established wholly or partly from public funds. They are not restricted to any class of persons in the community but freely available to all. They are the venue through which the overall information resources are made freely available to all (Edoka, 2000; Assoh, 2011; Shidi, Aju & Ashaver, 2014). Apparently, in order to achieve this mission, public libraries preserve a variety of information materials including print materials such as books, monographs, serials and periodicals; and non-print materials such as audio or audiovisual materials such as cassettes, microfiche and films among others. Public libraries anywhere they are established, is for the purpose of development. Their collection forms the body (theory) of knowledge from which the young members of the society are taught from. Quite so, public library's primary role in the society is to collect information sources of diverse kinds among others. This is their most crucial function of all. This is in line with Agber and Mngutyô (2013); Agber, Ugbagir, Mngutyo and Amaakaven (2014) who reiterated that for developing countries such as Nigeria, audio and audio-visual media are appropriate information formats for capturing information for the people. Benue State like other African Regions has farmers who cannot read nor write but who need information to develop their agricultural enclaves. The term cassava is most likely derived from the Arawak word for bread, casavi or cazabi, and the term manioc from the Tupi word maniot, which French explorers converted to manioc  (Lebot, 2009) . Agber (2007)  reported that a variety of cassava species are found in the Benue Basin including Aiv-kpenga, Pavnya (pronounced Panya) and Imande; and that Panya was discovered by a Tiv hunter called Adaga from Gaav Megaclan of the Tiv in about 1794 according to oral history. The crop therefore represents a household food bank that can be drawn on when adverse climatic conditions limit the availability of other foods. Therefore, the Tiv people develop different ways of processing it into durable forms soon after it is harvested, which forms part of management strategies for postharvest losses. Postharvest priorities across the globe have evolved considerably over the past four decades, from being exclusively technical in their outlook, to being more responsive to consumer demand. Consumer-driven trends which have contributed to this shift include rising incomes in urban areas, changing dietary habits, more women in the work-place, reduced time for meal preparation and growing demand for safety, quality and convenience  (Rolle, 2006) . Postharvest encompasses the conditions and situations surrounding the state of the food after separation from the medium and site of immediate growth or production of that food. Harris and Lindblad (1978)  asserted that postharvest begins when the process of collecting or separating food of edible quality from its site of immediate production has been completed. Apparently, postharvest losses therefore mean any change in the availability, edibility, wholesomeness or quality of the food that prevents it from being consumed by people. Food losses may be direct or indirect. A direct loss is disappearance of food by spillage, or consumption by insects, rodents, and birds. Peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips) 2. The public library owes the people a duty to go into the hinterlands of the Tiv nation in Benue State to collect audiovisual materials on these postharvest losses management strategies. The study adopted a survey design, which is the type of design that enables the researcher to collect data from a group of people through questionnaire, interview or observation techniques for the purpose of analysis and subsequent interpretation. There are 7 public library branches in the Tiv speaking local government areas of Benue State. These are the Benue state Library Board Headquarters in Makurdi and its branches in Gboko, Vandeikya, Katsina-Ala, Gungur, Aliade and Adikpo with about 33 workers  (Mngutyô & Amaakaven, 2013) . Significantly, the researchers got a hypothetical population of 993 library users and 18,974 cassava farmers making the total population of 20,000. The snowball sampling was adopted in selecting the subjects. They used the Fish Bowl Technique by writing Yes and No for the respondents to choose and those who chose Yes were finally given questionnaire to respond to it. The researchers did this until they arrive at the sample size of 680. The instrument used for data collection was Questionnaire constructed by the researchers. Section A of the questionnaire contained respondents' bio-data, which included sex and occupation. These research assistants were asked to administer and retrieve the questionnaire through personal contact to avoid delays associated with mailing and multiple filling. Data were analyzed using mean and standard deviations. Importantly, on each research question, data were collected on related items in the instrument. The collected data were analyzed using mean and standard deviation. Demographically, data were collected from 680 Library Staffs, Cassava Farmers and Library Users, out of which 348 were male representing 51.2% while 332 representing 48.8% were female. Apparently, 28 representing 4.1% were library staffs, 376 representing 55.3% were cassava farmers and 276 representing 40.6% were library users. What are the Tiv management strategies for postharvest losses of cassava? The collected data were analyzed and presented in  Table 1 . From  Table 1 , it can be seen that the mean rating scores of all the items are above 2.50. This means that peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips), peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour), peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom, slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base), peeling the cassava, fermenting, sieving and squeezing it into lumps known as Akpu, peeling the cassava, grating, squeezing (demoisturizing) it, mixing with salt and other ingredients and frying with palm or groundnuts oil known as Kweesi or Akweesa Akom and peeling the cassava, grating, squeezing and sieving the starch and preparing it for food as Kamu Akom are the management strategies for postharvest losses of cassava. In order to answer the research question, data were collected relating to the research question, analyzed and presented in  Table 2 . Table 2  shows the mean rating scores of all the items are below 2.50, which implies that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? The collected data were analyzed and presented in  Table 3 . Table 3 , it can be seen that the mean rating scores of all the items are above 2.50. This implies that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. Based on the results in  Table 1 , the findings from the study revealed that processing cassava into Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom, Mtuhem (Base), Akpu, Kweesi or Akweesa Akom and Kamu Akom are the Tiv management strategies for postharvest losses of cassava. Table 2 , findings of the study revealed that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). Findings from the study based on the result in  Table 3  showed that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. The research discovered that the Tiv people had various strategies they adopted in managing postharvest losses of cassava. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. Therefore, it was recommended that concerned policy makers should make policies that will allow public libraries to acquire store and preserve audiovisual materials on various indigenous knowledge for development to ensue; and for posterity.
paper_298	The study investigated the awareness and use of library catalogue for accessing information resources by users of Federal University of Kashere (FUK) Library. Survey research design was used, registered users of the University Library between 2016-2019 numbering 920 formed the population of the study where a sample of 272 (30% sample) was used based on 95% confidence level and 5% confidence interval of Sample Size Calculator. The instrument used for data collection was questionnaire. Data generated was analyzed using simple percentage and descriptive statistics. The findings of the study revealed that majority of the respondents were aware of library catalogue existence; they were more informed about card catalogue usage than OPAC for retrieving information resources. Poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. The study recommended that the library management should organize a periodic user education, orientation and sensitization programmes for the undergraduate users to create awareness and enable them gain the needed skills to use the library catalogue maximally when searching for information resources. Guidelines on how to use catalogue and library in general should be prepared and issued to registered users. Regular shelf reading should be done so as to establish right contact between library users and library materials. The university library goals and objectives are to provide adequate and relevant information resources both in print and online for university community to support teaching, learning and research (these for undergraduates students may refer to class work, assignments, research/project work, term papers, seminar presentation by providing relevant information and services provision for effective and efficient achievement of academic pursuit). University library provides well stocked information resources and trained personnel to organize available information materials and assist faculty members and student users in the retrieval and use of these resources. The traditional goals and objectives of the library catalogue are to enable users to search a library's collection to find items pertaining to specific titles, authors, or subjects. Library catalogue is considered as an interface of information retrieval system which assists information searchers to access resources of libraries using several access points. Like the card catalogues, sheaf catalogues, books, machine readable catalogues (MARCs) and online public access catalogues (OPACs). Following advancement in ICT and subsequent development of Online Public Access Catalogue (OPAC), the traditional concept of access to library resources which many scholars identified to be prone to numerous challenges has changed. OPAC has brought a changing relationship between libraries and their users in the changing ICT environment. Library OPACs first emerged in the late 1970s and early 1980s and have gone through several cycles of change and development all geared towards improvement. The library OPAC as an automated catalogue system was developed as a tool to locate those information resources that had been acquired, organized and preserved by the library itself. In this environment, the user is both the primary searcher of the system and the user of the information resources  [2] . Online public access catalogue (OPAC) is the most modern form of library catalogue, whereby bibliographic records of all the documents collection are stored in the computer memory or server. Again, awareness of the library catalogue is the ability of the students to have communication and consciousness of its essence, its retrieval technique as well as their relevance to the information user. KumarandVohra investigated the use of Online Public Access Catalogue by the users at Guru Nanak Dev University Library, Amritsar (Punjab) and discovered that majority of the respondents 68.7% were not aware regarding OPAC, 12.5% stated the reason to be lack of assistance from library staff and slow speed. The purpose of using OPAC majority of the respondents 63.2% stated that they use OPAC to know the availability of required document. The study suggests that the users should be made familiar with the use and operation of the OPAC by providing special training  [6] . Kumar revealed that 26.99% respondents use the OPAC fortnightly and 22.84% weekly. 61.59% use the OPAC to locate a document on shelves and 58.48% to know the particular book is available on the shelves or not, 37.71% to know the bibliographical details, 31.14%. 91% respondents used the title search approach and 83.04% used the author search approach, User also indicated that the information regarding the problem faced by the respondents while using the OPAC like 74.39% faced by the problem lack of proper guidance about OPAC followed by 67.47% lack of awareness, 36.33% satisfied with the OPAC and its services  [7] . If a user lacks skills to use a library catalogue, the user may not be able to make effective use of the library resources. The studies of Oghenekaro found that users exhibit patterns of library catalogue usage, that education, experience and sophistication of library users determine the pattern or level of library catalogue use  [1] . The author attributed the reason for the poor usage to lack of user education programme  [9] . In essence, students use the catalogue to enable them conduct research in the library. While this is a welcome development, it is important to occasionally assess the effectiveness of the library catalogue especially from the users' point of view. However, it is observed sometimes that the bibliographic tools that supposed to lead or guides user to the location of a particular item in the library are either found in adequate, misleading, totally not provided or somehow incomplete. The study is designed to achieve the following objectives: a. To find out methods employ by students to consult library catalogue to search for information resources. The study is designed to answer the following research questions: a The survey research method was adopted for the study, and questionnaire was the instruments use for collecting data. The stratification sampling technique was employed to sample the entire registered population of undergraduate library users from each level (100-500) in Federal University of Kashere respectively. The completed and returned questionnaires were analyzed using descriptive statistics, percentage and frequency distribution. This section presents questionnaire collected on the basis of gender of the research respondents. The gender breakdown is presented on  Table 1 . The question on the demographic information of the respondents was on gender presented in  Table 1 . It shows that majority of the respondents 188 (69%) were male, followed by females 84 (31%). This shows that majority of library users in the Federal University of Kashere Library were male. Table 2  revealed that majority of the respondents 262 (96.3%) were aware of the existence of the library catalogue in the university library. The awareness of respondents would depend on the way the library informs their users about the use and functions of library catalogue revealing the available resources and its location in the library. 26 (10%) of the respondents were aware of the existence of OPAC as a retrieval tools in the library. Their responses were presented in the  Table 4  below. Table 5  reveals that 106 (40%) got their awareness through library staff, followed by those that knew it through the user education programme (GST) 88 (16%). The result proves that the management of the university library provides awareness opportunity to users for retrieval and utilization of information resources, except that they need to put more emphasis during library orientation to add to existing ones. Never  Total  F  %  F  %  F  %  100  20  40  28  56  2  4  50  200  14  32  20  45  10  23  44  300  24  44  20  37  10  19  54  400  42  75  12  21  2  4  56  500  58  100  0  0  0  0  58  Total  158  60  80  31  24  9  262  Key: F=Frequency, %= Percentage. Table 5  indicated that majority of the respondents 158 (60%) used the library catalogue regularly when searching for materials in the library. 24 (9%) of the respondents never used the catalogue but they were aware of it existence in the library as indicated on table 2 above. This shows that these few users preferred browsing the shelves to search for information resources. This finding is contrary to that of Oghenekaro whose study found low usage of the library catalogue attributed the low frequency of use to lack of user education programmes  [1] . Total  F  %  F  %  F  %  F  %  F  %  F  %  100  10  20  0  0  6  12  28  56  6  12  0  0  50  200  10  23  6  14  10  23  16  36  2  5  0  0  44  300  10  19  6  11  14  26  12  22  6  11  6  11  54  400  14  25  0  0  36  64  6  11  0  0  0  0  56  500  2  3  0  0  54  93  2  3  0  0  0  0  58  Total  46  18  12  5  120  46  64  24  14  5  6  2  262  The finding indicated poor use of OPAC 12 (5%) at the university library. This finding is in agreement with Fabunmi & Asubiojo whose study on OPAC use by students of Obafemi Awolowo University, Ile-Ife found out that though many students were aware of the OPAC, few actually made use of it. The study found that majority of the respondents were aware of library catalogue in the library including OPAC  [12] . The finding in this case was surprising considering that majority of the respondents had indicated that they do not use the OPAC. It could be concluded that majority of the respondents used the library for research work rather than for leisure and recreational purposes. The results are presented in  Table 8 . Table 8  focused on the challenges associated with catalogue use for information retrieval. This indicates that good of the respondents had difficulties using the library catalogue because the respondents lack sound ICT skills that could enable them use the OPAC. Again, the later challenge could be attributed to lack proper shelf reading by the library staff, which made users not to locate material indicated available in the library by the catalogue and not visible on its shelf. Again with that of [Ogunniyi & Efosa whose study concluded that the problem of catalogue use is associated with lack of knowledge on how to use the library catalogue  [11] . The study revealed that majority of the university library users were male. It is disturbing to discover from the study, that most of the respondents were aware of the card catalogues as access and retrieval tool for searching for information resources in the library. Therefore, proper orientation and awareness campaign should be done to address this ugly situation. It could be concluded that majority of the respondents used the library for research work rather than for leisure and recreational purposes. Another source of frustration was reasonable number of the respondents indicated poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. Considering the important roles library catalogue plays in effective and efficiency use of library information resources, the research recommends the following to improve the use of library catalogue for information access and retrieval: a. Because most challenges associated with the use of the OPAC often have to do with lack of ICT skills on how to make use of it. c. Guidelines on how to use catalogue and library in general should be prepared and issued to registered users. d. Regular shelf reading should be done so as to establish right contact between library users and library materials and avoid misplacement or wrong shelving of information resources.
paper_305	The implementation was based on the java netbeans development platform to create an interface that was used to train a model and its subsequent use in predicting credit decisions. The results obtained proved that such a mechanism can be applied to augment manual credit appraising processes, especially where large volumes of applications are to be processed within limited timeframes. Therefore the screening of the customer's financial history as well as the ability to remain faithful to new financial obligations is a very significant factor before any credit decision is taken and it is a major step in reducing credit risk. We begin by highlighting the relevant base literature upon which the experiment was setup and the subsequent experimental setup and finally the results obtained with corresponding analysis and conclusion. After some experience, these officers develop their own experiential knowledge or intuition to judge the worthiness of a loan decision. Further, the complexity of loan decision tools and variation between applications is an opportunity for the use of a machine learning tool to provide learning capability that does not exist in other technologies. Ensemble meta modeling techniques, are empirically some of the best machines learning tools applicable to financial risk analysis. The purpose of this study was to develop a loan decision system using the logistic regression Meta modeling algorithm -Logitboost around Java based open source software for the Kenya commercial banks. The general objectives of this study were to: 1) Implement the meta learning algorithm -LogitBoost to develop as system for evaluating credit applications to support loan decisions in Kenyan financial institutions 2) Outline some of the challenges of using the learning algorithm in the decision-making process for the banking industry in Kenya 3) Champion the applicability of Java as an open source software in business intelligence applications From time immemorial in the banking sector, banks have relied on the personal assessment of loan risks or on the traditional statistical methods to predict the default of loans instead of using a standardized evaluation tool. Further, the study considered a binary output from the classifier, hence dependent variable can only take on accept or reject values with an emphasis on the banking industry in Kenya; though the results can easily be generalized to institutions elsewhere. Although the classifier takes this into account through voting -in which those values that meet certain thresholds are promoted to either of the classification values, most of such incidences are minimal and can be handled through judgmental procedures by re-examining those peculiar cases and applying policies as laid out. Further, the classifier labels every classification instance with a level of confidence value. The study has left such analysis to oversight procedures especially where the confidence level of the classifier does not meet a certain threshold. This calls for the use of more efficient and effective loan screening tools and procedures. One of the earliest automated procedures uses statistical tools which have fallen short of the inherent challenge for today's commercial banks is their desire to understand large amounts of information and reveal useful knowledge to improve decision-making. Today, lenders are making increased use of new and innovative techniques -the key being data mining and machine learning to evaluate loan applications for business and financial prospects  [2, 3] . Further, ensemble learning algorithms -those that combine a number of base algorithms, through empirical reports typically lead to better results. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced  [4] [5] [6] . One such algorithm that effectively addresses these issues is the LogitBoost Meta classifier -based on the log of the odds ratio for the dependent variable  [7, 8] . Although the model was reported to perform relatively better than models developed using other approaches; as part of the limitations and recommendation, they suggested that such a model is usually a black box and more insight the model parameters was required to make it more effective. The solution to the problem was an adaptation of ensemble machine learning strategies where a 'weak' classifier, commonly referred to as a base classifier was boosted through a series of adjustments through weighting and re-sampling to develop a better learner which was an additive aggregate of individual learners. The reported technique derives from the intuitive understanding that instead of putting all the effort on finding highly accurate base classifiers, it becomes sufficient or even desirable to use a set of weaker hypotheses. Resulting classifier is a simple threshold on a single feature a) Outputs a +1 if the attribute is above a certain threshold b) Outputs a -1 if the attribute is below the threshold In majority voting, to predict the class of a new item, each base classifier got to vote for either the 'accept' or the 'reject' class. It can be proven (as discussed here-under), that under the assumption that all individual classifiers have the same prediction rate and that the distribution of the data correctly classified by each base classifier is independent and random, this is the best possible strategy. The implementation detailed lay in the use of a logistic regression that models the posterior class probabilities Pr (G = k|X = x) for the K classes. In our study, the variable k was bi-valued and took on either 'accept' or 'reject' values and K was set at 2. The model was specified in terms of K −1 log-odds that separate each class from the base class K. a) With K attributes , there are K different decision stumps to choose from b) At each stage of boosting i. given reweighted data from previous stage ii. Classify and obtain performance metrics. Select the next partition as testing and use the rest as training data. v. Classify until each partition has been used as the test set. This strategy relies on two separate files, one for training and the other for testing. This strategy is similar to the use of two files as discussed earlier but relies on the learner to automatically partition a given data set into two given a split percentage Class =Accept: The number of correctly classified instances is 12 and that of instances classified as belong to the class is 13. This gives a precision value of 12/13=0.92 Class =Reject: The number of correctly classified instances is 7 and that of instances classified as belong to the class is 7. This gives a recall value of 12/12=1 Class =Reject: The number of correctly classified instances is 7 and the number of instances belonging to the class is 8. The ROC graph is a plot of two measures: Sensitivity: The probability of true classifications given true instances i.e. Three options were investigated for training the algorithm namely: a) The use of single file both for training and testing the model through stratified cross validation. This is a strategy where the training file was portioned into complementary data sets called the training set and the validation set. The model accuracy using this procedure was 86.86% making it a fairly reliable strategy b) The use of separate training and testing data sets returned an accuracy of 95% making it a relatively better strategy c) The use of a ratio to determine the size of the training and testing files from one data set returned an accuracy of 88.64% Therefore, it implies from these findings that the use of separate files for training and testing of the model returns the best model accuracy and hence should be adopted. Finally, as stated earlier in the introduction, it is not prudent to completely rely on an automated credit appraising as some cases might require subjective interpretation and personal judgment. As a conclusion, the reported work indeed confirmed that: 1) Machine learning procedures can be applied in financial modeling applications to augment manual underwriting techniques 2) These procedures can greatly improve the efficiency of such techniques because of their ability to handle large items of data generating very useful statistics 3) This work can be improved through the use of enhanced data set pre-processing procedures, the use of a cost matrix as well as parameter tuning to settle on the most effective set for various data mining requirements.
paper_310	We are now living in the 21 st century. This project describes how to control a robot using mobile through Bluetooth communication, some features about Bluetooth technology, components of the mobile and robot. We present a review of robots controlled by mobile phone via moving the robot upward, backward, left and right side by the android application such as Arduino, Bluetooth. We derived simple solutions to provide a framework for building robots with very low cost but with high computation and sensing capabilities provided by the smart phone that is used as a control device. This paper represents android application based Bluetooth controlled robotic car. We interface the Bluetooth module with the system so that we can easily control the system by smart phone application. Here in the project the Android smart phone is used as a remote control for operating the Robot. The controller acts accordingly on the DC motors of the Robot. In achieving the task the controller is loaded with a program written using Embedded 'C' language. Still there exists a requirement of a cost-effective automation system, which will be easy to implement. An example of such a costeffective project has been proposed here. The design of the system is kept as simple as possible. Few things like cost-effectiveness and simplicity in design, lowprofile structure etc. have been kept in mind before designing the project. Our system aims to achieve the target to design a system that can provide following functionalities with a simple and easy-to-use interface: a) Develop an android application that will act as an remote of a robot. g) The Bluetooth module is connected with the Arduino UNO board for the connection with the user. The microcontroller is programmed with the help of the Embedded C programming. Bluetooth module will give the commands given by smart-phone to the microcontroller. It sends the data to microcontroller through Bluetooth module. The novelty lies in the simplicity of the design and functioning. Arduino software (  Figure 5 ) is used to put the instruction of whole functions of this system to the microcontroller. The program is stored in the EEPROM of the microcontroller, which is present in the Arduino board. By this software we put the data and instruction for forward, backward, left, right operation of this system. Motor driver switches accordingly the data bit, if the data bit is low then the corresponding pin of the motor driver doesn't work else highbit then thecorrespondingpinof the motor driver is on. There are two steps of the programming. Second loop part where the program runs continuously. The working principle is kept as simple as possible. The working principle of the circuit has been elaborated with the help of a block diagram, of the system interconnection as shown in  Figure 6 . The microcontroller, thereby, sends instructions, which when executed, helps in functioning of the motor driver. The Bluetooth module is connected with the Arduino UNO board for the connection with the user. When user sends any data to the Arduino board then the corresponding pin of Arduino goes to high state and switches the motor driver ic in the on mode. The circuit diagram of this project is shown below: Here at first we construct the circuit as shown in  Figure 7 . Here the Bluetooth module act as a receiver which receives the instruction from the smart phone (remote or transmitter). Then the microcontroller decides the operation for the instruction which is coming from the smart phone. The functions of the given instructions are operated by the microcontroller. When any input is given then the motors moves as per the preloaded functions in the microcontroller. The novelty lies in the fact that it is a cost-effective project with a simple and easy to use interface compared to existing ones.
paper_333	The diagnosis of diseases on the plant is a very important to provide large quantity and good qualitative agricultural products. Enset is an important food crops produced in Southern parts of the Ethiopia with great role in food security. There are several issues and diseases which try to decline the yield with quality. The aim of this study is to design a model for Enset diseases diagnosis using Image processing and Multiclass SVM techniques. The strategy of K-fold stratified cross validation was used to enhance generalization of the model. Food security is a challenge in many developing countries like Ethiopia. It is also the sector that is given an overriding focus in the government's plan for growth of the economy as a whole. This is commonly known as false banana, the Ethiopian banana or the Abyssinian banana. Ethiopia is one of the grand producer of Enset in African continent countries. There are several issues and diseases which tries to decline the yield with quality. Particularly, diagnosis of potential diseases on Ethiopian banana is based on traditional ways and due to limited research attention given to Enset crop production. There is a potential need for technology supported alternative systems to support the manual identification of Enset crop diseases so as to optimize the accuracy for remedial action. If the Enset crop diseases can be minimized then definitely the production will significantly be increased to contribute to the country's economy towards a new boost. In our country few researchers found the promising solutions to different plant diseases diagnosis such as maize, rose flower, coffee and others using computer vision and machine learning techniques  [3, 4, 5] . The remaining part of this paper is organized as follows. In Section III, we present the architecture of proposed system. Experimental results are reported in Section IV. Section V presents the conclusion and discusses of future works. In this work, disease identification was done by using characteristics of Enset plant. The feature of normal and diseased Enset image features was extracted to train kernel support vector machine. Figure 2  shows the architecture of the proposed system A. Dataset Partitioning The images of the dataset are collected from different parts of southern Ethiopia. Those images are collected by using a digital camera and some of them are collected from secondary sources like research centers which collects pictures for visual inspection of Enset disease. As it is mentioned in the above section, for the experiment two diseases and one normal of Enset were identified. From all those disease category a total of 460 Enset leaf images are collected from which 368 is used for training and 92 images are used for testing. Test Result For the experimentation a total of fifteen experimental setups which is color, texture morphological and a combination of each and a multiclass Kernel support vector machine model: Linear Kernel, polynomial Kernel, RBF kernel and Quadratic Kernel was used. In order to measure the performance of the classifiers K-fold (in this case k is 10) cross validation method was used. The detail classification result is shown in  figure 4 . From the figure it is shown that the RBF kernel classifier has classified the given enset image in its correct class category by 94.04% and 92.44% respectively. We designed a system which can detect Enset disease automatically and this will help the farmers to detect the diseases in its early stage and to take relevant action.
paper_389	This paper systematically describes the definition, model structure, parameter estimation and corpus selection of the conditional random field model, and applies the conditional random field to the Chinese word segmentation and the Chinese word segmentation method. Word probability, the paper explores the probability characteristic of word location. Experiments on the corpus show that the introduction of the word position probability feature has improved the accuracy, recall and the value of Fl. The so-called Chinese word segmentation, is the process of word segmentation as each Chinese character classification process, by marking each character in the sentence to segmentation. Zhou J built a hybrid method of Chinese word segmentation around CRF model. For example, "O" can be used to represent individual Chinese characters, "B" means Chinese characters appear in the head, "I" means Chinese characters appear in the middle or the end of the word. So the word segmentation problem is transformed into a pure sequence data labeling problem, you can use a lot of sequence tag algorithm for word segmentation. Figure 1  is an example of the use of Chinese characters marked word segmentation. Finally, according to the mark of each Chinese character, the mark result is "OOBIO", so the result is "这 / 是 / 武汉 /." As mentioned earlier, the CRF makes it easy to add any feature in the observed sequence to the model, so that not only the transfer and emission characteristics of the traditional HMM sequence model can be incorporated into the model, but also some other The feature information associated with the observation sequence or with the language itself is added to the model. In this paper, CRF automatic word segmentation experiments, the use of features include the following two: a single word characteristic: a position on the word characteristics. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. CRF word segmentation system is a word segmentation system based on conditional random field, and the Chinese word segmentation method is adopted. Secondly, feature extraction is carried out to generate training corpus and test corpus which can be recognized by CRF model tool. Then the training corpus is trained to generate a CRF model, and some training parameters such as iteration number are added in the training process. Using the CRF model generated by the training, the test corpus is tested and a prediction result is obtained. The performance of a word segmentation system mainly depends on two aspects: segmentation precision and word segmentation speed. Word segmentation accuracy, also known as word segmentation accuracy, is the core performance index system. The performance of Chinese automatic word segmentation is evaluated by the following three indexes: correct rate (P), recall rate (R) and F value. Where, P refers to the accuracy of word segmentation; R refers to the word recall rate; F value refers to the P and R integrated value. CRF The word segmentation system is a word segmentation system based on the conditional random field, and uses the feature template one, the feature template two and the feature template three. Moreover, since the word position probability feature is extracted completely from the training corpus, some of the participle criterion information of the corpus is extracted to a certain extent, so that when the test set is tested, Corpus. This chapter first briefly introduces the CRF tools, experiment corpus and standard of experimental evaluation in Chinese word segmentation experiments. The experiment not only demonstrates the influence of the choice of conditional random field model parameters and Chinese character annotation set on the experimental results, but also verifies the validity of the new features and the feasibility of the new method. In the experiment, only some feature information is used, and most of the features are extracted from the training corpus, we have achieved good results. As the Yangtze River Daily corpus is from the newspaper news, for the news corpus in the special format, such as title, poetry, weather forecasting, etc., to our model training has a certain impact, so if the corpus in the handling of these disturbances, Our model should have better performance.
paper_391	Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. Globally, tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The data for this study were the Tetanus Toxoid data set from the Ethiopian Demographic and Health Survey (EDHS) 2011, and analyzed using the Knowledge discovery process of Selection, Processing, Transforming, mining, and interpretation. The WEKA 3.6.1 tool was used for classification, clustering, association and attribute selection. The accuracy rate of the classifiers on training data is relatively higher than on test data and the multilayer perceptron is the best classifier in our data set on Tetanus toxoid. In the cross-validation with 10 folds, correctly classified best are by naïve Bayesian 63.30% and the least accurate were by k-nearest neighbor 60.52%. Single data instance test using Naïve Bayesian was done by creating test 1, test 2, test 3, and test 4 data test instance, three of them are correctly predicted but one of them incorrectly classified. The maximum confidence attained in the general association is 0.98. But, in the class attribute, it is 0.72. The literacy status of the mother has high information gain with the value 0.046. As a conclusion, the best algorithm based on the TT vaccination data is multilayer perceptron classifier with an accuracy of 67.28% and the total time taken to build the model is at 0.01 seconds. Multilayer perceptron classifier has the lowest average error at 32.72% compared to others. These results suggest that among the machine learning algorithm tested, multilayer perceptron classifier has the potential to significantly improve the conventional classification methods for use in EDHS data of Tetanus toxoid. Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. And the major cause of early infant deaths in many developing countries is often due to failure to observe hygienic procedures during delivery. Infection is acquired through environmental exposure of any broken skin or dead tissue such as a wound or when the umbilical cord is cut to the spores of the bacteria. Globally tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually, accounting for up to 25% neonate's death in some African countries  [3] [4] [5] . As of December 2012, maternal and neonatal tetanus persist as a public health problem in 30 countries, mainly in Africa and Asia  [2] . However, until now maternal and neonatal tetanus persist as public health problems in 36 countries, mainly in Africa and Asia. The TT vaccination schedule in Ethiopia for childbearing women follows the schedule recommended by WHO for developing countries  [6] . Five doses of TT can ensure protection throughout the reproductive years and even longer. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. It is a young and fast-growing field also known as knowledge discovery from data (KDD) and used for discovering interesting patterns from data in various applications  [7] . The health care industry is one of the world's largest and fastest growing industries having a huge amount of healthcare data. This health care data include relevant information about Client, their treatment, and resource management data. The information is rich and massive. Data mining techniques are more effective that has used in healthcare research. This problem caused the loss of time and effort in the healthcare system environment and spending lots of efforts and costs without having evidence-based information for planning and intervention. The standards used are a percentage of accuracy and error rate of every classification techniques used. This process must have a model to control its execution steps. Knowledge discovery from data for prediction of the tetanus toxoid immunization among the women of childbearing age in Ethiopia following the standard process, guiding us in the analysis process, and exposing those aspects that could otherwise be neglected. The  Figure 1 (Adapted from  [7] ) shows, the basic phases of the knowledge discovery from data, we have undergone. Preprocessing solves issues about noise, incomplete and inconsistent data. The next phase is the transformation of the preprocessed data into a suitable form for performing the desired data mining task. In the data mining phase, a procedure is run that executes the desired task and generates a set of patterns. However, not all of the patterns are useful. The methodology of this study was the practical research method applied on the Tetanus Toxoid data of the Ethiopian DHS 2011. The data used in this investigation are the TT immunization data. It has a dimension of 7033 rows and 12 columns. Only 80% of the overall data is used for training and the rest 20% was used for testing the accuracy of the classification of the selected classification methods. csv" file formats and stored as an ". Classification is one of the data mining techniques and it is used to group the instances which belong to same class  [8] . Classification also extracts models describing important data classes. The classification methods used in this study is to classify data according to their classes putting the data in a single group that belongs to a common class. The approaches are; (a). And the topmost node in a tree is the root node. This method of classifiers is based on learning by analogy, by comparing a given test tuple with training tuples that are similar to it. The training tuples are described by m attributes. In this way, all the training tuples are stored in an m-dimensional pattern space. When given an unknown tuple, a k-nearest-neighbor classifier searches the pattern space for the k training tuples that are closest to the unknown tuple. These k training tuples are the k "nearest neighbors" of the unknown tuple  [7, 10] . Bayes Classification Methods Bayes classifiers are statistical classifiers based on Bayes' theorem, which is probabilistic learning method. It is made to simplify the computations involved and, in this sense, is considered "Naïve"  [7] . Multilayer preceptor Multilayer preceptor is a simple two-layer neural network classifier with no hidden layers. We will consider the case of where the class tuples are more or less evenly distributed, as well as the case where classes are unbalanced. Confusion matrix; is used in this study, include accuracy, sensitivity, specificity, and precision. Understanding them will make it easy to grasp the meaning of the various measures. To discover acceptable classes using Simple K-Means based on the principle of maximizing the similarity between objects in the same class i.e., intra-class similarity and minimizing the similarity between objects of different classes i.e., inter-class similarity  [7] . "How does the k-means ( ) algorithm work?" The k-means algorithm defines the centroid of a cluster as the mean value of the points within the cluster. First, it randomly selects k of the objects in D, each of them initially represents a cluster mean or simply center. The k-means algorithm then iteratively boost the within-cluster variation. For each cluster, it computes the new center using the objects assigned to the cluster in the previous iteration. All the objects are then re-assigned using the updated center as the new cluster centers. The association rules that contain a single predicate are referred to as single-dimensional association rules. Another threshold is Confidence, which is the conditional probability than an attribute appears in a transaction using the Apriori algorithm. The InfoGainAttributeEval with Ranker T used with respect to the class attribute and visualization was done in the attribute selection. (Table 1)  As can be seen in  Figure 2 , the accuracy rate of the classifiers on training data is relatively high. A multilayer perceptron is the best classifier in our data set. Using evaluation with cross-validation (10 folds) correctly classified best are by naïve Bayesian 63.30% and the least accurate were by K-nearest neighbor 60.52%. (Table 3)  Simple K-Means preferred the method of clustering for this project we have adjusted the attributes of our cluster algorithm by clicking Simple K-Means. The only attribute of the algorithm we are interested in adjusting here is the "Num clusters field", which tells us how many clusters into five based on the instruction given. Cluster 3-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 4-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, orthodox in religion, Amhara in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education no education, women age 35-49, mother marital status married, hh_head Female, tt-vaccinated yes. As explained in the    The literacy status of the mother has high information gain (0.046) and followed by the literacy status of the mother (0.041) and the least information gained was head of household (0.00000147). In this study, the data mining tool and algorithm like (J48, k-nearest, and Bayes) is used for selecting the training and test data, for classification, and K-means approach for clustering, single-dimension association rule to identify the best association. csv" file and features were described using WEKA performance. As a conclusion, the best algorithm based on the TT vaccination data is multilayer perceptron classifier with an accuracy of 67.28% and the total time taken to build the model is at 0.01 seconds. Multilayer perceptron classifier has the lowest average error at 32.72% compared to others. These results suggest that among the machine learning algorithm tested, multilayer perceptron classifier has the potential to significantly improve the conventional classification methods for use in medical data.
paper_402	The BPNN and SFS were used interchangeably to identify the relevant features that contributed with the response variable. As a result, the BPNN with the selected features was able to interpret more accurate results (r 2 = 0.991) than the model with all the features (r 2 = 0.816). The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. Several types of machine learning algorithms such as Artificial Neural Network (ANN) have been used in different fields for the development of models that predict response parameters (experimental dataset) using certain independent input parameters. However, an experiment could have a large number of independent parameters most of which are redundant and have negligible effects on the response parameters. The need for soft computing tools and models for the prediction of behavioural properties of engineering components, systems and materials is continuously rising. ANN emerged as one of soft computing paradigms that have been successfully applied in several engineering fields  [1] . Recently, research interest has revolved around the development of ANN models to interpret the behaviour of structural materials such as steel, concrete, and composites  [9] [10] [11] [12] [13] [14] . The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete  [14] [15] [16] [17] . In this study ANN is employed with other machine learning techniques to identify the parameters that capture the compressive strength of UHPC using data collected from the literature. However, most of the aforementioned mixtures result in exhausting a large amount of resources and performing tests on many batches, while barely predicting the strength of UHPC  [19] . [19]  used the back-propagation neural network (BPNN) and statistical mixture design (SMD) in predicting the required performance of UHPC. Despite the statistical advantages of ANN, it has been long regarded as a black box that evaluates functions using input covariates and yielding outputs. This will reduce the amount of parameters in the model, which will improve the computation complexity of the ANN model and simplify the derivation strategies of a mathematical model used to predict the compressive strength of UHPC. There are several machine learning techniques, in the literature, that assist researchers in identifying the underlying covariates impacting the prediction model. Sequential feature selection (SFS) is a machine learning tool that sequentially selects features and inputs them into a fitting model (i.e. This technique makes use of ANN's complex computation and allows the SFS tool to select and remove the influential and redundant parameters, respectively. [29]  used the Markov Blanket with a wrapper method to select the most relevant features of human motion recognition. Four sets of open human motion data and two types of machine learning algorithms were used. Moreover, Rodriguez-Galiano et al. [30]  used SFS when tackling ground water quality problems, where 20 datasets of parameters were extracted from a GIS database. Four types of machine learning algorithms were used as wrappers for the SFS. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. The input neurons are responsible for containing the independent parameter presented by the user, the wires represent the randomly generated matrices called weights that manipulate the function's slope or steepness, the hidden neurons map the weights variables using an activation function, and the bias units control the output function's shift, either upward or downward. Equation  1 shows the linear combination of mapping weights from each input neuron, via wires, to the hidden neurons. SFS is composed of two components: the objective function, which is the criteria the algorithm follows when selecting the features (i.e. the NMSE), and the search algorithm, which are the methods of how the machine add/removes the features from the subset. There are two types of search algorithms: sequential forward selection and sequential backward selection. In this study, the previously verified ANN model was used as the objective function and the forward selection was used in selecting the relevant features. The ANN numerical solver, Levenberg-Marquardt, was verified by testing different number of neurons using a basis like the normalized mean square error (NMSE) to measure the error. Hence, for each neuron tested, ten NMSE values will be stored in a column vector, where each column vector will be averaged and plotted against its corresponding number of neuron(s). Figure 2  shows the plot of all the scenarios with the minimum point circled at 11 neurons. Therefore, 11 neurons is, approximately, the number of neurons that is sufficient enough for BPNN to facilitate an accurate ANN model for the collected dataset. The SFS algorithm was run 200 times to capture all possible combinations of independent features when using ANN. Table 2  tabulates the percentage of features that were used during the 200 trials. Based on the results of these trials, the most abundant combination during the SFS analysis, within a 20% threshold, was selected as the important parameters that contribute mostly in the model. In this study, four variables (Cement, Sillica Fume, Flyash, and Water) were selected as the most relevant features for the prediction model. The selected features, using SFS, were analyzed by the previous BPNN model. As a result, the model that used the selected features showed stronger agreement with the experimental results in contrast with that prior to the selection. Table 3  shows the statistical measurements calculated for both cases. It was observed that the r 2 and NMSE before and after selection yielded 81.6% and 99.1%, respectively, and 0.0594 and 0.026, respectively. The correlation plots between the predicted and experimental results for the ANN models, with and without selected features using SFS, are summarized in  Figure 4(a)  presents the percent deviation, where an arbitrary percent deviation was plotted above and below the perfect fit line with a deviation value of ±20%. As a result, the ANN model with the relevant features was capable of predicting 89.6% of its values within the aforementioned boundaries, as opposed to the ANN model with all the features which predicted 58.7% of its values within the boundaries. Table 4  shows the coefficient values, with their corresponding symbols, for each UHPC constituent with the statistical measurements of the LSR model. Since the developed LSR model is capable of accurately predicting the experimentally measured compressive strength, a parametric study was conducted, using this model, to study the effect of Fly Ash and Silica Fume on the compressive strength of UHPC. Using Fly Ash quantities that range between 0-200 kg/m 3 and Silica Fume quantities that range between 40-160 kg/m 3 while fixing the quantity of cement at 1400 kg/m 3 and water at 175 kg/m 3 , several plots showing the variation of strength of UHPC were generated as shown in  Figure 5 . This study was conducted to detect the correlation between the material constituents of UHPC and its compressive strength. BPNN was used and three major steps were executed: (1) verification of ANN; (2) application of both SFS; and NID, and (3) analysis of selected features using ANN and LSR. 2) The use of ANN with selected input parameters improved the accuracy of prediction of compressive strength of UHPC and reduced the computational effort. The correlation coefficient (r 2 ) before and after the use of SFS improved from 81.6% to 99.1% while the NMSE improved from 0.0594 to 0.026, respectively. 3) The ANN model with the selected relevant input parameters also showed a lower deviation (89.6 %) than the ANN model with all the features (58.7%). 4) LSR was implemented using the selected input parameters to develop an analytical model that can be used to accurately predict the compressive strength of UHPC.
paper_418	Table 1 shows that the column variances of Buys-Ballot table is constant for additive model but depends on slope and seasonal effects for mixed model. Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model. Also, when there is no trend (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The decomposition models are Additive model: formula_0 Multiplicative model: formula_1 Mixed model: formula_2 where t M is the trend-cycle component, t S is the seasonal component and t e is the error. In particular, this means that the fluctuations overlapping the trend-cycle are not dependent on the series level. They do not depend on the level of the trend  [3] . On the other hand, the appropriate model is multiplicative when the seasonal standard deviations show appreciable increase/decrease relative to any increase /decrease in the seasonal means. Here again, no statistical test was provided for the choice. The emphasis is to compare the row, column and overall means and variances of the Buys-Ballot table for additive and mixed model when trend-cycle component of time series is linear. In such cases it is appropriate to use a multiplicative model. Gupta  [8]  observed that, the additive model assumes that all the four components of the time series operate independently of each other so that none of these components has any effect on the remaining three. According to him, this series are not independent of which other. Oladugba et al  [9]  gave brief description of additive and multiplicative seasonality. 2 (a) The column means mimic the shape of the trending parameters and contain seasonal indices for additive model. The column variances of the Buys-Ballot table is constant for additive model, but a function of slope and seasonal indices for the mixed model. These properties of row, column and overall averages and variances of the Buys-Ballot table are what could be used for estimation and assessment of trend parameters, estimation and assessment of seasonal indices and choice of the appropriate for time series decomposition. That is when 0 b =  (Table 1)  For mixed model, we obtain using the expression in  Table  1  ( ) ( )  formula_8 when there is no trend. That is when ( 0 b = ), it is clear from  For mixed model, we obtain using the expression in  Table 1  . formula_10 When there is no trend ( 0 b = ) we obtain from  (Table 1)  . (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The real life example is based on monthly data on number of registered road traffic accidents in Owerri, Imo State, Nigeria for the period of 2009 to 2018 shown in  Table A1 . The actual and transformed series are given in figures 3.1 and 3.2. Results show that seasonal variances of the Buys-Ballot table is constant for additive model and a function of slope and seasonal effects for mixed model. Also, when there is no trend ( ) 0 b = , the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models.
paper_428	The present article examines the influence of thermal radiation on two-dimensional incompressible magnetohydrodynamic (MHD) mixed convective heat transfer flow of Williamson fluid flowing past a porous wedge. An adequate similarity transformation is adopted to reduce the fundamental boundary layer partial differential equations of Williamson fluid model in to a set of non-linear ordinary differential equations. The solutions of the resulting nonlinear system are obtained numerically using the fifth order numerical scheme the Runge-Kutta-Fehlberg method. Williamson analyzed the flow of pseudoplastic materials and presented model to described the behavior of pseudoplastic material and explain convenient importance of plastic flows, and also recognized that viscous flows are very varied from plastic flows  [1] . studied the peristaltic motion of non-Newtonian fluid through asymmetric channels along porous wall by means of various phase and amplitude, and also studied the manipulation of different wave structures on the fluid flow model  [8] . By considering the approximation of long wave length and small Reynolds number the peristaltic pumping of Williamson fluid in a planar channel was investigated by Vasudev et al. found numerically convergent solutions of two dimensional flows of non-Newtonian fluids along chemically reactive species  [11] . Jabar addressed the influence of viscous dissipation and joule heating on MHD flow through a stratified sheet subjected to power Law Heat Flux having heat source  [22] . studied both forced and natural convection boundary layer transport by perpendicular surface in a stratified medium along connective boundary conditions  [34] . Bau investigated the thermal convection in a saturated stratified medium bounded between two parallel eccentric cylinders with the help of a regular perturbation expansion along Daarcy-Rayleigh number; it was observed that the appropriate preference of eccentricity values can maximize the heat transfer inside annulus of various thermal insulators  [37] . Ferdows & Liu obtained the similarity solutions of mixed convection heat convey in parallel surface with internal heat production  [44] . Malleswaran & Sivasankaran carried an analysis for mixed convection flow and noticed that the average heat transfer decreases with an increase in Richardson number but in general heat transfer is better at force convection mode than free convection mode  [45] . studied boundary layer flow of fluid in a porous wedge subject to Newtonian heating along heat generation or absorption  [69] . Deka   is wedge angle parameter. The nondimensional form of the given system of partial differential equations is obtained by introducing the following stream function and the similarity variables  [76] . E 0 = E 6 , E′ 0 = 0, F 0 = 1 (9) E′ ∞ = 1, F ∞ = 0 Where E 6 is injection/suction parameter. The system of ordinary differential equations 7 and 8 subject to the boundary conditions 9 and 10 is first reduced to a system of first order ordinary differential equations using the substitutions E K =`, $ K = a, F K = b. This With the boundary conditions E 0 E 6 , $ 0 0, F 0 1, $ ∞ 1, F ∞ 0 (13) The resulting system in Eq. (11) (12) (13)  is solved numerically with the help of 5 th order Runge-Kutta-Fehlberg method. The transformed governing equations  (11) (12)  subjected to boundary conditions  (13)  are solved numerically by employing the fifth order Runge-Kutta-Fehlberg method. Figures (1-2)  illustrate the influence of wedge angle parameter < with on velocity and temperature profile. Figure 6  drafts the non-dimensional velocity E′ for different values of suction parameter E 6 . Figures 7-8  illustrate the behavior of thermal radiation and Prandtl number on fluid flow region with M= E 6 = O = < =1. Because the prandtl number is the relation of momentum diffusivity to thermal diffusivity, when it increases then it decreases the thermal boundary layer thickness and temperature but increases thermal capacity of fluid.
paper_432	And found some results; The theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds, possibility study of Hamiltonian tubes when the simplistic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models. The Cotangent Bundles * of manifold d provides the basic model of a symplectic manifold. The total space of a cotangent bundles naturally has the structure of a symplectic manifold. Let be dimensional differentiable manifold of class ∞ and * the cotangent bundles over . Then the Cotangent Bundle has a dual space * . We can introduce the Legendre transformation we need some basic facts about the structure of the cotangent bundles * of a nd dimensional differentiable manifold . 1, for any 0 ∈ , the tangent space at 0, denoted , is the space of linear derivations on 1 2, , that vanish on 3 2, , . Thus,T 5 M can be identified with (1 2, , /3 2, , )* the space formula_0 , is called the cotangent space at 0; it is isomorphic to the dual * , of M . The Cotangent Space T 5 * (M) of a manifold at 0 ∈ is defined as the dual vector space to the tangent spaceT 5 M. A dual vector space is defined as follow ∶ given an − dimensional vector space G, with basis H , I=1, 2, 3,…, , the basis J = of the dual space G * is determined by the inner product. Let ∈ and let I; T { * Q → Τ * be the natural inclusion mapping. b) Let exp ∶ → 3 > be given by exp(% =J •" … ∈ 3 > . c) Recall that the n-dimensional real projective space 0 is defined by 0 =3 /o, where ~ , for ∈ 3 ⊂ Š> . For the mapping.∅: & → | to have a lift ‹ satisfying the condition ( ‚ ) =J ‚ , where ‚ ∈ &,J ‚ ∈ H, 0 (J ‚ ) =d 0 =( ‚ ), it is necessary that∅ (π n (&, ‚ )) ⊆0 (a n (H,J ‚ )) (3. A , principal bundles is a quintuple (0, , , R , where: 0 V →0 is a , right at action with the property of local triviality: Each point e ∈ has an open neighborhood for which there exists a , -diffeomorphism. šJ% ± ∈ , with isotropy subgroup = ² . A tube for the action at ± is a −equivariant diffeomorphism from some twisted product × ° to onopen neighborhood of ±in , that maps [J, 0] H to ±. The space o may be embedded in × ° as {[J, ] : ∈ 3}; the image of the latter by the tube is called a slice theorem. In this part we study the symplectic geometry of cotangent -lifted action induced by a smooth proper action of a Lie group on a smooth manifold. The first work studying symplectic normal forms in the specific case of cotangent bundles. The symplectic reduction of the cotangent bundles * Q has more structure than a symplectic manifold. Let act freely and properly by cotangent lifts on * Q, and let Jbe the momentum map of the G action (with respect to the canonical symplectic form on * Q. Leta ³ : → / is projection. Then ∅isc , invariant surjective submersion and descends to a symplectic homeomorphic. The first result of the theory in cotangent bundles reduction, the theory developed for the problem with a single or bit type playing an important role in the solution to the general problem of a singular cotangent bundles reduction for base manifolds, Hamiltonian tubes when the symplectic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models and the first work studying symplectic normal forms in the specific case of cotangent bundles. Conclude that the theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds and found that the phase space is the cotangent bundle T * Q of a configuration space .
paper_444	Resource constrained project scheduling problem is to make a schedule for minimizing of the completion time or total cost subject to precedence rules and resource constraints. Traditional resource constrained project scheduling problem research takes into account achieve management goal in certain environment. In this paper, for better described the uncertain resource constrained project scheduling problem, we firstly consider the uncertain resource availability project scheduling problem based on uncertainty theory. To meet the manger goals, it is assumed that the increased quantities of resource are uncertain variables and the finish time of each activity is a decision variable. One of the constraints is the finish-start precedence relationship among the project activities. The resource constrained project scheduling problem (RCPSP) takes into account the balance of makespan and total cost through resource allocation and reasonable activity schedules while the precedence constrains between the activities and resource constrains are all satisfied. Xie  [9]  supposed that the increased quantities of resource were real-value variables, and built a multi-mode resource constrained project scheduling model for minimizing both the project makespan and cost. In this paper, we consider uncertain resource availability project scheduling problem based on uncertainty theory, in which the increased quantities of resource are uncertain variables, and the finish time of each activity is a decision variable. Then, we build a multi-objective model which is under resource and precedence rule constrained to minimize the resource cost and the project completion time. For solving the above model, the equivalent form of the model is provided and the proof is given. In order to solve the problem of time-cost trade-off in project scheduling, we consider a project which is described as an activity-on-the-node network ( , ) , where = {1, 2, ⋯ , } is the set of activities and is the set of pairs of activities with precedence relations. A multi-objective programming model is built under the resource restricts and precedence rules of activities constrains in order to balance the completion time and the total cost of the project. (2)  Suppose that the increased quantities of resource are independent uncertain variables with regular uncertainty distributions. The uncertain resource constrained time-cost trade-off problem can be described as following optimization model: formula_0 In the above model, objective ○ 1 is to minimize the project total completion time; Objective ○ 2 is to minimize the expected project total cost which consists of the activity cost and the additional resource cost. Constraint ○ 6 shows the range of decision variables. In order to transform the model into deterministic form, we introduce the following several theorems. [16]  Let G be an uncertain variable with regular uncertainty distribution H. If the excepted value exists, then +IGJ = K Φ MA (<) A = <. (2) For instance, let G be a linear uncertain variable, it has inverse uncertainty distribution H MA = (1 − <)N + <O. Then excepted value of G is formula_1 Theorem 2. [16]  Let G A , G P , ⋯ , G ) be independent uncertain variables with regular uncertainty distributions H A , H P , ⋯ , H ) , respectively. If the function Q(R A , R P , ⋯ , R ) ) is strictly increase with respect to R A , R P , ⋯ , R S and strictly decreaseing with respect to R STA , R STP , ⋯ , R ) , then formula_2 holds if and only if formula_3 To solve the model (1), according to the operational law of uncertain variables, we transformed it into its equivalent form, as shown below. formula_4 Proof: Because is an uncertain variable with regular uncertainty distribution Φ , and formula_5 by the definition of uncertain variable  [16] , we know that ? is formula_6 By Theorem 1, we know that the excepted value of ? By Theorem 2, we have ∑ ∈8 9 − − Φ MA (1 − <) ≤ 0，  (12)  i.e. formula_10 Therefore, the model (1) is equivalent to the model (2). In the real-life project, due to the influence of uncertain environment, managers should consider the trade-off between the completion time and cost. By describing the increased quantities of resource as uncertain variables, an uncertain resource constrained project scheduling problem is discussed in this paper. Then an uncertain optimal model was built with objective of minimizing the completion time and the cost with resource constrained based on uncertainty theory. To solve this model, we transformed the uncertain model into equivalent form and proved it. Finally, we used genetic algorithm to search quasi-optimal solution of the model and gave a numerical example to illustrate the validity of the model.
paper_462	To deepen the reform of clinical medical personnel training in an all-round way with the cooperation of medicine and education is the strategic adjustment direction of clinical postgraduate education in China. Since 1998, the postgraduate education for master's degree in clinical medicine has been piloted in China. Through improving the conditions of enrollment, innovating the training mode, adjusting the award marks and improving the system guarantee, it has effectively constructed the professional degree education of master of clinical medicine and standardized resident training. Chongqing Medical University was founded in 1956. It was founded by the former Shanghai First Medical College (now Shanghai Medical College of Fudan University) and moved to Chongqing. In 1998, it became the first batch of doctoral and master's degree in clinical medicine in China. Pilot units for professional degrees. Since 2009, the school has been focusing on reforming the training mode of clinical master degree postgraduates (hereinafter referred to as clinical master). For a long time, the medical postgraduates trained in our country are seriously lacking in practical operation ability, resulting in the embarrassing situation that "medical doctor will not see a doctor". The development of postgraduate education of clinical medicine degree in China is faced with the characteristics of short time, complex training objects, multiple administrative departments involved in medical degree education, and diversified training channels  [2] . It has effectively solved the problems of imperfect training mode of clinical master, lack of mature training mode of high-level clinicians, low overall training quality of clinical master, inability of clinical master to apply for the qualification of practicing doctor, difficulty of effective connection with regular training, difficulty of seamless connection between professional degree education and industry admission standards, and inadaptability of current management system and mechanism to professional degree postgraduates. To vigorously develop professional degree postgraduate education and improve the quality of training high-level applied talents is the focus of the current degree and postgraduate education work. First, after one year's clinical training in the tertiary and first-class affiliated hospitals of the school, the clinical master who has not obtained the license of a licensed doctor is allowed to apply for the qualification examination of a licensed doctor, which solves the contradiction that the clinical master cannot obtain the license of a licensed doctor during the period of study  [5] . Thirdly, we allow our clinical master not to take the entrance examination, but to be directly incorporated into the standardized resident training system, so that they can participate in the standardized resident training. Under the above policy guarantee, the clinical master trained by the school has the dual status of postgraduate and regular trainee. After completing the relevant training content and passing the examination, the qualified certificate of licensed physician qualification and resident standardized training can be obtained. After completing the dissertation defense and meeting the award requirements, the graduate certificate and master's professional degree certificate can be obtained. To ensure the quality of clinical master's training, the school has set up a guidance group composed of the first tutor and rotating responsibility tutor. Effective management during the transition period. The staff of the Graduate Management Department of the school conducted a thorough investigation on the effect, problems and suggestions of the training mode of clinical master's degree in various clinical departments, and held dozens of meetings to solve common problems. In order to speed up the development of professional degree postgraduate education, the school has formulated an enrollment policy conducive to the development of professional degree  [9] . At the same time, the workload of guiding professional degree postgraduates is directly linked to the promotion of their professional titles. Through the above measures, the problem of low enthusiasm of faculties and tutors in guiding graduate students with professional degrees has been solved, and the construction of professional degree tutors has been accelerated. Aiming at the characteristics of professional degree postgraduate education and adapting to the innovation of its training mode, the school has established a funding system different from academic degree postgraduates, and continuously improves the standards and coverage of grants. Excluding very few phenomena, the school grants cover 100% of the total, and the living allowance for clinical master's degree has also been greatly increased. Therefore, the school has improved the conditions for applying for clinical master's examination, so that it is consistent with the admission conditions of residents' standardized training: undergraduate majors should be clinical medicine, full-time national education series undergraduate graduates, to obtain bachelor's degree. After the admission of clinical master, the school will submit the list to Chongqing Municipal Health Bureau, and register it to Chongqing Municipal Health Bureau resident standardized training registration system without test. The reformed curriculum system not only meets the training objectives of clinical master, but also meets the requirements of theoretical knowledge for trainees. It is closely related to clinical practice. The curriculum involves many independent modules, such as clinical research methods, clinical diagnostics, internal medicine, surgery and so on  [11] . Students are brought into the "two levels, two stages" training after they enter school. According to whether the students have obtained the certificate of licensed physician, they are divided into two levels, and different awarding standards are formulated. Requirements for the first stage of training. Students who have obtained the certificate of resident standardization training are directly transferred to specialist training (i.e. How to objectively and effectively assess the clinical competence of postgraduates is the key to ensure the quality of clinical medicine degree award. The main reason for this phenomenon is that since the trial implementation of clinical medical professional degree, the state has not evaluated the quality of the pilot units and lacked an effective monitoring mechanism. There are mainly "pre-job training", "centralized departure assessment", "stage assessment" and "annual assessment"  [13] . The Interim Measures for the Implementation of the Regulations on Academic Degrees of the People's Republic of China promulgated in 1981 stipulates that before the postgraduate's reply, the degree-granting unit shall employ one or two experts from the disciplines concerned with the thesis to review the thesis. The clinical master emphasizes on examining the clinical competence of postgraduates. If the reviewers are hired, some experts will evaluate the papers according to the requirements of scientific degree papers. The staff of the Graduate Management Department of the school conducted a thorough investigation on the effect, problems and suggestions of the reform of the training mode of clinical master in various clinical colleges and departments, and organized a meeting to solve some common problems. Thirdly, we should formulate an enrollment system conducive to the development of professional degrees. The number of professional degree postgraduates enrolled by tutors in clinical departments is clearly required. All tutors working in clinical departments must enroll professional degree postgraduates. There are three difficult problems in the training process of master's degree postgraduates of clinical medicine specialty: first, the graduates of clinical medicine specialty with bachelor's degree must work in medical institutions for one year before they can apply for the qualification of practicing physician; second, the graduates with bachelor's degree can't obtain the qualification certificate of practicing physician and can't carry out clinical training; third, some of the graduates with bachelor's degree must work in medical institutions for one year. By promoting the combination of clinical master training and regular training, the problem of clinical master qualification certification has been solved, and a new "5 + 3" training mode has been constructed. The school has thoroughly reformed the training mode of clinical master, organically docked school education with post-graduation education and lifelong education, promoted the seamless docking of professional degree education with industry access standards, accelerated the internationalization process of higher medical personnel training mode, and formed a new systematic training mode for clinical doctors, which has great application value in the whole country. In view of the characteristics of professional degree postgraduate education, the school grasps the development trend of postgraduate education, and closely combines the admission criteria with the industry admission criteria in all aspects of enrollment, cultivation and award of posts, which ensures the complete docking of personnel training and qualifications, and provides mature experience for the seamless docking of professional degree education and Industry admission criteria in China. It provides a mature reference model for the docking of professional degree postgraduate education and industry access standards in other fields in China. It fully guarantees the quality of clinical master training and brings up a large number of high-level applied medical talents for Chongqing and even the whole central and Western regions, which has produced remarkable economic and social effects. Since 2009, the school began to explore the reform of the training mode of clinical master in an all-round way. It has overcome many obstacles and steadfastly promoted the reform. Over the past five years, the reform has achieved fruitful results and basically achieved the expected goals. The main results are as follows: The reform breaks through the restrictions of relevant industry policies on the training objectives and modes of clinical master, and provides innovative modes for brothers to learn from. Dozens of brothers such as Fudan University learn from our experience. In the past five years, a total of 2063 clinical masters enrolled in the school have been integrated into the training. The employment rate of graduates has been guaranteed to be 100% for a long time. They have trained a large number of high-level applied medical talents for Chongqing and the whole central and Western regions, promoted the development of health undertakings in the central and Western regions, and produced remarkable economic and social benefits  [16] . At present, the school has completed more than 10 research reports and published more than 30 academic papers on the sub-project "Construction and Practice of Quality Assurance System for Medical Degree Postgraduates" of the Ministry of Education Innovation Project. Our school has 10 affiliated hospitals of Grade A and 28 key national clinical specialties. Since the reform of training mode, the enrollment of clinical masters in our university has been increasing rapidly. In the past five years, a total of 2063 clinical masters have been enrolled in the standardized resident training system. The "5 + 3" reform of clinical medicine master's degree in Chongqing Medical University has established a "one goal", that is, to train "doctors who really can see a doctor", and established the basic principles of the reform of training mode of clinical medicine master's degree and the training of clinicians. The establishment of "two sets of systems", namely curriculum system and assessment system, has solved the problem of insufficient clinical practice ability of master of clinical medicine and improved the overall quality of training. The implementation of "three combinations", namely, the combination of enrollment and enrollment, training and training, degree award and physician access. The "four certificates association" should be implemented, namely "licensed physician qualification certificate", "standard training qualification certificate", "diploma certificate" and "degree certificate"  [17] . We should build "five guarantees", innovate the management system and mechanism, and ensure that the reform is in place. It has effectively solved the problems of imperfect training mode of clinical master, lack of mature training mode of high-level clinicians, low overall training quality of clinical master, inability to apply for the qualification of licensed physician, difficulty in effectively linking up with regular training, and difficulty in seamlessly linking professional degree education with industry access standards. Through a series of reforms, it has achieved relatively ideal results and accumulated rich experience in reform. It hopes that through continuous exploration and reform, it can play a role in attracting more brothers.
paper_476	The results show that: (1) The cost and resources operational efficiency are closely related to the order splitting ratio under the same delivery frequency; (2) The delivery frequency has different effects on the operational efficiency of the resources in different links. (3) Through the proper splitting and loading of orders, staff scheduling, and area adjustment of delivery station, the optimal delivery frequency can be achieved under the balance of cost and resource operational efficiency. In order to reduce the operating costs of logistics enterprises on the basis of ensuring service levels, one should first properly allocate the amount of orders and splitting ratios to achieve an economic increase in the delivery frequency. As the volume of orders and consumers' demand of rapid delivery services increasing, express companies are required to increase the daily delivery frequency to cope with the pressure of delivery during peak period and meet the consumers' needs effectively. The order volume, the orders' splitting ratio, the operational efficiency of delivery resource and the delivery costs will affect the delivery frequency. System dynamics models not only offer a different Operational Efficiency: A Case Study of Jingdong Logistics perspective while whole system approach to transport planning, but also demonstrate to policy makers the importance of these feedbacks and lagged responses  [1] . At present, the researches on delivery frequency, resource operation efficiency and cost utilization were mainly focusing on the following two aspects: (1) Delivery efficiency improvement through the choice of delivery model, And (2) Delivery link optimization. Wang Wei  [17]  analyzed the advantages and disadvantages of self-operated, outsourced, and federated modes in ecommerce delivery model, and constructed a system dynamics model for the combination selection of delivery modes by selecting six core factors, which can provide decision-making advice for managers to improve the delivery efficiency. Hongtao Yang and Jianbang Du  [19] [20] [21]  uses system dynamics to analyze the problems of supply chain and delivery efficiency from the perspectives of transportation routes, drivers and cooperation agreements, and its sensitivity analysis provides some reference to this paper for the corporate in this case to improve the delivery efficiency. Lin Wanting [22]  considers the phenomenon of unfilled orders, information leakage, and product damage which exists in crowdsourcing delivery, then built system dynamics model from five dimensions of reliability, responsiveness, assurance, empathy and security, which provide decision-making advice for enterprise delivery efficiency management. Then, it establishes a simulation model for the operation of the delivery system on the base of system dynamics. Next, it uses the simulation model to study the effect of changes in delivery frequency on delivery costs and resource operation efficiency under different quantity of delivery orders. Secondly, it constructs subsystems of cost and resource operation efficiency for the delivery activities in the boundary. Next, it analyzes the interaction between delivery frequency, cost, and resource efficiency, and builds dynamics simulation model to get the equilibrium of cost and resource operation efficiency under different delivery frequency. The relationship of the workflows in the delivery system is shown in  Figure 1 . As shown in  Figure 1 , JDL delivery process nodes in the area include: warehouses, three sorting centers and two delivery stations; resources include: storage facilities, sorting facilities, various transport vehicles and delivery workers. The delivery cost mainly includes the equipment usage costs and labor costs in the three stages of sorting, transportation and terminal delivery. The variable costs occurred due to the volume of orders, which include operating cost of the sorting equipment, rental cost of the site, units fixed cost of transport vehicle, fuel costs, toll and so on. Labor costs are composed of fixed wages and performance wages for employees in the three links mentioned above. The on-site logistics personnel is the person who puts the sorted goods into a mail packages and places them on a pallet and then sends it to the transportation vehicles. This article considers the utilization efficiency for the facilities, personnel, and technology such as turnover rate of transportation vehicles, the utilization rate of delivery site, and the delivery personnel loading rate, etc., which are put into the sorting, transportation, and terminal delivery links. The utilization rate of transportation vehicles is calculated by dividing the actual traffic volume at each sorting center by the vehicle capacity. The personnel load rate in the process is calculated by dividing the actual workload by the authorized workload. The delivery frequency refers to the number of times of terminal delivery by the company in unit time (in days). The increase in the delivery frequency will result in the following two changes: first, the increase in consumer satisfaction; second, the corresponding changes in the cost and resources operational efficiency. At the same time, the factors such as sorting time requirements, proportion of per-order batch, efficiency of the delivery personnel, and unit fuel consumption of the vehicle, will have a direct impact on the delivery frequency. After that, the validity of the model was verified though the error analysis by comparing the simulation data and actual operational data, which includes the average cost of sorting, transportation and terminal delivery. Different delivery frequencies have different splitting ways of total order quantity per day. The delivery frequency in scenarios 2 and 3 was increased by 9.5% over that in scenario 1. In scenarios 2 and 3, when the order quantity was lower than 3,500, the differences between the total delivery costs in Operational Efficiency: A Case Study of Jingdong Logistics different scenarios were very small; when the order quantity was higher than 3,500, the increased rate of cost in scenario 3 was higher for a while than that in scenario 2. The reason is that in scenario 3 the sorting center dispatched more vehicles in advance, which increased the cost of transportation. After the order volume reached 5,100, the total delivery cost in scenario 4 showed a downward trend and reached the minimum. Within the orders volume ruled in this paper, the delivery frequency and the cumulative delivery costs are positively correlated. The four scenarios showed a lower average unit cost when the order quantity was around 4,700 units indicating that this order quantity was a batch of economic orders. In  Figures 10 and 11 , it can be seen that the sorting cost of scenario 2 increased by an average of 157 yuan per day compared to scenario 1, which was due to the increase of delivery Higher frequency led to an increase in the total working time of sorting equipment and equipment costs. This is because that under the current total delivery volume, the order quantity after splitting in the scenario 4 was higher. The difference of average unit sorting costs among scenarios 1, 2 and 4 decreased as the order increased, which is because the change in the sorting costs under different order quantities was small and the difference in the delivery order quantity increased. Due to the splitting of order, the transportation costs of scenarios 2 and 4 remained unchanged when the order volume was small. In scenarios 2 and 4 the orders can be met by just increasing the frequency of existing vehicles, however, in scenario 1, new vehicles were needed to meet the transportation needs, which resulted in higher transportation costs. The main reason is that the increase in the delivery frequency improved the demand for the number of delivery personnel and caused the delivery time of delivery personnel to be shorter in a single batch. The vehicles utilizations of the three sorting centers in scenario 1 showed the first drop point when the order volume was 2530, 2567, and 2350, and the second drop point occurred at 5000, 5200, and 4800. The vehicle utilization efficiency for the three sorting centers in scenario 4 increased with the order volume since only one transportation vehicle was required to be dispatched from each sorting center within the current order volume. As a result, the company had to increase the number of delivery personnel and the area of shipments. The results of this study indicate that under the impact of order volume there is no fixed relationship between delivery frequency with delivery cost and resource operational efficiency. With the same delivery frequency, the orders splitting ratio influences delivery cost and resource utilization efficiency significantly. The change of delivery frequency has different effects on the resource operational efficiency in different stages of the delivery process. With the same delivery frequency, different order splitting ratios affect delivery cost and resource operational efficiency. Therefore, JDL needs to consider the increase in delivery frequency, the increase in delivery costs, the overloading of resources, and the ratio of orders and splits. For example, when the JDL order volume fluctuates between 3086-3154, one should adopt scenario 2 or scenario 3; when the order volume is greater than 5100, scenario 4 should be adopted. In the same delivery link the demand for delivery resources can be different due to different delivery operational capacities. Therefore, JDL needs to consider different constraints when making delivery frequency decisions. In this case, if scenario 2, 3, or 4 is adopted, one can increase the total number of order shipments by 2,400, 1,400, and 4,450 units respectively without increasing the number of transport vehicles. When the delivery frequency is increased from 3 times per day to 4 times per day, scenario 3 should be adopted from the perspective of increasing consumer satisfaction; scenario 2 should be adopted when the area of delivery stations and the delivery personnel are tight. The delivery frequency has different effects on the resource operational efficiency in different delivery stages. Therefore, JDL should combine the simulation results with the actual situation before making the delivery frequency decision.
paper_479	Handwriting is also a key element in document examination as it leaves a forensic document examiner with the task of determining who the writer of a particular document is and this is achieved through the likelihood ratio (L R) paradigm. This research employed back propagation neural network (BPNN) to model the writing pattern of individuals with input layer as the features of handwriting characters, two hidden layers of three neurons each, activation function sigmoid (s) and an output handwriting. With the help of handwriting model for individual writers, little or no assumptions and no nuisance parameters were employed in achieving full likelihood ratio for comparative handwriting analysis in forensic science. From the research carried out, it can be concluded that modeling an individual's handwriting is a crucial factor in achieving a full likelihood ratio, little/or no inconclusiveness in result reporting and a less degree of disagreements for handwriting identification in a forensic environment. The likelihood ratio paradigm has been studied as a means for quantifying the strength of evidence for a variety of forensic evidence types in handwriting and other types of forensic evidence such as earmark, speech, footprint, fingerprint, glass fragments and DNA  [1] [2] [3] [4] [5] [6] [7] [8] . For the handwriting evidence, previous literatures reported many factors and the inability of the forensic examiner to model the writing profile of a writer because it is often believed that an individual's writing profile is a latent characteristic that cannot be observed directly, and is not easily modeled because an individual's writing profile cannot have static characteristics and may change over time  [9] [10] [11]  which has resulted to adopt ad-hoc methods to compute likelihood  [9, 12, 13]  when full likelihood ratio is not achievable. The purpose of this paper is to illustrate how neural network approach to comparative handwriting analysis can greatly affect the outcome of a forensic investigation and make full likelihood ratio achievable. The task of verification, which is to determine whether two writing samples compared side-by-side, originates from the same person, was the principal problem addressed. A statistical model for writer verification was developed; it allows computing the likelihood ratio based on a variety of different feature types. [17]  based their approach on distribution of measurements from comparing items but their approach was similar to that of marginal L R when full L R was not an option due to the presence of nuisance parameter. [19]  described a statistical model for the writer verification task to determine if two documents were written by the same writer, their model followed the scenario proposed by  [17]  thus suffers from the same weakness  [20]  presented one conceivable way to deal with gauge a probability proportion in near handwriting analysis was delineated. The oddity of the methodology depends on producing reproduced composing samples from an accumulation of composing samples acquired from a realized source to frame a database for evaluating the dissemination related with the numerator of a L R . In a related work, similar examination of probability proportion based proof appraisal strategies in both evaluative and analytical procedures was carried out using a sample collected from female and male author. Assessment of signature handwriting evidence through score-based likelihood ratio based on comparative measurement of relevant dynamic features was carried out by  [22, 9]  presented score-based approaches to calculating forensic probability ratios that have been established as becoming more progressively common in forensic literature. [23]  Asserted that recent analytical developments paired with modern statistical computational tools have led to the proliferation of adhoc techniques for quantifying the probative value of forensic evidence. The handwritings were preprocessed using the Otsu method after which they were segmented into different words using the Sobel edge detection algorithm. Features were extracted via local binary pattern from each clustered characters, while the back-propagation neural network was used to learn the writing pattern for each writer and these writing pattern were then stored in the database. The target will help to know which handwriting is original and disguised. BPNN is designed to process information on how the human brain processes information by gathering its knowledge by finding trends and associations in data and learning from experience. Various things come together to form a BPNN, such as hundreds of single units, artificial neurons or processing elements (PE), connected to coefficients (weights), which represent a neural structure and are organized in layers. The transition of the neuron functions determines the behavior of the neural networks, the laws of learning and the structure itself. The activation signal is passed through the transfer feature to produce a single neuron output. In addition, that each individual's writing pattern was modeled using BPNN with the algorithm below, as BPNN can combine and incorporate both literature-based and experimental data to solve problems. Each character variable has a weight W i which shows its contribution in the training process. The feature information extracted from student handwriting through LBP was entered in NN through input layer and the participation of each class of character variables is calculated by the hidden layer of the network using: formula_1 net j is ith type of factor, n is the number of factors in net i , & ' ! This study used a network concealed with n input points, k 1 and k 2 hidden, and m output units. The weight from input points i and two hidden unit j is ! " Weight from second hidden unit i and output unit j is ! ) Weight of additional edge for each unit is bias− θ, where input unit and output vector from the hidden layer are expanded with a 1 component as seen in  Figure 2 . After randomly selecting the weights of the network, the backpropagation algorithm is used to measure the necessary corrections. Step 1: Feed-Forward Computation The vectors & " , & ( and & ) are computed and stored, evaluated derivative also stored. Step 2: Backpropagation to the output layer This research looked for the first partial derivatives 12 1 ! ) This research work was able to model the handwriting for individual in the presence of large-scale database using the back-propagation neural network (BPNN). An inconclusive state is declared if and only if L R value = 1 With the BPNN algorithm to model handwriting pattern for each writer and L R estimation described in  [10]  this paper was able to deal with the problem of inconclusiveness as there was no inconclusive results reported in the investigation, determine who the writer of a questioned document is as well as eliminate the inclusion of parameters considered nuisance in investigation because each writer's handwriting profile could be ascertained due to the BPNN training of the Handwriting categories. Table 1  shows the results of our Estimated Likelihood Ratio (ELR) for a writer against any other author in a collection of appropriate databases where the upper confidence interval (CiU) and the lower confidence interval (CiL) are indicated. This research shows that modeling handwriting pattern for a writer is an important factor in achieving a full L R which will eliminate the use of nuisance parameters in the computation process, lower the disagreement rate with respect to the Hypothesis in support of the prosecutor and also produce no inconclusive result after estimation of L R .
paper_492	Based on known functions of neuroscience the neural network that performs serial parallel conversion and its inverse transformation is presented. The activity of the upper neural networks changes in response to the context structure inherent in the time series data and have both function of accepting and generating of general time series data. Eating behavior in animals in the early stages of evolution is also processing time series data, and it is possible to predict behavior although be limited short term by learning the contextual structure inherent in time series data. Although status of nervous system of the animal change according to the recognition by sensory organ and to the manipulation of the object by muscle in the vicinity of the animal itself, the evolved animals have in addition another nervous system so-called long-term memory or episodic memory being involved experience and prediction. By hierarchical connection of the circuits, it is possible to accept and generate general time series data. The nervous system related to short-term memory is activated in synchronization with environmental events, but the nervous system involved in long-term memory is highly layered to form an image corresponding to past and future events. Whether the eating behavior is evaluated as intellectual behavior aside, it is possible to mimic this degree of behavior by electronic work at the junior high school level by combining sensors and logic ICs. In the following, rather than the engineering usefulness, to extend the function of the circuit according the evolution of the neural system of the animal. The eating behavior of animals evolved from the aforementioned animals is composed of time series of actions such as extending arms toward the target, opening the palm of hand when approaching the target, closing the palm to grasp the target and bringing to the mouth. The recognition technology of the figure has evolved by the neural network which starts with the perceptron, but it is forced to judge by the relation of the part and the whole as in the example of kanji when the scale of the figure increases. In other words, a trade-off is made between the recognition power of the part in the complex figure recognition and the processing power of the data sequence of the recognition result, and by learning the animal might got the most efficient processing method. It is desirable that the behavior of any neural network system can be expressed by combinations of the simple action parts of animals in the early stages of evolution. In this chapter, it is presented that arbitrary time series data can be divide into basic sequences as the logical basis of neural networks. Finally, it shows the hierarchically connected neural network that can process for general time series data. The neural circuit corresponding to each basic subsequence can be easily configured in the neural network, leading to the realization of processing by hardware of general time series data. Because the elements activated by c 1 is randomly connected to input, not all elements activated by c 0 are additionally activated. Other portion will return to the initial state because no activation factors (may be activated by another time series data). The number and their position of the activated elements is the conversion output corresponding to the serial input  [1] . In the neural network shown in  Figure 2 , the elements are activated one after another by the time series data (in this case, the basic subsequence) from below, and the result is output upward. Its output is the result of the serial parallel conversion, it is the result of AND logic of the output of the activated elements. Therefore, the couplings between the elements are enhanced (Hebb rule) by repeating this conversion, as a result the elements involved in the conversion will be activated by only receiving the first element of the time series data. This operation is a generation of (learned) time series data. On the flow direction of data, this parallel to serial conversion is upside down with the serial to parallel conversion described above, but the basis of neural network operation is the same. When the state transition diagram of the  Figure 3  is seen as a serial parallel conversion, the bottom is inputs connected to such sensory organs. After the first data reception, the connected elements are activated as described above. The essence of the neural network which performs the parallel serial conversion and the neural network which performs the serial parallel conversion is the same except that the conversion result comes out downward or upward. On other words, both operations of the serial parallel conversion and the reverse conversion are essentially the same on the point both operations are triggered by the first data and proceed waiting for input state change. For each basic subsequence a neural network that accepts and generates the basic subsequence is considered. Since general time series data is consisted of hierarchy of basic subsequences, basic units can process general time series data by identifying the outputs of lower layer basic units as the new time series data. Animal's behavior is considered a time series data consisted of the data that is couple of behavior data and received data. The time series data handled in the first 2 steps are consists of various stimuli of the internal and endocrine system, as well as data that captured by sensory organ from the environment in which the animal is placed. After time sequence data learning, animals are possible to quickly start the operation that continues only by receiving the beginning of the time series data to be corresponding a slight sign. In brief, images were advantageous even if an organism were not conscious of the images formed within it. Animals in the early stages of evolution will spend most of their living time obtaining food and avoiding danger. Animals with some evolved sensory organs must recognize, for example, environmental changes from sunrise to sunset as repetitions of a time series data. One of the factors that made the language possible to dramatically expand the range of expression is that it can be used as expression of object even if it is not near. For example, a family might have conversation like this toward you who is about to have a birthday the next day. "Tomorrow I'd like to buy a birthday cake at Store A to celebrate you with everyone." "Let's ask store A to put a chocolate plate on the cake with your name on it." And you'll think of the shape of the cake and the action of lifting the chocolate plate and mouthing it. On your birthday day, you may want to see the cake in front of you, identify the cake from store A, which was the one that was the subject of a conversation with your family the day before, and worry about the difference from the previous day's expectations. Because the episode about the cake is remembered, communication is possible and feelings are transmitted. One is time series data based on visual information of the cake in front of you and muscle movements that manipulate chocolate plates, which is produced by the nervous system that animals have from the early stages of evolution. The other is generated by the nervous system that is called " image" by Damasio, is time series data based on the shape of cake produced from family conversations the day before. While the behavior of the former nervous system is reflection of the visual information and movement of the objects nearby, the latter nervous system not only no needs to be synchronized with the former, but also moves independently. Like talking about your childhood while eating cake. However, if the nervous system of the episodic memory is activated by remembering the A shop and the chocolate plate while looking at the cake, the difference between reality and the expectation might become a problem. Stimulus from the sensory organs from the bottom becomes time series data and is transmitted to the upper part. The part shown in red is a part that is particularly activated, the lower red disk is the part activated by the visual data of the chocolate plate that placed in front, the upper red disk is the part that is activating by recalled episode about the chocolate plate. As a result, the shape of the chocolate in the episodic memory is identified as the thing seen in front. When studying objects that are intertwined with portion and whole system such as the nervous system, the idea of category theory can be incorporated to develop the whole without focusing on the details of the object. In neuroscience, the morphism corresponds to synapses and axons, and is responsible for the transmission of information between objects. Thus, the behavior of both categories can be migrated to each other. In this paper, short-term memory and long-term memory are both regarded as time series data brought about by activity in the brain. It is a short-term memory to be involved in the recognition and manipulation of the thing in the vicinity of the animal itself. From among the random connections, the necessary connections for the desired operation are selected and enhanced, and the target function is realized. It can be said that the essence of the logic of the operation exists not in the basic unit, but in the connecting situation among basic units selected from randomly initialized connection. Therefore, even if the circuit is partially damaged, it is possible to supplement by learning the function of the peripheral circuit is lost. From the viewpoint of neuroscience, even if there are no "parts" equivalent to the basic unit in the process of stimulation from the sensory organs propagating to the cerebral cortex, axons parallel to the propagation direction connect between the layers, and it can be said that it is a passage of serial parallel mutually converted information. Constructing neural networks based on known functions of neural circuits, including the Hebb rule, and considering the correspondence between the movements of biological neural circuits will lead to new discoveries in both fields.
paper_507	Landslide is defined as a slow to rapid downward movement of instable rock and debris masses under the action of gravity. The area chosen in the present study is Uttarkashi district of Uttarakhand, suffering from frequent landslides every year. In present study we used the already existing topographical maps, satellite imageries and field work. The main aim of present study is to integrate the result of our study with spatial data, soil parameters, land inventory and used the output as a user friendly application using GIS which could predict the future susceptibility of region to landslide and% contribution of each factor for the same. Efficiency of the application will be calculated by the help of previously acquired data of the study area at different places and then the reliability of the application will be judged. A frequently used definition of landslide is "movement of mass of rock, earth or debris down a slope" in the words of Cruden  [1] . They are the catastrophic phenomenon taken lives of many a hundred and destroyed the hard earned money, disrupting the communication facilities. The active areas include Himalayan region of India and the process of development of this region thus slows down. Uttarakhand is an appropriate choice for the study since the newly developed state has been fighting with the catastrophe and is making front paper headlines for landslide at Vishnuprayag, Baldora, Lambagharchatti, Jharkula, phatabyung, and Amiya landslides  [4] . The recent landslide of 2012 took a heavy toll on life and property, many people lost their lives, and thousands of tourist were stranded due to disrupted communication services  [5] . The phenomenon can be easily classified and described by two nouns. The result obtained i.e. vector maps could be used for accessing the features for a particular location. Identification of factors which affects to the landslide. Preparation of a landslide hazard zonation map that would divide Uttarkashi into different zones depending upon the factors. There are 793 villages with area drained by major river (s), Yamuna, Ganga. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area (Mohammad Onagh, 2012). slope, aspect, lithology, rainfall, land cover etc. were generated using ERDAS and ARCGIS v. 9.3. Four control points were selected at the corner of the concerned points, the geo-referencing of these coordinates was done by finding the coordinates from the Google Earth. DEM (Digital elevation model) was obtained from BHUVAN. The chosen data are related with the various factors causing landslide at a place. in the form of images and have to be converted into the Vector Form for use in the susceptibility analysis. Four control points are selected on the four corners of the map such that the points mark the spatial extent of the whole map as shown: formula_0 . An artificial neural network is a "computational mechanism able to acquire, represent, and compute a mapping from one multivariate space of information to another, given a set of data representing that mapping". The ANN is a black box model is a multi-layered neural network, which consists of an input layer, hidden layers, and an output layer. An artificial neural network "learns" by adjusting the weights between the neurons in response to the errors between the actual output values and the target output values. A neural network consists of a number of interconnected nodes. Each node is a simple processing element that responds to the weighted inputs it receives from other nodes. In the present study we selected 107 points and all the six factors namely soil depth, soil type, rock type, land cover, slope and elevation and a excel database is created. The dataset is categorized into 60% training and 40% validation. In the study we used multiple layers with structure of 6 X 20 X 1 that is 6 input and 1 output neuron. ANNs can be grouped into two major forward and feedback (recurrent) networks. In the former network, no loops are formed by the network connections, while one or more loops may exist in the back propagation neural network with feed forward approach. The data is categorized into training, testing and validation, and all the six factors namely soil depth, soil type, rock and a excel database is created. The dataset is categorized into 60% training and 40% validation. In these points we categorized them into two categories landslide prone and non-landslide prone. Since ANN does not understand the 'landslide prone' and 'non-landslide prone' region we explain it by giving value '1' and '0' respectively. As we have seen neural network can compute the output for a given input. However, this is possible only if we know the coefficients called weights. Here, "feed-forward" denotes that the interconnections between the layers propagate forward to the next layer. To calculate weights for different factors we have to train the network, thus interrelationship between the nodes of different factors are given. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer (20), and between the hidden layer (20) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). In the training process we change the weights in that way in which the network output and the true values get closer and closer to each other. For a new dataset the weights are unknown. Most of the training datasets met the 0.01 RMSE goal. For easy interpretation, the average values were calculated, and these values were divided by the average of the weights of the some factor that had a minimum value. This study was concerned to the region of Uttarkashi due to the limitation of resources and time, we have been able to generate the results for a limited area Rishikesh Uttarkashi-Gangotri-Gaumukh route from latitude 78°19'55.14'' to 78°47'36.27" and longitude 30°32'30" to31°1'9.33". It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area  [13] . slope, aspect, lithology, rainfall, land cover etc. were generated using ERDAS and ARCGIS v. 9.3. We have used only six factors to limit the bulk of data. An artificial neural network technique was used. The back-propagation algorithm was then applied to calculate the weights between the input layer (6) and the hidden layer  (15) , and between the hidden layer (15) and the output layer (1), by modifying the number of hidden node and adjusting the learning rate (0.01). Other values for landslide susceptibility for the adjoining areas have been calculated using interpolation technique therefore the Rishikesh-Uttarkashi-Gangotri-Gaumukh route has been mapped for landslide Hazard. With the further advancement in such type of study, we could interpret results for future from past records, if the site is inaccessible, or the test results are erroneous. Here we have used the already existing topographical maps, satellite imageries and field work integrating them together using GIS and ANN MODEL to create a database that has generated the output for the future use. The result of present study with spatial data, soil parameters, land inventory and presented by a landslide hazard zonation map and a user friendly application using GIS that could predict the future susceptibility of region to landslide and percentage contribution of each factor for the same.
