paper_1	Tweets and other updates have become so important in the world of information and communication because they have a great potential of passing information very fast. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. Its objective is to get groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups  [1] . It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . A number of techniques can be used to do clustering. Some of them include summarization, compression and k-nearest neighbor which localizes search to one or a small number of clusters. Good clustering methods produce high quality clusters with either a high intra-class similarity within clusters or a low inter-class similarity between clusters. The quality of clustering also depends on both the similarity measure used by the method and its implementation  [10] . The quality of a clustering method is also measured by its ability to discover more or all the hidden patterns. The system design methodology used was incremental prototyping. In incremental prototyping, the whole requirements are broken down into building blocks which are incremented each time a new component is integrated based on an overall design solution. The first task was to retrieve details of each of the students from their twitter accounts using an extension script which is part of the twitter Application Programming Interface. The second task involved identifying the right kind of data to use for training the expected prototype as well as testing it. This involved picking the details of a new student from twitter and trying to predict the class hence group that he should join. On this question touching on the overall goal, 90% of the students emphatically agreed that the system actually enabled them to be classified into groups and they were therefore able to know their group members and comfortably interact with them on a given task that they were assigned  [4] . They also confirmed that the system simplified the process of group formation and made inclusivity of distant students in the groups possible. This is summarized in the chart below. This classifier was doing the classification using the unigrams. Through the study, it can be underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. They therefore come with a lot of challenges including time wastage. Through social media a better and more efficient way can be used to enable online learning generally and group formation specifically. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. It was able to extract tweets from various social media accounts based on a given hash tag (task) and then pass them to a Naïve Bayes classifier as input. The classifier was also able to assign other or new users groups also according to their tweets and the learning that the system had undergone. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Through the study, it was underscored that inasmuch as the social media has a great potential in education, this has not been exploited to a greater percentage. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. They therefore come with a lot of challenges including time wastage. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_2	The developed system uses a microcontroller of PIC89C51 microcontroller duly interfaced with sensors. The signal timing changes automatically based on the traffic density at the junction, thereby, avoiding unnecessary waiting time at the junction. Junction timings allotted are fixed. In these fixed traffic control systems, vehicles have to wait at a road crossing even though there is little or no traffic in the other direction. There are other problems as well, like ambulances getting caught up by a red traffic signal and wasting valuable time  [2] . Sometimes higher traffic density at one side of the junction demands longer green time as compared to the standard allotted time. The suggested case study, Jakpa junction is a typical example of a traffic congested area. The conventional traffic light system based on fixed time is employed to control the traffic in this area. As a result of this a lot of time is wasted in the process. In order to overcome the aforementioned problem, this research adopted a density based approach in controlling vehicular traffic. The signal timing changes automatically on sensing the traffic density at the junction. Junction timings allotted are fixed. The image captured in the traffic signal is processed and converted into grayscale image then its threshold is calculated based on which the contour has been drawn in order to calculate the number of vehicles present in the image. An intelligent traffic lights control system using a Fuzzy Logic approach was developed by  [5] . The algorithm implementation was done using Mathworks, MATLAB software, and the results were simulated using a Simulink Tool to create traffic scenarios and comparisons between simple time-based algorithms and the developed system. Data was analyzed and presented using descriptive statistics; tables and graphs by using excel 2003. For testing the adaptive traffic light controllers, a simulation system using Qt, C++ software integrated with MATLAB tools was developed. The simulation runs results showed that the adaptive algorithms can strongly reduce average waiting times of cars compared to the conventional traffic controllers. The top down design approach was adopted here. This is converted by a bridge rectifier to a dc voltage. Below are the ratings of the transformer 0.7 volts for silicon diodes). For convenience, a capacitor of 1000uF is used. Figure 4  shows the circuit diagram of the system. IR sensors are placed on the intersections on the road at fixed distances from the signal placed in the junction. The time delay in the traffic signal is set based on the density of vehicles on the roads. The IR sensors are used to sense the number of vehicles on the road. According to the IR count, microcontroller takes appropriate decisions as to which road is to be given the highest priority and the longest time delay for the corresponding traffic light. The Vero board is also called a strip board. The PIC Microcontroller was programmed using Embedded C language. The codes are as shown in the Appendix. Simulation was done via Proteus software. Jakpa Junction, in Effurun, Delta State, Nigeria is increasingly becoming chaotic by reason of the recurring traffic gridlock it experiences. On these days traffic rules are usually violated because of the complex traffic situation.
paper_3	They build a personalized model of the learner and apply this model to adapt the content and/or the appearance of the hypermedia, according to the learner's specific characteristics, such as the cognitive goal, level of knowledge, pre-existing or prior knowledge, interests, preferences, stereotypes, cognitive preferences and cognitive or learning style  [4] . (c) Adaptive Navigation Support: It adapts the link structure in such a way that the learner is guided towards interesting and relevant information, kept away from nonrelevant information either by suggesting the most relevant links to follow or by providing adaptive comments to visible links. (d) Meta-adaptive Navigation Support: It selects or suggests the most appropriate adaptive navigation technique that suits the given learner best relatively to the given context, either by observing and evaluating the success of each technique in different contexts and the resulting learning from these observations, or by assisting the learner in selecting the navigation technique that best suits to him or her. (h) Adaptive Collaboration Support; Adaptive Group Formation and/or Peer Help: These techniques support the collaboration process either just like the interactive problem solving support systems assist an individual learner in solving a problem, or they use knowledge about possible collaborating peers in order to form a matching group relatively to the kind of the collaborative task. The teaching strategies are based on  [9]  learning cycle and learning style model. The general purpose of the AEHS MATHEMA is to support learners through an interactive and constructivist educational environment in order to construct their knowledge in Physics and Technology, individually and/or collaboratively, and to overcome their misconceptions and/or learning difficulties AEHS MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and meta-adaptive navigation, interactive problem solving, and adaptive group formation. More specifically it supports: (1) adaptive presentation according to learner's learning style; (2) four adaptive navigation techniques which are direct guidance (uses a "next" button), link hiding (hide nonrelevant links), link annotation (use colors to indicate relevance), and link sorting (like in search engines); (3) a meta-adaptive navigation technique in order for the learner selects the more appropriate navigation technique that best suits him or her; (4) an interactive problem-solving method through personalized and collaborative problem-solving activities that exploits the teaching approaches of experimentation through simulations, guided inquiry and discovery, investigations and peer collaboration; and (5) a method of adaptive group formation for peer matching. The rest of the paper is organized as follows: In the section 2 related works for various models of architecture of AEHSs is presented. In the section 3 the architecture of the AEHS MATHEMA is presented in detail. The AHAM architecture model proposed by  [6]  is shown in the  Figure 1 . The AEHSs fit so nicely in this model. This leads to a structure of prerequisite relationships. The prerequisites are most common in educational material. Proper's architecture is a combined architecture of SCORM LMS and AEHS. Educational content can be either SCO or Asset. In the  Figure 3  the architecture of the WELSA  [14]  system is presented. [12] proposed a framework for AEHS (  Fig. 4 ) based on JSP, Java servlets, and Javabeans. ULUL-ILM  [1]  is an AEHS that focuses on student's learning styles. The content loaded to the MySQL database is accessed via JDBC API. A servlet is a Java class that implements the Java servlet API, a protocol to respond to HTTP requests. These servlets are complete programs that are capable of creating JSPs. This allows for much more flexibility in creating the page than XML. Servlets dynamically create JSP pages according to student requirements. JavaBeans technology comes into play. (3) Protects your intellectual property by keeping source code secure. Figure 5  shows the architecture of the AEHS MATHEMA. A component of an adaptive educational system is the representation of knowledge. The domain knowledge is structured in a way that supports the ability of the system to choose the educational material, depending on the learner's requirements and current status. The domain knowledge of the AEHS MATHEMA is the basis of the system's adaptation. The model that supports the AEHS MATHEMA is the overlay model. The characteristics that the system maintains in learner model for each learner are: name, gender, username, password, learning style, level of knowledge for each cognitive goal (general performance), level of knowledge for each basic meaning of the cognitive goal (quantitative and qualitative characterization), concepts that have been successfully evaluated for each cognitive goal, current page, current teaching method, current navigation technique, navigation history, meta-adaptive navigation status, initial Web experience and knowledge level (pre-existing knowledge) in the current cognitive goal, ways of presenting feedback messages, preference for assistance by Advisor in terms of its navigation, course sequence, and phases of the guided dialogues in interactive problem solving process in which each learner participates. The learner during the study selects the cognitive goal he or she wishes to study, following his or her personal course in the educational content, exploits the system's assistance, navigating and studying, is informed about the information that the system maintains, who can change them by intervening and directing the system's adaptation in this way, in terms of curriculum sequencing, adaptive presentation and navigation and adaptive group formation, selects his or her learning style, selects the navigation technique, activates and deactivates the curriculum sequencing, selects the way that the feedback messages are presented, changes his or her knowledge level for each basic concept of the cognitive goal, and activates-deactivates the Advisor. This is an innovation on the architecture of AEHSs. It is the second level meta-adaptation engine that is responsible for monitoring the cognitive improvement of the learner in the cognitive goal that he or she studies, after n successful evaluations of his or her knowledge in basic concepts, showing him or her advantages and disadvantages of the four navigation techniques that it supports. It also offers additional information about the navigation techniques that it supports. In AEHS MATHEMA, meta-adaptive navigation works as follows: The first time that the learner enters the system, he or she is asked to state his or her Web experience and level of knowledge in the cognitive goal he chose to study (preexisting knowledge). Following the learner's statement, the system suggests the most appropriate navigation technique to him or her, taking into account the level of his or her Web experience and his or her level of knowledge in the subject he or she chose to study as follows: (1) Direct guidance: Little or no Web experience and little or no knowledge of the cognitive goal. The meta-adaptation engine proposes to the learner to change the navigation technique if he or she wishes after n successful assessments of the basic concepts of a cognitive goal he or she studied, showing him or her the pros and cons and additional information on each of four navigation techniques, and the learner decides whether to change it or not. Figure 7  shows a snapshot of the meta-adaptation result. A snapshot of a meta-adaptation result. The pages displayed to the learner are dynamically generated. Figure 9  shows a snapshot of the page responsible for selecting preferences of the learner. MATHEMA supports the learner to find the most suitable peer for the formation of a collaboration team. Following a study  [13]  conducted on the formation adaptive groups in the MATHEMA, it was decided that: when the system creates a priority list of candidate peers for an interested learner, in the first and second position of the priority list, the algorithm will place the candidate collaborators with a concrete or abstract style and with the same active or reflective dimension of their learning style as follows: If the learner has a concrete learning style, then the algorithm will place the candidate collaborators with a concrete learning style in the first position, and in the second position, the candidate collaborators with an abstract learning style. In candidate collaborators belonging in the same position, the classification is according to their level of knowledge in the current cognitive goal up to that moment. Figure 10  shows a snapshot of adaptive group formation in the MATHEMA. Assessment of the pedagogical and technological aspects of the AEHS MATHEMA has been carried out.
paper_21	The proposed scheme ZSISMP is validated on all BCH codes of known minimum distance. BCH codes are a family of cyclic codes, which are used in many applications, due to their powerful algebraic decoding algorithms and their error-correcting capability. The error-correcting capability of these codes is directly related to their minimum distance. For these codes, only a lower bound is known and the minimum distance is known only for some lengths and special cases  [2-3-4-5-6-7-8] . In this paper, our work will focused on finding the minimum distance of large BCH codes. The section 3 presents the proposed scheme ZSISMP. The section 4 presents the main results. Determining the minimum distance of BCH codes is an important, but difficult, problem. For these codes, only a lower bound is known but the true value is still unknown for large codes. For this reason, many researchers have explored several ways to attack the difficulty of the minimum distance search problem for large BCH Codes. This section summarizes the most important ones. The artificial intelligence Simulated Annealing presented in  [16] , Tabu Search  [17] , Hill-Climbing  [18] , Genetic Algorithm  [18] [19] , Ant Colony Optimization  [20] , Metropolis Algorithm  [21] , was shown to be useful to attack the difficulty of the minimum distance search problem for BCH Codes. For finding the minimum distance of BCH codes. This section presents a validation of the proposed method on BCH codes of known minimum distance and its application for finding the minimum distance of BCH codes of unknown minimum distance. It is well known that the smallest primitive narrow-sense BCH code whose minimum distance is greater than its designed distance is BCH (127, 43, 29) and it is the only one for this length. It is known also, that All the narrow-sense primitive binary BCH codes of length 255 have their minimum distance equal to their designed distance except BCH (255, 63, 61), and BCH (255, 71, 59). The table 2 summarizes the obtained results. The table 3 presents a comparison between the proposed scheme and MIM-RSC method  [24] . In order to find the minimum distance of some large BCH codes, the proposed scheme is applied by using a simple machine of the configuration given above. In this paper, we have proposed a new efficient scheme to find the minimum distance for large BCH codes. The experimental results show that the proposed scheme outperforms several known powerful techniques.
paper_31	This paper takes chronic diseases as the research object, and proposes a design of intelligent medical system for chronic diseases based on semantic matching by the adaptation of ZigBee technology in the front-end data acquisition. Through the ZigBee wireless sensor network, this system sends the physiological parameters collected by various medical sensors to the intelligent medical system, and innovatively proposes semantic matching algorithm to solve the queuing problem of data transmission, to ensure the accuracy of data transmission. This system employs the improved spatial vector model to process the data uploaded, and uses AES encryption algorithm in the process of data transmission to ensure the security of data transmission. The realization of the intelligent system provides a scientific means of disease management for chronic disease patients, and realizes the effective management of chronic disease, which meets the design requirements and receives great patients' evaluation. It will cause many problems: firstly, it is unable to check the patient's continuous vital signs data; secondly, it is impossible for patients to get their own condition on time; thirdly, it is liable to doctors' misjudgment. Therefore, it is of great significance to develop an intelligent data analysis platform to record, store, and share and handle various personal health signs in time through wireless transmission. Chronic diseases can easily cause damage to patients' vital organs such as the brain, heart, and kidneys  [3]  to seriously affects working ability and quality of life, and its high medical expenses increases the burden of economy on society and families. In the same year, American scholar Jutra had founded teleradiology. At the end of the last century, the rapid development of semiconductor technology drove the revolution of information technology. This is the second stage of the development of WITMED. This is the third stage of the development of WITMED. Zigbee has been implemented on the Health Care Profile. As an alternative for aiding healthcare systems, sensors and wearable devices are used for monitoring patient physiological data to help guide health services or the self-care of patients  [9] . For example, when using 4G technology and WIFI technology to transmit data, patients must keep focusing on whether the device is online at all times. (2) The problem of power consumption. Chronic diseases patients are always required to be monitored for multiple vital signs simultaneously. At present, ad-hoc network cannot work no matter using GPRS or WIFI devices, and only one kind of vital-sign data can be transmitted at a time. (4) The problem of data processing. Based on ZigBee technology, a design of intelligent medical system based on semantic matching is proposed. The system architecture diagram is as follow: The data acquisition structure charts After receiving the relevant data, the data analysis platform needs to analyze the corresponding data and feedback the processed results to relevant users. The general technique route of the algorithm is as follows: Propose a comprehensive sorting algorithm, on the basis of semantic matching and a first-come-first-served principle. First, match the Word segmentation information by queued delivery with the taglib of the analysis system. If there are X words appearing in the taglib and N-X words not appearing in it, the matching degree of the information is (N-X) / N * 100%. When two pieces of information are with the identical matching degree, the first arriving one will be regarded as with high matching degree. Finally, the request is inserted into the message queuing sequence by the matching degree. The general algorithm of this part is as follows: Define priority function: f x, y ax by ε, with a basic definition as follows: x: semantic matching degree, 0≤x≤1 y: a request arriving at the serial position, 0<y≤1 a is defined as the weight of semantic matching degree, 0<a<1 b is defined as the weight of serial position of Request arrives, 0<b<1, a+b=1 ε is defined as the disturbance value, 0≤ε≤0.1, with the default as 0. The patients' vital signs data will be transferred into the data analysis system by the sensor, and then the data will be analyzed by the big data system, various signs data will be compared with the characteristic values of related diseases and the results will be returned. This research proposes an improved space vector model algorithm to ensure whether the patient has a related disease by the similarity between the physical signs and the disease on the basis of calculation of the sign value, continuous sign value and the disease characteristic value. Definition 3: The concept similarity is calculated as:  formula_1 Patients' health information involves their privacy. Therefore, it is crucial to guarantee the security of data transmission in the process of information transmission. The calculation is shown as  Figure 7 : After logging in the system, enter the sign input interface and put on the sensor, the system will automatically input the relevant data collected, as shown in  Figure 8 : After inputting the data of relevant signs, the system will automatically generate diagnosis & treatment files and health assessment according to the relevant data. The method of backstage data giving warnings and putting forward corresponding disposal measures by big data processing as shown in  Figure 9  is better than clustering, and the choice of Space vector model is more accurate than such models as Zigebee Health Care Profile chosen in the references. In this study, a design scheme of intelligent medical system based on semantic matching is proposed, with the front-end data acquisition which adopts ZigBee technology, and a semantic matching algorithm is innovatively proposed to solve the queuing problem of data transmission to ensure the accuracy of data transmission. Through ZigBee wireless sensor network, the system sends the physical parameters collected by various medical sensors to the information control terminal, and the improved spatial vector model is employed to process the uploaded data in the process of data transmission by the intelligent data analysis platform, and the security of data transmission is ensured by adopting the AES encryption algorithm in the whole process. The intelligent system realizes data acquisition, data encryption and processing, and big data analysis. The semantic matching algorithm and the space vector model algorithm proposed by the system can be widely used in the data acquisition of clinical big data to provide theoretical and technical support for artificial intelligence to assist disease risk prediction.
paper_38	The study adopted the descriptive research design. The instruments for data collection were OL, UTME and first-year Cumulative Grade Point Average (CGPA) results, which were coded and analysed with the aid of Computational Statistical Package for Social Sciences (SPSS). Education is an essential issue regarding the development of any country in the world. It is a progressive development of knowledge and skills of examinees through stages of teaching and learning at various levels  [1] . In order to accomplish and improve the value of education, it is necessary to find other ways to enhance the academic performance of students. The emphasis on academic performance, which is also dominant worldwide, has encouraged many studies about the conditions that promote it. There have been various perspectives presented by researchers and psychologists about what is academic performance and its importance. In Nigerian universities, an academic performance frequently is defined in connection with semester examination performance. In this study, the academic performance is categorised by the entire performance each year, which culminates in a Cumulative Grade Point Average (CGPA). The CGPA score takes into consideration students' tests, assignments, practicals, examinations and sometimes lecture attendance. Formula 1 is used for calculating the CGPA. As such, the CGPA is considered to be a good predictor indicator of a student's academic performance. Reference  [6]  stated that students' academic performance is not only dependent on various factors such as personal, socio-economic, psychological and other environmental variables but also quite challenging. The rest of this paper is organised as follows: section 2 is the review of related literature, section 3 presents the methodology, section 4 discusses the results obtained, and the last section presents the conclusion and recommendation. This study aims to investigate the relationship between the entry requirements into any of the Faculty of Science undergraduate full-time degree programmes in the Kaduna State University and the students' academic performance at the end of the first year of study. The objectives of this study are to: i. Investigate the relationship between the students' performance of their entry qualification and the academic performance in the 100 level CGPA examinations. The following research questions directed the study: 1. How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? The authors further explained that academic performance is usually used to determine "how well an individual assimilates, retains, recalls and communicates his knowledge of what is learnt and acquired". There are a lot of definitions of students' performance based on previous works of literature. A student's academic performance usually is measured in either examinations or continuous assessment tests, and this is expressed in various ways depending on what the scores should be used for. Reference  [11] , in his study, monitored the performance of science education students admitted through Post UME screening in 2005/2006 academic session. A sample of 214 students records was used for data collection. The records of students who graduated in the 2009/2010 and 2010/2011 academic sessions from seven faculties were obtained using the stratified random sampling technique. The study by  [13]  found a significant relationship between students' scores in three examinations, namely: UTME, PUTME and 100-Level Psychology course, Faculty of Agriculture, Federal University of Agriculture, Makurdi, and thus concluded that the UTME has predictive validity for performance in the university. Furthermore,  [16]  used the PPMCC analysis to investigate to which extent the scores of UTME and PUTME predicted the academic performance of university undergraduates. The author further recommended the need for the PUTME exercise to be strengthened to have a fruitful admission of candidates. The design adopted in the study is the correlational ex-post facto, which is used to measure the degree of association between two or more variables or sets of scores. The Faculty of Science consists of nine undergraduate B. Sc. The sample distribution is as shown in  Table 1 . The semester examinations were mostly essay type questions. The data used in the study are the OL grades, and JAMB UTME scores obtained from the University's central database, and the academic standing of first-year CGPA results collected from the various departmental examination officers (DEOs) with the approval from the Dean of the Faculty of Science. However, the OL results obtained from the database was for students admitted during 2010/2011 to 2014/2015 academic sessions. The coding for the CGPA is also shown in  Table 4 . It was used in this research study. The data were regrouped and analysed by academic session and programme of study. Table 5  shows the summary of correlations coefficient between OL results and CGPA scores (OL-CGPA), UTME and CGPA scores (UTME-CGPA) and PUTME and CGPA scores (PUTME-CGPA) aimed at all the academic sessions for Computer Science, Mathematics and Physics degree programmes. The Pearson Correlation analysis was carried out to find out if there exists a strong positive correlation between OL and CGPA, UTME and CGPA and PUTME and CGPA. For the Computer Science programme, as shown in  Table 5 , the correlation coefficient indicated a low negative correlation in UTME-CGPA (-0.092) and PUTME-CGPA (-0.001) and a low positive correlation for OL-CGPA (0.072). How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? Table 6  is the likelihood ratio test results for the Computer Science, Mathematics and Physics degree programmes. Likelihood Ratio Tests is a statistical test of the goodness-offit between two models. A Multinomial Logistic Regression (MLR) was performed to model the relationship between the predictors and membership in the six groups, i.e., CGPA categories (Fail, Pass, 3 rd Class, 2 nd Class Lower, 2 nd Class Upper and 1 st Class) for each degree programme. The traditional 0.05 criterion of statistical significance was employed for all tests in  Table 7 . For the Computer Science programme, the slopes (B) of OL in all the CGPA categories are positive. These showed that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. However, the relative strength of OL, UTME and PUTME on CGPA performance of Computer Science students is not statistically significant. What is the relationship between OL results, UTME scores and Post-UTME scores of students and their firstyear CGPA across the academic session, 2010/2011 to 2014/2015? Table 8  shows the summary of correlations coefficient between OL-CGPA, UTME-CGPA and PUTME-CGPA aimed at all the degree programmes for the academic sessions ranging from 2010/2011 to 2014/2015. PPMC was used to analyse the data for this research question. In the 2010/2011 academic session, the correlation coefficient indicated a low positive correlation in OL-CGPA (0.198), UTME-CGPA (0.189), and PUTME-CGPA (0.232). In 2013/2014, there exist a low positive association for OL-CGPA (0.061), UTME-CGPA (0.056), and PUTME-CGPA (0.038). The OL-CGPA (-0.071), UTME-CGPA (-0.090), and PUTME-CGPA (-0.040) in the 2014/2015 session signifies a low negative correlation. A Multinomial Logistic Regression (MLR) was performed to model the relationship between the predictors and membership in the CGPA categories for each academic session. Table 10 , the traditional 0.05 criterion of statistical significance was also used. For the students admitted during the 2010/2011 academic session, the slopes (B) of OL in all the CGPA categories are positive. Conclusively, the slope (B) of PUTME in the CGPA category of '3 rd Class' is positive and the rest negative. The relative strength of OL, UTME and PUTME on CGPA performance of students admitted in the 2010/2011 session is not statistically significant except for the slope (B) of OL in the CGPA category of '2 nd Class Upper', which is statistically significant. As for the students admitted in the other sessions, 2011/2012 to 2014/2015, the relative strength of OL, UTME and PUTME on CGPA performance are statistically insignificant except for the slope (B) of UTME in the CGPA category of '2 nd Class Upper' and '1 st Class' for those admitted in the 2012/2013 academic session, which is statistically significant. This predictor holds true especially for students who are in the CGPA category of '2 nd Class Lower' and '1 st Class' respectively.
paper_57	Ant colony optimization represents an efficient tool for optimization and design of graph oriented problems. [2]  as Ant system (AS) algorithm. The aim is to find the shortest path. Let's mention ant colony system (ACS) with pseudo-random proportional rule  [3]  in which random uniformly distributed variable q ∈ (0,1) is compared with a tunable parameter formula_5 else the probability selection rule (3) is applied; random selection applied to AS rank  [4]  where random selection rate r is the probability of random selection and represents a user parameter which adjusts the balance between exploration and exploitation; prevention of quick convergence (i) and stagnation avoidance (ii) mechanisms applied to AS  [5] . If q ≥ p k ij (t), then choose the next node randomly. Genetic algorithms (GA) were proposed by  Holland (1975) . The population size is given by the number of genomes, i.e. The simplest form is one point mutation on  Fig. For feasibility reason the replacement node n r (new gene) is such a node from the node n i neighborhood N i , to which an arc from n i predecessor n p to n i successor n s exists  (Fig. If more such nodes occur, random selection is applied. If no such node exists, another gene is randomly picked up from the list. In ACO algorithm crossover position is represented by a common node of parental strings except the first and the last node  (Fig. If more of such nodes exist, random selection is applied. At the end of each cycle t, when all the ants finish their tours T k (t), genetic operations are applied on the T k (t) strings which represents the list of nodes. At first mutation is applied. If mutation is not feasible, another node is chosen. If more candidates by which the selected node can be replaced occur, the new node is random chosen from the candidates. Parent strings are random selected. If crossover operation is not feasible, another second string is selected. If no tour has common node with the first selected tour, another first tour is selected and the random selection process is repeated. Genetic operations do not have to be necessarily feasible. The value for the number of cycles represents three macro cycles of ACO MC  [9]  for the same graph and parameters. Test graph is a symmetrical multi-graph with 80 nodes and 300 arcs  (Fig. The task is to find the shortest path between start node n s = 1 and end node n e = 80. For each setting 500 trials were performed. The reference value of n [%] was received without any genetic operation and is 5.6 (Table 2, row 1). For better results representation three graphs are provided. Their color map was set to show green -blue when the results are worse than reference value and yellow to red otherwise (  Fig. However, results received with two crossover pairs have higher values along with the Mutations per path axis  (Fig. This behavior may be caused by the execution order of the GO: crossover is applied after mutation, thus crossover may re-distribute mutation substrings between more paths. The results for one crossover pair show different behavior. The results vary  (Fig. Limit of crossover is 60% of crossover rate. This can be explained by the nature of the mutation operation which creates new paths whilst crossover operation can only combine already existing solutions.
paper_78	Glycyrrhiza uralensis is an endangered medicinal plant species and mainly distributed in North China. Each formation has its own composition, structure, distribution range and environment, and all of them should be protected effectively. The results suggest that fuzzy C-means clustering is an useful technique for classification of plant community. It is applicable in ecology because the description of ecological systems is not always possible in terms of a binary approach  [4] . Fuzzy C-means clustering is the only soft method in the clustering family and should have some advantages  [5] . Ecological communities, such as Glycyrrhiza uralensis communities, have great variations in their species composition respond more or less independently to environmental variables. Like licorice, most medicinal plants are harvested in the wild and the extended use has led to some medicinal species endangered and tending to be extinct, and the protection of such medicinal species has been urgent  [12, 13] . Further more, conservation of medicinal plant species is important in term of biodiversity conservation  [11, 14] . This study aims to identify Glycyrrhiza uralensis communities and analyze their characteristics on composition, structure and environment in North China. Classification is the process of dividing plots into clusters (communities) so that plots in the same cluster are as similar as possible, and plots in different clusters are as dissimilar as possible. Fuzzy C-means clustering is a soft classification technique  [5, 7]  in which a plot can belong to more than one cluster, and associated with each plot is a set of membership levels. The Fuzzy C-means clustering attempts to divide a finite collection of n elements (plots) formula_0 into a collection of C fuzzy clusters with respect to some given criterion. Given a finite set of data matrix (species × plots), the method calculates a list of C cluster centres  formula_1 formula_2 Where i = 1, 2, …, N = the number of plots; j = 1, 2, …, C = the number of clusters; U = {U ij } = the matrix of membership values, U ij is the membership of plot i in cluster j; V is a matrix of cluster centers; m is fuzzifier which determines the level of cluster fuzziness (1 ≤ m < ∞). (dA ij ) 2 is the distance index: formula_3 X i is the vector of attribute measurements in plots, usually a vector of ordination scores; V j is the centre of cluster j, if A is a unit matrix, then formula_4 Based on the algorithm above, the procedure of fuzzy C-means clustering is as follows: (1) Selecting a common ordination method, we use Detrended Correspondence Analysis (DCA), and do ordination analysis for species data. For a plot, any value can be given as its membership value in cluster j, but the sum of memberships for a plot must be equal to 1:  2  is calculated using equations (2) and (3). formula_5 formula_7 Based on the new membership values U, we go back to the fourth step and calculated the next turn V j , (dA ij )  2  and U ij iteratively, and until the membership values become approximately stable. 7 Classified plots into clusters based on the final U. However for clustering purpose, a plot should belong to the cluster in which it had the maximum membership value. Based on a general survey of Glycyrrhiza uralensis and its community distribution, five study regions, Chifeng (in inner Mongolia), Hengjinqi (in inner Mongolia), Minqin (in Ganshu), Aletai (in Xinjing) and Kashi (in Xinjing), were selected as sampling sites  [7] . Twenty plots of 5 m × 5 m were established randomly at each site which is over 40 ha in area. The coverage, mean height, individual number for shrub and herb species was measured in each plot. Elevation, slope and aspect for each plot were also measured and recorded. The elevation was measured by a GPS, slope and aspect measured by a compass meter. The Importance Value (IV) of species was calculated and used as data in clustering analysis. The importance value was calculated by the formulas  [7] :  formula_8 It classified 100 plots into 12 clusters, representing 12 Glycyrrhiza uralensis communities, e.g. The name and characteristics in species composition, structure and environment of each community are described below. spinosa, Lespedeza darurica, Pedicularis resupinata, Potentilla anserine, Saussurea epilobioides, Artemisia sacrorum, Artemisia mongolica, Cynanchum hancockianum, and Vicia amoena. The average cover of Glycyrrhiza uralensis in this community is 25% with a density of 3600 ha -1 . Its disturbance intensity is medium and heavy. The community has a total cover of 70%, a shrub layer cover of 5% and an herb layer cover of 70%. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 60%. The common species are Caragana korshinskii, Elaeagnus, mooceroftii, Suaeda prostrate, Artemisias phaerocephala, Saussurea laciniata, Saposhnikovia divariicata, Oxytropis glabra, and Artemisia ordosica. Glycyrrhiza uralensis + Artemisia frigida. It is distributed from 350 to 650 m in hills with slope 15 -35° in sunny and semi-sunny slope and sandy soil. Its disturbance intensity is heavy. The average cover of Glycyrrhiza uralensis in this community is 33% with a density of 5500 ha -1 . The common species are Artemisia scoparia, Kochia prostrate, Potencilla acaulis, Artemisia frigida, Ceratoides lates and Atraphaxis frutescus. It is distributed from 400 to 800 m in hills with slope 20 -35° in sunny and semi-sunny slope and sandy soil. Theoretically, fuzzy C-means clustering is the only one soft clustering method and can deal with much imprecise and incomplete information and has advantages in solving non-linear problems and in studying complex system  [2, 17] . This study proved that fuzzy C-means clustering is fully usable in classification analysis of plant communities  [1, 19] . These further confirm that fuzzy C-means clustering is an effective technique in vegetation analysis  [28, 29] .
paper_96	With the rapid development of China's transportation, the frequency of traffic accidents is also high. I think that if electronic guide dogs guide them to avoid obstacles such as vehicles, the blind people are able to avoid such accidents, aren't they? After that I began to design the structure of the entire system, which made the overall implementation of "electronic guide dog" take a welcome step. In fact, this "warning system of embedded electronic guide dog avoiding obstacles" is similar to a car anti-collision avoidance system. The design of obstacle-avoiding early warning system of embedded electronic guide dog is an inheritance and innovation, based on the design of traffic information collecting system, which is generally used in vehicle anti-collision at present. In this way, the design of obstacle-avoiding early warning system of embedded electronic guide dog has the following advantages. Also, it is easy to extend the functions, which is convenient for field operations, greatly reducing the cost of the traffic information collecting system in the obstacle-avoiding early warning system of electronic guide dog. [3]  The alarm system of guide dog adopts the combination of light and sound. Sound will produce auditory stimulus for the blind. Moreover, the obstacle-avoiding early warning system of embedded electronic guide dog also selects USB protocol to transfer data, saving the collected data in the hardware after being managed. The overall design plan of obstacle-avoiding early warning system of embedded electronic guide dog mainly is the velocity and distance measurement module, the power circuit module, the LED screen show module, USBcommunication circuit module, and clock circuit module. The hardware of the obstacle-avoiding early warning system of embedded electronic guide dog can reference the Figure 1:  Figure 1 : Schematic diagram of hardware system structure  [4] We should optimize the choice of the obstacle-avoiding early warning system technique of embedded electronic guide dog, based on China's national situation. The design of obstacle-avoiding early warning system of embedded electronic guide dog also adopts the generally used embedded processor. The full name of ARM is Advanced RISC Machines. It has a variety of merits, like small size, high performance, low power consumption, and cheap cost. It extensively uses the registers with a fast speed of instruction execution. At present, the embedded RTOS (Real Time Operate System) mainly includes two categories--commercial and free. These are the most remarkable features that uCLinux owned. In addition, the design of obstacle-avoiding early warning system of embedded electronic guide dog also makes a full use of USB. The reference model is shown in  Figure 2 . of USB reference model  [4] The traffic safety problem of the blind is an urgent issue to solve. This humanized technology innovation is the embodiment of environmental science and technology aesthetics theory in the field of science and technology innovation practice.
paper_134	The most jobbery ways during olive oil production consist of mixing other oils such as maize, sunflower, Canola and corn into the olive oil. Detecting the purity of different materials can be done in a variety of ways  [3] . These methods are very damaging, costly and time-consuming. In recent years, nondestructive methods have been considered in purification. Nowadays, many non-destructive methods have been investigated, which have the ability to identify various components of the quality and purity of a substance at a widespread level. Among the non-destructive methods, the dielectric method has advantages due to the relationship between the steady-state dielectric with some qualitative materials, the cost less than other methods, high speed and high efficiency in the construction of systems Different recognition of quality has attracted the attention of many researchers  [5] . Dielectric properties are one of the most important physical properties of agricultural and food products. When in a country where milk is eaten up by 30 percent as a result of a rise in the price of a few hundred tons, what else would you expect from a 60 to 80 thousand Riels per liter consumption of lubricants  [9] ? The second pertain to data mining algorithms; third part related to samples and used methods in the article. Also, processing of data and results were presented in fourth section  [11] . (2010) conducted a research on olive oil for detecting adulteration using a dielectric spectrum. They also used the partial least squares model (PLS) to detect oil falsification. The results of the PLS calibration model showed good results for the adulterated oils with soybean oil as a counterfeit oil. Also, the charts sorted by the method showed clear performance for all oil samples and easily categorize them in different clusters. Soltani et al (2010) used a non-destructive capacitive method to detect the amount of banana fruit juice. The relationship between the rates of fruit dielectric constant in a frequency of 1 kHz to 10 MHz was investigated. The results showed a significant difference between DC / DV ratio during storage period. In this article the experiment was done by olive, sunflower, Canola and corn oil. The samples were blended with sunflower canola and corn oil, and they made up a total of 9 classes with varying percentages (from 60 to 100% in 5% increments), each of which was shed separately, according to the desired amount, of 100 centimeters inside the sensor. The device used consists of the Arduino board, ICL8083 and AD8302. The Arduino board is used on the Uno device, which uses the Atmega2560 microcontroller. One of the new ideas in identifying and categorizing patterns is a backup vector machine or SVM. Using this technique, the amount of RMSE for the samples were equal to 3.56, 2.45, 3.05 and the amount R 2 was equal to 0.92, 96 and 94, respectively. According to the results, Most R 2 related to Olive-Canola (0.90) and the least amount of R 2 pertain to Olive-Sunflower (0.86). Also, in regard to the amount of RMSE, result has shown 4.81, 2.54, and 4.38 for Olive-Sunflower, Olive-Canola and Olive-corn, respectively. The results were predicted and modeled using regression methods. In this study, Quadratic function was used to regression test data. Figure 2  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn Using this technique, the amount of RMSE for the samples were equal to 3.65, 2.46, 3.31 and the amount R 2 was equal to 0.92, 96 and 93, respectively. In the current research, three different techniques were applied to predict olive oil adulterated. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud.
paper_139	Web mining is categorized into three group Web Content mining  [6] , Web structure mining, Web usage mining. Traditional web mining algorithms handle with structured document  [7] [8] [9] [10] [11] [12] [13] [14] [15]  than the advanced methodology of mining algorithm can dealthe entire heterogeneous document comprises of images  [9] , graphs, videos  [16] , etc. A query is searched in a web search tool to recover some significant and required data for the client, either the search query is known or unintelligible to the client, it generally to reply with relevant data rather than redundant, however we can't guarantee that the reply for the query about the significance and redundancy. The Document retrieved must follow some constraints which have less time & space requirements, based upon the criteria the extracted web document must be preprocessed, for preprocessing & information selection, need to apply some techniques such as stop word removal, Stemming of word, phrasing, normalization of tokens. Initially calculate measure for all the vertices and maintain the set U which contains a minimum and maximum degree for all vertices and isolated measure vertex. After applying the above steps the entire vertex without redundant information available in the set U. Pseudo Code forThe Proposed Algorithm KTMIN-JAK-MAXAM Step1: Compute degree measure for all vertices in the setup. Step2: Pick the minimum degree vertex 'v' in the set-up and include in the set U. Step3:While U doesn't include all vertices Step3A: Include the entire isolated vertex which is adjacent to the vertex 'v' to U. Step3B: Find the adjacent vertex 'u' to 'v' which is not in U and has maximumdegree. Case I:Regular set-up Connected Regular Setup Case I: A Consider the following connected set-up G 1 in figure 2, having 12 nodes having 3 degree in all vertices along with redundant links. Now the set U consists of the nodes A, B, 2. Now the set U consists of the nodes A, B, 2, 1, 3. After Applying The Proposed KTMIN-JAK-MAXAM ALGORITHM To G 2 , we get  After applying the proposed KTMIN-JAK-MAXAM ALGORITHM to the above graph G 3 Here notice that, Regular connected network G 3 , after applying the proposed KTMIN-JAK-MAXAM ALGORITHM to G 3 , we get path of length 9 in figure 7. Case III:Connected Irregular network Case III: A Consider the following irregular set-up G 5 in figure 10     After applying the proposed KTMIN-JAK-MAXAM ALGORITHM to the above graph G 7 in figure 15. Also we observed derived linked graph need not be unique but this approach will provide the optimized cost analysis report in future in data science field.
paper_145	Among them 44.3 percent were overweight and obese. Majority (70.6%) were diabetic patients. With the increase in age significant increase in prevalence rate of obesity was observed. Higher prevalence of obesity was noted among females. According to factor weights it was noted that the important factors for variation in the level of obesity were mainly gender variation followed by occupation, education and type of work. The major contribution of obesity is to lead the increase in the prevalence of chronic diseases and cancers  [7] [8] [9] [10] . The investigated diabetic patients were 544. To study the variability of socioeconomic variables for diabetic and nondiabetic people, some respondents were also investigated as a control group. The number of this latter group of respondents was 346. However, from the filled-in questionnaires 356 were found in completed form and the information of these 356 respondents were included in the analysis. The variables included for factor analysis were residence of the respondents, their age, gender, marital status, religion, level of education, occupation, type of work, monthly income and smoking habit. The analysis was done by using SPSS [version 20.0]. The level of obesity was measured by BMI [weight in kg /(height in m)  2  ] and it is a most commonly used measure of level of obesity  [18] . The respondents were classified as underweight  [BMI < 20] , normal [BMI,  20 -25] , overweight [BMI < 30] and obese [BMI 30+]. Besides the factor analysis, the association of different socioeconomic variables with level of obesity were investigated. Significant association was decided by chi-square test with p-value< 0.05 and odd ratio was calculated for respondents who were overweight and obese compared to normal group. Maximum (43.1) of the respondents were overweight and 20.9 percent of them were rural people. However, urban people were more exposed to overweight and obesity by thirty two percent compared to rural people [O. R.= 1.32]. There were 58.9 percent males among the respondents and 47.2 percent of them were normal. But more Muslim respondents (43.8%) were normal compared to Non-Muslim respondents (38.8%). Among the investigated respondents 92.6 percent were currently married and 43.1 percent of them were normal  [Table  4 ]. Similar normal group was noted among the other group of respondents. Majority (52.9%) of the respondents were of age 50 years and above and 48.5 percent of them were normal [  Table 5 ]. Higher proportion of respondents (23.2%,  Table 7 ) were businessmen and 45.5 percent of them were normal. Maximum normal group of respondents (53.8%) was observed among agriculturists. The overall normal group was maximum. More respondents of normal group of people were observed (49.0%) among them who had income 20,000.00 -< 30,000.00.This group of people were 20.2 percent. More overweight people was observed among them who had income 30,000.00 -< 40,000.00 taka followed by the group of people who had income 50,000.00+. Obese group was also more (30.3% 0) among them. The value of O. R= 0.40 for higher income group of people compared to other income level did not indicate that rich people had more chance to become obese and overweight. Table 10  showed that 67.6 percent underweight respondents were affected by diabetes. There was no significant association between level obesity and prevalence of diabetes [P(χ2 ≥ 0.851) = 0.837]. The association between smoking habit and level of obesity was significant [P(χ2 ≥ 20.189) = 0.0.002]. The association between smoking habit and level of obesity was significant [P(χ2 ≥ 20.189) = 0.0.002] and smokers were 21 percent more exposed to overweight and obesity compared to non-smokers [O. R. = 1.21]. This was done by factor analysis. From the results of the communality it could be concluded that the variable marital status was more important followed by gender and education. From the results of the communality of the variables it could be concluded that 85.6 percent variation of the variable marital status would be explained by the extracted factors  [22] . These three variables were more important for the variation in the level of obesity. The factor analysis extracted 5 components as these components explained 73.309 percent variation in the observations of obesity. This component explained 25.733 percent variation in the data of obesity. The second component explained 16.161 percent variation of the data. This component explained 10.86 percent variation of the obesity. The analysis presented here was done from the data collected from 635 diabetic patients and 265 control group of respondents. The investigated respondents were divided into 4 groups according to their level of obesity, where levels of obesity were decided by their levels of BMI. Around 50 percent respondents were overweight and obese. The analysis was done from the collected information of 900 respondents. They were classified as underweight (BMI <20), normal (BMI, 20 -<25), overweight (BMI, 25 -< 30) and obese (BMI = 30+). The percentages of these four groups of respondents were 7.6, 43.1.34.0 and 15.3, respectively. Most the respondents were in normal and overweight groups. Again, prevalence of diabetes was more among these groups. The factor analysis showed that sex, occupation, education and type of work were more important to explain the variation in the level of obesity. These are: a) Halt the rise in body weight by encouraging people so that they can take healthy home made food and avoid restaurant food / first food.
paper_212	The models developed however faced considerable challenges ranging from inaccurate representation of natural data for deterministic models, to methods of forecasting such as statistical extrapolation which assumes that current conditions will prevail which is not always the case. The values produced through simulation by the model developed in this paper using a tau value as the time step of the model were compared to HIV/AIDS data from 1985 to 2018, given by NACC. We conclude that the simulated model reflects reality. A serological test, was then made available. In 1984, Robert Gallo discovered that HIV was responsible for AIDS. This virus, being highly transmittable is analyzed and capturing how it is transmitted is crucial in attempting to model the disease. In 1984, several HIV and AIDS cases were documented in Kenya. This accounts for 4% of new pediatric infections worldwide. Among the expectant women there are 13,000 new HIV infections among children. These questions are encompassed in the modeling of the HIV immunology, the HIV dynamics as well as the AIDS dynamics such as the dynamic distribution of the disease in the population and its likely magnitude. The formulated model was used to forecast the number of PLWHA. This model approximated that there would be 6000000 cases of HIV and 400000 cases of AIDS in China if there were no forms of interventions implemented. In 2007 the government of China alongside UNAIDS made an estimate of 700000 cases of HIV and 85000 cases of AIDS in China at the time, which is much lower that the estimates made by Liu  [4] . The number of HIV infections in 2010 was predicted to approximately 1000000. There are several challenges facing models used for HIV estimates developed by UNIADS. Furthermore, it provides inaccurate estimates where an epidemic has not gone beyond its peak  [6] . Even with the 2013 updates of Spectrum where adjustments were made in the parameter values empirically to improve the fit to program data, the estimates given by Spectrum still differed with data available. More adjustments are needed as they desire to make the process where Spectrum selects the incidence curve for the data an automatic process  [7] . In their works they build a disease spread prediction model based on the SIR model and applied parameter values to a stochastic model based on Gillespie's algorithm. This study seeks to incorporate a stochastic aspect in the deterministic SIR epidemiological model. This paper will help bridge the gap between conceptual epidemiological models and its simulated version by providing a developed version of an SIR model that solves one inherent problem that deterministic models do not reflect the natural data. The transmission and infection rates were considered to be variant. The model explored how altering transmission dynamics affected the model as a whole. The reliability of the simulated values would set the precedent for the valued to be predicted based on the model is also explored. The graphical representation of the developed stochastic model is shown;  Figure 1 . The stochastic SIR model. The interaction between states is made possible by events outlined in this model as birth, infection, non-AIDS death an AIDS death. Assuming that the resulting state is . A propensity function is the probability of one event occurring in the time interval ! Discrete evolution is modelled in discrete time. In order to assess how the simulated data performs against natural data, a modified chi-square test was used. The data was obtained from NACC for HIV/AIDS cases. The means and variances of the simulated and natural data were computed. Considering the hypotheses, 4 = If the mean and variance of the simulated and natural data are equal, the simulated mean does not fit the data. 4 5 = If the mean and variance of the simulated and natural data are not equal, the simulated data fits the natural data. A modified chi-square test for simulation models was used to see how well the simulated data fit the natural data  [18] . A stochastic SIR model was simulated with a mean step size of 0.006336446. 537 tau steps were made in the model. Variables in the model were S = 3507162, I = 45820, R = 4597, parameters in the model are crude birth rate of 0.06, non-AIDS death rate of 0.025, transition rate of 0.1 and AIDS death rate of 0.48. Curves produced are illustrated below. In order to have confidence in the predicted value, we apply a test to check the simulated values against the natural data values by employing hypotheses. where C represents the variance of D The calculated value found was 64.958. The critical value was 47.4. Therefore, the conclusion is that the simulated data model fits the natural data model. The simulated curves were compared to HIV/AIDS data. Mathematical modeling is an area that requires more research.
paper_214	The paper presents the usage of databases that store business data into a warehouse star model that permits to create queries using SQL language and business intelligence tools. Optimizing the use of a certain class of models for finding the best solutions. Among the methods most commonly used are decision analysis and mathematical programming  [1, 2] . Each alternative are attached estimates and probabilities of achievement. Solving the problem is to build decision tables or decision trees, from which it selects the best alternative. The simulation, conducted testing process is carried out using computers on a defined pattern  [2, 6] . It is the only method that can be applied to unstructured problems. Among the advantages are: support the simulation model provides a functional form of expression of the links between the phenomena studied. simulation models have a procedural nature, their solution involving the processing of experiments created within the system. data model can be used in the construction of real observations (numerical values) or knowledge. Simulation becomes a technical coordination of procedures using the computer. The simulation can be conventionally divided into the following steps: the problem and research purpose; model development and data collection system; model verification and validation; describing experiments on the computer; simulation execution and achieving results; analyze the simulation results. Interaction replaces classical execution, procedural, with a performance conducted by decider according to the stages of solving a problem decisions that necessitate different inputs. The activity of the coordinations of inputs is done in most cases with specialized software systems that create analytical databases or modeling languages. To be more precise, a specific problem highlighted in a model is called one of the most used tools in the decision making simulation. The decident system uses a dialog interface with the key users of the company, enabling connectivity and communication between networks with different topologies and areas. Solving the result of communication between all stakeholders, sharing the general manager responsibilities both at decisionmaking levels and the corresponding subproblems defined. In the model design phase defines a model for decision shall be tested and validated under real system. Modeling takes expression of reality by means of abstract entities possessing quantitative and qualitative attributes. Based on patterns defined by an efficient simulation can generate alternatives. For choosing the solution which takes the results of the previous stages, the action is chosen according to the criterion of selection and decision-making model. After the final resolution of the model, select the best alternative is chosen implementation plan. The choice of solution is closely linked to proper evaluation of the results of said solution. Structural problems use mathematical formulas and analytical method to achieve an optimum solution. In order to improve efficiency the best solution search algorithms are used. Methods called heuristics, based on a thorough analysis of the issue. Basically successive tests are performed, the search progressing from a solution to another. Implementation is the phase that involves the integration model chosen solution in context and simulating the real system. Personal data is data that relates to the behavioral aspects of decisionmakers in making decisions. The data source, internal or external, data is extracted and managed by a management database. The management of the database depends on the organization of data. It contains data definitions, data sources and their intrinsic significance. It is the component that differentiates interactive decision support systems to other systems. The models are domain-specific and models can be classified into strategic, tactical and operational models models. The models are used currently in operational and transactional system that aims of the organization. Database management system allows creating new models models using programming languages, update and modify existing models, establish interdependencies between models. In building a data warehouse is based on the analysis of data. Extract information in order to obtain information for decision making. Basically there are two steps: designing and populating data. The models may change depending on the context, presenting the data in a structure bed, easily designed and accessible to end users.
paper_216	The Black-Scholes model is a well-known model for hedging and pricing derivative securities. The objective of this study is to value a European call option using a non-parametric model and a parametric model. This model is found as promising alternative as far as pricing of European options is concerned, due to its varied volatility of the underlying security and estimation of the risk neutral MGF. This study made an attempt to improve the accuracy of option price estimation using Wavelet method and it improves the accuracy due to its ability to estimate the risk neutral MGF. The study was carried out using simulated stock prices of 1024 observations. An option is a type of derivative that gives the option holder the right but not obligation to buy or sell an underlying security at a specified price, at any time on or before a given date in the future as agreed on. The holder of a call option has the right but not obligation to buy the underlying security at a specified price, at a specified time in the future. The holder of a put option has the right but not the obligation to sell underlying security under some predefined terms of an agreement which includes; strike or exercise price, maturity date and option volume. The Black Scholes model therefore belongs to the parametric continuous time models with a closed form solution family. The implied risk neutral moment-generating functions (MGF) is estimated by the wavelet method. This research therefore compared the performance of Wavelet based pricing model and Black-Scholes model in the valuation of a European option. Lastly, section 5 concludes the study. The derivation of the pricing formula is independent on the parametric form of the underlying security's price dynamics and hence the success of this model depends on the accurate capture of dynamics of the underlying security's price process. The approximation of the implied risk-neutral MGF of the underlying security (asset) returns was focused by developing a non -parametric option pricing model called Wavelet model  [4] . is the cumulative distribution function. As it has been seen from the reviewed literature, this model is a new method in the field of finance. Based on some general assumptions like independent and identically distribution (iid) for asset returns and that the moment generating function is defined well, the wavelet based pricing model can be expressed as follows; formula_1 Where, t is the current time, is the underlying asset price at time t, # $ is the time t price for a European call option written on asset, K is the strike price and T is the future maturity date. ( is the bilateral inverse Laplace transform. Interest rate r is assumed to be constant. This needs to be approximated by wavelets. In order to estimate the risk neutral Moment Generating Function of the underlying security from option prices, we applied the methodology of wavelet. This is evident from the values of the RMSE and MSE, whereby MSE of wavelet model is lower than that of Black-Scholes model.
paper_219	This paper introduces a framework to determine data quality on enterprise networks for net-centric and net-ready initiatives as introduced by the US Department of Defense (DoD). Traditionally quality of data delivered to an enterprise user focuses on network performance, i.e. quality of service (QoS). The QDS attribute brings distinction to the resultant data quality of the network's quality of service. The DR attribute is introduced; it is important in enabling enterprise data consumers to sort, filter and prioritize data. There is also a need to assess the quality of data sharing across the enterprise network. The overall value of data quality on enterprise networks is decided using a minimax decision model consisting of the three attributes. The presented framework offers decision support tools to enable agencies to allocate limited resources towards improving the performance of their net-centric service offerings to the enterprise network. That same data abundance challenges the network capacity and overloads the capacity of the human operator. All of this is in addition to the historical problems of network management and quality of service. Importantly, a model for valuing enterprise data quality is introduced to bridge the gap between measure of technical performance and operational benefit. The value of data quality can be used to indicate the priority of data delivery to the consumer. Thus the term data relevancy (DR) is introduced into the model for valuing data quality in the context of net-centric / net-ready. Section 2 of this paper provides a brief overview of the origins of net-centric and net-ready policies. Section 3 defines a new model for valuing the quality of data by measuring user satisfaction. The attributes in the new model represents data quality of the enterprise system within three contexts: net-centric measures, traditional network quality of service, and cyber-security. Vital to the value of NCW is "the content, quality, and timeliness of information moving between nodes on the [enterprise] network." The DoD introduced four criteria that must be satisfied for "Data Sharing in a Net-Centric Environment" via DoD Directive 8320.02 in 2004  [4]  which later in  [5]  expanded to the seven listed in this section. Making data visible is achieved via deployment of discovery capabilities that access and search data asset catalogs and registries in the enterprise  [6] . These enterprise catalogs and registries contain discovery metadata entries for an individual data asset or group of data assets  [6] . To satisfy the attribute of entered and managed on the network, the IT connections to external networks required to perform net-centric operational tasks must be identified  [3] . The identification of the connections must be specific (i.e. The specific data elements and assets exchanged with external networks as part of executing net-centric operational tasks are specified by the exchange information attribute  [3] . Evaluation of the data sharing enterprise requires a holistic view that considers the net-centric attributes of the data simultaneously with the quality of service for the data network. 1  illustrates the mapping relationship for each enterprise attribute to various DoD objectives of net-centric, net-ready and cyber-security. Sampling users is the preferred and direct method for measuring QoE. The question arises how to get the users' opinions. Usually the MOS is formed from arithmetic mean of a population of user opinions. For a sample of size n consider v strata with sample s k in each stratum, i.e. ∑ i=1 to v n i = n and s = U k=1 to v s k . where n ≤ N and s = {1, 2,…, n}. An important part of assessing the end-to-end performance of a data system is consideration of the inherent quality of the originating data prior to its entry into the network; to be referred to as the quality of data at source (QDS). QDS is a subjective rating from the perspective of the end user  [11] . Over the last several decades there have been many papers on the topic of QoS. But just for completeness a brief explanation of QoS is given here. QoS is in essence an engineering optimization problem where the objective is to maximize users' satisfaction while minimizing cost of delivery of the supporting network services. User satisfaction is traditionally associated with network metrics: delay, jitter, throughput, packet loss, order preservation. And the service level agreement (SLA) is the users' agreement with network provider(s) on acceptable ranges for the metrics. The most widely deployed QoS architecture used to deliver a SLA on an enterprise IP/MPLS network is referred to as a differentiated service (Diffserv)  [11] . User-satisfaction provides a true gauge of a network QoS  [11]  and the subjective assessment of that satisfaction is provided by QoE. However there are a number of challenges to QoE discussed in  [19]  and  [20] . data at the source as well as objective measurements of that data on the network that can be used to correlate with the end user's QoE. These models of QoE based on QoS objective measures are often referred to as "QoE\QoS" correlators  [21]  and  [12] . Models have been developed to correlate the QoS with QoE for multimedia applications  [12] . A number of authors  [12, 15] The measure of data relevance expresses the utility provided by the data towards the consumer's objective(s). The three principal elements of data relevance and their respective effects on the QoE for data relevance are now discussed, as depicted in  Fig. Intrinsic data relevance represents the relative value (i.e. Enterprise data systems can offer multiple forms of data to the consumer (e.g. metadata) with those of the producer. Just as the performance of tagged relevance is improved by an understanding of the consumer by the producer, discovered relevance benefits from awareness that the consumer has of the full range of data offerings of the enterprise and how they may be described. We first discuss the design philosophy for the overall assessment of the data quality for the enterprise: (1) The enterprise data quality is determined by the attribute with the lowest user satisfaction (QoE). (2) A high user satisfaction for the enterprise data quality can only occur when all three attributes have high QoEs. Alternatively the data relevance user satisfaction to match the other attributes can be improved. Thus, aside from determining the value of enterprise data quality for purposes of data prioritization, we also have an analytical tool to identify areas of improvement and to allocate resources more effectively across the overall system. f d (s) = u HT . The x-axis of the chart in  Fig. 3  is organized to reflect the connection between QDS and QoS. The final enterprise data quality, as shown in  Fig. 3 , as formula_6 In  Fig. This paper introduced a framework to determine data quality on enterprise networks for DoD net-centric and net-ready initiatives. The framework's data quality model consists of three attributes: quality of data at the source, data relevance, and network QoS. The paper described the data quality using minimax decisions based on users' survey ratings for a given enterprise configuration. The final value of data quality for the enterprise network was demonstrated as the attribute with the lowest score. The implemented strategy provides a tool for decision makers to prioritize data and manage their resources without comprising any part of the data sharing system. The paper notes the definition of the relationships between attribute's objective measures and the final quality of experience.
paper_241	Transformers are the key equipment in electrical power transmission. C transmission, power transformer is one of the most important equipment. The system is capable of sustaining a variety of environmental and operating impacts that resemble normal operating conditions which may include lightning striking the transmission lines, excessive loading, deterioration or breakdown of the equipment insulation resulting to power systems experiencing occasional faults such as phase to phase or phase to ground faults, over current, over voltage or even temperature based faults. differential current) and can be prevented using differential protection and microcontroller based relay protection. The Differential current protection scheme is based on the principle that the input power to the transformer under normal condition is equal to the output power and is concerned with having a differential or balanced current between the primary and secondary side which would be digitally displayed  [2] . The protection techniques employed differential relay mechanism with Arduino. At all times, the Arduino senses the condition of the transformer. The fundamental idea of the differential current protection scheme is to provide protection to the transformer if any fault occurs in the protected zone that will cause an imbalance in the differential currents. The output power in a transformer is equal to that of the input power hence, for differential current protection of the current transformers reduces the currents at the primary and secondary sides to a measureable value and in such a way that they are equal  [5] . The Arduino microcontroller acts as differential current comparator connected to both current sensors ACS712 and as control unit in this design. The LCD display is used to display the differential current values of the transformer and to display the voltage and current level of the system as well. The following table shows the result from the conditions of the system From the test carried out, it was observed that the current values gotten from the primary and secondary side of the system in Faulty conditions were larger than that gotten in Normal conditions and also the current difference were also larger for Faulty than Normal condition.
paper_251	Cloud computing is associated with a new paradigm for the provision of computing infrastructure and services. Advancement of electronics and telecommunication field has done the job. In the cloud computing. They are able to communicate i.e. social in nature, mobile i.e. autonomous and distributive in nature. Integration of the agent in the propose system provides the cost effective and reliable with dynamic pace, solution for efficient scheduling (elasticity of the resource and services) and proposer monitoring of the cloud computing systems. For developing proposed agent based system three types of public cloud and their services has been selected as test bed for better evaluation and measurement of the accuracy of the propose system. Codenvy -To develop an / are application i.e. SaaS (Software as a service). Platform as a Service (PaaS). monitoring and scheduling using software agent New Relic service has been subscribed. In this the java agent has been customized to meet the monitoring and scheduling of the SaaS services. These are following 1. Service scheduling delay 2. Better Provisioning of the SaaS 4. Fault tolerance While author  [1]  and  [2]  proposed an agent based solution to solve the above listed QoS parameter that greatly affect the performance of cloud service especially SaaS. But the main problem while looking  [1]  and  [2]  is the realization and effectiveness of the agent with cloud for better optimization of the service delivery. Our main research work is to enhance the agent based model for SaaS delivery in the cloud as depicted in the  [1]  and  [2] . Detail Objective of the proposed agent based SaaS Service. To Evaluate and delivered the cloud computing services (SaaS) using agent (for better and fast delivery) using public cloud such as "New Relic and cloud bees". Deploying a web services under SaaS paradigm and evaluate the effectiveness of the web application in the cloud environment with the help of agent. For SaaS development Codenvy has been subscribed. Cloud bees service integrating a SaaS deployment on it and delivering through agent. Measuring the performance of the proposed analytical approach (influenced from Aneka) in cloud services such as public Cloud bees. Model for Proposed Work Our proposed work is to schedule and monitor cloud SaaS application onto the cloud and evaluate the performance of the same using proposed agent based. Summary MAP of the SaaS using Agent -  Figure 1  has shows the overall MAP of the proposed SaaS with the help of the customized new relic agent. For evaluating the performance of the proposed agent system, the obtained results have been compared with  [2]  in which author has proposed "MABOCCF" the realization of the federation of different cloud (cloud interoperability) using agent. Time to execute the task 3. Table 3  shows the user satisfaction of the deployed SaaS obtained results and has been compared with existing malty agent based method  Rather than performing analytical calculation, proposed system has been compared with the author's MABOCCF and NMABOCCF method. Hence proposed system performs better than  [2]  and any other methods as shown in figure 3. For evaluation of the propose agent based method data set (jsp) has been developed using jsp web pages and deployed onto the cloud evaluating the elasticity and its monitoring. The developed java web application (SaaS) has been developed with the help of codenvy SaaS developed platform. Security enhancement using Agent for following attack internal attacks and DoS (Denial of Service) attack.
paper_272	Parameters of a vacuum interrupter are essential. Moreover the switching process associated with power electronics offers many modern benefits as a static switching system when joints together by using a snubber circuits for damping chopping current and transient over voltages or other facilities  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] . A more exact name would be metal vapor arc inside vacuum electroplate. J=2*10 6 (A/m 2 ) Joule heating as thermionic emission Where Ø is the thermionic work function. K is Boltzmann's constant (1.37x10-23 Joules/K) The constant A is approximately 6x10 5 for most metals If the high frequency accompanying re-ignitions and voltage escalation in one phase couple into other two phases, the process of virtual current chopping can occur  [6] . Virtual current chopping involves the load current in the other two phases being forced to zero by superimposed highfrequency re ignition coupled current. Pre-striking of the breaker in picking up a transformer load is somewhat similar to the multiple re-ignition event which occurs on opening a breaker  [7] . A high frequency current governed by the circuit parameters flows. Restrike overvoltage due to the multiple re-ignition of circuit breaker when a switching interrupter process is initiated before current zero. The process of current chopping is the premature suppression of 50Hz or 60Hz circuit current before normal current zero due to instability of the arcs in a vacuum interrupter  [6] [7] . Although the current in the vacuum interrupter can chop to zero almost instantaneously (fraction of a microsecond), the current in the load inductance-3ph coils in the power transformer cannot attain zero value instantaneously. These two high frequency transients and the voltage loop are associated with the current chop and the immediate re ignition of current. Notice that the negative polarity of the voltage loop and the negative direction of the two high frequency transients agree with the negative polarity of the last cycle of the current interrupted. This has been done in Figure. By calculating the inductor current in parallel RLC circuit under conditions of a subsidence transient, but have a far wider application. Suppose Vo =13.8√2 KV So. Zo = √L/C = 5X104 ohms η. Chopping current times (400us-800us). Transient over-voltages (450us -480us-500us).
paper_294	The cultural heritage of any people can be redressed through the preserved cultural values tangibly stored and preserved in media and forms retrievable and usable. Audiovisual heritage are quite essential that the public library system must not afford to elude its storage not only for users but also for posterity. Cassava has played an important role as a staple crop in the feeding of the Tiv people. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. It was therefore, recommended that concerned policy makers should make policies that will allow public libraries to acquire store and preserve audiovisual materials on various indigenous knowledge for development to ensue; and for posterity; and that governments and other stakeholders should ensure that the public library sector is adequately funded and manpower is adequately provided as well. The cultural heritage of any people can be redressed through the preserved cultural values tangibly stored and preserved in media and forms retrievable and usable. The photographs people take, videos they shoot, speeches and music they record, capture in bits, every moment of their life, culture, event and times that inevitably speak volumes of their history. It is a legacy that people would want to impart to their grandchildren, so that the next generation would have the opportunity to understand their heritage. Audiovisual heritage are quite essential that the public library system must not afford to elude its storage not only for users but also for posterity. Cassava has played an important role as a staple crop in the feeding of the Tiv people. The Tiv had different management strategies of postharvest losses of cassava (Akom or Alogo) that includes dried cassava chips (kpor), imyorun or imorun (gari) processing, fermented cassava (akpu) and production of base (mtuhem) among others, which were potent managing postharvest losses of cassava. Moreover, the public library should also ensure that the host communities and users are met with their particular information needs; but if this is not done, then the aim of establishing such libraries will be defeated. Cassava is the principal source of dietetic food energy for a significant portions of world populace particularly those living in the lowland tropics, and much of the sub-humid tropics. Incidentally, cassava is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world and it plays a crucial food security role to the populace due to the fact that its matured edible roots can be left in the ground for up to 36 months. Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. The study intends to investigate the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. Specifically, the study sought to: 1. Identify Tiv management strategies of postharvest losses of cassava 2. Determined whether public library has impacted on Tiv management strategies of postharvest losses of cassava. What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? Public library is a library that is established and managed with public funds. Public libraries are established wholly or partly from public funds. They are the venue through which the overall information resources are made freely available to all (Edoka, 2000; Assoh, 2011; Shidi, Aju & Ashaver, 2014). Information processing and retrieval are the core aims and objectives of librarianship, which warrants adequate coverage at all levels of education and above all service to all users both learned and non-learned. Apparently, in order to achieve this mission, public libraries preserve a variety of information materials including print materials such as books, monographs, serials and periodicals; and non-print materials such as audio or audiovisual materials such as cassettes, microfiche and films among others. The term cassava is most likely derived from the Arawak word for bread, casavi or cazabi, and the term manioc from the Tupi word maniot, which French explorers converted to manioc  (Lebot, 2009) . Moreover, it is not in the least likely to concur with the Tiv people's belief that the term cassava is derived from Tiv word kasseve, which means stick-fence it. The Tiv people used to make a ridge round their houses and plant cassava on it; and when the cassava grew up, it became a cassava fence surrounding the house. Agber (2007)  reported that a variety of cassava species are found in the Benue Basin including Aiv-kpenga, Pavnya (pronounced Panya) and Imande; and that Panya was discovered by a Tiv hunter called Adaga from Gaav Megaclan of the Tiv in about 1794 according to oral history. Cassava (Manihot esculenta Crantz, Euphorbiaceae, Dicotyledons) is the sixth most important crop after wheat, rice, maize, potato and barley and is the primary staple for more than 800 million people in the world, mostly in the poorest tropical countries. Postharvest priorities across the globe have evolved considerably over the past four decades, from being exclusively technical in their outlook, to being more responsive to consumer demand. Consumer-driven trends which have contributed to this shift include rising incomes in urban areas, changing dietary habits, more women in the work-place, reduced time for meal preparation and growing demand for safety, quality and convenience  (Rolle, 2006) . The food need not be removed any great distance from the harvest site, but it must be separated from the medium that produced it by a deliberate human act with the intention of starting it on its way to the table. Food losses may be direct or indirect. A direct loss is disappearance of food by spillage, or consumption by insects, rodents, and birds. Essentially, cassava postharvest losses can be defined as both the physical losses such as weight and quality suffered during postharvest handling operations of cassava and the loss of opportunities as a result of inability of producers to access markets or only lower value markets. Cassava has a short lifespan after harvest and as a result, the Tiv people process it into various forms for easy storage as a stratagem for postharvest loss management. Peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips) 2. Slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base) and 5. Incidentally, these methods of cassava processing among the Tiv people practiced as strategies for the management of postharvest losses of cassava were passed down from one generation to another orally. The public library owes the people a duty to go into the hinterlands of the Tiv nation in Benue State to collect audiovisual materials on these postharvest losses management strategies. This would have been achieved by capturing of audio narrations of elderly cassava farmers by recording, as well as organizing custodians of the knowledge to shot informative video clips and snap shot for video slides aimed at educating the younger generation and storing the materials for posterity. The study adopted a survey design, which is the type of design that enables the researcher to collect data from a group of people through questionnaire, interview or observation techniques for the purpose of analysis and subsequent interpretation. The target population of the study was public library staff, library users, and cassava farmers in Tiv speaking local government areas of Benue State. There are 7 public library branches in the Tiv speaking local government areas of Benue State. These are the Benue state Library Board Headquarters in Makurdi and its branches in Gboko, Vandeikya, Katsina-Ala, Gungur, Aliade and Adikpo with about 33 workers  (Mngutyô & Amaakaven, 2013) . This was due to the fact that there is no adequate list of cassava farmers and library users, which could be used as a sampling frame. They used the Fish Bowl Technique by writing Yes and No for the respondents to choose and those who chose Yes were finally given questionnaire to respond to it. Section C also consisted of 7 variables of ways public library has impacted on Tiv management strategies for postharvest losses of cassava and Section D consisted of 5 variables of challenges faced by public libraries in acquisition and storage of audiovisual materials on Tiv management strategies of postharvest losses of cassava in Benue state. The 19 item questionnaire adapted a 4 point rating scale and respondents were asked to respond by ticking the correct or applicable responses (SA) strongly agree, (A) agree, (D) disagree and (SD) strongly disagree. Consequently, for the cassava farmers who had no western education, the research assistants read the questionnaire to their hearing and gave interpretation in Tiv language, and the options they selected were ticked for them. Demographically, data were collected from 680 Library Staffs, Cassava Farmers and Library Users, out of which 348 were male representing 51.2% while 332 representing 48.8% were female. Apparently, 28 representing 4.1% were library staffs, 376 representing 55.3% were cassava farmers and 276 representing 40.6% were library users. The collected data were analyzed and presented in  Table 1 . From  Table 1 , it can be seen that the mean rating scores of all the items are above 2.50. This means that peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips), peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour), peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom, slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base), peeling the cassava, fermenting, sieving and squeezing it into lumps known as Akpu, peeling the cassava, grating, squeezing (demoisturizing) it, mixing with salt and other ingredients and frying with palm or groundnuts oil known as Kweesi or Akweesa Akom and peeling the cassava, grating, squeezing and sieving the starch and preparing it for food as Kamu Akom are the management strategies for postharvest losses of cassava. Table 2  shows the mean rating scores of all the items are below 2.50, which implies that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? Table 3 , it can be seen that the mean rating scores of all the items are above 2.50. This implies that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. Table 2 , findings of the study revealed that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). Findings from the study based on the result in  Table 3  showed that lack of adequate funding to facilitate acquisition and storage of audiovisual materials on various aspects of indigenous knowledge, inadequate technical knowledge and skills of staffs, lack of adequate staffs, lack of Policy Statement on the acquisition and storage of audiovisual materials on Tiv indigenous knowledge such as management of postharvest losses of cassava, and lack of initiative on the part of library management to make attempts to acquire and store audiovisual materials on various forms of indigenous knowledge are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. Therefore, it was recommended that concerned policy makers should make policies that will allow public libraries to acquire store and preserve audiovisual materials on various indigenous knowledge for development to ensue; and for posterity. Management of public libraries should also ensure that initiatives on going from one community to another to record and shot films on indigenous knowledge are in place. The study was carried out to investigate the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. Importantly, if public library has acquired and stored audiovisual materials on Tiv management strategies for postharvest losses of cassava, cassava farmers would have accessed these information materials to improve on various ways of managing postharvest losses of cassava in Benue State of Nigeria. The study concludes that public library must make it a point of duty to start acquiring, storing and preserving information materials on indigenous knowledge, particularly in audiovisual format, since this is the only way the library will be useful to the host communities.
paper_298	The study investigated the awareness and use of library catalogue for accessing information resources by users of Federal University of Kashere (FUK) Library. The instrument used for data collection was questionnaire. In effect, the respondents use card catalogue regularly compared to the OPAC. Majority of the respondents used the library for research work rather than for leisure and recreational purposes. Poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. Again, awareness of the library catalogue is the ability of the students to have communication and consciousness of its essence, its retrieval technique as well as their relevance to the information user. Students' skills in search catalogue can be created through library tours, orientations, seminars, lectures, library staff, handbooks and use of library studies. Although, much awareness of the retrieval tools may be created in the libraries, it does not necessarily mean its accessibility, not to speak of its use. Kumar revealed that 26.99% respondents use the OPAC fortnightly and 22.84% weekly. In essence, students use the catalogue to enable them conduct research in the library. It is on this note that this study investigates the use of library catalogue by undergraduate student in the university library. Therefore, the study investigates the access and use of library catalogue by students of Federal University of Kashere, Gombe State. The study is designed to achieve the following objectives: a. To find out the students awareness about library catalogue as access point to library collection. To find out methods employ by students to consult library catalogue to search for information resources. c. To ascertain the extent of use of the library catalogue by students to access information resources. d. To identify the constraints associated with the use of library catalogue. The study is designed to answer the following research questions: a It shows that majority of the respondents 188 (69%) were male, followed by females 84 (31%). This shows that majority of library users in the Federal University of Kashere Library were male. Table 2  revealed that majority of the respondents 262 (96.3%) were aware of the existence of the library catalogue in the university library. Awareness of access points to library collections are the most important factors influencing the success of the retrieval and utilization of library resources. The awareness of respondents would depend on the way the library informs their users about the use and functions of library catalogue revealing the available resources and its location in the library. In order to find out how respondents became aware of the library catalogue in the university library, options were provided in the questionnaire on the sources of library catalogue and they were asked to indicate the sources of their awareness. Their responses were presented in the  Table 4  below. Table 5  reveals that 106 (40%) got their awareness through library staff, followed by those that knew it through the user education programme (GST) 88 (16%). This implies that majority of respondents became aware of information retrieval tools through library staff and user education programmes these are more formal sources of awareness about library catalogue function and use. Table 5  indicated that majority of the respondents 158 (60%) used the library catalogue regularly when searching for materials in the library. 24 (9%) of the respondents never used the catalogue but they were aware of it existence in the library as indicated on table 2 above. Total  F  %  F  %  F  %  F  %  F  %  F  %  100  10  20  0  0  6  12  28  56  6  12  0  0  50  200  10  23  6  14  10  23  16  36  2  5  0  0  44  300  10  19  6  11  14  26  12  22  6  11  6  11  54  400  14  25  0  0  36  64  6  11  0  0  0  0  56  500  2  3  0  0  54  93  2  3  0  0  0  0  58  Total  46  18  12  5  120  46  64  24  14  5  6  2  262  The finding indicated poor use of OPAC 12 (5%) at the university library. The study found that majority of the respondents were aware of library catalogue in the library including OPAC  [12] . It could be concluded that majority of the respondents used the library for research work rather than for leisure and recreational purposes. The results are presented in  Table 8 . Table 8  focused on the challenges associated with catalogue use for information retrieval. Higher number of the respondents indicated poor computer skills hindering navigation when searching OPAC 96 (37%) and inability to locate materials on the shelves, indicated in the catalogue as being available 58 (22%) were the challenges affecting the use of catalogue as an information retrieval tools. ICT skill is needed by the respondents to be able to browse the library OPAC. Higher number of them got their awareness through library staff and above average used the library catalogue regularly. It could be concluded that majority of the respondents used the library for research work rather than for leisure and recreational purposes. Another source of frustration was reasonable number of the respondents indicated poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools.
paper_305	This is primarily due to difficulties in uncovering uncertainties in information provided by credit applicants and also due to lack of reliable automated techniques that would improve the efficiency of manual underwriting procedures. In this paper, we report on the results of a MSc. The results obtained proved that such a mechanism can be applied to augment manual credit appraising processes, especially where large volumes of applications are to be processed within limited timeframes. Loans constitute the cornerstone of the banking industry's financial portfolios. Despite the increase in consumer loans defaults and competition in the banking market, most of the Kenyan commercial banks are reluctant to use artificial intelligence technologies in their decision-making routines. Generally, bank loan officers rely on traditional methods to guide them in evaluating the worthiness of loan applications. A checklist of bank rules, conventional statistical methods and personal judgment are used to evaluate loan applications. Furthermore, a loan officer's credit decision or recommendation for loan worthiness is subjective. Given the absence of objectivity, such judgment is biased, ambiguous and nonlinear and humans have limited capabilities to discover useful relationships or patterns from a large volume of historical data. Generally, loan application evaluations are based on a loan officers' subjective assessment. Further, the complexity of loan decision tools and variation between applications is an opportunity for the use of a machine learning tool to provide learning capability that does not exist in other technologies. Further, the study champions the use of open source software tools in business intelligence applications. From time immemorial in the banking sector, banks have relied on the personal assessment of loan risks or on the traditional statistical methods to predict the default of loans instead of using a standardized evaluation tool. These traditional methods often require a great deal of subjective input from underwriters, making them un-reliable and often lack empirical and scientific backing. Loan appraisal decisions can easily extend beyond the "accept" or "reject" kind of classifications to include such other spectral values as "fairly good", "good" and so on. This calls for the use of more efficient and effective loan screening tools and procedures. Automated techniques have progressively become popular in contemporary loan appraisal processes. One of the earliest automated procedures uses statistical tools which have fallen short of the inherent challenge for today's commercial banks is their desire to understand large amounts of information and reveal useful knowledge to improve decision-making. This is largely because the sustainability of banks depends largely on their abilities to sift through large volumes of data, to extract useful knowledge and enforce this knowledge in their decisions. Today, lenders are making increased use of new and innovative techniques -the key being data mining and machine learning to evaluate loan applications for business and financial prospects  [2, 3] . These techniques have been found to outperform earlier approaches leading to increased competitiveness. Credit appraisal often amounts to making a decision whether to grant or to reject an application. A decision stump is a decision tree with only a single root node. It works as follows: 1. Looks at all possible thresholds for each attribute 2. Figure 1  illustrates the combination criterion. In our study, the variable k was bi-valued and took on either 'accept' or 'reject' values and K was set at 2. Train all K decision stumps iii. Select the single best classifier at this stage iv. Reweight the data vii. Learn all K classifiers again, select the best, combine, viii. Repeat until you have T classifiers selected Weka which is open source software issued under the GNU General Public License providing a collection of machine learning algorithms for data mining tasks was integrated into the development platform. v. Classify until each partition has been used as the test set. Calculate an average performance. This gives an accuracy of 19/20=95% Class =Accept: The number of correctly classified instances is 12 and that of instances classified as belong to the class is 13. This gives a precision value of 7/7=1 Class =Accept: The number of correctly classified instances is 12 and the number of instances belonging to the class is 12. This gives a recall value of 7/8=0.88  Figure 2 . Test Split ROC graph Indicates a perfect prediction . Finally, as stated earlier in the introduction, it is not prudent to completely rely on an automated credit appraising as some cases might require subjective interpretation and personal judgment. This is a good basis for manually investigating such cases whose levels of confidence go below a certain threshold.
paper_310	Now, smart phone has become the most essential thing in our daily life. Android application based smart phones are becoming each time more powerful and equipped with several accessories that are useful for Robots. Here we use mainly Arduino UNO (ATMEGA 328P), Bluetooth module (HC-05). The controlling device of the whole system is a Microcontroller. Bluetooth module, DC motors are interfaced to the Microcontroller. b) Develop a robot which will be helpful for travelling. f) Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. h) Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. The microcontroller is programmed with the help of the Embedded C programming. Cprogram is very easy to implement for programming the Arduino UNO. The smart phone is the transmitter of this circuit. It sends the data to microcontroller through Bluetooth module. The program for executing this project has been written in C language. The program is burnt in the microcontroller using burner software. When signal data arrives the Arduino the pin which corresponds to the particular input is set to high. Motor driver switches accordingly the data bit, if the data bit is low then the corresponding pin of the motor driver doesn't work else highbit then thecorrespondingpinof the motor driver is on. The working principle is kept as simple as possible. As seen from the  Figure 6 . A DC power supply is required to run the system. The DC power supple feeds the Microcontroller and the Bluetooth module. Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. When user sends any data to the Arduino board then the corresponding pin of Arduino goes to high state and switches the motor driver ic in the on mode. The corresponding motor moves as per the input data. The instructions are sent by the smart phone. When any input is given then the motors moves as per the preloaded functions in the microcontroller. The robot can be used for surveillance.
paper_333	There are several issues and diseases which try to decline the yield with quality. This study presented a general process model to classify a given Enset leaf image as normal or infected. This diagnosis apply K-means clustering, color distribution, shape measurements, Gabor texture extraction and wavelet transform as key approaches for image processing techniques. The government committed 15% to 17% of expenditures to the sector; it covers 41% of gross domestic product (GDP), over 90% of export value and directly supports 80% of the populations' livelihoods  [1] . There are several issues and diseases which tries to decline the yield with quality. As a matter of facts, visual or manual detections may have defects in terms of accuracy in detection along with lower precision. There is a potential need for technology supported alternative systems to support the manual identification of Enset crop diseases so as to optimize the accuracy for remedial action. If the Enset crop diseases can be minimized then definitely the production will significantly be increased to contribute to the country's economy towards a new boost. Therefore, the implementation of IT based solution in the sector will have a paramount importance to facilitate the activities such as economic, social and ecologic development in the country by increasing efficiency in both quality and quantity of Enset crops production, to sustain dependability of customer preferences and to preserve the ecology  [2] . In our country few researchers found the promising solutions to different plant diseases diagnosis such as maize, rose flower, coffee and others using computer vision and machine learning techniques  [3, 4, 5] . Experimental results are reported in Section IV. The occurrence, distribution and the incidence level also indicated to vary from one Enset-growing locality to the other. Figure 2  shows the architecture of the proposed system Those images are collected by using a digital camera and some of them are collected from secondary sources like research centers which collects pictures for visual inspection of Enset disease. From all those disease category a total of 460 Enset leaf images are collected from which 368 is used for training and 92 images are used for testing. Test Result For the experimentation a total of fifteen experimental setups which is color, texture morphological and a combination of each and a multiclass Kernel support vector machine model: Linear Kernel, polynomial Kernel, RBF kernel and Quadratic Kernel was used. From the total dataset 20% is used for testing and the distribution of each disease category is shown in table 2. From the figure it is shown that the RBF kernel classifier has classified the given enset image in its correct class category by 94.04% and 92.44% respectively. We designed a system which can detect Enset disease automatically and this will help the farmers to detect the diseases in its early stage and to take relevant action. If there is a shortage of experts in the area of those disease identification the system is going to be a replacement for the experts.
paper_389	The experimental corpus has been tested by Changjiang Daily for many years. Peng F establishes a Chinese character segmentation model based on CRF. Therefore, Chinese word segmentation method has become a frequently used method to study word segmentation. Enter the sentence as "This is Wuhan." As mentioned earlier, the CRF makes it easy to add any feature in the observed sequence to the model, so that not only the transfer and emission characteristics of the traditional HMM sequence model can be incorporated into the model, but also some other The feature information associated with the observation sequence or with the language itself is added to the model. The following  Table 1, Table 2 ,  Table 3  is used in the experiment some of the characteristics of the template. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. Using the CRF model generated by the training, the test corpus is tested and a prediction result is obtained. The main corpus used in this paper is the training corpus and test corpus of Changjiang Daily. The corpus is from 1950 to 2005, and the scale is 2564168000 sentences. The corpus content mainly comes from newspaper news. The format consists of a sentence segment consisting of words marked with spaces. The performance of a word segmentation system mainly depends on two aspects: segmentation precision and word segmentation speed. The performance of Chinese automatic word segmentation is evaluated by the following three indexes: correct rate (P), recall rate (R) and F value. CRF The word segmentation system is a word segmentation system based on the conditional random field, and uses the feature template one, the feature template two and the feature template three. The results of "+ feature template 3" model are obviously better than that of "+ feature template 2" model, that is, under the condition of adding feature template 3, F-score is 4.4% higher than that of feature template 3, Played a better effect. (CRF) word segmentation model, the experiment uses the combination of "feature template one", "feature template two" and "feature template three" in the common daily closed test set Test, the performance comparison of the results shown in  Table 5 . And then use these tools and the corpus carried out a number of experiments.
paper_391	Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. Globally, tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The WEKA 3.6.1 tool was used for classification, clustering, association and attribute selection. But, in the class attribute, it is 0.72. The literacy status of the mother has high information gain with the value 0.046. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The information is rich and massive. In this study, we use different data mining techniques that have been tested on TT dataset. Selection phase generates the target data set from the whole data set of EDHS 2011. However, not all of the patterns are useful. Those patterns that remain represent the discovered knowledge. The EDHS of 2011 dataset was used as a source for this study and WEKA 3.6.1 machine learning tools are used. Finally, data have been saved in ". csv" file formats and stored as an ". How does this classification work? The approaches are; (a). And the topmost node in a tree is the root node. This assumption is called class conditional independence. Training and testing are performed k times. Thus, in this study, we have used five clusters. "How does the k-means ( ) algorithm work?" (Table 1)  As can be seen in  Figure 2 , the accuracy rate of the classifiers on training data is relatively high. A multilayer perceptron is the best classifier in our data set. Briefly described as follows: Cluster 0-This group have 1534 (27%) instances of which consist of mothers who live in the rural, no access to radio, no access to TV, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 1-This group consists of 1004 (18%) instances of which mothers who live in the rural, no access to radio, access to TV yes, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, head of household male, tt-vaccinated yes. Cluster 2-This group consists of 2063 (37%) instances of which mothers who live in the rural, no access to radio, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated No. Cluster 3-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 4-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, orthodox in religion, Amhara in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education no education, women age 35-49, mother marital status married, hh_head Female, tt-vaccinated yes.
paper_402	The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. The need for soft computing tools and models for the prediction of behavioural properties of engineering components, systems and materials is continuously rising. The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete  [14] [15] [16] [17] . Other researchers proposed different mixtures by adding fly ash and sand to reduce the amount of cement and silica fume, and acquire an optimum mix that is both economical and sustainable  [25, 26] . Despite the statistical advantages of ANN, it has been long regarded as a black box that evaluates functions using input covariates and yielding outputs. Moreover, Rodriguez-Galiano et al. The steps that were are followed in developing a robust and accurate numerical model using SFS include (1) design and validation of ANN model by manipulating the number of neurons and hidden layers; (2) execution of SFS using ANN as a wrapper; and (3) analysis of selected features using both ANN and nonlinear regression. There are two types of ANN models: (1) feed forward; and (2) feed backward. The input neurons are responsible for containing the independent parameter presented by the user, the wires represent the randomly generated matrices called weights that manipulate the function's slope or steepness, the hidden neurons map the weights variables using an activation function, and the bias units control the output function's shift, either upward or downward. Figure 1  shows the algorithm SFS uses when performing forward selection. Based on the results of these trials, the most abundant combination during the SFS analysis, within a 20% threshold, was selected as the important parameters that contribute mostly in the model. The selected features, using SFS, were analyzed by the previous BPNN model. The LSR model is a linear function and its form is shown in  (2) . fc = θ1C + θ2SI + θ3FA + θ4W (2) Using Fly Ash quantities that range between 0-200 kg/m 3 and Silica Fume quantities that range between 40-160 kg/m 3 while fixing the quantity of cement at 1400 kg/m 3 and water at 175 kg/m 3 , several plots showing the variation of strength of UHPC were generated as shown in  Figure 5 . The correlation coefficient (r 2 ) before and after the use of SFS improved from 81.6% to 99.1% while the NMSE improved from 0.0594 to 0.026, respectively.
paper_418	Also, when there is no trend (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The models most commonly used to describe time series data are additive, multiplicative and mixed models. They do not depend on the level of the trend  [3] . According to  [4] , the appropriate model is additive when the seasonal standard deviations show no appreciable increase or decrease relative to any increase or decrease in the seasonal means. On the other hand, the appropriate model is multiplicative when the seasonal standard deviations show appreciable increase/decrease relative to any increase /decrease in the seasonal means. The higher the trend, the more intensive these variations are. Iwueze and Akpanta  [6]  pointed out that an additive model is appropriate when the seasonal standard deviations show no appreciable increase or decrease relative to any increase or decrease in the seasonal means while a multiplicative model is usually appropriate when the seasonal standard deviations show appreciable increase/decrease relative to any increase or decrease in the seasonal means. In such cases it is appropriate to use a multiplicative model. Oladugba et al  [9]  gave brief description of additive and multiplicative seasonality. However, this approach can only identify the additive model (when the column variance is constant), but does not tell the analyst the alternative model when the variance is not constant. The implication of this is that when the test for constant variance says the appropriate model for a study series is not the additive model; an analyst still faces the challenge of distinguishing between mixed model and the multiplicative model. The method adopted in this study is the Buys-Ballot procedure in descriptive time series. formula_5 Source: Iwueze and Nwogu (2014), Nwogu el al (2019). formula_10 When there is no trend ( 0 b = ) we obtain from  (Table 1)  . Table 3  that when there is no trend i.e. (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The actual and transformed series are given in figures 3.1 and 3.2. The expressing of a linear trend and seasonal indices for an additive model is given as Estimates of trend parameters and seasonal indices are discussed. Also, when there is no trend ( ) 0 b = , the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models.
paper_428	The present article examines the influence of thermal radiation on two-dimensional incompressible magnetohydrodynamic (MHD) mixed convective heat transfer flow of Williamson fluid flowing past a porous wedge. An adequate similarity transformation is adopted to reduce the fundamental boundary layer partial differential equations of Williamson fluid model in to a set of non-linear ordinary differential equations. In Non-Newtonian fluids, the most frequently encountered fluids are pseudoplastic fluids, and Navier-stokes equations alone are insufficient to describe the rheological properties of these fluids, therefore, to overcome this defect, several rheological model like Ellis model, Power law model, Carreaus model and Cross model are presented, but little attention has been compensated to the Williamson fluid model and estimated to explain the rheological properties of pseudoplastic fluids. studied the peristaltic motion of non-Newtonian fluid through asymmetric channels along porous wall by means of various phase and amplitude, and also studied the manipulation of different wave structures on the fluid flow model  [8] . analytically discussed MHD flow of viscous fluid through Stretching sheet using DTM-pade approach to solve boundary layer equations of given flow problem  [21] . Jabar addressed the influence of viscous dissipation and joule heating on MHD flow through a stratified sheet subjected to power Law Heat Flux having heat source  [22] . Bau investigated the thermal convection in a saturated stratified medium bounded between two parallel eccentric cylinders with the help of a regular perturbation expansion along Daarcy-Rayleigh number; it was observed that the appropriate preference of eccentricity values can maximize the heat transfer inside annulus of various thermal insulators  [37] . studied unsteady combined convection flow in a cavity in presence of nanofluid  [41] . Chaudhary and Jain studied the impact of mass transfer, radiation and hall on MHD mixed convection flow of viscoelastic fluid in an infinite plate  [43] . Malleswaran & Sivasankaran carried an analysis for mixed convection flow and noticed that the average heat transfer decreases with an increase in Richardson number but in general heat transfer is better at force convection mode than free convection mode  [45] . Deka   is wedge angle parameter. [78] , Yih  [79]  and Rashidi et al. It is observed that velocity increases by increasing the wedge angle parameter < , but the thermal boundary layer thickness is decreased. It is clear from graph that an increase in thermal radiation parameter leads to increase in temperature and thermal boundary layer thickness. The important conclusions of the analysis are 1. The non-dimensional velocity profile increases by increasing the wedge angle parameter <.
paper_432	The mathematical analysis method used. And found some results; The theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds, possibility study of Hamiltonian tubes when the simplistic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models. Time -dependent smooth Hamiltonian on Τ * , the cotangent bundles of . Generically, the corresponding Hamiltonian system `(%) =& H '%, % ). Given any , manifold , of dimensionn, with -. b) Let exp ∶ → 3 > be given by exp(% =J •" … ∈ 3 > . Let 0 ∶ 3 → 0 be the projection map. Let be a lie group• ∈ o⋃ ~∞•. 8 ' : K V → R s> ( K ), satisfying R o 8 ' u 0 ' and the equivariance property. 8 ' ( K , n, ' ) = 8 ' ( K, n ). ' ), the left multiplication of : n`. Now consider a G on a manifold. šJ% ± ∈ , with isotropy subgroup = ² . The first result of the theory in cotangent bundles reduction, the theory developed for the problem with a single or bit type playing an important role in the solution to the general problem of a singular cotangent bundles reduction for base manifolds, Hamiltonian tubes when the symplectic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models and the first work studying symplectic normal forms in the specific case of cotangent bundles. Conclude that the theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds and found that the phase space is the cotangent bundle T * Q of a configuration space .
paper_444	Resource constrained project scheduling problem is to make a schedule for minimizing of the completion time or total cost subject to precedence rules and resource constraints. The other constraint is the resource constraint in which the demand of resource shall not exceed the total supply of resource for each resource type at any time. Furthermore, the equivalent form of the above model is given and its equivalence is proved. Finally, a genetic algorithm is applied to search for quasi-optimal scheduling, and a project example is given to illustrate the effectiveness of the model. In this case, many scholars begin to consider the uncertain resource constrained project scheduling problem. For solving the above model, the equivalent form of the model is provided and the proof is given. The construction of this paper is organized as follows. In Section 2, an uncertain resource constrained project scheduling model will be built and transformed into a crisp form. (1) This paper only considers renewable resources. The uncertain resource constrained time-cost trade-off problem can be described as following optimization model: formula_0 In the above model, objective ○ 1 is to minimize the project total completion time; Objective ○ 2 is to minimize the expected project total cost which consists of the activity cost and the additional resource cost. Constraint ○ 4 reflects that for any time # and each resource type , the demand for resources shall not exceed the total supply of resources with at least given confidence level < = . In order to transform the model into deterministic form, we introduce the following several theorems. Model (1) is equivalent to the following model. is an uncertain variable, and the inverse uncertainty distribution of ? is formula_7 Since ℳ6∑ ∈8 9 ≤ + ; ≥ < = , then, ℳ6∑ ∈8 9 − − ≤ 0; ≥ < = . formula_10 Therefore, the model (1) is equivalent to the model (2). The manager tends to the demand of resource shall not exceed the total resource supply with at least given confidence level 0.9 for any time # and each resource type . The constrains are recourse constraint and precedence constraint. The information of the activities. By describing the increased quantities of resource as uncertain variables, an uncertain resource constrained project scheduling problem is discussed in this paper. Then an uncertain optimal model was built with objective of minimizing the completion time and the cost with resource constrained based on uncertainty theory. Finally, we used genetic algorithm to search quasi-optimal solution of the model and gave a numerical example to illustrate the validity of the model.
paper_462	The goal of postgraduate education for master's degree in clinical medicine is to cultivate high-level medical and health professionals. Since 1998, the postgraduate education for master's degree in clinical medicine has been piloted in China. There are many problems such as insufficient practical ability training. Through improving the conditions of enrollment, innovating the training mode, adjusting the award marks and improving the system guarantee, it has effectively constructed the professional degree education of master of clinical medicine and standardized resident training. "5 + 3" new training mode of training in combination. Chongqing Medical University was founded in 1956. Since 1979, it has enrolled doctoral and master's degree students. In 1998, it became the first batch of doctoral and master's degree in clinical medicine in China. Pilot units for professional degrees. Since 1998, the school has vigorously developed professional degree postgraduate education  [1] . Since 2009, the school has been focusing on reforming the training mode of clinical master degree postgraduates (hereinafter referred to as clinical master). After more than five years of exploration and practice, the reforms have achieved remarkable results and have provided a training model for clinical master in China. It not only guarantees the quality of professional degree postgraduate training, but also supplies a large number of high-quality talents for related industries. How to reform the training mode of clinical master, improve the quality of training, and bring up a large number of high-level applied medical talents is the main problem for graduate educators. In the long-term practice of training clinical master, Chongqing Medical University has gradually explored a new "5+3" training mode which combines clinical master education with regular training, and completed the educational practice of "Innovation and Practice of the Training Mode of Clinical Master Professional Degree in China"  [3] . Educational development and other key problems of clinical master training. The system reform of training mode for clinical master of Medical University has effectively solved the problem of insufficient practical ability of clinical master, and has realized the degree education and professional qualification recognition of clinical medicine specialty. The organic cohesion of syndromes has effectively improved the quality of clinical master training. Under the above policy guarantee, the clinical master trained by the school has the dual status of postgraduate and regular trainee. After completing the relevant training content and passing the examination, the qualified certificate of licensed physician qualification and resident standardized training can be obtained. After completing the dissertation defense and meeting the award requirements, the graduate certificate and master's professional degree certificate can be obtained. The current single tutorial responsibility system can no longer meet the requirements of clinical rotation training, which is not conducive to the management and supervision of postgraduates. To ensure the quality of clinical master's training, the school has set up a guidance group composed of the first tutor and rotating responsibility tutor. Effective management during the transition period. The staff of the Graduate Management Department of the school conducted a thorough investigation on the effect, problems and suggestions of the training mode of clinical master's degree in various clinical departments, and held dozens of meetings to solve common problems. After the admission of clinical master, the school will submit the list to Chongqing Municipal Health Bureau, and register it to Chongqing Municipal Health Bureau resident standardized training registration system without test. Our school has made corresponding reforms in the curriculum system of clinical master's degree. The reformed curriculum system not only meets the training objectives of clinical master, but also meets the requirements of theoretical knowledge for trainees. Secondly, the curriculum of Master of Clinical Science is adapted to the requirement of training students' theoretical knowledge and foreign language. It is closely related to clinical practice. The rotation requirement not only meets the requirements of the state for clinical master, but also closely combines with the regular training. Requirements for the first stage of training. Students who have obtained the certificate of resident standardization training are directly transferred to specialist training (i.e. the second stage of standardization training), which not only saves the time for graduate students to carry out clinical rotation, but also ensures the quality of training, and achieves seamless docking in the rotation cycle. System for Clinical Ability Clinical competence is the core of clinical master training. School research and explore more rigorous clinical training and assessment methods. The clinical master emphasizes on examining the clinical competence of postgraduates. At the same time, the cost of training clinical masters has increased substantially. Compared with academic degree postgraduates, the training objectives of professional degree postgraduates are obviously different. The school revises the standard of clinical master's degree award, which reduces the requirement of publishing articles. The staff of the Graduate Management Department of the school conducted a thorough investigation on the effect, problems and suggestions of the reform of the training mode of clinical master in various clinical colleges and departments, and organized a meeting to solve some common problems. Thirdly, we should formulate an enrollment system conducive to the development of professional degrees. All tutors working in clinical departments must enroll professional degree postgraduates. By promoting the combination of clinical master training and regular training, the problem of clinical master qualification certification has been solved, and a new "5 + 3" training mode has been constructed. The curriculum system is guided by the improvement of vocational competence and solves the contradiction between curriculum learning and clinical competence training. The two systems complement each other and organically combine to realize the training of clinical master and regular training. It has overcome many obstacles and steadfastly promoted the reform. It has basically realized the seamless connection between the degree education of clinical medicine specialty and the certification of professional qualification. The main results are as follows: In the past five years, a total of 2063 clinical masters enrolled in the school have been integrated into the training. They have achieved the ultimate goal of training "doctors who can really see a doctor". The employment rate of graduates has been guaranteed to be 100% for a long time. In December 2013, the school sponsored the National Symposium on the Reform of the Training Model of Clinical Master's Degree Postgraduates. In the past five years, a total of 2063 clinical masters have been enrolled in the standardized resident training system. The school assists in formulating the standardized training policy for general practitioners and residents in Chongqing, the construction of bases, the training and assessment system, and teacher training. The implementation of "three combinations", namely, the combination of enrollment and enrollment, training and training, degree award and physician access. The "four certificates association" should be implemented, namely "licensed physician qualification certificate", "standard training qualification certificate", "diploma certificate" and "degree certificate"  [17] . The current management system and mechanism are not suited to the key issues of clinical medicine master training, such as the development of professional degree postgraduate education  [18] .
paper_476	This paper uses JD Logistics as an example to quantitatively analyze the relationship mentioned above. In addition, it is important for the logistics enterprises to consider the constraints such as delivery resources and consumer satisfaction to achieve the appropriate decision of delivery frequency. At present, most express companies are operating different delivery frequencies in different regions. The order volume, the orders' splitting ratio, the operational efficiency of delivery resource and the delivery costs will affect the delivery frequency. There are various of applications of system dynamics in the area of transportation  [2, 3] . Fan Xuemei et al. [5]  pointed out that joint delivery can effectively improve the efficiency of urban delivery. It proposed an assessment framework for joint delivery. This paper uses Jingdong Logistics (JDL for short) as research objects. Each delivery operation mainly includes: storage, ferry, sorting, transportation and terminal delivery. The relationship of the workflows in the delivery system is shown in  Figure 1 . The delivery cost mainly includes the equipment usage costs and labor costs in the three stages of sorting, transportation and terminal delivery. The equipment usage costs can be divided into two parts: the fixed costs and the variable costs. Performance wage is determined by the actual amount of work performed during the delivery process. Sorting staffs consist of sorters and on-site logistics personnel. The sorter is the person who operates on the sorting equipment. Based on the surveys of JDL and interviews with related professionals, this paper summarizes 55 influencing factors on delivery frequency. These factors present a complex, nonlinear, and inverse relationship between each other. As is shown in  Figure 2 , 58 causal loops are formed. The extreme condition when the order quantity equals to zero was examined as well. Different delivery frequencies have different splitting ways of total order quantity per day. On the other hand, the cost index was calculated based on the summed number of shipments as JDL adopts single-batch delivery. Second  Third  Fourth   1  60  30  40  20  2  30  30  20  20  3  10  30  30  20  4  -10  10  30  5  ---10  Table 4 . The delivery frequency in scenarios 2 and 3 was increased by 9.5% over that in scenario 1. When the volume of orders increased from 1496 to 5300, the increased percentage of total delivery cost showed a downward trend followed by an upward trend. The minimum value of 3.9% appeared in the order volume of 3086-3154 units. The reason is that in scenario 3 the sorting center dispatched more vehicles in advance, which increased the cost of transportation. The total delivery cost in scenario 4 was increased by 25% in average than that in scenario 1. As the order volume increased, the increased percentage of total cost tended to decrease in fluctuation. After the order volume reached 5,100, the total delivery cost in scenario 4 showed a downward trend and reached the minimum. In this situation, the delivery frequency should be increased accordingly. Within the orders volume ruled in this paper, the delivery frequency and the cumulative delivery costs are positively correlated. Compared to scenario 1, scenario 4 had an average increase of 315 yuan per day in total sorting costs. This is because that under the current total delivery volume, the order quantity after splitting in the scenario 4 was higher. Figures 12 and 13  show that the transportation cost in scenario 2 was increased by an average of 14% than that in scenario 1. The increased percentage behaved a negative growth trend followed by a positive growth trend as the order increased. The transportation cost in scenario 4 was increased by an average of 45% over scenario 1. In addition, scenario 4 has the lowest transportation cost among the three scenarios as it has a higher order volume. The transportation cost in scenario 1 showed a rising trend with the increase of orders. When the order volume was 2406, 2530 and 4700, the transportation cost rose rapidly. Due to the splitting of order, the transportation costs of scenarios 2 and 4 remained unchanged when the order volume was small. When the volume of orders increased to a certain value, the transportation cost of scenario 1 became the highest among the three scenarios. As a result, the overall efficiency of the delivery staff was reduced. At the same time, the increase in the delivery frequency also improved the rental cost of the delivery station. The difference between the vehicle utilization efficiency in scenarios 2 and 3 increased with order volume until it reached 3600 units. Figures 19, 20, 21  and 22show that the delivery frequency had different effects on terminal delivery operations. In some cases, the area utilization efficiencies in Scenario 1 and Scenario 3 were the same. This is because the difference of orders volume in the largest batch between two scenarios was small. In addition, the delivery area utilization efficiency was the same at some points between scenario 4 and scenario 2. This is because the order volume was so small in scenario 4 that in the unit batch delivery personnel was working inefficiently. As a result, the company had to increase the number of delivery personnel and the area of shipments. As shown in  Figures 21 and 22 , the overall utilization efficiency of delivery station A was approximately 5% higher than that of delivery station B. In addition, scenario 4 can effectively ease the utilization of area and personnel in the delivery site. The results of this study indicate that under the impact of order volume there is no fixed relationship between delivery frequency with delivery cost and resource operational efficiency. In addition, the average split of order quantity can effectively reduce the tension of delivery resources. When transportation vehicles are in tight supply, the delivery frequency should be increased when the order volume is 2530.
paper_479	Inability to model an individual's handwriting over time has made estimating a full likelihood ratio for comparative handwriting analysis impossible thereby employing nuisance parameters and subjectivity in computation of L R that is not full. With the help of handwriting model for individual writers, little or no assumptions and no nuisance parameters were employed in achieving full likelihood ratio for comparative handwriting analysis in forensic science. From the research carried out, it can be concluded that modeling an individual's handwriting is a crucial factor in achieving a full likelihood ratio, little/or no inconclusiveness in result reporting and a less degree of disagreements for handwriting identification in a forensic environment. Let us presume that there are two opposing ideas. In ' competing, ' it can be said that they are mutually exclusive, but may not necessarily include all possible alternatives. One proposition is 'The suspect is the author of the document in question'. A counter argument may be' The defendant is not the author of the document in question'. For the handwriting evidence, previous literatures reported many factors and the inability of the forensic examiner to model the writing profile of a writer because it is often believed that an individual's writing profile is a latent characteristic that cannot be observed directly, and is not easily modeled because an individual's writing profile cannot have static characteristics and may change over time  [9] [10] [11]  which has resulted to adopt ad-hoc methods to compute likelihood  [9, 12, 13]  when full likelihood ratio is not achievable. The novelty of our proposed implementation relies on natural handwriting samples over a period of six months from known individuals to form the database to model the writing profile for each writer. Strength of proof serves an integral part of this problem. The problem of estimating a L R for handwriting has proven to be a non trival task due to the inability to model the writing pattern of an individual and due to the absence of a large database of handwriting and other factors. [15] [16] [17] [18]  estimated a L R for handwriting using Bayesian approach but in the presence of nuisance parameter, and their works had no underlying principle and model in which this L R was estimated. [17]  based their approach on distribution of measurements from comparing items but their approach was similar to that of marginal L R when full L R was not an option due to the presence of nuisance parameter. [19]  described a statistical model for the writer verification task to determine if two documents were written by the same writer, their model followed the scenario proposed by  [17]  thus suffers from the same weakness  [20]  presented one conceivable way to deal with gauge a probability proportion in near handwriting analysis was delineated. Figure 1  shows the graphical representation for the handwriting modeling. Back Propagation Neural Network (BPNN) served the sake of supervised learning of the system. We target is set for each character in the handwriting. The weighted number of the inputs is the activation of the neuron. Transfer function can add non-linearity to the network. weight-recognition factor with weight ! " The weight from input points i and two hidden unit j is ! " After randomly selecting the weights of the network, the backpropagation algorithm is used to measure the necessary corrections. Due to the complexity of modeling the handwriting of a writer and the absence of industrial size databases from which different handwriting can be described  [17, 9]  estimated a marginal L R when the full L R was not possible also in the presence of some parameters considered to be nuisance. The developed writing model for each writer is one of the criteria to eliminate the presence of nuisance parameters when estimating a full L R . Base on decision law i.e. If L R value greater than 1 H p is true If L R value less than 1 H p is false. An inconclusive state is declared if and only if L R value = 1 With the BPNN algorithm to model handwriting pattern for each writer and L R estimation described in  [10]  this paper was able to deal with the problem of inconclusiveness as there was no inconclusive results reported in the investigation, determine who the writer of a questioned document is as well as eliminate the inclusion of parameters considered nuisance in investigation because each writer's handwriting profile could be ascertained due to the BPNN training of the Handwriting categories. A more elaborate and collated result table is presented in  Table  2 . Thus there must be agreement in sign otherwise there is disagreement. Full L R void of nuisance parameters is needed for most forensic investigators. This research shows that modeling handwriting pattern for a writer is an important factor in achieving a full L R which will eliminate the use of nuisance parameters in the computation process, lower the disagreement rate with respect to the Hypothesis in support of the prosecutor and also produce no inconclusive result after estimation of L R .
paper_492	Eating behavior in animals in the early stages of evolution is also processing time series data, and it is possible to predict behavior although be limited short term by learning the contextual structure inherent in time series data. This function is the behavior of so-called short-term memory. By the workings of long-term memory, lot of information are exchanged between fellows, and lot of time series data are conserved by characters in human society. Advance of the amount and complexity of the connection on nervous system must have been enabled to get more advanced processing. In Chapter 2, the circuits by combining the basic functions of neural circuits for serial parallel conversion and inverse of the conversion are shown. Although for learning process Hebb rules is used on the circuits, operations such as back propagation and Markov process are not used. The number of logical elements used may not differ much from the sum of nerve cells in insects or zooplankton. The same is true for the recognition process. Many East Asian recognize kanji as a combination of parts. It is desirable that the behavior of any neural network system can be expressed by combinations of the simple action parts of animals in the early stages of evolution. The divided subsequence is defined the basic subsequence. Example shown in  Figure 1  shows that the time series data (1) arranged by randomly selecting the elements is divided into five basic subsequences (2). The dividing is done by the following procedure. (1) The first element is the beginning of the first subsequence. In this example a6 is the concerned element. Figure 2  shows the affinity with the neural circuit. When the first data c 0 is received activate the bottom. For the next data c 1 additionally activates the elements which has been activated by the first data c 0 . Because the elements activated by c 1 is randomly connected to input, not all elements activated by c 0 are additionally activated. Four portions are activated in the  Figure 2 . Other portion will return to the initial state because no activation factors (may be activated by another time series data). The output of the element holding the activity when receiving the last c 4 of the time series data is the recognition result of the time series data c 0 c 1 c 2 c 3 c 4 . It may be seen as the output of the connected AND logic element. The number and their position of the activated elements is the conversion output corresponding to the serial input  [1] . In the neural network shown in  Figure 2 , the elements are activated one after another by the time series data (in this case, the basic subsequence) from below, and the result is output upward. Its output is the result of the serial parallel conversion, it is the result of AND logic of the output of the activated elements. The elements involved in the conversion are still activated at the time of output. This operation is a generation of (learned) time series data. After the first data reception, the connected elements are activated as described above. Both state transition diagram is shown in the existing  Figure 3 . This neural network is called a basic unit. Animal's behavior is considered a time series data consisted of the data that is couple of behavior data and received data. It can be said as "stance to the event" that most animals have  [2] . And in the nervous system, the eating behavior shown above will be nothing more than a brief occurrence that appears in a constantly continuing life. The reason why drawn a hierarchy further in the upper part of the  Figure 4 , is to suggest existence of the nervous system that processes intellectual judgment. The movement will be mentioned in the next chapter. Neuroscientist Damasio calls "image" the internal representation built in the nervous system by stimulation inside and outside the body. Animals in the early stages of evolution will spend most of their living time obtaining food and avoiding danger. However, recognition is limited to the area of the vicinity that the sensory organ catches. Before long, animals evolve to be able to act in groups. They exchange information with their peers through squeals and gestures to enhance their ability to survive as a species. The ability to imitate fellow's action is indispensable. In addition, it spreads to the story beyond space and time. And you'll think of the shape of the cake and the action of lifting the chocolate plate and mouthing it. If there is a problem in the episodic memory, it causes difficulties in social activities. Two types of time series data can be considered in the above situation. The other is generated by the nervous system that is called " image" by Damasio, is time series data based on the shape of cake produced from family conversations the day before. Like talking about your childhood while eating cake. However, if the nervous system of the episodic memory is activated by remembering the A shop and the chocolate plate while looking at the cake, the difference between reality and the expectation might become a problem. In order to avoid confusion, the episode must be corrected by reality. Stimulus from the sensory organs from the bottom becomes time series data and is transmitted to the upper part. The connection between the two sides is enhanced (Hebb rule) as indicated by a bold line, and visual information is copied to the top to reinforce the episodic memory. As a result, the shape of the chocolate in the episodic memory is identified as the thing seen in front. In neural networks, it is a coupling element with the property that the binding is enhanced as the activity of both ends increases. According to the Hebb law, if there are two nervous system activated, the binding between the elements in the two categories will be enhanced. Thus, the behavior of both categories can be migrated to each other. It has been vaguely thought that the nervous system responsible for long-term memory may be in a different place from the nervous system responsible for short-term memory. It is a short-term memory to be involved in the recognition and manipulation of the thing in the vicinity of the animal itself. And the time series data brought by other brain with highly hierarchically connected structure activities is concerning long-term memory. In the study of memory and language, it is a meaningful theme that covers the homogeneity of behavior between parts of the nervous system  [6, 7] .
paper_507	The main aim of present study is to integrate the result of our study with spatial data, soil parameters, land inventory and used the output as a user friendly application using GIS which could predict the future susceptibility of region to landslide and% contribution of each factor for the same. Finally, an overlay analysis will be carried out by evaluating the layers obtained according to their accepted coefficient in final model.. Efficiency of the application will be calculated by the help of previously acquired data of the study area at different places and then the reliability of the application will be judged. As far as Indian scenario is concerned approximately 0.49 million km  2  or 15% of land area of country is vulnerable to landslide hazard and 80% is spread over Himalayas, Nilgiris, Ranchi plateau and eastern ghats  (GSI 2006)    [3] . Uttarakhand is an appropriate choice for the study since the newly developed state has been fighting with the catastrophe and is making front paper headlines for landslide at Vishnuprayag, Baldora, Lambagharchatti, Jharkula, phatabyung, and Amiya landslides  [4] . The traditional practice of Landslide prevention is enabling people with Landslide Hazard Zonation Maps. al, 1999 and varnes 1984)    [7, 8, 9] . With the help of ANN model, we generated weightage for each factor and using this the hazard zonation map is produced  [10, 11] . The result obtained i.e. Preparation of a landslide hazard zonation map that would divide Uttarkashi into different zones depending upon the factors. Uttarkashi falls under the physiographic division (s) Rohilkhand plains, Nepal Himalayas, Ganga, Yamuna daob, Siwalik range, Kumaun Himalaya, Dhaoladhar range. In the present paper landslide hazard zonation map has been prepared for the Rishikesh-Uttarkashi-Gaumukh-Gangotri. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area (Mohammad Onagh, 2012). In the present study we selected 107 points and all the six factors namely soil depth, soil type, rock type, land cover, slope and elevation and a excel database is created. The dataset is categorized into 60% training and 40% validation. The data is categorized into training, testing and validation, and all the six factors namely soil depth, soil type, rock and a excel database is created. The number of epochs was set to 3,000. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area  [13] . The dataset is categorized into 60% training and 40% validation. The most contributing factor is slope carrying 93% and the least one is soil depth. The study has to led the determination of factors on the basis of past studies and determination of weightage for the chosen six factors namely soil depth, soil texture, rock type, height, slope and land cover. The result of present study with spatial data, soil parameters, land inventory and presented by a landslide hazard zonation map and a user friendly application using GIS that could predict the future susceptibility of region to landslide and percentage contribution of each factor for the same. The reliability of ANN is high over other methods.
paper_1	This helps in solving the problem of losing vital information that is generated from the social media. Clustering is a descriptive task of data mining. It can be applied in various fields for instance taxonomy of living things, information retrieval from a document, identification of areas of similar land use in an earth observation database, discovering distinct groups by marketers in their customer bases for development of targeted marketing programs and identifying groups of houses according to their house type, value, and geographical location  [2] . A number of techniques can be used to do clustering. The system design methodology used was incremental prototyping. The fourth step was testing the system. The prototype was then subjected to testing using the test data. Finally, the model was used to classify a new user into a group. The illustration of the proposed prototype is given below. This is summarized in the chart below. The Naïve Bayes Classifier was also tested to evaluate its accuracy, precision and recall  [9] . This classifier was doing the classification using the unigrams. Precision and recall were however average. The study has proved that it can actually be used constructively in learning in various institutions.
paper_2	The developed system uses a microcontroller of PIC89C51 microcontroller duly interfaced with sensors. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. As a result of this a lot of time is wasted in the process. The proposed system would use a microcontroller of PIC family duly interfaced with sensors, to change the junction timing automatically to accommodate movement of vehicles, thereby, avoiding unnecessary waiting time at the junction. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Junction timings allotted are fixed. An intelligent traffic lights control system using a Fuzzy Logic approach was developed by  [5] . The algorithm implementation was done using Mathworks, MATLAB software, and the results were simulated using a Simulink Tool to create traffic scenarios and comparisons between simple time-based algorithms and the developed system. Data was analyzed and presented using descriptive statistics; tables and graphs by using excel 2003. The top down design approach was adopted here. Below are the ratings of the transformer 0.7 volts for silicon diodes). For convenience, a capacitor of 1000uF is used. Figure 4  shows the circuit diagram of the system. The time delay in the traffic signal is set based on the density of vehicles on the roads. The IR sensors are used to sense the number of vehicles on the road. The PIC Microcontroller was programmed using Embedded C language. The codes are as shown in the Appendix. Simulation was done via Proteus software. In order to address this problem, an advanced traffic congestion control system is required. The sensors help in keeping count of vehicles entering roads and the microcontroller subsequently allots time delay thereby giving accurate priority to each road.
paper_3	In the beginning, a related work on the architecture trends of Web-based AEHSs is presented. (d) Meta-adaptive Navigation Support: It selects or suggests the most appropriate adaptive navigation technique that suits the given learner best relatively to the given context, either by observing and evaluating the success of each technique in different contexts and the resulting learning from these observations, or by assisting the learner in selecting the navigation technique that best suits to him or her. (h) Adaptive Collaboration Support; Adaptive Group Formation and/or Peer Help: These techniques support the collaboration process either just like the interactive problem solving support systems assist an individual learner in solving a problem, or they use knowledge about possible collaborating peers in order to form a matching group relatively to the kind of the collaborative task. The teaching strategies are based on  [9]  learning cycle and learning style model. The general purpose of the AEHS MATHEMA is to support learners through an interactive and constructivist educational environment in order to construct their knowledge in Physics and Technology, individually and/or collaboratively, and to overcome their misconceptions and/or learning difficulties AEHS MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and meta-adaptive navigation, interactive problem solving, and adaptive group formation. More specifically it supports: (1) adaptive presentation according to learner's learning style; (2) four adaptive navigation techniques which are direct guidance (uses a "next" button), link hiding (hide nonrelevant links), link annotation (use colors to indicate relevance), and link sorting (like in search engines); (3) a meta-adaptive navigation technique in order for the learner selects the more appropriate navigation technique that best suits him or her; (4) an interactive problem-solving method through personalized and collaborative problem-solving activities that exploits the teaching approaches of experimentation through simulations, guided inquiry and discovery, investigations and peer collaboration; and (5) a method of adaptive group formation for peer matching. The key idea is the decentralization of their functions. The rest of the paper is organized as follows: In the section 2 related works for various models of architecture of AEHSs is presented. In the section 3 the architecture of the AEHS MATHEMA is presented in detail. The AHAM architecture model proposed by  [6]  is shown in the  Figure 1 . The AEHSs fit so nicely in this model. When designing a course it is important to first list the concepts. This leads to a structure of prerequisite relationships. The prerequisites are most common in educational material. The rules together form the adaptation model in AHAM. AEHSs applications need to maintain a permanent user model. Educational content can be either SCO or Asset. In the  Figure 3  the architecture of the WELSA  [14]  system is presented. [12] proposed a framework for AEHS (  Fig. 4 ) based on JSP, Java servlets, and Javabeans. ULUL-ILM  [1]  is an AEHS that focuses on student's learning styles. The result of that analysis is called domain model. The content loaded to the MySQL database is accessed via JDBC API. These servlets are complete programs that are capable of creating JSPs. Servlets dynamically create JSP pages according to student requirements. JavaBeans technology comes into play. Figure 5  shows the architecture of the AEHS MATHEMA. A component of an adaptive educational system is the representation of knowledge. The domain knowledge of the AEHS MATHEMA is the basis of the system's adaptation. The model that supports the AEHS MATHEMA is the overlay model. The characteristics that the system maintains in learner model for each learner are: name, gender, username, password, learning style, level of knowledge for each cognitive goal (general performance), level of knowledge for each basic meaning of the cognitive goal (quantitative and qualitative characterization), concepts that have been successfully evaluated for each cognitive goal, current page, current teaching method, current navigation technique, navigation history, meta-adaptive navigation status, initial Web experience and knowledge level (pre-existing knowledge) in the current cognitive goal, ways of presenting feedback messages, preference for assistance by Advisor in terms of its navigation, course sequence, and phases of the guided dialogues in interactive problem solving process in which each learner participates. The learner during the study selects the cognitive goal he or she wishes to study, following his or her personal course in the educational content, exploits the system's assistance, navigating and studying, is informed about the information that the system maintains, who can change them by intervening and directing the system's adaptation in this way, in terms of curriculum sequencing, adaptive presentation and navigation and adaptive group formation, selects his or her learning style, selects the navigation technique, activates and deactivates the curriculum sequencing, selects the way that the feedback messages are presented, changes his or her knowledge level for each basic concept of the cognitive goal, and activates-deactivates the Advisor. This is an innovation on the architecture of AEHSs. It is the second level meta-adaptation engine that is responsible for monitoring the cognitive improvement of the learner in the cognitive goal that he or she studies, after n successful evaluations of his or her knowledge in basic concepts, showing him or her advantages and disadvantages of the four navigation techniques that it supports. In AEHS MATHEMA, meta-adaptive navigation works as follows: The first time that the learner enters the system, he or she is asked to state his or her Web experience and level of knowledge in the cognitive goal he chose to study (preexisting knowledge). Following the learner's statement, the system suggests the most appropriate navigation technique to him or her, taking into account the level of his or her Web experience and his or her level of knowledge in the subject he or she chose to study as follows: (1) Direct guidance: Little or no Web experience and little or no knowledge of the cognitive goal. knowledge of the cognitive goal. The meta-adaptation engine proposes to the learner to change the navigation technique if he or she wishes after n successful assessments of the basic concepts of a cognitive goal he or she studied, showing him or her the pros and cons and additional information on each of four navigation techniques, and the learner decides whether to change it or not. Figure 7  shows a snapshot of the meta-adaptation result. A snapshot of a meta-adaptation result. The pages displayed to the learner are dynamically generated. Figure 10  shows a snapshot of adaptive group formation in the MATHEMA. Assessment of the pedagogical and technological aspects of the AEHS MATHEMA has been carried out. Regarding adaptive and intelligent techniques of the AEHS MATHEMA mentioned above, the students having evaluated them consider that the adaptive presentation of different teaching strategies is quite useful (93.0 percent) and fairly easy to use (86.0 percent), the different ways of navigating are quite useful (100.0 percent) and handy (90.7 percent), the meta-adaptive navigation is quite useful (88.4 percent) and fairly easy to use (86.0 percent), the adaptive group formation of collaborative teams to assist them in collaborative problem solving is useful (83.7 percent), but less user-friendly (58.1 percent). MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and metaadaptive navigation, interactive problem solving, and adaptive group formation. The key idea is the decentralization of their functions.
paper_21	This scheme consists in evaluating the minimum distance of the reduced dimension sub code fixed by a Self Invertible Stabilizer Multiplier Permutation by Zimmermann algorithm. An efficient solution of this problem is the use of error correcting codes. For these codes, only a lower bound is known and the minimum distance is known only for some lengths and special cases  [2-3-4-5-6-7-8] . The section 3 presents the proposed scheme ZSISMP. The section 4 presents the main results. For these codes, only a lower bound is known but the true value is still unknown for large codes. This section summarizes the most important ones. From these stabilizers, we take only a Self Invertible stabilizer if it exist and by using a mathematical tool, we find the sub code fixed by this involution and then we evaluate the minimum distance by using the famous Zimmermann algorithm. For finding the minimum distance of BCH codes. In order to validate the proposed method, it is applied on all BCH codes of known minimum distance presented in table 1. the obtained results show that the minimum weight found by the proposed method is equal to the true value of the minimum distance of all BCH codes of length up to 255. The table 2 summarizes the obtained results. The table 3 presents a comparison between the proposed scheme and MIM-RSC method  [24] .
paper_31	This paper takes chronic diseases as the research object, and proposes a design of intelligent medical system for chronic diseases based on semantic matching by the adaptation of ZigBee technology in the front-end data acquisition. Through the ZigBee wireless sensor network, this system sends the physiological parameters collected by various medical sensors to the intelligent medical system, and innovatively proposes semantic matching algorithm to solve the queuing problem of data transmission, to ensure the accuracy of data transmission. With the means of the Internet, with the carrier of the mobile terminal, intelligent medical treatment founded on the units of family is a new trend. Meanwhile, some also have applied various wireless sensor technology to medical monitoring system. Chronic diseases can easily cause damage to patients' vital organs such as the brain, heart, and kidneys  [3]  to seriously affects working ability and quality of life, and its high medical expenses increases the burden of economy on society and families. Therefore, the word "Telemedicine" arose. This is the second stage of the development of WITMED. Haji Bagheri Fard present methods based on deep learning algorithms to achieve high classification accuracy  [8] . Zigbee has been implemented on the Health Care Profile. (2) The problem of power consumption. (3) The problems of unitary monitoring data. Chronic diseases patients are always required to be monitored for multiple vital signs simultaneously. (4) The problem of data processing. Due to the large amount of medical resources occupied by chronic patients, the phenomenon of difficulty and high cost of getting medical service is more prominent. Attracted by the broad prospect of smart medicine, BAT and other Internet enterprises have participated in the smart medical industry. According to the Research Report on market development and investment trend of China's smart medical industry from 2017 to 2021, the annual compound growth rate of smart medical market reach 29.6% from 2015 to 2020, and the market scale will exceed 50 billion yuan in the future. Based on ZigBee technology, a design of intelligent medical system based on semantic matching is proposed. Then the coordinator transmits the data collected to the intelligent medical system. The system architecture diagram is as follow: The general algorithm of this part is as follows: Define priority function: f x, y ax by ε, with a basic definition as follows: x: semantic matching degree, 0≤x≤1 y: a request arriving at the serial position, 0<y≤1 a is defined as the weight of semantic matching degree, 0<a<1 b is defined as the weight of serial position of Request arrives, 0<b<1, a+b=1 ε is defined as the disturbance value, 0≤ε≤0.1, with the default as 0. This research proposes an improved space vector model algorithm to ensure whether the patient has a related disease by the similarity between the physical signs and the disease on the basis of calculation of the sign value, continuous sign value and the disease characteristic value. Definition 2: let {X1, Y1}, {X2, Y2} is concept form of {X, Y}, {X2, Y2b {X1, Y1}. Patients' health information involves their privacy. The calculation is shown as  Figure 7 : In this study, a design scheme of intelligent medical system based on semantic matching is proposed, with the front-end data acquisition which adopts ZigBee technology, and a semantic matching algorithm is innovatively proposed to solve the queuing problem of data transmission to ensure the accuracy of data transmission.
paper_38	The study aimed to determine if any of the entry requirements such as Ordinary Level (OL) results, Unified Tertiary Matriculation Examination (UTME) scores or Post-UTME (PUTME) scores could predict an outstanding academic performance of first-year undergraduate students admitted into the Faculty of Science in the Kaduna State University, Kaduna. The study adopted the descriptive research design. A purposive sample of nine hundred and forty-three (943) first-year students constituted the population for the study were drawn from Computer Science, Mathematics and Physics undergraduate degree programmes from the Faculty of Science of the university who were admitted from the 2010/2011 to 2014/2015 academic sessions. Education is an essential issue regarding the development of any country in the world. It is a progressive development of knowledge and skills of examinees through stages of teaching and learning at various levels  [1] . Students admitted into any of the Faculty of Science undergraduate degree programmes in the Kaduna State University must have been subjected to serious academic scrutiny. Also, each student is expected to have obtained at least the minimum score required in the Unified Tertiary Matriculation Examination (UTME), conducted every year by the Joint Admissions Matriculation Board (JAMB) since 1979. In order to accomplish and improve the value of education, it is necessary to find other ways to enhance the academic performance of students. In this study, the academic performance is categorised by the entire performance each year, which culminates in a Cumulative Grade Point Average (CGPA). Formula 1 is used for calculating the CGPA. Students' academic records show that after admissions, some students perform poorly even after going through a series of screening of their OL results, and writing of UTME and PUTME examinations before offering them admissions. This study aims to investigate the relationship between the entry requirements into any of the Faculty of Science undergraduate full-time degree programmes in the Kaduna State University and the students' academic performance at the end of the first year of study. The objectives of this study are to: i. Determine if any of the following entry qualification used for admission, OL results only, UTME scores only or Post-UTME scores (average of OL results and UTME scores) best predict the academic performance of students in the 100 level CGPA examinations; ii. Investigate the relationship between the students' performance of their entry qualification and the academic performance in the 100 level CGPA examinations. The following research questions directed the study: 1. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? What is the relationship between OL results, UTME scores and Post-UTME scores of students and their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? There are a lot of definitions of students' performance based on previous works of literature. Several studies have criticised the use of UTME and PUTME as an imperfect instrument for predicting academic performance of students. Reference  [11] , in his study, monitored the performance of science education students admitted through Post UME screening in 2005/2006 academic session. A sample of 214 students records was used for data collection. The authors in  [8]  comparatively analysed the academic performance of graduates admitted through UTME and preliminary programmes (Certificate, Basic Studies and School of Science Laboratory Technology [SSLT]) in the University of Port Harcourt. The study by  [13]  found a significant relationship between students' scores in three examinations, namely: UTME, PUTME and 100-Level Psychology course, Faculty of Agriculture, Federal University of Agriculture, Makurdi, and thus concluded that the UTME has predictive validity for performance in the university. In contrast to the studies from the earlier mentioned authors,  [15]  investigated the relationship between 276 students' performance in the entrance examination and their performance in Mathematics in two selected Colleges of Education (CoE) in Osun and Oyo states each. The results indicated no significant relationship between students' performance in the entrance examination and their Mathematics performance at the CoE thereby ascertaining that UTME was the best predictor. The author concluded that the entry qualification or the entrance examination performance could not individually predict Mathematics performance at the CoE. This study aimed to investigate which of the University's entry requirements used for the admission process best predicts the academic performance of students in the 100 level CGPA examinations. The Faculty of Science consists of nine undergraduate B. Sc. The population of the study was limited to consist of all students admitted into three Faculty of Science undergraduate degree programmes of Kaduna State University for five academic sessions from 2010/2011 to 2014/2015 using the OL, UTME and CGPA results. This limitation is due to the non-availability of CGPA results of the other undergraduate degree programmes at the time required. The sample distribution is as shown in  Table 1 . The instrument used to derive the data for this study were: the JAMB UTME scores from 2010 to 2014 and OL grades in the five relevant subjects which were the pre-admission criteria and first-year CGPA results obtained from the semester examinations from 2010/2011 to 2014/2015 academic sessions. The semester examinations were mostly essay type questions. The data used in the study are the OL grades, and JAMB UTME scores obtained from the University's central database, and the academic standing of first-year CGPA results collected from the various departmental examination officers (DEOs) with the approval from the Dean of the Faculty of Science. However, the OL results obtained from the database was for students admitted during 2010/2011 to 2014/2015 academic sessions. The coding for the CGPA is also shown in  Table 4 . It was used in this research study. The data were regrouped and analysed by academic session and programme of study. PPMC is used to determine the degree of relationship between two sets of variables and compute the strength of association between the variables  [19] . The results of the data analysis are presented in tables according to the research questions that guided the study in this section. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? The Pearson Correlation analysis was carried out to find out if there exists a strong positive correlation between OL and CGPA, UTME and CGPA and PUTME and CGPA. For the Computer Science programme, as shown in  Table 5 , the correlation coefficient indicated a low negative correlation in UTME-CGPA (-0.092) and PUTME-CGPA (-0.001) and a low positive correlation for OL-CGPA (0.072). How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? Likelihood Ratio Tests is a statistical test of the goodness-offit between two models. For the Computer Science programme, the slopes (B) of OL in all the CGPA categories are positive. What is the relationship between OL results, UTME scores and Post-UTME scores of students and their firstyear CGPA across the academic session, 2010/2011 to 2014/2015? PPMC was used to analyse the data for this research question. In the 2010/2011 academic session, the correlation coefficient indicated a low positive correlation in OL-CGPA (0.198), UTME-CGPA (0.189), and PUTME-CGPA (0.232). In 2011/2012, OL-CGPA (-0.114) and PUTME-CGPA (-0.043) display low negative correlation while UTME-CGPA (0.114) has low positive correlation. In 2013/2014, there exist a low positive association for OL-CGPA (0.061), UTME-CGPA (0.056), and PUTME-CGPA (0.038). The OL-CGPA (-0.071), UTME-CGPA (-0.090), and PUTME-CGPA (-0.040) in the 2014/2015 session signifies a low negative correlation. How well do OL results, UTME scores and Post-UTME Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria scores of students predict their first-year CGPA across the academic session, 2010/2011 to 2014/2015? Table 9  is the likelihood ratio test results for all academic sessions ranging from 2010/2011 to 2014/2015. Table 10 , on the other hand, shows the results of the Parameter estimates for the MLR for each academic session. For the students admitted during the 2010/2011 academic session, the slopes (B) of OL in all the CGPA categories are positive. Conclusively, the slope (B) of PUTME in the CGPA category of '3 rd Class' is positive and the rest negative. The primary purpose of this study is to investigate if OL results, UTME and PUTME scores do predict the academic performance among first-year undergraduate students in the Faculty of Science. Also, there is a need to do a further study by including some more variables, such as age and senior secondary school mock examinations results, as criteria to significantly predict the academic performance of students successfully.
paper_57	This paper attempts to overcome stagnation problem of Ant Colony Optimization (ACO) algorithms. [2]  as Ant system (AS) algorithm. The aim is to find the shortest path. where ρ ∈ (0,1) is the pheromone persistence (1 -ρ is evaporation rate) and m is the number of ants. how long the acquired information will be available. Genetic algorithms (GA) were proposed by  Holland (1975) . The original GA is known as simple genetic algorithm (SGA). The population size is given by the number of genomes, i.e. Mutation (ii) mimics random gene changes. Crossover operation makes sense only if both child strings differ from their parents. The above described genetic operations have been applied to one of the best performing ACO algorithms of Kumar, Tiwari and Shankar (ACO KTS )  [5] . At first mutation is applied. If mutation is not feasible, another node is chosen. After all mutation operations are performed, crossover operations are applied. Parent strings are random selected. Since genetic operations may produce strings with loops, in ACO framework prior and immediately after each genetic operation a loop removal procedure is performed. Genetic operations do not have to be necessarily feasible. The above described ACO GO algorithm has been tested on a random generated graph. The task is to find the shortest path between start node n s = 1 and end node n e = 80. Variable parameters were set to determine the influence of the genetic operations quantity on algorithm performance and effect of distribution of mutation operations between paths. For each setting 500 trials were performed. The reference value of n [%] was received without any genetic operation and is 5.6 (Table 2, row 1). It can be seen that the higher number of mutation operations, the better the performance is (Tables 2). Results received without crossover operation have higher values along with the Mutation paths axis  (Fig. However, results received with two crossover pairs have higher values along with the Mutations per path axis  (Fig. with no crossover pair certain amount of mutation operations should be spread out among more paths, but with 2 crossover pairs concentration of mutation operation on less paths tends to perform better. It does not have the highest value on edges of the surface where the highest amount mutation operation is. The highest value 13% was received with four mutation paths with three mutation operation per path. Genetic operations where nearly always feasible; ratio accomplished / required mutation operations is 100% and for the crossover operation over 99%. In order to determine the effect of crossover operation crossover rate was let to grow up to 100%  (Table 3) . To prevent interference, no mutation operation was allowed. The results vary  (Fig. As the crossover rate increases, ratio accomplished / required operation decreases  (Fig. This is caused by the search space dimension. It has been proved that genetic operations increase ACO algorithm performance. Even small number of any genetic operation causes positive effect. Limit of crossover is 60% of crossover rate. Mutation operation causes better results than crossover operation. This can be explained by the nature of the mutation operation which creates new paths whilst crossover operation can only combine already existing solutions. The higher amount of mutation operations the higher the performance gain is. Without crossover operation distributed mutation operation has better performance, but with two crossover pairs concentrated mutation operation on less paths tends to perform better. Further research and more experiments are needed to determine the distribution and optimal amount of mutation operation with respect to the number of ants and length of the paths.
paper_78	Each formation has its own composition, structure, distribution range and environment, and all of them should be protected effectively. The results suggest that fuzzy C-means clustering is an useful technique for classification of plant community. Quantitative methods, such as numerical classification and ordination, are significant in ecological analysis of plant communities  [1] . It is applicable in ecology because the description of ecological systems is not always possible in terms of a binary approach  [4] . Ecological communities, such as Glycyrrhiza uralensis communities, have great variations in their species composition respond more or less independently to environmental variables. Because of this, methods based on fuzzy mathematics might be more appropriate in ecological analysis  [6, 7] . Like licorice, most medicinal plants are harvested in the wild and the extended use has led to some medicinal species endangered and tending to be extinct, and the protection of such medicinal species has been urgent  [12, 13] . Classification is the process of dividing plots into clusters (communities) so that plots in the same cluster are as similar as possible, and plots in different clusters are as dissimilar as possible. Fuzzy C-means clustering is a soft classification technique  [5, 7]  in which a plot can belong to more than one cluster, and associated with each plot is a set of membership levels. The Fuzzy C-means clustering attempts to divide a finite collection of n elements (plots) formula_0 into a collection of C fuzzy clusters with respect to some given criterion. Given a finite set of data matrix (species × plots), the method calculates a list of C cluster centres  formula_1 formula_2 Where i = 1, 2, …, N = the number of plots; j = 1, 2, …, C = the number of clusters; U = {U ij } = the matrix of membership values, U ij is the membership of plot i in cluster j; V is a matrix of cluster centers; m is fuzzifier which determines the level of cluster fuzziness (1 ≤ m < ∞). (dA ij ) 2 is the distance index: formula_3 X i is the vector of attribute measurements in plots, usually a vector of ordination scores; V j is the centre of cluster j, if A is a unit matrix, then formula_4 Based on the algorithm above, the procedure of fuzzy C-means clustering is as follows: (1) Selecting a common ordination method, we use Detrended Correspondence Analysis (DCA), and do ordination analysis for species data. For a plot, any value can be given as its membership value in cluster j, but the sum of memberships for a plot must be equal to 1:  2  is calculated using equations (2) and (3). formula_5 formula_7 Based on the new membership values U, we go back to the fourth step and calculated the next turn V j , (dA ij )  2  and U ij iteratively, and until the membership values become approximately stable. However for clustering purpose, a plot should belong to the cluster in which it had the maximum membership value. Based on a general survey of Glycyrrhiza uralensis and its community distribution, five study regions, Chifeng (in inner Mongolia), Hengjinqi (in inner Mongolia), Minqin (in Ganshu), Aletai (in Xinjing) and Kashi (in Xinjing), were selected as sampling sites  [7] . Twenty plots of 5 m × 5 m were established randomly at each site which is over 40 ha in area. The coverage, mean height, individual number for shrub and herb species was measured in each plot. Fuzzy C-means clustering is a nonhierarchical method which provides C clusters and their plot compositions  (Table  1) . It classified 100 plots into 12 clusters, representing 12 Glycyrrhiza uralensis communities, e.g. The name and characteristics in species composition, structure and environment of each community are described below. The community has a total cover of 75%, a shrub layer cover of 20% and an herb layer cover of 65%. spinosa, Lespedeza darurica, Pedicularis resupinata, Potentilla anserine, Saussurea epilobioides, Artemisia sacrorum, Artemisia mongolica, Cynanchum hancockianum, and Vicia amoena. The average cover of Glycyrrhiza uralensis in this community is 35% with a density of 5700 ha -1 . The average cover of Glycyrrhiza uralensis in this community is 40% with a density of 59500 ha -1 . The common species are Salicornia Bigelivii, Carex duriuscula, Stipa sareptana, Artemisias phaerocephala, Alopecurus pratensis, Saposhnikovia divariicata, and Carex pediformis. Its disturbance intensity is heavy. It is distributed from 400 to 700 m in hills with slope 10 -30° in sunny and semi-sunny slope and sandy soil. Its disturbance intensity is heavy. Glycyrrhiza uralensis + Aneurolepidium chinense +Stipa sareptana. The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 65%. It is distributed from 400 to 800 m in hills with slope 20 -35° in sunny and semi-sunny slope and sandy soil. The common species are Artemisia parvula, Scorzonera divaricata, Roegneria kamoji, Potentilla bifurca, Carex duriuscula, and Ranunculus japonicas. Fuzzy C-means clustering successfully classified 100 plots into 12 communities dominated by Glycyrrhiza uralensis. Theoretically, fuzzy C-means clustering is the only one soft clustering method and can deal with much imprecise and incomplete information and has advantages in solving non-linear problems and in studying complex system  [2, 17] . Therefore, fuzzy C-means clustering should describe ecological phenomena and rules better  [5, 6] . This study proved that fuzzy C-means clustering is fully usable in classification analysis of plant communities  [1, 19] . They distribute from temperate meadow grassland, through typical temperate steppe and desert steppe to temperate desert regions from east to west in North China  [15] . These communities are important not only for protection of medicinal plant species but also for conservation of ecosystems and their environments in semi-arid and arid regions in North China  [23, 24] . The classification results by fuzzy C-means clustering are reasonable according to vegetation classification system of China  [25] [26] [27] .
paper_96	Such warning system of obstacle avoidance of embedded electronic guide dog can effectively eliminate the impact of the traffic environment and the subjective factors of the blind, warning in advance for the travelling blind in time, effectively avoiding obstacles such as vehicles, to reduce traffic accidents caused by the their blindness. In fact, this "warning system of embedded electronic guide dog avoiding obstacles" is similar to a car anti-collision avoidance system. The design of obstacle-avoiding early warning system of embedded electronic guide dog is an inheritance and innovation, based on the design of traffic information collecting system, which is generally used in vehicle anti-collision at present. In this way, the design of obstacle-avoiding early warning system of embedded electronic guide dog has the following advantages. What's more, the design of obstacle-avoiding early warning system of embedded electronic guide dog has utilized the core processor, based on the realization of the system function, and development conditions. [3]  The alarm system of guide dog adopts the combination of light and sound. Sound will produce auditory stimulus for the blind. The hardware of the obstacle-avoiding early warning system of embedded electronic guide dog can reference the Figure 1:  Figure 1 : Schematic diagram of hardware system structure  [4] We should optimize the choice of the obstacle-avoiding early warning system technique of embedded electronic guide dog, based on China's national situation. The design of obstacle-avoiding early warning system of embedded electronic guide dog also adopts the generally used embedded processor. The full name of ARM is Advanced RISC Machines. And its way to addressing is much easier and more flexible, and its operating efficiency is high. These are the most remarkable features that uCLinux owned. In addition, the design of obstacle-avoiding early warning system of embedded electronic guide dog also makes a full use of USB. The reference model is shown in  Figure 2 . of USB reference model  [4] The traffic safety problem of the blind is an urgent issue to solve. The warning system of embedded electronic guide dog avoiding obstacles studied in the project can effectively eliminate the influence of traffic environment and blind subjective factors, send the pre-trip alarm for the blind in time, effectively avoid obstacles such vehicles, to reduce traffic accidents caused due to the blindness.
paper_134	Both of these areas work in model identification and data classification issues and will, in effect, be directly used in data mining, and both groups are active in identifying and using neural networks and decision trees  [2] . These methods are very damaging, costly and time-consuming. In recent years, nondestructive methods have been considered in purification. Also, processing of data and results were presented in fourth section  [11] . (2010) conducted a research on olive oil for detecting adulteration using a dielectric spectrum. In this article the experiment was done by olive, sunflower, Canola and corn oil. The AD8302 phase detection chip provides a simple method for measuring the input ratio of the domain and the phase difference of the two signals simultaneously (22). In particular, SVM does not have the problem of local optimization in its training, it builds the categorizer with maximum generalization, determines its structure and topology optimally, and functions nonlinear differentiation easily with low computing using It forms the concept of an inner product in the Hilbert space. Using this technique, the amount of RMSE for the samples were equal to 3.56, 2.45, 3.05 and the amount R 2 was equal to 0.92, 96 and 94, respectively. Figure 5  shows the results of adulterated oil boosted tree regression. The results were predicted and modeled using regression methods. Figure 2  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn Using this technique, the amount of RMSE for the samples were equal to 3.65, 2.46, 3.31 and the amount R 2 was equal to 0.92, 96 and 93, respectively.
paper_139	Traditional web mining algorithms handle with structured document  [7] [8] [9] [10] [11] [12] [13] [14] [15]  than the advanced methodology of mining algorithm can dealthe entire heterogeneous document comprises of images  [9] , graphs, videos  [16] , etc. Initially calculate measure for all the vertices and maintain the set U which contains a minimum and maximum degree for all vertices and isolated measure vertex. After applying the above steps the entire vertex without redundant information available in the set U. Pseudo Code forThe Proposed Algorithm KTMIN-JAK-MAXAM Step1: Compute degree measure for all vertices in the setup. Step2: Pick the minimum degree vertex 'v' in the set-up and include in the set U. Step3:While U doesn't include all vertices Step3A: Include the entire isolated vertex which is adjacent to the vertex 'v' to U. Step3B: Find the adjacent vertex 'u' to 'v' which is not in U and has maximumdegree. Iterate throughall adjacent vertices if possible. Repeat step 3 till all nodes are included in the set U. Step4: Finally network U contains no cyclic information. Now the set U consists of the nodes A, B. 4.2 Search the node B, the unvisited adjacent node is from B as 2 and 7. Now the set U consists of the nodes A, B, 2. Now the set U consists of the nodes A, B, 2, 1, 3.
paper_145	Among them 44.3 percent were overweight and obese. Majority (70.6%) were diabetic patients. Higher prevalence rate was also observed among housewives. Higher prevalence of obesity was noted among females. According to factor weights it was noted that the important factors for variation in the level of obesity were mainly gender variation followed by occupation, education and type of work. The study was based on data collected from both urban and rural people of Bangladesh. The investigated diabetic patients were 544. The analysis was done by using SPSS [version 20.0]. The respondents were classified as underweight  [BMI < 20] , normal [BMI,  20 -25] , overweight [BMI < 30] and obese [BMI 30+]. Factor analysis was done to identify the factors for obesity. The levels of obesity were significantly different among males and females [  Table 2 , P (χ2 ≥ 27.546) = 0.000]. There were 58.9 percent males among the respondents and 47.2 percent of them were normal. The corresponding figure among females were 37.3 percent. However, compared to males more females were obese. The data indicated that 54.4% respondents had income less than 30,000.00 taka. Table 10  showed that 67.6 percent underweight respondents were affected by diabetes. There was no significant association between level obesity and prevalence of diabetes [P(χ2 ≥ 0.851) = 0.837]. Thus, we were in search of identification of most important variables to explain the variation in the levels of obesity in the present data  [14] . This was done by factor analysis. The analysis helps to identify the important variables to explain the variation in the data set  [15, 21] . The variables which were included in the analysis were sufficient to explain the variation as KMO = 0.633, χ2 = 256.371, p-value = 0.000. From the results of the communality it could be concluded that the variable marital status was more important followed by gender and education. From the results of the communality of the variables it could be concluded that 85.6 percent variation of the variable marital status would be explained by the extracted factors  [22] . Similar percentages of variation for the variables gender and religion were 74.2 and 70.0, respectively. These three variables were more important for the variation in the level of obesity. The factor analysis extracted 5 components as these components explained 73.309 percent variation in the observations of obesity. This component explained 25.733 percent variation in the data of obesity. The second component explained 16.161 percent variation of the data. The second component showed that the most important variables to explain the variation in the data of obesity were residence followed by religion and income. The third component showed that the variable income was important for explaining the variability in the obesity. This component explained 10.86 percent variation of the obesity. Around 50 percent respondents were overweight and obese. Similar finding was also noted in another study  [21] . The prevalence of overweight and obesity were significantly associated with age, religion, education, occupation, marital status, income and smoking habit. Around 50.6 percent people of urban area were overweight and obese. They were classified as underweight (BMI <20), normal (BMI, 20 -<25), overweight (BMI, 25 -< 30) and obese (BMI = 30+). The percentages of these four groups of respondents were 7.6, 43.1.34.0 and 15.3, respectively. Most the respondents were in normal and overweight groups. Among the respondents 84.1 percent were of the age 40 years and above and among them 42.8 percent were overweight and obese. The factor analysis showed that sex, occupation, education and type of work were more important to explain the variation in the level of obesity. c) Counseling is needed for the obese children and adolescents. The public health authority can play a decisive role for the above steps.
paper_212	Mathematical modeling of disease has been an indispensable tool in accounting for disease transmission dynamics as well as disease spread. Epidemiological disease models have been used to explain the dynamics of HIV/AIDS in the population from the early 1900s. The models developed however faced considerable challenges ranging from inaccurate representation of natural data for deterministic models, to methods of forecasting such as statistical extrapolation which assumes that current conditions will prevail which is not always the case. This algorithm is able to give a statistically correct of the course of a disease with initial conditions to begin with and propensity functions to update the system. The purpose of this paper is to build on the concept of Gillespie's Algorithm based SIR models by developing a stochastic SIR model to simulate disease evolution in the population setting. A serological test, was then made available. In 1984, several HIV and AIDS cases were documented in Kenya. This accounts for 4% of new pediatric infections worldwide. Among the expectant women there are 13,000 new HIV infections among children. These questions are encompassed in the modeling of the HIV immunology, the HIV dynamics as well as the AIDS dynamics such as the dynamic distribution of the disease in the population and its likely magnitude. The formulated model was used to forecast the number of PLWHA. The group most affected would be the 31-40 years group. Despite the fact that a lot of research has been done on modeling disease trajectory, not much literature is available on the use of Gillespie based SIR models to simulate the trajectory of a disease in the population. Infection-transmission deterministic models are based on the characteristics of population growth, disease occurrence, and spread within a population. In spite having a lot of work done on mathematical modeling, there isn't adequate literature on the modeling the evolution of disease in the population through simulation. This paper will contribute and build on to the existing literature on modeling disease dynamics in the population with the model tested on HIV/AIDS data 1985-2018 to investigate if the simulated values would reflect results that are close to reality. denotes the birth rate and death rate denotes the infection rate denotes the recovery rate t denotes time point Model development The Gillespie algorithm was used to simulate a statistically correct trajectory given initial SIR conditions. The stochastic SIR model. , , This Gillespie's algorithm based stochastic SIR model generates a statistically correct trajectory from the initial vector as , , where i=s, i, r S+I+R=N i denotes the population size of the state at time t denotes a function characterized by two quantities as a state change vector and a propensity function. Assuming that the resulting state is . A propensity function is the probability of one event occurring in the time interval ! Discrete evolution is modelled in discrete time. The data was obtained from NACC for HIV/AIDS cases. 537 tau steps were made in the model. Curves produced are illustrated below. The critical value was 47.4. Mathematical modeling of disease trajectory using Gillespie based algorithms is yet to be explored extensively in literature. In this study, a simulation was carried out on the SIR model to explain the trajectory of the disease by employing a stochastic element using Gillespie's simulation algorithm. The simulated curves were compared to HIV/AIDS data.
paper_214	The paper presents the usage of databases that store business data into a warehouse star model that permits to create queries using SQL language and business intelligence tools. This kind of model allow to the decision maker to create complex reports and graphs based on the columns from the dimension tables and measures from fact tables, that can be the base for creating alternatives and scenarious acording to the economical indicators. This data warehouse star model allow complex analyses such as rollup, drill down, slice and dice through the dimensions and fact tables by using special tools such as online analythical processes and complex queries based on views and snapshots. Objectives of decision-making process aimed at adopting the best solution from many possible alternatives. Among the methods most commonly used are decision analysis and mathematical programming  [1, 2] . It is the only method that can be applied to unstructured problems. These are translated into algorithms which are executed by a computer system. Quantitative mathematical models are embedded in base models, managed by subsystem a management model that requires separate users from the physical aspects of data processing and storage that extract, create, delete and modify models. Besides maintaining traditional information representation formats like charts, maps and diagrams used currently to represent multidimensional data there are used new types of dynamic graphs. After the final resolution of the model, select the best alternative is chosen implementation plan. The assessment in turn depends on the search method. Structural problems use mathematical formulas and analytical method to achieve an optimum solution. In order to improve efficiency the best solution search algorithms are used. Basically successive tests are performed, the search progressing from a solution to another. The database contains no internal data, external data and personal data.Internal data consist from the current activities of the organization and operations of various functional departments image. Personal data is data that relates to the behavioral aspects of decisionmakers in making decisions. In current systems, the company's intranet, are increasingly present data accessible through web browsers and multimedia items such as maps, images, sounds. In most cases there is SGBS transactional relational data system and a management database for multidimensional data warehouses created. The data dictionary is a catalog of all data from the database. The models are used currently in operational and transactional system that aims of the organization. Database management system allows creating new models models using programming languages, update and modify existing models, establish interdependencies between models. Manage in a logical manner a variety of models to consistency of the data model and provides integration of application systems components maker. In building a data warehouse is based on the analysis of data. Extract information in order to obtain information for decision making. Basically there are two steps: designing and populating data. In such a model is highlighted: quantitative data centralized called measures of activity quantitative criteria for centralized aggregation  ,  referred sizes  relational table that stores the measures identified by   the facts dimensions is called table  Tables where aggregation criteria has explicit codes,  called type tables list. The advantages of using databases for Decision Support Systems in businesses involves creating numerous queries by the decision maker and also allow to create many scenarious and variants to choose from. In the business area is crucial to store data into a warehouse such as a star modell and create many queries and reports so the decision maker can see the trends of the economical indicators and helps him to make a decision that can improve those numbers or it can be useful to implement a strategy for a further development  [1, 3] . It is very important to store data into a warehouse model and then to create future strategies that can help a company to expand or to grow the profit in a realistic way based on complex analyses and alternatives.
paper_216	The Black-Scholes model is a well-known model for hedging and pricing derivative securities. The study was carried out using simulated stock prices of 1024 observations. Derivatives includes; Forwards, futures, options and swaps. An option is a type of derivative that gives the option holder the right but not obligation to buy or sell an underlying security at a specified price, at any time on or before a given date in the future as agreed on. The holder of a put option has the right but not the obligation to sell underlying security under some predefined terms of an agreement which includes; strike or exercise price, maturity date and option volume. In the Kenyan market, derivatives are yet to be developed. The Wavelet based pricing model is another nonparametric method alternative used to price derivatives  [4] . The remainder of the paper is structured as follows: Section 2 discusses the existing literature; Section 3 presents an overview of the Black-Scholes model and Wavelet based pricing model. Lastly, section 5 concludes the study. The derivation of the pricing formula is independent on the parametric form of the underlying security's price dynamics and hence the success of this model depends on the accurate capture of dynamics of the underlying security's price process. The Black Scholes model does not correctly price in high volatility markets  [5] . According to the researches as far as pricing derivative securities is concerned, Wavelet based option pricing model is the latest option pricing model in the literature  [6] . The approximation of the implied risk-neutral MGF of the underlying security (asset) returns was focused by developing a non -parametric option pricing model called Wavelet model  [4] . The following are some examples of these advantages; when there is presence of jumps in the underlying process MGF is more tractable, the option's obtained MGF is a continuous function, statistical moments of the asset distributions can be obtained using MGF and the risk neutral MGF can be used to estimate the out of sample options that has maturity dates which are different. The de-noising ability of wavelets was also recognized in  [7] . is the cumulative distribution function. Based on some general assumptions like independent and identically distribution (iid) for asset returns and that the moment generating function is defined well, the wavelet based pricing model can be expressed as follows; formula_1 Where, t is the current time, is the underlying asset price at time t, # $ is the time t price for a European call option written on asset, K is the strike price and T is the future maturity date. ( is the bilateral inverse Laplace transform. Interest rate r is assumed to be constant. The underlying asset dynamics and investor expectation in option prices is captured by the MGF Θ !$ + of the logarithmic returns /0 7 . When T -t = 1, Θ (v) is the risk-neutral MGF for the rate of return per unit of time. This needs to be approximated by wavelets. This function also emulates the probability density function of asset returns. In this study Monte Carlo simulation was used to generate 1024 stock prices. Therefore, one of the most significant input of this study is that the wavelet -based pricing model is an alternative model for pricing options and other derivatives on the same underlying asset with varying times to maturity and different strike values. Moreover, hedging performance may be evaluated with respect to each of the two models.
paper_219	This paper introduces a framework to determine data quality on enterprise networks for net-centric and net-ready initiatives as introduced by the US Department of Defense (DoD). Traditionally quality of data delivered to an enterprise user focuses on network performance, i.e. quality of service (QoS). The QDS attribute brings distinction to the resultant data quality of the network's quality of service. The DR attribute is introduced; it is important in enabling enterprise data consumers to sort, filter and prioritize data. There is also a need to assess the quality of data sharing across the enterprise network. One recent method subjectively assess the quality of data is to measure the user satisfaction referred to as quality of experience (QoE). The overall value of data quality on enterprise networks is decided using a minimax decision model consisting of the three attributes. The presented framework offers decision support tools to enable agencies to allocate limited resources towards improving the performance of their net-centric service offerings to the enterprise network. That same data abundance challenges the network capacity and overloads the capacity of the human operator. All of this is in addition to the historical problems of network management and quality of service. When discussing the theoretical, it is easy to dismiss the challenges of limited resources to implement new policies such as schedule, budget, manpower, etc. Importantly, a model for valuing enterprise data quality is introduced to bridge the gap between measure of technical performance and operational benefit. Thus the term data relevancy (DR) is introduced into the model for valuing data quality in the context of net-centric / net-ready. Section 2 of this paper provides a brief overview of the origins of net-centric and net-ready policies. Section 3 defines a new model for valuing the quality of data by measuring user satisfaction. The attributes in the new model represents data quality of the enterprise system within three contexts: net-centric measures, traditional network quality of service, and cyber-security. Section 4 presents a method for applying the new model attributes to evaluate data quality across the enterprise. Vital to the value of NCW is "the content, quality, and timeliness of information moving between nodes on the [enterprise] network." Making data visible is achieved via deployment of discovery capabilities that access and search data asset catalogs and registries in the enterprise  [6] . The performance of the IT must be quantifiable with threshold and objective values that are traceable to measures of effectiveness (MOEs)  [3] . The identification of the connections must be specific (i.e. The specific data elements and assets exchanged with external networks as part of executing net-centric operational tasks are specified by the exchange information attribute  [3] . Evaluation of the data sharing enterprise requires a holistic view that considers the net-centric attributes of the data simultaneously with the quality of service for the data network. Additionally, the interdependence between cyber security and net-centric principles are indicated in the most recent update to the DoD's instruction for enterprise data sharing  [5] . 1  illustrates the mapping relationship for each enterprise attribute to various DoD objectives of net-centric, net-ready and cyber-security. The subjective measure of overall user-satisfaction of a service or application is referred to as quality of experience (QoE)  [9] . Sampling users is the preferred and direct method for measuring QoE. The question arises how to get the users' opinions. Usually the MOS is formed from arithmetic mean of a population of user opinions. develop or enhance the prediction model of QoE for each attribute. For a sample of size n consider v strata with sample s k in each stratum, i.e. ∑ i=1 to v n i = n and s = U k=1 to v s k . where n ≤ N and s = {1, 2,…, n}. An important part of assessing the end-to-end performance of a data system is consideration of the inherent quality of the originating data prior to its entry into the network; to be referred to as the quality of data at source (QDS). The QDS takes into account the effects of environmental conditions on sensor performance for given design parameters. QDS is a subjective rating from the perspective of the end user  [11] . Traditionally these QoE MOS ratings were undertaken by panels of experts. A standard to address the rating of motion imagery (i.e. Over the last several decades there have been many papers on the topic of QoS. But just for completeness a brief explanation of QoS is given here. The most widely deployed QoS architecture used to deliver a SLA on an enterprise IP/MPLS network is referred to as a differentiated service (Diffserv)  [11] . However there are a number of challenges to QoE discussed in  [19]  and  [20] . To understand cause and effect it is ideal to have the full reference i.e. Models have been developed to correlate the QoS with QoE for multimedia applications  [12] . A number of authors  [12, 15] The measure of data relevance expresses the utility provided by the data towards the consumer's objective(s). A scalar measure is required to support evaluating the degree of relevance on the enterprise performance. Intrinsic data relevance represents the relative value (i.e. Intrinsic relevance reflects three properties of the data: form, spatial, and temporal. Enterprise data systems can offer multiple forms of data to the consumer (e.g. metadata) with those of the producer. (2) A high user satisfaction for the enterprise data quality can only occur when all three attributes have high QoEs. By focusing on all three attributes we can reduce resources required in one attribute based on the overall value of data quality of the enterprise. The rest of this section assumes that there is some ability to adjust the designs of the systems associated with the enterprise attributes. Player one has three action options U, D, N to choose from. The minimax strategy in game theory inspired the decision theory approach of Abraham Wald's minimax model  [25] . f d (s) = u HT . The x-axis of the chart in  Fig. 3  is organized to reflect the connection between QDS and QoS. The final enterprise data quality, as shown in  Fig. 3 , as formula_6 In  Fig. This paper introduced a framework to determine data quality on enterprise networks for DoD net-centric and net-ready initiatives. The framework's data quality model consists of three attributes: quality of data at the source, data relevance, and network QoS. The final value of data quality for the enterprise network was demonstrated as the attribute with the lowest score. The implemented strategy provides a tool for decision makers to prioritize data and manage their resources without comprising any part of the data sharing system.
paper_241	Transformers are the key equipment in electrical power transmission. C transmission, power transformer is one of the most important equipment. The system is capable of sustaining a variety of environmental and operating impacts that resemble normal operating conditions which may include lightning striking the transmission lines, excessive loading, deterioration or breakdown of the equipment insulation resulting to power systems experiencing occasional faults such as phase to phase or phase to ground faults, over current, over voltage or even temperature based faults. differential current) and can be prevented using differential protection and microcontroller based relay protection. The protection techniques employed differential relay mechanism with Arduino. At all times, the Arduino senses the condition of the transformer. The differential protection scheme is concerned with the faults that arise from associated difficulties such as the magnetizing inrush current and saturation. The power transformer primary and secondary currents are measured by the current transformers, which are to be protected by the Arduino and the circuit breaker. The step-down transformer is used to step down the primary or source voltage from 220 to 12V. This 12V is regulated to 5V (using L7805CV voltage regulator) which is the required voltage for the Arduino. The Arduino microcontroller acts as differential current comparator connected to both current sensors ACS712 and as control unit in this design. The LCD display is used to display the differential current values of the transformer and to display the voltage and current level of the system as well. The flow chart above is a brief description of how the differential protection scheme works using arduino microcontroller as the differential relay.
paper_251	In this paper we propose a concept of multi agent based batch scheduling and monitoring in cloud computing environment, where the number of agent are more than or equal to two with reducing the complexity of accessing and responding time. Secondly, Now computing power or connected computing power (with network) has more demanding and significant role in almost all areas of epoch including market analysis, searching, map, accounting, medical, trading, shopping, rescue operations and many more, the list is endless. In the cloud computing. They are able to communicate i.e. can roam in the network, perform the task at remote stations and send back the results to source platform (where they been originated), agents are also clone themselves and one of the core property of the agent is autonomy i.e. Hence agent based solution has been proposed to meet the requirement of the modern cloud computing with pace of dynamic provision to insure shrink in shrink out (elasticity) of the cloud service provider to achieve highest scalability and reliability in extent of the maximum availability of the service to the requisites. SaaS (Software as a service). Platform as a Service (PaaS). Fault tolerance While author  [1]  and  [2]  proposed an agent based solution to solve the above listed QoS parameter that greatly affect the performance of cloud service especially SaaS. Provisions of service and resources in cloud PaaS is an important function that provides analytical statistics about the current view of cloud (running instance for a user or group of users). Resources: Algorithm for Scheduling (influenced from Aneka) developed onto the Cloudbees Proposed Algorithm Step1: Register user login Step2: Sign in the user login Step3: Select multi monitoring agent in cloud environment (for each subc_request with QoS constraints) Step 4: Mapping of resources between cloud client, cloud agent and cloud service providers. Proposed multi agent based solution has influenced from [2] but it's not the realization of cloud federation rather it has to evaluate the scheduling and monitoring of the SaaS (task) application in public cloud's (cloud federation not interoperability). Table 2  shows the response time of the deployed SaaS obtained results and has been compared with existing agent based method (in seconds) . Table 3  shows the user satisfaction of the deployed SaaS obtained results and has been compared with existing malty agent based method  Rather than performing analytical calculation, proposed system has been compared with the author's MABOCCF and NMABOCCF method. This paper presents the enhanced agent based solution to ensure better elasticity and monitoring solution. Proposed agent based solution for guaranteed better elasticity and their efficient monitoring of the resources in the cloud which helps to gather analytical statistics of the resources currently held and will be used such a memory, number of instances and CPU.
paper_272	Since many years up to date now, we are still constructing the vacuum circuit breaker by classical design, but the interacting among the characteristics inside each vacuum interrupter must be scientifically analysis as a high values of the general specification which must be thoroughly understood before the breaker can be applied with safety confidence. Moreover the switching process associated with power electronics offers many modern benefits as a static switching system when joints together by using a snubber circuits for damping chopping current and transient over voltages or other facilities  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] . In addition the inside electroplates some characteristics can be described as followings: Non-sustained disruptive discharge voltages In general, the above characteristics are not involving associated with only vacuum switchgear itself a lone, but when connects the switchgear with power distribution transforms or synchronous motors, there are also many negative interactive processes for instance hysteresis currents, eddy currents and high voltage capacitive inside each transformer (Parasites electrostatics charged) feedback to the interrupter process. The term multiple re-ignitions refers to the series of alternate re-ignitions and high frequency (typical several hundred KHz)interruptions usually resulting in an increasing train of voltage peaks; this overall phenomena is usually defined as voltage escalation  [3, 4] . It is important to appreciate that while current chopping and voltage escalation can occur in a single -phase circuit, virtual current chopping is specifically a 3-phase characteristic: the effects of normal current chopping, multiple re ignition and over voltage escalation in one phase can generate surge over voltages in the second and third phases. However pre-striking transient over voltages is less severe than multiple re-ignitions occurring during load-dropping, first because the contact gap at the first prestrike is very small and second because the contact gap is rapidly decreasing rather than increasing with respect of time. EMTP which is called electromagnetic transient program or others such as SIMULINK/MATLAB are excellent numerical tools that allow for depth studies of switching transients in industrial as well as utility power systems. Since the high frequency characteristics of the power system equipment are depend on stray capacitances that mentioned above and inductances also and they in turn depend on physical dimensions, equipment layout, materials, cable lengths, all these can be represented as follows  [17] ; The figure represents TRV & Re-ignitions computed in the system when switching off power transformer load that the transient voltage approximately (350µs-450 µs) and shows very high frequency sub transients both going in the negative direction of the voltage -Left side. The analysis shows that the effect of damping in an oscillatory circuit can be described in terms of a single parameter, designed ŋ, or its reciprocal ʎ, which is the ratio of the resistance to the surge impedance of the circuit: This fact permits the construction of generalized damping curves for different values of ŋ, from which the solutions to many practical problems can be extracted with about the same effort as one expand in using a table logarithms. Zo = √L/C = 5X104 ohms η. The electrostatic charges -static charged -parasitism are almost depleted in the a mount current begin to decrease in the same time of first peak value which generates chopping currents, moreover the formula of the transition processing rate of du/dt or di/dt have to be considered. Transient over-voltages (450us -480us-500us).
paper_294	Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. This study therefore, looks at the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. The study sampled 377 out of the population of 20,000. The snowball sampling technique was used in selecting the subjects for the study. Mean and standard deviation statistics was used in answering the research questions. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. The Tiv had different management strategies of postharvest losses of cassava (Akom or Alogo) that includes dried cassava chips (kpor), imyorun or imorun (gari) processing, fermented cassava (akpu) and production of base (mtuhem) among others, which were potent managing postharvest losses of cassava. Moreover fresh cassava has a very short postharvest storage lifespan and as such, the Tiv people developed different ways of processing it into durable forms soon after harvest, as well as organizing their local markets where the cassava products would be sold. Despite these postharvest losses management strategies, the different varieties of food stuffs processed from cassava are threatened to extinction due to the flooding of expensive western food stuffs in the markets; and the indigenous local markets where the products could be sold are not accessible due to lack of good access roads. In addition, these indigenous postharvest losses management strategies seem to be facing total extinction due to lack of documentation and storage of audiovisual materials on them. The study intends to investigate the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. Specifically, the study sought to: 1. Identify Tiv management strategies of postharvest losses of cassava 2. storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. What are the Tiv management strategies for postharvest losses of cassava? In what ways does the public library impact on Tiv management strategies for postharvest losses of cassava? What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? Public library is a library that is established and managed with public funds. Fresh cassava has a very short postharvest storage life  (Karuri, Mbugua, Karugia, Wanda & Jagwe, 2001 ). Therefore, the Tiv people develop different ways of processing it into durable forms soon after it is harvested, which forms part of management strategies for postharvest losses. Food losses may be direct or indirect. Essentially, cassava postharvest losses can be defined as both the physical losses such as weight and quality suffered during postharvest handling operations of cassava and the loss of opportunities as a result of inability of producers to access markets or only lower value markets. In pursuance to boost agricultural development in Benue State, postharvest losses must be managed; and to achieve this, the Tiv people developed different management strategies for postharvest losses of cassava. Peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom 4. Incidentally, these methods of cassava processing among the Tiv people practiced as strategies for the management of postharvest losses of cassava were passed down from one generation to another orally. The sample size of 680 out of the population of 20,000 was drawn using the sample size table, (Emaikwu, 2015). The snowball sampling was adopted in selecting the subjects. Section B consisted of 7 variables of forms or Tiv management strategies for postharvest losses of cassava. Section C also consisted of 7 variables of ways public library has impacted on Tiv management strategies for postharvest losses of cassava and Section D consisted of 5 variables of challenges faced by public libraries in acquisition and storage of audiovisual materials on Tiv management strategies of postharvest losses of cassava in Benue state. Data were analyzed using mean and standard deviations. Data was collected using 4 point rating scale instrument. The collected data were analyzed using mean and standard deviation. The collected data were analyzed and presented in  Table 1 . This means that peeling the cassava, slicing it and sun drying known as Kpor Akom (Cassava Chips), peeling the cassava, slicing, sun drying it and grinding into flour known as Mwem ma Kpor (Cassava Flour), peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom, slicing the cassava, sun drying and burning it into ashes known as Mtuhem (Base), peeling the cassava, fermenting, sieving and squeezing it into lumps known as Akpu, peeling the cassava, grating, squeezing (demoisturizing) it, mixing with salt and other ingredients and frying with palm or groundnuts oil known as Kweesi or Akweesa Akom and peeling the cassava, grating, squeezing and sieving the starch and preparing it for food as Kamu Akom are the management strategies for postharvest losses of cassava. In what ways does the public library impact on Tiv management strategies for postharvest losses of cassava? Table 2  shows the mean rating scores of all the items are below 2.50, which implies that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? To answer the research question, data were collected relating to challenges faced by public libraries on acquisition and storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. The collected data were analyzed and presented in  Table 3 . Based on the results in  Table 1 , the findings from the study revealed that processing cassava into Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom, Mtuhem (Base), Akpu, Kweesi or Akweesa Akom and Kamu Akom are the Tiv management strategies for postharvest losses of cassava. Table 2 , findings of the study revealed that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. The study was carried out to investigate the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. The study found out that public library does not impact on Tiv management strategies of postharvest losses of cassava through storage of audiovisual materials on the subject matter. Importantly, if public library has acquired and stored audiovisual materials on Tiv management strategies for postharvest losses of cassava, cassava farmers would have accessed these information materials to improve on various ways of managing postharvest losses of cassava in Benue State of Nigeria.
paper_298	Four research questions guided the study. The instrument used for data collection was questionnaire. Data generated was analyzed using simple percentage and descriptive statistics. Poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. Library catalogue exist in different form. Kumar revealed that 26.99% respondents use the OPAC fortnightly and 22.84% weekly. The study is designed to achieve the following objectives: a. To find out the students awareness about library catalogue as access point to library collection. c. To ascertain the extent of use of the library catalogue by students to access information resources. d. To identify the constraints associated with the use of library catalogue. The study is designed to answer the following research questions: a The survey research method was adopted for the study, and questionnaire was the instruments use for collecting data. The completed and returned questionnaires were analyzed using descriptive statistics, percentage and frequency distribution. This section presents questionnaire collected on the basis of gender of the research respondents. The gender breakdown is presented on  Table 1 . The question on the demographic information of the respondents was on gender presented in  Table 1 . It shows that majority of the respondents 188 (69%) were male, followed by females 84 (31%). This shows that majority of library users in the Federal University of Kashere Library were male. Table 2  revealed that majority of the respondents 262 (96.3%) were aware of the existence of the library catalogue in the university library. Their responses were presented in the  Table 4  below. The study found that majority of the respondents were aware of library catalogue in the library including OPAC  [12] . The results are presented in  Table 8 . Higher number of the respondents indicated poor computer skills hindering navigation when searching OPAC 96 (37%) and inability to locate materials on the shelves, indicated in the catalogue as being available 58 (22%) were the challenges affecting the use of catalogue as an information retrieval tools. This finding corroborates with that of Ogbole & Morayo which revealed that (41.1%) of the respondents rarely use OPAC. The study revealed that majority of the university library users were male. Another source of frustration was reasonable number of the respondents indicated poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools.
paper_305	In this paper, we report on the results of a MSc. Boosting is one of the most important recent developments in classification methodology. Although the model was reported to perform relatively better than models developed using other approaches; as part of the limitations and recommendation, they suggested that such a model is usually a black box and more insight the model parameters was required to make it more effective. It works as follows: 1. Figure 1  illustrates the combination criterion. Train all K decision stumps iii. Reweight the data vii. Repeat until you have T classifiers selected We report on cross validation as under. v. Classify until each partition has been used as the test set. Calculate an average performance. This gives an accuracy of 19/20=95% This gives a precision value of 7/7=1 This gives a recall value of 7/8=0.88  Figure 2 . Test Split ROC graph Indicates a perfect prediction . The model accuracy using this procedure was 86.86% making it a fairly reliable strategy b) The use of separate training and testing data sets returned an accuracy of 95% making it a relatively better strategy c) The use of a ratio to determine the size of the training and testing files from one data set returned an accuracy of 88.64% Therefore, it implies from these findings that the use of separate files for training and testing of the model returns the best model accuracy and hence should be adopted.
paper_310	We are using Arduino software to interface the Bluetooth module with microcontroller. Here we use mainly Arduino UNO (ATMEGA 328P), Bluetooth module (HC-05). The controlling device of the whole system is a Microcontroller. Bluetooth module, DC motors are interfaced to the Microcontroller. h) Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. Cprogram is very easy to implement for programming the Arduino UNO. The smart phone is the transmitter of this circuit. It sends the data to microcontroller through Bluetooth module. Motor driver IC is used to control the dc motors. Here we use programming language 'C' for coding. The program is burnt in the microcontroller using burner software. There are two steps of the programming. The working principle is kept as simple as possible. As seen from the  Figure 6 . A DC power supply is required to run the system. The DC power supple feeds the Microcontroller and the Bluetooth module. Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. This device is connected with the Arduino board (microcontroller section) by the means wirelessly i.e. The instructions are sent by the smart phone. This is indeed a cost-effective and efficient project. The robot can be used for surveillance.
paper_333	Enset is an important food crops produced in Southern parts of the Ethiopia with great role in food security. There are several issues and diseases which try to decline the yield with quality. This study presented a general process model to classify a given Enset leaf image as normal or infected. This diagnosis apply K-means clustering, color distribution, shape measurements, Gabor texture extraction and wavelet transform as key approaches for image processing techniques. Therefore, an efficient practice of IT based solution in this domain will increases productivity and quality of Enset products. It is also the sector that is given an overriding focus in the government's plan for growth of the economy as a whole. Enset crop is related to and resembles the banana plant which is an indigenous plant classified under the monocarpic genus Enset and family Musaceae. There are several issues and diseases which tries to decline the yield with quality. As a matter of facts, visual or manual detections may have defects in terms of accuracy in detection along with lower precision. There is a potential need for technology supported alternative systems to support the manual identification of Enset crop diseases so as to optimize the accuracy for remedial action. If the Enset crop diseases can be minimized then definitely the production will significantly be increased to contribute to the country's economy towards a new boost. Therefore, the implementation of IT based solution in the sector will have a paramount importance to facilitate the activities such as economic, social and ecologic development in the country by increasing efficiency in both quality and quantity of Enset crops production, to sustain dependability of customer preferences and to preserve the ecology  [2] . In our country few researchers found the promising solutions to different plant diseases diagnosis such as maize, rose flower, coffee and others using computer vision and machine learning techniques  [3, 4, 5] . The remaining part of this paper is organized as follows. So far a number of fungal, nematode, viral and bacterial diseases were reported to cause damage at different degrees of intensity that was mainly explored by Quimio research finding since 1992. Figure 2  shows the architecture of the proposed system Those images are collected by using a digital camera and some of them are collected from secondary sources like research centers which collects pictures for visual inspection of Enset disease. Test Result For the experimentation a total of fifteen experimental setups which is color, texture morphological and a combination of each and a multiclass Kernel support vector machine model: Linear Kernel, polynomial Kernel, RBF kernel and Quadratic Kernel was used. From the figure it is shown that the RBF kernel classifier has classified the given enset image in its correct class category by 94.04% and 92.44% respectively. We designed a system which can detect Enset disease automatically and this will help the farmers to detect the diseases in its early stage and to take relevant action. If there is a shortage of experts in the area of those disease identification the system is going to be a replacement for the experts.
paper_389	In this paper, a large number of experiments have been carried out using conditional random fields. Experiments on the corpus show that the introduction of the word position probability feature has improved the accuracy, recall and the value of Fl. Peng F establishes a Chinese character segmentation model based on CRF. Figure 1  is an example of the use of Chinese characters marked word segmentation. Enter the sentence as "This is Wuhan." 3, Mark B can only be followed by the mark I. The probability of the i-character of the position as the prefix: pY> 95% 昨狭抨第竞耽 3 Z The probability of the i-character as the suffix: pZ> 95% 丸役袄侣丸瞰 4 R The probability of the i-character individual word of the position: 85% <= pR <= 95% 吧枚刘磊躺却 5 U Position of the i-character as a prefix of the probability: formula_0 The probability of the i-character as the suffix: 85% <= pV<= 95% 貌型胁帘午岸 7 D The probability of the i-character position of the individual word: pD <= 5% 言辽改信申仪 8 E The probability of the i-character of the position as the prefix: pE <= 5% 业络王场姆杨 9 F The probability of the i-character as the suffix: pF<= 5% 增隐晋香浓伊 In order to deal with the long-distance information, this paper takes the context distance as 2. Table 2 and Table 3  are two new feature templates. The probabilities of the position probabilities in  Table 3  are extracted from the training corpus and the probability of each position is calculated according to the following probability formula: P (individual word) = number of occurrences of the individual idiom of the word / total number of occurrences of the word * 100% (1) P (prefix) = number of occurrences of the word as a prefix / total number of occurrences of the word * 100% (2) P (suffix) = number of occurrences of the word as a suffix / total number of occurrences of the word * 100%  3 After many experiments and comparisons, the probability of more than 85% or probability of less than 5% of the word as a location probability feature, but also on the selected word for some filtering, the elimination of some unnecessary words, such as At the same time, select the probability of greater than or equal to 85% of the word is subdivided into the probability of 85% <p <95% and p> 95% of the two sets. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. Secondly, feature extraction is carried out to generate training corpus and test corpus which can be recognized by CRF model tool. The main corpus used in this paper is the training corpus and test corpus of Changjiang Daily. The corpus is from 1950 to 2005, and the scale is 2564168000 sentences. The coding method is GB code. The formula is as follows: Correct rate P = number of words correctly recognized / total number of system output words * 100% Recall rate IP correctly identify the number of words / test words in the total number of words * 100% F value F=*P*R/(P+R)*100％ The speed of word segmentation is another important index of word segmentation performance. In different applications, the performance requirements of the word segmentation system have different emphases. Moreover, since the word position probability feature is extracted completely from the training corpus, some of the participle criterion information of the corpus is extracted to a certain extent, so that when the test set is tested, Corpus. The experimental results show that conditional random field is an efficient segmentation method. And then use these tools and the corpus carried out a number of experiments. As the Yangtze River Daily corpus is from the newspaper news, for the news corpus in the special format, such as title, poetry, weather forecasting, etc., to our model training has a certain impact, so if the corpus in the handling of these disturbances, Our model should have better performance.
paper_391	Globally, tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The WEKA 3.6.1 tool was used for classification, clustering, association and attribute selection. But, in the class attribute, it is 0.72. The literacy status of the mother has high information gain with the value 0.046. Tetanus toxoid (TT) vaccine is given to women of childbearing age to prevent neonatal tetanus and maternal mortality attributed to tetanus. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The information is rich and massive. In this study, we use different data mining techniques that have been tested on TT dataset. However, not all of the patterns are useful. It has a dimension of 7033 rows and 12 columns. Finally, data have been saved in ". csv" file formats and stored as an ". How does this classification work? The approaches are; (a). And the topmost node in a tree is the root node. The training tuples are described by m attributes. This assumption is called class conditional independence. Training and testing are performed k times. Thus, in this study, we have used five clusters. "How does the k-means ( ) algorithm work?" From the selected 7037 mothers, 3351 of mothers received TT Immunizations. A multilayer perceptron is the best classifier in our data set. Briefly described as follows: Cluster 0-This group have 1534 (27%) instances of which consist of mothers who live in the rural, no access to radio, no access to TV, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 1-This group consists of 1004 (18%) instances of which mothers who live in the rural, no access to radio, access to TV yes, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, head of household male, tt-vaccinated yes. Cluster 2-This group consists of 2063 (37%) instances of which mothers who live in the rural, no access to radio, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated No. Cluster 3-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 4-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, orthodox in religion, Amhara in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education no education, women age 35-49, mother marital status married, hh_head Female, tt-vaccinated yes. csv" file and features were described using WEKA performance.
paper_402	The BPNN and SFS were used interchangeably to identify the relevant features that contributed with the response variable. The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete. ANN emerged as one of soft computing paradigms that have been successfully applied in several engineering fields  [1] . The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete  [14] [15] [16] [17] . As a result, BPNN proved to be more accurate than SMD in the prediction of compressive strength and slump flow of UHPC. ANN) until the model's error function increases. Four sets of open human motion data and two types of machine learning algorithms were used. Moreover, Rodriguez-Galiano et al. Four types of machine learning algorithms were used as wrappers for the SFS. There are two types of ANN models: (1) feed forward; and (2) feed backward. Equation  1 shows the linear combination of mapping weights from each input neuron, via wires, to the hidden neurons. There are two types of search algorithms: sequential forward selection and sequential backward selection. Table 2  tabulates the percentage of features that were used during the 200 trials. In this study, four variables (Cement, Sillica Fume, Flyash, and Water) were selected as the most relevant features for the prediction model. The selected features, using SFS, were analyzed by the previous BPNN model. The LSR model is a linear function and its form is shown in  (2) . fc = θ1C + θ2SI + θ3FA + θ4W (2)
paper_418	An important aspect of descriptive time series analysis is the choice of model for time series decomposition. Also, when there is no trend (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. The models most commonly used to describe time series data are additive, multiplicative and mixed models. For short series, the cyclical is embedded in the trend  [2] . They do not depend on the level of the trend  [3] . An important aspect of descriptive time series analysis is the choice of model for time series decomposition. Iwueze and Akpanta  [6]  pointed out that an additive model is appropriate when the seasonal standard deviations show no appreciable increase or decrease relative to any increase or decrease in the seasonal means while a multiplicative model is usually appropriate when the seasonal standard deviations show appreciable increase/decrease relative to any increase or decrease in the seasonal means. Linde  [7]  observed that, the differences between the Additive and Multiplicative and the models are (i) for the additive model, the seasonal variation is independent of the absolute level of the time series, but it takes approximately the same magnitude each year while in the multiplicative model, the seasonal variation takes the same relative magnitude each year. In such cases it is appropriate to use a multiplicative model. Oladugba et al  [9]  gave brief description of additive and multiplicative seasonality. This is an improvement over what is in existence. The method adopted in this study is the Buys-Ballot procedure in descriptive time series. formula_5 Source: Iwueze and Nwogu (2014), Nwogu el al (2019). Table 3  that when there is no trend i.e. (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. Estimates of trend parameters and seasonal indices are discussed. Also, when there is no trend ( ) 0 b = , the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models.
paper_428	In Non-Newtonian fluids, the most frequently encountered fluids are pseudoplastic fluids, and Navier-stokes equations alone are insufficient to describe the rheological properties of these fluids, therefore, to overcome this defect, several rheological model like Ellis model, Power law model, Carreaus model and Cross model are presented, but little attention has been compensated to the Williamson fluid model and estimated to explain the rheological properties of pseudoplastic fluids. Williamson analyzed the flow of pseudoplastic materials and presented model to described the behavior of pseudoplastic material and explain convenient importance of plastic flows, and also recognized that viscous flows are very varied from plastic flows  [1] . studied the peristaltic motion of non-Newtonian fluid through asymmetric channels along porous wall by means of various phase and amplitude, and also studied the manipulation of different wave structures on the fluid flow model  [8] . developed a model for the transport of Williamson fluid in an annular region  [10] . analytically discussed MHD flow of viscous fluid through Stretching sheet using DTM-pade approach to solve boundary layer equations of given flow problem  [21] . Reddy discussed unsteady MHD transport of rotating fluid past a permeable surface confined by infinite vertical permeable plate and concluded that by increasing rotating parameter the velocity field is also increased  [23] . It play significant role in field of technology. discussed combine convection in vertical tubes by using constant wall temperature and constant wall heat flux conditions  [38] . investigated flow reversal of mixed convection in a three dimensional channel and concluded that an increase in Richardson number, natural convection dominates the flow and thermal field of combine convection  [39] . studied unsteady combined convection flow in a cavity in presence of nanofluid  [41] . General example of porous wedge is sand, soil, sandstone and foams. Deka   is wedge angle parameter. E 0 = E 6 , E′ 0 = 0, F 0 = 1 (9) E′ ∞ = 1, F ∞ = 0 Where E 6 is injection/suction parameter. This With the boundary conditions E 0 E 6 , $ 0 0, F 0 1, $ ∞ 1, F ∞ 0 (13) The resulting system in Eq. (11) (12) (13)  is solved numerically with the help of 5 th order Runge-Kutta-Fehlberg method. [78] , Yih  [79]  and Rashidi et al. Furthermore an increase in λ may cause increase in temperature of flow. The important conclusions of the analysis are 1. The non-dimensional velocity profile increases by increasing the wedge angle parameter <.
paper_432	The mathematical analysis method used. And found some results; The theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds, possibility study of Hamiltonian tubes when the simplistic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models. The Cotangent Bundles * of manifold d provides the basic model of a symplectic manifold. Generically, the corresponding Hamiltonian system `(%) =& H '%, % ). Given any , manifold , of dimensionn, with -. b) Let exp ∶ → 3 > be given by exp(% =J •" … ∈ 3 > . Let be a lie group• ∈ o⋃ ~∞•. 8 ' : K V → R s> ( K ), satisfying R o 8 ' u 0 ' and the equivariance property. 8 ' ( K , n, ' ) = 8 ' ( K, n ). ' ), the left multiplication of : n`. Now consider a G on a manifold. šJ% ± ∈ , with isotropy subgroup = ² . In the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models. Then ∅isc , invariant surjective submersion and descends to a symplectic homeomorphic. The first result of the theory in cotangent bundles reduction, the theory developed for the problem with a single or bit type playing an important role in the solution to the general problem of a singular cotangent bundles reduction for base manifolds, Hamiltonian tubes when the symplectic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models and the first work studying symplectic normal forms in the specific case of cotangent bundles.
paper_444	The other constraint is the resource constraint in which the demand of resource shall not exceed the total supply of resource for each resource type at any time. In recent years, many scholars discussed different types of resource constrained project scheduling problems, such as multi-mode RCPSR  [1] [2] [3] , multi-project RCPSP  [4] [5] , robust RCPSP  [6] [7] , and so on. Lambrechts  [10]  established a stochastic project scheduling model in which the resource availability was a random variable in order to increase robustness. Liu  [13]  firstly established an uncertain project scheduling model, aiming to minimize the total cost under the constraint that the completion time does not exceed the deadline. Ma  [15]  considered resource constrained project scheduling problem with uncertain durations, and an uncertain excepted value model was built with the objective was to minimize the completion time. Then, we build a multi-objective model which is under resource and precedence rule constrained to minimize the resource cost and the project completion time. Furthermore, a project example is proposed and the optimal scheduling scheme of the project is obtained by genetic algorithm. The construction of this paper is organized as follows. (1) This paper only considers renewable resources. : The set of underway activities at time #. Constraint ○ 4 reflects that for any time # and each resource type , the demand for resources shall not exceed the total supply of resources with at least given confidence level < = . Then excepted value of G is formula_1 Theorem 2. is an uncertain variable, and the inverse uncertainty distribution of ? is formula_6 By Theorem 1, we know that the excepted value of ? is formula_7 Since ℳ6∑ ∈8 9 ≤ + ; ≥ < = , then, ℳ6∑ ∈8 9 − − ≤ 0; ≥ < = . The duration time, cost and resource requirement of activities are presented in  Table 1 . The manager tends to the demand of resource shall not exceed the total resource supply with at least given confidence level 0.9 for any time # and each resource type . The cost per time unit of additional resource ! The constrains are recourse constraint and precedence constraint. The information of the activities. By describing the increased quantities of resource as uncertain variables, an uncertain resource constrained project scheduling problem is discussed in this paper. Then an uncertain optimal model was built with objective of minimizing the completion time and the cost with resource constrained based on uncertainty theory. In future research, We can also focus on more types of project scheduling problems based on uncertainty theory.
paper_462	The goal of postgraduate education for master's degree in clinical medicine is to cultivate high-level medical and health professionals. Since 1998, the postgraduate education for master's degree in clinical medicine has been piloted in China. After 17 years of experiment and exploration, great progress has been made in clinical medical degree education. There are many problems such as insufficient practical ability training. Through improving the conditions of enrollment, innovating the training mode, adjusting the award marks and improving the system guarantee, it has effectively constructed the professional degree education of master of clinical medicine and standardized resident training. "5 + 3" new training mode of training in combination. Chongqing Medical University was founded in 1956. Since 1979, it has enrolled doctoral and master's degree students. In 1981, it was approved as the first authorized unit of doctoral and master's degree in China. In 1998, it became the first batch of doctoral and master's degree in clinical medicine in China. Pilot units for professional degrees. Since 1998, the school has vigorously developed professional degree postgraduate education  [1] . Since 2009, the school has been focusing on reforming the training mode of clinical master degree postgraduates (hereinafter referred to as clinical master). It not only guarantees the quality of professional degree postgraduate training, but also supplies a large number of high-quality talents for related industries. Educational development and other key problems of clinical master training. The system reform of training mode for clinical master of Medical University has effectively solved the problem of insufficient practical ability of clinical master, and has realized the degree education and professional qualification recognition of clinical medicine specialty. The organic cohesion of syndromes has effectively improved the quality of clinical master training. The above policy support ensures that our clinical master can fully participate in clinical practice skills training and ensure the quality of training. Under the above policy guarantee, the clinical master trained by the school has the dual status of postgraduate and regular trainee. After completing the relevant training content and passing the examination, the qualified certificate of licensed physician qualification and resident standardized training can be obtained. To ensure the quality of clinical master's training, the school has set up a guidance group composed of the first tutor and rotating responsibility tutor. Effective management during the transition period. Our school has made corresponding reforms in the curriculum system of clinical master's degree. The reformed curriculum system not only meets the training objectives of clinical master, but also meets the requirements of theoretical knowledge for trainees. Secondly, the curriculum of Master of Clinical Science is adapted to the requirement of training students' theoretical knowledge and foreign language. It is closely related to clinical practice. Students are brought into the "two levels, two stages" training after they enter school. The rotation requirement not only meets the requirements of the state for clinical master, but also closely combines with the regular training. Requirements for the first stage of training. System for Clinical Ability Clinical competence is the core of clinical master training. School research and explore more rigorous clinical training and assessment methods. The clinical master emphasizes on examining the clinical competence of postgraduates. At the same time, the cost of training clinical masters has increased substantially. The school revises the standard of clinical master's degree award, which reduces the requirement of publishing articles. It stipulates that clinical master can apply for a degree only by publishing a review or case analysis. Postgraduates can devote all their energy and time to clinical ability training. Schools from several aspects to improve the mentors, managers and related personnel of clinical master's education awareness: First, extensive publicity. Thirdly, we should formulate an enrollment system conducive to the development of professional degrees. All tutors working in clinical departments must enroll professional degree postgraduates. By promoting the combination of clinical master training and regular training, the problem of clinical master qualification certification has been solved, and a new "5 + 3" training mode has been constructed. The two systems complement each other and organically combine to realize the training of clinical master and regular training. The school has reformed the single tutor system and explored the establishment of clinical master tutor group system. In view of the characteristics of clinical master's degree, the school has established a funding system different from academic degree postgraduates. Since 2009, the school began to explore the reform of the training mode of clinical master in an all-round way. It has overcome many obstacles and steadfastly promoted the reform. It has basically realized the seamless connection between the degree education of clinical medicine specialty and the certification of professional qualification. The main results are as follows: In the past five years, a total of 2063 clinical masters enrolled in the school have been integrated into the training. Many excellent students have become an indispensable new force in clinical colleges. They have achieved the ultimate goal of training "doctors who can really see a doctor". The employment rate of graduates has been guaranteed to be 100% for a long time. In December 2013, the school sponsored the National Symposium on the Reform of the Training Model of Clinical Master's Degree Postgraduates. Since the implementation of the reform of the training mode of clinical master, the combination of degree education and vocational education in our school has become closer. Since the reform of training mode, the enrollment of clinical masters in our university has been increasing rapidly. In the past five years, a total of 2063 clinical masters have been enrolled in the standardized resident training system. The implementation of "three combinations", namely, the combination of enrollment and enrollment, training and training, degree award and physician access. It has solved the difficult problem of registering for medical examination for master of clinical medicine in our university and effectively connected the training of master of clinical medicine with the regular training of master of clinical medicine. The current management system and mechanism are not suited to the key issues of clinical medicine master training, such as the development of professional degree postgraduate education  [18] .
paper_476	This paper uses JD Logistics as an example to quantitatively analyze the relationship mentioned above. (3) Through the proper splitting and loading of orders, staff scheduling, and area adjustment of delivery station, the optimal delivery frequency can be achieved under the balance of cost and resource operational efficiency. In order to reduce the operating costs of logistics enterprises on the basis of ensuring service levels, one should first properly allocate the amount of orders and splitting ratios to achieve an economic increase in the delivery frequency. In addition, it is important for the logistics enterprises to consider the constraints such as delivery resources and consumer satisfaction to achieve the appropriate decision of delivery frequency. What's more, it is also crucial to reasonably arrange vehicle loading, staff scheduling and distribution station leasing for the logistics enterprises. at various stages in the delivery system, which will more likely to result in uneconomical performance. The order volume, the orders' splitting ratio, the operational efficiency of delivery resource and the delivery costs will affect the delivery frequency. System dynamics models not only offer a different Operational Efficiency: A Case Study of Jingdong Logistics perspective while whole system approach to transport planning, but also demonstrate to policy makers the importance of these feedbacks and lagged responses  [1] . Fan Xuemei et al. That research explored three scenarios, which are joint delivery, autonomous delivery, and third-party delivery, which also pointed out that, in order to improve delivery efficiency, enterprises should adequately consider relevant factors such as own resources, competitors' delivery strategies, and urban transport policies before determining delivery methods. [5]  pointed out that joint delivery can effectively improve the efficiency of urban delivery. It proposed an assessment framework for joint delivery. That research took Lyon in France as an example with the using of radar map to visually show CO2 emissions, risk values, delivery costs, traffic impact and delivery time of joint delivery under different scenarios. Wang Wei  [17]  analyzed the advantages and disadvantages of self-operated, outsourced, and federated modes in ecommerce delivery model, and constructed a system dynamics model for the combination selection of delivery modes by selecting six core factors, which can provide decision-making advice for managers to improve the delivery efficiency. Hongtao Yang and Jianbang Du  [19] [20] [21]  uses system dynamics to analyze the problems of supply chain and delivery efficiency from the perspectives of transportation routes, drivers and cooperation agreements, and its sensitivity analysis provides some reference to this paper for the corporate in this case to improve the delivery efficiency. Lin Wanting [22]  considers the phenomenon of unfilled orders, information leakage, and product damage which exists in crowdsourcing delivery, then built system dynamics model from five dimensions of reliability, responsiveness, assurance, empathy and security, which provide decision-making advice for enterprise delivery efficiency management. This paper uses Jingdong Logistics (JDL for short) as research objects. The number of employees is affected by factors such as order quantity, transportation efficiency, delivery frequency, and the number of transport vehicles. The sorter is the person who operates on the sorting equipment. The on-site logistics personnel is the person who puts the sorted goods into a mail packages and places them on a pallet and then sends it to the transportation vehicles. This article considers the utilization efficiency for the facilities, personnel, and technology such as turnover rate of transportation vehicles, the utilization rate of delivery site, and the delivery personnel loading rate, etc., which are put into the sorting, transportation, and terminal delivery links. The utilization rate of transportation vehicles is calculated by dividing the actual traffic volume at each sorting center by the vehicle capacity. The utilization of the site space is obtained by dividing the actual leased area of the site by the available area. The personnel load rate in the process is calculated by dividing the actual workload by the authorized workload. The increase in the delivery frequency will result in the following two changes: first, the increase in consumer satisfaction; second, the corresponding changes in the cost and resources operational efficiency. Among them, the positive loop represents that there is a mutually reinforcing relationship between the factors, while the negative loop indicates that there is a balanced relationship between factors (Such as: Delivery frequency --> + Consumer demand response ability --> + Impact of demand response capacity --> + Consumer satisfaction --> + Order quantity due to change in satisfaction --> + Total daily order quantity --> + Per batch delivery order quantity-->+ Sorting center cargo volume-->+ Number of on-site logistics personnel--> -Onsite logistics personnel operating time-->+On-site logistics personnel costs-->+ Sorting personnel costs-->+ Sorting costs-->+ Total cost --> Total profit --> + Delivery facility input --> + Delivery frequency). Second  Third  Fourth   1  60  30  40  20  2  30  30  20  20  3  10  30  30  20  4  -10  10  30  5  ---10  Table 4 . When the volume of orders increased from 1496 to 5300, the increased percentage of total delivery cost showed a downward trend followed by an upward trend. The minimum value of 3.9% appeared in the order volume of 3086-3154 units. The total delivery cost in scenario 4 was increased by 25% in average than that in scenario 1. As the order volume increased, the increased percentage of total cost tended to decrease in fluctuation. The difference of average unit sorting costs among scenarios 1, 2 and 4 decreased as the order increased, which is because the change in the sorting costs under different order quantities was small and the difference in the delivery order quantity increased. Figures 12 and 13  show that the transportation cost in scenario 2 was increased by an average of 14% than that in scenario 1. The increased percentage behaved a negative growth trend followed by a positive growth trend as the order increased. The transportation cost in scenario 4 was increased by an average of 45% over scenario 1. The transportation cost in scenario 1 showed a rising trend with the increase of orders. When the order volume was 2406, 2530 and 4700, the transportation cost rose rapidly. The vehicle utilization efficiency for the three sorting centers in scenario 4 increased with the order volume since only one transportation vehicle was required to be dispatched from each sorting center within the current order volume. In some cases, the area utilization efficiencies in Scenario 1 and Scenario 3 were the same. In addition, scenario 4 can effectively ease the utilization of area and personnel in the delivery site. With the same delivery frequency, the orders splitting ratio influences delivery cost and resource utilization efficiency significantly. Recommendation I: one should properly allocate the amount of orders and splitting ratios to achieve an economic increase in the delivery frequency. Recommendation II: one should consider constraints such as delivery resources and consumer satisfaction to achieve the appropriate decision of delivery frequency. With different delivery frequency, JDL's delivery resources and consumer service quality are different. Therefore, JDL needs to consider different constraints when making delivery frequency decisions. In this case, if scenario 2, 3, or 4 is adopted, one can increase the total number of order shipments by 2,400, 1,400, and 4,450 units respectively without increasing the number of transport vehicles. Recommendation III: One should reasonably arrange vehicle loading, staff scheduling and distribution station leasing. The delivery frequency has different effects on the resource operational efficiency in different delivery stages. In addition, the area utilization efficiency of delivery stations may exceed 100% under large orders.
paper_479	Handwriting is also a key element in document examination as it leaves a forensic document examiner with the task of determining who the writer of a particular document is and this is achieved through the likelihood ratio (L R) paradigm. With the help of handwriting model for individual writers, little or no assumptions and no nuisance parameters were employed in achieving full likelihood ratio for comparative handwriting analysis in forensic science. [19]  described a statistical model for the writer verification task to determine if two documents were written by the same writer, their model followed the scenario proposed by  [17]  thus suffers from the same weakness  [20]  presented one conceivable way to deal with gauge a probability proportion in near handwriting analysis was delineated. Assessment of signature handwriting evidence through score-based likelihood ratio based on comparative measurement of relevant dynamic features was carried out by  [22, 9]  presented score-based approaches to calculating forensic probability ratios that have been established as becoming more progressively common in forensic literature. Figure 1  shows the graphical representation for the handwriting modeling. BPNN in the context of this paper was to model the handwriting pattern of each writer over a period of time. The feature information extracted from student handwriting through LBP was entered in NN through input layer and the participation of each class of character variables is calculated by the hidden layer of the network using: formula_1 net j is ith type of factor, n is the number of factors in net i , & ' ! weight-recognition factor with weight ! " Weight from second hidden unit i and output unit j is ! ) This research work was able to model the handwriting for individual in the presence of large-scale database using the back-propagation neural network (BPNN). The developed writing model for each writer is one of the criteria to eliminate the presence of nuisance parameters when estimating a full L R . Base on decision law i.e. A more elaborate and collated result table is presented in  Table  2 . Statistically the estimation of the interval is considered to be robust over the estimation point, therefore we do consider the estimation of the interval and take into account the decision condition, and we conclude that both the down and top intervals must be of the same sign; that is, either both positive or both negative. Several factors have to be put in place to estimate a full L R for forensic handwriting investigations.
paper_492	Based on known functions of neuroscience the neural network that performs serial parallel conversion and its inverse transformation is presented. This function is the behavior of so-called short-term memory. In Chapter 2, the circuits by combining the basic functions of neural circuits for serial parallel conversion and inverse of the conversion are shown. The same is true for the recognition process. The same applies to general figures. Deductive logical development is desired. Next, by providing a two-way function to the neural network both serial parallel conversion and vice versa on basic sequences is realized. The neural circuit corresponding to each basic subsequence can be easily configured in the neural network, leading to the realization of processing by hardware of general time series data. The dividing is done by the following procedure. In this example a6 is the concerned element. (3) If the maximum length of subsequence is defined, a new subsequence is allocated after the subsequence that reaches the maximum length, add new element to the concerned new subsequence. Figure 2  shows the affinity with the neural circuit. When the first data c 0 is received activate the bottom. Four portions are activated in the  Figure 2 . The number and their position of the activated elements is the conversion output corresponding to the serial input  [1] . Its output is the result of the serial parallel conversion, it is the result of AND logic of the output of the activated elements. The elements involved in the conversion are still activated at the time of output. Therefore, the couplings between the elements are enhanced (Hebb rule) by repeating this conversion, as a result the elements involved in the conversion will be activated by only receiving the first element of the time series data. This operation is a generation of (learned) time series data. On the flow direction of data, this parallel to serial conversion is upside down with the serial to parallel conversion described above, but the basis of neural network operation is the same. When the state transition diagram of the  Figure 3  is seen as a serial parallel conversion, the bottom is inputs connected to such sensory organs. On the other hand, when the state transition diagram of the  Figure 3  is seen as a parallel serial conversion, another waiting state is needed. The essence of the neural network which performs the parallel serial conversion and the neural network which performs the serial parallel conversion is the same except that the conversion result comes out downward or upward. On other words, both operations of the serial parallel conversion and the reverse conversion are essentially the same on the point both operations are triggered by the first data and proceed waiting for input state change. Both state transition diagram is shown in the existing  Figure 3 . For each basic subsequence a neural network that accepts and generates the basic subsequence is considered. This neural network is called a basic unit. Since general time series data is consisted of hierarchy of basic subsequences, basic units can process general time series data by identifying the outputs of lower layer basic units as the new time series data. The movement will be mentioned in the next chapter. The ability to imitate fellow's action is indispensable. In addition, it spreads to the story beyond space and time. Two types of time series data can be considered in the above situation. Like talking about your childhood while eating cake. In order to avoid confusion, the episode must be corrected by reality. In neuroscience, the morphism corresponds to synapses and axons, and is responsible for the transmission of information between objects. Of course, the description does not convey all the features of the hippocampus.
paper_507	The main aim of present study is to integrate the result of our study with spatial data, soil parameters, land inventory and used the output as a user friendly application using GIS which could predict the future susceptibility of region to landslide and% contribution of each factor for the same. A frequently used definition of landslide is "movement of mass of rock, earth or debris down a slope" in the words of Cruden  [1] . As per the official figures of United Nations International strategy of disaster reduction (UN/ISDR) and Centre for Research on Epidemiology of Disasters CRED for the year 2006, landslide ranked 3 rd in terms of number of death among the top ten natural disasters  [2] . The recent landslide of 2012 took a heavy toll on life and property, many people lost their lives, and thousands of tourist were stranded due to disrupted communication services  [5] . The material can be rock, debris, earth or a mix and movement can be fall, topple, slide, spread and flow. These map divides the land into homogeneous areas or domain and their ranking according to degree of actual hazard potential caused by mass movement  (Guzzetti et. al, 1999 and varnes 1984)    [7, 8, 9] . The result obtained i.e. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area (Mohammad Onagh, 2012). were generated using ERDAS and ARCGIS v. 9.3. DEM (Digital elevation model) was obtained from BHUVAN. In the present study we selected 107 points and all the six factors namely soil depth, soil type, rock type, land cover, slope and elevation and a excel database is created. The data is categorized into training, testing and validation, and all the six factors namely soil depth, soil type, rock and a excel database is created. The number of epochs was set to 3,000. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area  [13] . The study has to led the determination of factors on the basis of past studies and determination of weightage for the chosen six factors namely soil depth, soil texture, rock type, height, slope and land cover. With the further advancement in such type of study, we could interpret results for future from past records, if the site is inaccessible, or the test results are erroneous. The result of present study with spatial data, soil parameters, land inventory and presented by a landslide hazard zonation map and a user friendly application using GIS that could predict the future susceptibility of region to landslide and percentage contribution of each factor for the same.
paper_1	These numerous amounts of information can be extracted, processed and properly utilized in areas like marketing and electronic learning. This paper reports on the successful development of a way of searching, filtering, organizing and storing the information from social media so that it can be put to some good use in an electronic learning environment. It addresses this limitation by using the data from twitter to cluster students and by so doing support group electronic learning. A number of techniques can be used to do clustering. The quality of clustering also depends on both the similarity measure used by the method and its implementation  [10] . Clustering is the concept that was used in this research to create groups from social media data which can be used for learning on electronic learning platforms. This section looks at how the system for creating discussion groups was developed as well as a detailed explanation of the research method that was used to realize the objective of the study. The system design methodology used was incremental prototyping. The fourth step was testing the system. The prototype was then subjected to testing using the test data. They are part of the data that was used to train the system but its results are already known. Finally, the model was used to classify a new user into a group. The illustration of the proposed prototype is given below. This is summarized in the chart below. The Naïve Bayes Classifier was also tested to evaluate its accuracy, precision and recall  [9] . The results below illustrate a summary of what was obtained when 200 tweets were used to test the Naïve Bayes Classifier. This classifier was doing the classification using the unigrams. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. Through social media a better and more efficient way can be used to enable online learning generally and group formation specifically. The system that was developed by the researcher demonstrated the learning capability of the social media by coming up with a way of creating study groups from the information shared across the social media. The classifier was also able to assign other or new users groups also according to their tweets and the learning that the system had undergone. The system was therefore able to address the limitation of the social media of not being properly utilized as a platform for supporting learning activities like group formation. This paper addresses the limitation of social media of not being properly utilized as a platform for supporting learning activities like group formation. Most of the information that passes through social media was being used majorly for social interaction. The techniques that are currently used in group formation and learning are mostly manual and so not efficient. Through social media a better and more efficient way of clustering can be used to enable electronic learning generally and group formation specifically.
paper_2	In addition to this, the current traffic light systems encourage extortion by corrupt traffic officials as commuters often violate traffic rules because of the insufficient time allocated to their lanes or may want to avoid a long waiting period for their lanes to come up. The density of the vehicles is measured in three zones i.e., low, medium and high based on which timings were allotted accordingly. Traffic congestion is a common occurrence in many major cities across the world, especially in third world cities, and this has caused untold hardship to commuters in these cities in diverse ways  [1] . Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Sometimes higher traffic density at one side of the junction demands longer green time as compared to the standard allotted time. In order to overcome the aforementioned problem, this research adopted a density based approach in controlling vehicular traffic. The proposed system would use a microcontroller of PIC family duly interfaced with sensors, to change the junction timing automatically to accommodate movement of vehicles, thereby, avoiding unnecessary waiting time at the junction. The density of the vehicles is measured in three zones i.e., low, medium, high based on which timings were allotted accordingly. Conventional traffic light system is based on fixed time concept allotted to each side of the junction which cannot be varied as per varying traffic density. Sometimes higher traffic density at one side of the junction demands longer green time as compared to standard allotted time  [4] . In a bid to overcome this challenge  [4]  adopted an approach whereby a camera is placed on the top of the signal to get a clear view of traffic on the particular side of the signal so that it will capture the image. The image captured in the traffic signal is processed and converted into grayscale image then its threshold is calculated based on which the contour has been drawn in order to calculate the number of vehicles present in the image. After calculating the number of vehicles we will came to know in which side the density is high based on which signals will be allotted for a particular side. However, this system has the disadvantage of the controller since it depends on the preset quantification values for fuzzy variables  [6]  conducted a cross sectional study targeting traffic control in the city of Nairobi's Central Business District and its surroundings. The simulation runs results showed that the adaptive algorithms can strongly reduce average waiting times of cars compared to the conventional traffic controllers. 0.7 volts for silicon diodes). The PIV of the diode is calculated using the relation formula_1 Where Vp = peak voltage of transformer Vrms = root mean square voltage The Vrms of the transformer is given as 12v formula_2 Recall PIV ≥ 2Vp Therefore, PIV of the selected diode is given as 2 x 16.9706 = 33.9412v For current, the maximum current of the transformer I (max) = 500mA The forward current of the diode I f is given as:   formula_3 Another purpose of an optocoupler is to prevent rapidly changing voltages or high voltages on one side of a circuit from distorting transmissions or damaging components on the other side of the circuit. According to the IR count, microcontroller takes appropriate decisions as to which road is to be given the highest priority and the longest time delay for the corresponding traffic light. It is a widely used type of electronic prototyping board characterized by a 0.1-inch rectangular grid holes with parallel strips of copper cladding running in one direction all the way across one side of the board. The codes are as shown in the Appendix. On these days traffic rules are usually violated because of the complex traffic situation. The sensors help in keeping count of vehicles entering roads and the microcontroller subsequently allots time delay thereby giving accurate priority to each road.
paper_3	(d) Meta-adaptive Navigation Support: It selects or suggests the most appropriate adaptive navigation technique that suits the given learner best relatively to the given context, either by observing and evaluating the success of each technique in different contexts and the resulting learning from these observations, or by assisting the learner in selecting the navigation technique that best suits to him or her. The teaching strategies are based on  [9]  learning cycle and learning style model. The general purpose of the AEHS MATHEMA is to support learners through an interactive and constructivist educational environment in order to construct their knowledge in Physics and Technology, individually and/or collaboratively, and to overcome their misconceptions and/or learning difficulties AEHS MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and meta-adaptive navigation, interactive problem solving, and adaptive group formation. More specifically it supports: (1) adaptive presentation according to learner's learning style; (2) four adaptive navigation techniques which are direct guidance (uses a "next" button), link hiding (hide nonrelevant links), link annotation (use colors to indicate relevance), and link sorting (like in search engines); (3) a meta-adaptive navigation technique in order for the learner selects the more appropriate navigation technique that best suits him or her; (4) an interactive problem-solving method through personalized and collaborative problem-solving activities that exploits the teaching approaches of experimentation through simulations, guided inquiry and discovery, investigations and peer collaboration; and (5) a method of adaptive group formation for peer matching. In the section 3 the architecture of the AEHS MATHEMA is presented in detail. In this section some models that are most suited to the MATHEMA system are presented. The AHAM architecture model proposed by  [6]  is shown in the  Figure 1 . The AEHSs fit so nicely in this model. The next thing to do is to determine dependencies between the concepts. This leads to a structure of prerequisite relationships. The rules together form the adaptation model in AHAM. Proper's architecture is a combined architecture of SCORM LMS and AEHS. They use the Apache Tomcat 5.5 as Web and application server and the MySQL 5 as database server. The DM structure is exported by the manifest file and is stored into Java Object Files. The adaptive navigation techniques have been applied to it via Java servlets and Java Server Pages (JSP). In the  Figure 3  the architecture of the WELSA  [14]  system is presented. [12] proposed a framework for AEHS (  Fig. 4 ) based on JSP, Java servlets, and Javabeans. The Adaptation Decision Model is responsible for deciding what the system should do in terms of presentation and navigation adaptation on the basis of the conclusions draw by the Interaction Analyzer, parameters from the Learner Model and information from the Application Model. ULUL-ILM  [1]  is an AEHS that focuses on student's learning styles. The result of that analysis is called domain model. The content loaded to the MySQL database is accessed via JDBC API. These servlets are complete programs that are capable of creating JSPs. Servlets dynamically create JSP pages according to student requirements. JavaBeans technology comes into play. (3) Protects your intellectual property by keeping source code secure. Figure 5  shows the architecture of the AEHS MATHEMA. It is a set of pedagogical rules that combine the learner's model with the domain knowledge for adaptive performance. In the field of Adaptive Group Formation Module selecting of the most appropriate teaching strategy, the learner's learning style is taken into account during the learner's study. Content is on separate pages, enabling it to be re-used. The model that supports the AEHS MATHEMA is the overlay model. The characteristics that the system maintains in learner model for each learner are: name, gender, username, password, learning style, level of knowledge for each cognitive goal (general performance), level of knowledge for each basic meaning of the cognitive goal (quantitative and qualitative characterization), concepts that have been successfully evaluated for each cognitive goal, current page, current teaching method, current navigation technique, navigation history, meta-adaptive navigation status, initial Web experience and knowledge level (pre-existing knowledge) in the current cognitive goal, ways of presenting feedback messages, preference for assistance by Advisor in terms of its navigation, course sequence, and phases of the guided dialogues in interactive problem solving process in which each learner participates. The learner during the study selects the cognitive goal he or she wishes to study, following his or her personal course in the educational content, exploits the system's assistance, navigating and studying, is informed about the information that the system maintains, who can change them by intervening and directing the system's adaptation in this way, in terms of curriculum sequencing, adaptive presentation and navigation and adaptive group formation, selects his or her learning style, selects the navigation technique, activates and deactivates the curriculum sequencing, selects the way that the feedback messages are presented, changes his or her knowledge level for each basic concept of the cognitive goal, and activates-deactivates the Advisor. The adaptation engine decides on adaptive presentation and navigation in relation to Student, Domain and Didactic models. This is an innovation on the architecture of AEHSs. It is the second level meta-adaptation engine that is responsible for monitoring the cognitive improvement of the learner in the cognitive goal that he or she studies, after n successful evaluations of his or her knowledge in basic concepts, showing him or her advantages and disadvantages of the four navigation techniques that it supports. In AEHS MATHEMA, meta-adaptive navigation works as follows: The first time that the learner enters the system, he or she is asked to state his or her Web experience and level of knowledge in the cognitive goal he chose to study (preexisting knowledge). Following the learner's statement, the system suggests the most appropriate navigation technique to him or her, taking into account the level of his or her Web experience and his or her level of knowledge in the subject he or she chose to study as follows: (1) Direct guidance: Little or no Web experience and little or no knowledge of the cognitive goal. knowledge of the cognitive goal. The meta-adaptation engine proposes to the learner to change the navigation technique if he or she wishes after n successful assessments of the basic concepts of a cognitive goal he or she studied, showing him or her the pros and cons and additional information on each of four navigation techniques, and the learner decides whether to change it or not. Figure 7  shows a snapshot of the meta-adaptation result. A snapshot of a meta-adaptation result. It is responsible for what will be presented to the learner as a result of the processing of the information that arrives from the adaptation engine, the meta-adaptation engine, adaptive group formation module, and application module. The pages displayed to the learner are dynamically generated. More specifically it: (1) initializes and updates the Student Model; (2) collects and stores the data that the learner answers to open-ended questions, entries in tables, registers in notes, etc; (3) monitors the interactions between the learner and the system to get information about his or her choices in terms of the questionnaires, the phases of guided dialogs, the links, the options for meta-adaptive navigation, the visited pages, and more. Figure 9  shows a snapshot of the page responsible for selecting preferences of the learner. Following a study  [13]  conducted on the formation adaptive groups in the MATHEMA, it was decided that: when the system creates a priority list of candidate peers for an interested learner, in the first and second position of the priority list, the algorithm will place the candidate collaborators with a concrete or abstract style and with the same active or reflective dimension of their learning style as follows: If the learner has a concrete learning style, then the algorithm will place the candidate collaborators with a concrete learning style in the first position, and in the second position, the candidate collaborators with an abstract learning style. In candidate collaborators belonging in the same position, the classification is according to their level of knowledge in the current cognitive goal up to that moment. This module is responsible for monitoring and supporting synchronous communication between learners via a chattool. Assessment of the pedagogical and technological aspects of the AEHS MATHEMA has been carried out. The evaluation of the system was carried out by students of the Department of Informatics and Telecommunications of the University of Athens, Greece. Regarding adaptive and intelligent techniques of the AEHS MATHEMA mentioned above, the students having evaluated them consider that the adaptive presentation of different teaching strategies is quite useful (93.0 percent) and fairly easy to use (86.0 percent), the different ways of navigating are quite useful (100.0 percent) and handy (90.7 percent), the meta-adaptive navigation is quite useful (88.4 percent) and fairly easy to use (86.0 percent), the adaptive group formation of collaborative teams to assist them in collaborative problem solving is useful (83.7 percent), but less user-friendly (58.1 percent). MATHEMA supports adaptive and intelligent techniques, such as curriculum sequencing, adaptive presentation, adaptive and metaadaptive navigation, interactive problem solving, and adaptive group formation. The main contributions of the architecture of the MATHEMA in the improvement of functionality of AEHSs are the meta-adaptation technique, the adaptive group formation technique, the problem-solving technique, and the synchronous communication protocol that it supports.
paper_21	The proposed scheme ZSISMP is validated on all BCH codes of known minimum distance. For these codes, only a lower bound is known and the minimum distance is known only for some lengths and special cases  [2-3-4-5-6-7-8] . In this paper, our work will focused on finding the minimum distance of large BCH codes. The section 3 presents the proposed scheme ZSISMP. For these codes, only a lower bound is known but the true value is still unknown for large codes. This approach consists in adding to all-zero codeword a level of noise and considering the minimum distance as the smallest level of noise from which the Soft-In decoder fails in correction. From these stabilizers, we take only a Self Invertible stabilizer if it exist and by using a mathematical tool, we find the sub code fixed by this involution and then we evaluate the minimum distance by using the famous Zimmermann algorithm. For finding the minimum distance of BCH codes. The proposed scheme works as follows: Inputs: formula_0 Step 1: Find the sub code SC fixed by σ Step 2: Find the estimated minimum distance d of SC by using the Zimmermann algorithm. This section presents also a comparison between the proposed scheme and previous work on minimum distance for BCH codes. All results have been done using a simple configuration machine: Intel (R) Core (TM) i3-4005U CPU @1.70GHz RAM 4GO and are made by running the considered algorithm in 1day for each code. It is known also, that All the narrow-sense primitive binary BCH codes of length 255 have their minimum distance equal to their designed distance except BCH (255, 63, 61), and BCH (255, 71, 59). In order to validate the proposed method, it is applied on all BCH codes of known minimum distance presented in table 1. the obtained results show that the minimum weight found by the proposed method is equal to the true value of the minimum distance of all BCH codes of length up to 255. The table 2 summarizes the obtained results. In this paper, we have proposed a new efficient scheme to find the minimum distance for large BCH codes. In the perspectives, we will apply this powerful scheme to construct good large cyclic codes, and adapt this scheme for other linear codes.
paper_31	Through the ZigBee wireless sensor network, this system sends the physiological parameters collected by various medical sensors to the intelligent medical system, and innovatively proposes semantic matching algorithm to solve the queuing problem of data transmission, to ensure the accuracy of data transmission. Therefore, it is crucial to effectively prevent and control chronic diseases. This passive recording method greatly raises the economic cost and time cost of patients with chronic diseases. With the development of Internet, people's life has been profoundly changed. With the means of the Internet, with the carrier of the mobile terminal, intelligent medical treatment founded on the units of family is a new trend. The third Military Medical University studied on the home digital medical monitoring project, which designs a network system to monitor human physiological parameters using mobile terminals. With the great progress of mobile communication technology, Internet and WSN technology with low power consumption, some scholars have applied wireless transmission technology to medical detection system. In the meantime, some other scholars use 4G and the Internet transmission technology to send physiological parameters to the medical system through the Internet or 4G network, so as to realize medical monitoring. In the same year, American scholar Jutra had founded teleradiology. By the limitation of hardware performances and the immature internet technology, it could not meet the data transmission requirements for telemedicine. This is the second stage of the development of WITMED. With the great improvement of the data transmission capability, rapid development in telemedicine consultation and distance transmission of medical images occurred  [4] . This is the third stage of the development of WITMED. Zigbee has been implemented on the Health Care Profile. (2) The problem of power consumption. (4) The problem of data processing. Due to the large amount of medical resources occupied by chronic patients, the phenomenon of difficulty and high cost of getting medical service is more prominent. Attracted by the broad prospect of smart medicine, BAT and other Internet enterprises have participated in the smart medical industry. Based on ZigBee technology, a design of intelligent medical system based on semantic matching is proposed. Then the coordinator transmits the data collected to the intelligent medical system. The system architecture diagram is as follow: The data acquisition structure charts The general technique route of the algorithm is as follows: Propose a comprehensive sorting algorithm, on the basis of semantic matching and a first-come-first-served principle. When two pieces of information are with the identical matching degree, the first arriving one will be regarded as with high matching degree. The general algorithm of this part is as follows: Define priority function: f x, y ax by ε, with a basic definition as follows: x: semantic matching degree, 0≤x≤1 y: a request arriving at the serial position, 0<y≤1 a is defined as the weight of semantic matching degree, 0<a<1 b is defined as the weight of serial position of Request arrives, 0<b<1, a+b=1 ε is defined as the disturbance value, 0≤ε≤0.1, with the default as 0. Generally, a is close to µ= 1 2 , formula_0 Therefore, the process of queuing algorithm based on Semantic matching is as follows: With the hypothesis that a= 1 2 (to take one half of the sum) x= 5 10 (there are 10 participles in semantic information, of which 5 are matched), y 1 (Before a new message arrives, assumes that the request queue has 10 participles waiting, then: f x, y y a x y ε =0.294, therefore, the priority value of the message is 0.294. Definition 2: let {X1, Y1}, {X2, Y2} is concept form of {X, Y}, {X2, Y2b {X1, Y1}. Definition 3: The concept similarity is calculated as:  formula_1 Patients' health information involves their privacy. The calculation is shown as  Figure 7 : The system can gather and process big health data of chronic patients, and realizes the discovery, tracking and treatment of chronic diseases. The intelligent system realizes the data collection of the whole process of outpatient service, examination and treatment, which can provide data support for the precise treatment of diseases.
paper_38	The study aimed to determine if any of the entry requirements such as Ordinary Level (OL) results, Unified Tertiary Matriculation Examination (UTME) scores or Post-UTME (PUTME) scores could predict an outstanding academic performance of first-year undergraduate students admitted into the Faculty of Science in the Kaduna State University, Kaduna. The study adopted the descriptive research design. Education is an essential issue regarding the development of any country in the world. It is a progressive development of knowledge and skills of examinees through stages of teaching and learning at various levels  [1] . At the inception of the Kaduna State University  (KASU)  in 2005, a total number of 409 students were admitted out of which 199 were for the Faculty of Science. In the 2017/2018 academic session, a total number of students admitted was 4,031, and 1,632 was admitted into the Faculty of Science. There have been various perspectives presented by researchers and psychologists about what is academic performance and its importance. In this study, the academic performance is categorised by the entire performance each year, which culminates in a Cumulative Grade Point Average (CGPA). Formula 1 is used for calculating the CGPA. formula_0 Most universities in Nigeria have been using 5.0 as their perfect CGPA score. As such, the CGPA is considered to be a good predictor indicator of a student's academic performance. Over the years in Nigerian tertiary institutions, there has been rife with complaints about students' poor academic performance. In order to solve the problems of students being withdrawn, spending extra years and being on probation as a result of poor academic performance, there is need to predict and find out what is/are responsible for the poor academic performance from their entry requirement(s). This study aims to investigate the relationship between the entry requirements into any of the Faculty of Science undergraduate full-time degree programmes in the Kaduna State University and the students' academic performance at the end of the first year of study. The objectives of this study are to: i. Determine if any of the following entry qualification used for admission, OL results only, UTME scores only or Post-UTME scores (average of OL results and UTME scores) best predict the academic performance of students in the 100 level CGPA examinations; ii. Investigate the relationship between the students' performance of their entry qualification and the academic performance in the 100 level CGPA examinations. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? What is the relationship between OL results, UTME scores and Post-UTME scores of students and their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA across each academic session, from 2010/2011 to 2014/2015? It is a yardstick that is used to ascertain the competences of a student from which his abilities could be determined. Several studies have criticised the use of UTME and PUTME as an imperfect instrument for predicting academic performance of students. The records of students who graduated in the 2009/2010 and 2010/2011 academic sessions from seven faculties were obtained using the stratified random sampling technique. Due to this, the recommendation was that the stakeholders should review the use of UTME and PUTME results for university admissions. The study by  [13]  found a significant relationship between students' scores in three examinations, namely: UTME, PUTME and 100-Level Psychology course, Faculty of Agriculture, Federal University of Agriculture, Makurdi, and thus concluded that the UTME has predictive validity for performance in the university. The sample population consisted of students who were admitted during the 2010/2011 academic sessions but have graduated at the end of the 2012/2013 academic session. The Faculty of Science consists of nine undergraduate B. Sc. This limitation is due to the non-availability of CGPA results of the other undergraduate degree programmes at the time required. The sample distribution is as shown in  Table 1 . The instrument used to derive the data for this study were: the JAMB UTME scores from 2010 to 2014 and OL grades in the five relevant subjects which were the pre-admission criteria and first-year CGPA results obtained from the semester examinations from 2010/2011 to 2014/2015 academic sessions. The data used in the study are the OL grades, and JAMB UTME scores obtained from the University's central database, and the academic standing of first-year CGPA results collected from the various departmental examination officers (DEOs) with the approval from the Dean of the Faculty of Science. However, the OL results obtained from the database was for students admitted during 2010/2011 to 2014/2015 academic sessions. The stanine grades in the OL results obtained at either NECO or WAEC were collected and coded as shown in  Table  2 . The coding for the CGPA is also shown in  Table 4 . It was used in this research study. Since the focus of the study is to determine the predictive validity of OL results and CGPA scores (OL-CGPA), UTME and CGPA scores (UTME-CGPA), and PUTME and CGPA scores (PUTME-CGPA), the statistics employed on the extracted data were Multinominal Logistic Regression (MLR) and Pearson Product Moment Correlation (PPMC) coefficient. The data were regrouped and analysed by academic session and programme of study. What is the relationship between OL results, UTME scores and Post-UTME scores (average of OL results & UTME scores) of students and their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? Table 5  shows the summary of correlations coefficient between OL results and CGPA scores (OL-CGPA), UTME and CGPA scores (UTME-CGPA) and PUTME and CGPA scores (PUTME-CGPA) aimed at all the academic sessions for Computer Science, Mathematics and Physics degree programmes. For the Computer Science programme, as shown in  Table 5 , the correlation coefficient indicated a low negative correlation in UTME-CGPA (-0.092) and PUTME-CGPA (-0.001) and a low positive correlation for OL-CGPA (0.072). How well do OL results, UTME scores and Post-UTME scores of students predict their first-year CGPA in each of the respective undergraduate degree programmes in the Faculty of Science? Table 6  is the likelihood ratio test results for the Computer Science, Mathematics and Physics degree programmes. Likelihood Ratio Tests is a statistical test of the goodness-offit between two models. A Multinomial Logistic Regression (MLR) was performed to model the relationship between the predictors and membership in the six groups, i.e., CGPA categories (Fail, Pass, 3 rd Class, 2 nd Class Lower, 2 nd Class Upper and 1 st Class) for each degree programme. For the Computer Science programme, the slopes (B) of OL in all the CGPA categories are positive. These showed that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. What is the relationship between OL results, UTME scores and Post-UTME scores of students and their firstyear CGPA across the academic session, 2010/2011 to 2014/2015? PPMC was used to analyse the data for this research question. In the 2010/2011 academic session, the correlation coefficient indicated a low positive correlation in OL-CGPA (0.198), UTME-CGPA (0.189), and PUTME-CGPA (0.232). In 2011/2012, OL-CGPA (-0.114) and PUTME-CGPA (-0.043) display low negative correlation while UTME-CGPA (0.114) has low positive correlation. The OL-CGPA (-0.071), UTME-CGPA (-0.090), and PUTME-CGPA (-0.040) in the 2014/2015 session signifies a low negative correlation. How well do OL results, UTME scores and Post-UTME Sa'adatu Abdulkadir and Francisca Nonyelum Ogwueleka: Predicting Students' First-Year Academic Performance Using Entry Requirements for Faculty of Science in Kaduna State University, Kaduna -Nigeria scores of students predict their first-year CGPA across the academic session, 2010/2011 to 2014/2015? Table 9  is the likelihood ratio test results for all academic sessions ranging from 2010/2011 to 2014/2015. For all the academic sessions as shown on the results, the first equation intercept is the log of the ratio of the possibility of a student having a 'Pass' degree to the possibility of that student having a 'fail' degree. For the students admitted during the 2010/2011 academic session, the slopes (B) of OL in all the CGPA categories are positive. These show that the relative strengths of their OL result performance on the CGPA categories of 'Pass', '3 rd Class', '2 nd Class Lower', '2 nd Class Upper' and '1 st Class' are higher than those with a CGPA category of 'Fail'. Conclusively, the slope (B) of PUTME in the CGPA category of '3 rd Class' is positive and the rest negative. The primary purpose of this study is to investigate if OL results, UTME and PUTME scores do predict the academic performance among first-year undergraduate students in the Faculty of Science. However, by combining all the criterion variables, that is OL, UTME, and PUTME, as one variable and performing the PPMC and MLR, findings show that OL is a good predictor on the dependent variable for academic performance with a weak correlation of 0.068 which is statistically significant at 0.04 level. Although OL and UTME are still necessary as instruments for admission, it is recommended that the University be advised to include some other instruments such as senior secondary school mock examinations results for selecting candidates into any of the undergraduate degree programmes in the Faculty of Science. The authors in  [21] , in their study also recommended the need of potential researchers to compare the OL, UTME and Post UTME terms and scores across Nigerian Universities for standardisation and a model for educational development in the twenty first century.
paper_57	This paper attempts to overcome stagnation problem of Ant Colony Optimization (ACO) algorithms. It is a multi-agent meta-heuristic approach and was first purposed by M. Dorigo et al. [2]  as Ant system (AS) algorithm. The aim is to find the shortest path. On their way back they use the same way from which abundant loops has been removed, but the amount of pheromone (1) ∆τ k ij (t) they produced is inversely proportional to the tour length L k (t). Disadvantages of ACO algorithms are (i) many user parameters and (ii) the selection pressure. The first point is a property of the algorithm, while the second point has had many papers devoted to it. The stagnation avoidance mechanism (ii) is based on the comparison of a randomly generated quantity q ∈ (0,1) with probability p k ij (t) of selected arc. If q ≥ p k ij (t), then choose the next node randomly. Genetic algorithms (GA) were proposed by  Holland (1975) . The four main components of GA are representation (i), mutation (ii), crossover (iii) and selection (vi) mechanism. Mutation (ii) mimics random gene changes. In ACO adaptation the first and the last node is excluded from mutation. If more such nodes occur, random selection is applied. In ACO algorithm crossover position is represented by a common node of parental strings except the first and the last node  (Fig. If more of such nodes exist, random selection is applied. Crossover operation makes sense only if both child strings differ from their parents. In GA many selection (vi) mechanisms are available, like roulette-wheel selection, tournament selection, stochastic universal sampling or reward-based selection  [7] . At first mutation is applied. If more candidates by which the selected node can be replaced occur, the new node is random chosen from the candidates. If mutation fails on all nodes of the tour, another tour is chosen. After all mutation operations are performed, crossover operations are applied. Parent strings are random selected. If crossover operation is not feasible, another second string is selected. If no tour has common node with the first selected tour, another first tour is selected and the random selection process is repeated. Test graph is a symmetrical multi-graph with 80 nodes and 300 arcs  (Fig. The reference value of n [%] was received without any genetic operation and is 5.6 (Table 2, row 1). It can be seen that the higher number of mutation operations, the better the performance is (Tables 2). It does not have the highest value on edges of the surface where the highest amount mutation operation is. In order to determine the effect of crossover operation crossover rate was let to grow up to 100%  (Table 3) . To prevent interference, no mutation operation was allowed. The results vary  (Fig. Mutation operation causes better results than crossover operation. The higher amount of mutation operations the higher the performance gain is. No limit for amount of mutation operation was found during the simulation. The impact of the GO execution order on the mutation operation distribution needs to be verified.
paper_78	Each formation has its own composition, structure, distribution range and environment, and all of them should be protected effectively. Quantitative methods, such as numerical classification and ordination, are significant in ecological analysis of plant communities  [1] . It is applicable in ecology because the description of ecological systems is not always possible in terms of a binary approach  [4] . Ecological communities, such as Glycyrrhiza uralensis communities, have great variations in their species composition respond more or less independently to environmental variables. Because of this, methods based on fuzzy mathematics might be more appropriate in ecological analysis  [6, 7] . Medicinal plant is important natural resource and significant for people health in many countries and regions, such as China, Japan, Korea, India and so forth  [8, 9] . Licorice (Glycyrrhiza uralensis) is one of the most popular Chinese herbal medicines and a significant resource plant species. Like licorice, most medicinal plants are harvested in the wild and the extended use has led to some medicinal species endangered and tending to be extinct, and the protection of such medicinal species has been urgent  [12, 13] . The studies on medicinal plants and their communities are the basis for their conservation and restoration. This study aims to identify Glycyrrhiza uralensis communities and analyze their characteristics on composition, structure and environment in North China. Classification is the process of dividing plots into clusters (communities) so that plots in the same cluster are as similar as possible, and plots in different clusters are as dissimilar as possible. Fuzzy C-means clustering is a soft classification technique  [5, 7]  in which a plot can belong to more than one cluster, and associated with each plot is a set of membership levels. The Fuzzy C-means clustering attempts to divide a finite collection of n elements (plots) formula_0 into a collection of C fuzzy clusters with respect to some given criterion. Given a finite set of data matrix (species × plots), the method calculates a list of C cluster centres  formula_1 formula_2 Where i = 1, 2, …, N = the number of plots; j = 1, 2, …, C = the number of clusters; U = {U ij } = the matrix of membership values, U ij is the membership of plot i in cluster j; V is a matrix of cluster centers; m is fuzzifier which determines the level of cluster fuzziness (1 ≤ m < ∞). A large m results in smaller memberships u ij and hence, fuzzier clusters. (dA ij ) 2 is the distance index: formula_3 X i is the vector of attribute measurements in plots, usually a vector of ordination scores; V j is the centre of cluster j, if A is a unit matrix, then formula_4 Based on the algorithm above, the procedure of fuzzy C-means clustering is as follows: (1) Selecting a common ordination method, we use Detrended Correspondence Analysis (DCA), and do ordination analysis for species data. formula_5 formula_7 Based on the new membership values U, we go back to the fourth step and calculated the next turn V j , (dA ij )  2  and U ij iteratively, and until the membership values become approximately stable. 7 Classified plots into clusters based on the final U. We can use U to identify the relationships among plots and communities directly. However for clustering purpose, a plot should belong to the cluster in which it had the maximum membership value. Based on a general survey of Glycyrrhiza uralensis and its community distribution, five study regions, Chifeng (in inner Mongolia), Hengjinqi (in inner Mongolia), Minqin (in Ganshu), Aletai (in Xinjing) and Kashi (in Xinjing), were selected as sampling sites  [7] . The coverage, mean height, individual number for shrub and herb species was measured in each plot. Fuzzy C-means clustering is a nonhierarchical method which provides C clusters and their plot compositions  (Table  1) . It classified 100 plots into 12 clusters, representing 12 Glycyrrhiza uralensis communities, e.g. The name and characteristics in species composition, structure and environment of each community are described below. It is distributed from 380 to 605 m in hills with slope 10 -20° in sunny and semi-sunny slope and chestnut soil. Its disturbance intensity is medium and heavy. The community has a total cover of 70%, a shrub layer cover of 5% and an herb layer cover of 70%. The common species are Oxytropis myriophylla, Polygonum divaricatum, Adenophora gmeliniia, Potencilla acaulis, Suaeda prostrate, Astragalus melilotoides, Allium condensatum, Artemisia ordosica, and Oxytropis grandiflora. The average cover of Glycyrrhiza uralensis in this community is 35% with a density of 5700 ha -1 . The community has a total cover of 70%, a shrub layer cover of 15% and an herb layer cover of 60%. The average cover of Glycyrrhiza uralensis in this community is 37% with a density of 4900 ha -1 . Glycyrrhiza uralensis + Astragalinae triloa + Stipa sareptana. It is distributed from 400 to 700 m in hills with slope 10 -30° in sunny and semi-sunny slope and sandy soil. The common species are Artemisia scoparia, Kochia prostrate, Potencilla acaulis, Artemisia frigida, Ceratoides lates and Atraphaxis frutescus. Its disturbance intensity is heavy. Glycyrrhiza uralensis + Festuca logae + Stipa sareptana. Theoretically, fuzzy C-means clustering is the only one soft clustering method and can deal with much imprecise and incomplete information and has advantages in solving non-linear problems and in studying complex system  [2, 17] . Therefore, fuzzy C-means clustering should describe ecological phenomena and rules better  [5, 6] . This study proved that fuzzy C-means clustering is fully usable in classification analysis of plant communities  [1, 19] . Glycyrrhiza uralensis communities recognizing by fuzzy C-means clustering varied greatly in species composition, structure and distribution area  [20] . These communities are important not only for protection of medicinal plant species but also for conservation of ecosystems and their environments in semi-arid and arid regions in North China  [23, 24] . The classification results by fuzzy C-means clustering are reasonable according to vegetation classification system of China  [25] [26] [27] .
paper_96	With the rapid development of China's transportation, the frequency of traffic accidents is also high. In the program, a new type of "warning system of obstacle avoidance of embedded electronic guide dog" has been developed on the basis of careful analysis of all kinds of present anti-collision warning systems, which has a core micro-controller, 32-bit ARM7 microprocessor, and takes the embedded operating system uCLinux as its platform. Such warning system of obstacle avoidance of embedded electronic guide dog can effectively eliminate the impact of the traffic environment and the subjective factors of the blind, warning in advance for the travelling blind in time, effectively avoiding obstacles such as vehicles, to reduce traffic accidents caused by the their blindness. I think that if electronic guide dogs guide them to avoid obstacles such as vehicles, the blind people are able to avoid such accidents, aren't they? In fact, this "warning system of embedded electronic guide dog avoiding obstacles" is similar to a car anti-collision avoidance system. The research on "warning system of embedded electronic guide dog avoiding obstacles" in this paper is an important part of intelligent electronic guide dog system, the study of which is mainly based on the embedded system, anti-collision warning system, and the USB technology is applied to warning system of embedded electronic guide dog avoiding obstacles. [1]  In the design process of the obstacle-avoiding early warning system of embedded electronic guide dog, we firstly has carried on a comprehensive evaluation and analysis for each scheme, and then put forward a new idea that it introduces the open source embedded real-time operating system uCLinux, with a high-powered ARM core as the core processor. Also, it is easy to extend the functions, which is convenient for field operations, greatly reducing the cost of the traffic information collecting system in the obstacle-avoiding early warning system of electronic guide dog. [3]  The alarm system of guide dog adopts the combination of light and sound. Sound will produce auditory stimulus for the blind. The overall design plan of obstacle-avoiding early warning system of embedded electronic guide dog mainly is the velocity and distance measurement module, the power circuit module, the LED screen show module, USBcommunication circuit module, and clock circuit module. The hardware of the obstacle-avoiding early warning system of embedded electronic guide dog can reference the Figure 1:  Figure 1 : Schematic diagram of hardware system structure  [4] In the anti-collision early warning system, referenced in the design of obstacle-avoiding early warning system of embedded electronic guide dog, there mainly exits four steps, that is, information collection, information processing, information judgment, and warning information. Therefore, in the design process of the obstacle-avoiding early warning system of embedded electronic guide dog, after the comprehensive and comparative analysis of the anti-collision warning techniques, we have compared the optimization principles of the anti-collision warning techniques, and finally adopt the approach of millimeter wave radar sensor + ARM microprocessor. The full name of ARM is Advanced RISC Machines. And its way to addressing is much easier and more flexible, and its operating efficiency is high. Also, it could complete most of the data manipulation in the register. At present, the embedded RTOS (Real Time Operate System) mainly includes two categories--commercial and free. These are the most remarkable features that uCLinux owned. The reference model is shown in  Figure 2 . of USB reference model  [4] The traffic safety problem of the blind is an urgent issue to solve. The warning system of embedded electronic guide dog avoiding obstacles studied in the project can effectively eliminate the influence of traffic environment and blind subjective factors, send the pre-trip alarm for the blind in time, effectively avoid obstacles such vehicles, to reduce traffic accidents caused due to the blindness. This humanized technology innovation is the embodiment of environmental science and technology aesthetics theory in the field of science and technology innovation practice.
paper_134	Mathematical modeling, quantitative analysis of data and new algorithms can identify new relationships between different data, which in turn leads to competitive advantage. A set of 16 samples of olive oil, sunflower, canola and corn oil which mixed with different ratio of Authentication, were used for calibration and evaluation of developed system. These methods are very damaging, costly and time-consuming. In recent years, nondestructive methods have been considered in purification. Dielectric properties are one of the most important physical properties of agricultural and food products. If the dielectric coefficient is bigger, it will be a better insulating property  [6] . The association of these properties with many quantitative and qualitative components of the product, high speed, low cost and high efficiency of its use as a new and non-destructive technology has been considered by many researchers to measure a wide range of qualitative and quantitative characteristics of agricultural products  [8] . One of the main reasons for this low consumption is the high price of this oil. They used a 4V sine voltage in the range of 10Hz to 1 MHz to determine the dielectric properties of a binary mixture of olive oil. They also used the partial least squares model (PLS) to detect oil falsification. The results of the PLS calibration model showed good results for the adulterated oils with soybean oil as a counterfeit oil. Soltani et al (2010) used a non-destructive capacitive method to detect the amount of banana fruit juice. (2006) predicted egg quality parameters using its capacitive properties. In this article the experiment was done by olive, sunflower, Canola and corn oil. The USB port is used to communicate or send and receive information between the device and the computer. One of the new ideas in identifying and categorizing patterns is a backup vector machine or SVM. Backup machines have very valuable properties that make it suitable for pattern recognition. Using this technique, the amount of RMSE for the samples were equal to 3.56, 2.45, 3.05 and the amount R 2 was equal to 0.92, 96 and 94, respectively. After providing adulterated samples and pouring them into a capacitive sensor, output data was analyzed by MATLAB software. Figure 2  shows response diagrams for adulterated samples of olive-sunflower, olive-canola, olive-corn Using this technique, the amount of RMSE for the samples were equal to 3.65, 2.46, 3.31 and the amount R 2 was equal to 0.92, 96 and 93, respectively. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud. As regards, the accuracy of the methods are close together, this can be concluded that all selected techniques, presented here, have a good potential to be used to determine olive oil fraud.
paper_139	Step2: Pick the minimum degree vertex 'v' in the set-up and include in the set U. Step3:While U doesn't include all vertices Step3A: Include the entire isolated vertex which is adjacent to the vertex 'v' to U. Step3B: Find the adjacent vertex 'u' to 'v' which is not in U and has maximumdegree. Case I:Regular set-up Connected Regular Setup Case I: A Consider the following connected set-up G 1 in figure 2, having 12 nodes having 3 degree in all vertices along with redundant links. Now the set U consists of the nodes A, B. Now the set U consists of the nodes A, B, 2. After inclusion of the node 1 the set U consists of the nodes A, B, 2, 1. 4.4 Discover the node 1, the unvisited adjacent node is from 1 as only 3. Now the set U consists of the nodes A, B, 2, 1, 3. After Applying The Proposed KTMIN-JAK-MAXAM ALGORITHM To G 2 , we get  After applying the proposed KTMIN-JAK-MAXAM ALGORITHM to the above graph G 3 Here notice that, Regular connected network G 3 , after applying the proposed KTMIN-JAK-MAXAM ALGORITHM to G 3 , we get path of length 9 in figure 7. Case II: Connected Complete Network G 4 in figure 8  Here notice that, all complete connected networks G, after applying the proposed KTMIN-JAK-MAXAM ALGORITHM to G, we get linear path of length (n-1). Case III:Connected Irregular network Case III: A Consider the following irregular set-up G 5 in figure 10     After applying the proposed KTMIN-JAK-MAXAM ALGORITHM to the above graph G 7 in figure 15.
paper_145	The present study was based on data collected from 900 respondents of both urban and rural areas of Bangladesh. Majority (70.6%) were diabetic patients. With the increase in age significant increase in prevalence rate of obesity was observed. Higher prevalence rate was also observed among housewives. Higher prevalence of obesity was noted among females. The prevalence of overweight and obesity has increased rapidly over the last decades especially in developed countries  [1] [2] [3] . In 2016 WHO estimated that globally approximately 1.9 billion adults (age > 20 years) were overweight and more than 650 million adults were obese  [4] . The investigated diabetic patients were 544. Some of the variables observed were qualitative in character and some were quantitative. The analysis was done by using SPSS [version 20.0]. The respondents were classified as underweight  [BMI < 20] , normal [BMI,  20 -25] , overweight [BMI < 30] and obese [BMI 30+]. Factor analysis was done to identify the factors for obesity. Significant association was decided by chi-square test with p-value< 0.05 and odd ratio was calculated for respondents who were overweight and obese compared to normal group. Maximum (43.1) of the respondents were overweight and 20.9 percent of them were rural people. Obesity was observed among 15.3 percent people and overweight respondents were 34.0 percent. Of course major respondents (81.4%) were from urban area. However, urban people were more exposed to overweight and obesity by thirty two percent compared to rural people [O. R.= 1.32]. There were 58.9 percent males among the respondents and 47.2 percent of them were normal. However, compared to males more females were obese. But more Muslim respondents (43.8%) were normal compared to Non-Muslim respondents (38.8%). Among the investigated respondents 92.6 percent were currently married and 43.1 percent of them were normal  [Table  4 ]. Similar normal group was noted among the other group of respondents. However, there was significant differences in proportions of different levels of obesity among the two marital groups of respondents [P (χ2 ≥ 22.933) = 0.028]. Majority (52.9%) of the respondents were of age 50 years and above and 48.5 percent of them were normal [  Table 5 ]. Levels of obesity was significantly associated with levels of ages [P (χ2 ≥ 18.34) = 0.008]. Higher proportion of respondents (23.2%,  Table 7 ) were businessmen and 45.5 percent of them were normal. Maximum normal group of respondents (53.8%) was observed among agriculturists. Maximum (25.5%) respondents of obesity was noted among housewives. But, all other professional groups of respondents had the similar risk of obesity compared to servicemen [O. R. = 1.04]. The lower income (< 20,000.00 Taka) group of people were more (34.2%) and 48.4% of them were normal [  Table  8 ]. More respondents of normal group of people were observed (49.0%) among them who had income 20,000.00 -< 30,000.00.This group of people were 20.2 percent. The data indicated that 54.4% respondents had income less than 30,000.00 taka. More overweight people was observed among them who had income 30,000.00 -< 40,000.00 taka followed by the group of people who had income 50,000.00+. Significant association was noted between the level of obesity and the level of income [P(χ2 ≥ 64.994) = 0.00]. These two groups of people were 450 and 164 of them were overweight. Again, those who were not doing any physical labor (23. Table 10  showed that 67.6 percent underweight respondents were affected by diabetes. There was no significant association between level obesity and prevalence of diabetes [P(χ2 ≥ 0.851) = 0.837]. The association between smoking habit and level of obesity was significant [P(χ2 ≥ 20.189) = 0.0.002]. This was done by factor analysis. So, the inclusion of variables were satisfactory. The respondents were investigated mostly by the doctors and nurses from their working places. The selection procedure was a convenient sampling plan. Around 50 percent respondents were overweight and obese. Higher (71.6%) prevalence rate of diabetes was noted among the overweight and obese group of respondents. Similar finding was also noted in another study  [21] . Around 50.6 percent people of urban area were overweight and obese. This result was also similar as was observed in another study  [21] . They were classified as underweight (BMI <20), normal (BMI, 20 -<25), overweight (BMI, 25 -< 30) and obese (BMI = 30+). The percentages of these four groups of respondents were 7.6, 43.1.34.0 and 15.3, respectively. Most the respondents were in normal and overweight groups. Again, prevalence of diabetes was more among these groups. This is for the in service, private or government, people. The public health authority can play a decisive role for the above steps.
paper_212	Mathematical modeling of disease has been an indispensable tool in accounting for disease transmission dynamics as well as disease spread. Epidemiological disease models have been used to explain the dynamics of HIV/AIDS in the population from the early 1900s. Despite the spread of HIV/AIDS having been explored widely, not much literature is available on the Gillespie Algorithm based SIR model. The purpose of this paper is to build on the concept of Gillespie's Algorithm based SIR models by developing a stochastic SIR model to simulate disease evolution in the population setting. The values produced through simulation by the model developed in this paper using a tau value as the time step of the model were compared to HIV/AIDS data from 1985 to 2018, given by NACC. On June 5, 1981 a mysterious disease was recognized among MSM in the USA. In 1982 the CDC identified the same disease among IVDU, hemophiliacs and Haitian residents. It was characterized by its etiological agent HIV in 1983,  [1] . A serological test, was then made available. In 1984, Robert Gallo discovered that HIV was responsible for AIDS. In 1984, several HIV and AIDS cases were documented in Kenya. This accounts for 4% of new pediatric infections worldwide. Among the expectant women there are 13,000 new HIV infections among children. Sex workers have the highest percentage of HIV prevalence at 29.3% according to the Kenya HIV estimates report-National Aids Control Council. These questions are encompassed in the modeling of the HIV immunology, the HIV dynamics as well as the AIDS dynamics such as the dynamic distribution of the disease in the population and its likely magnitude. In this model, there were no forms of intervention. The number of HIV infections in 2010 was predicted to approximately 1000000. The group most affected would be the 31-40 years group. There are issues estimating prevalence in high risk groups and the size of high risk groups. Furthermore, it provides inaccurate estimates where an epidemic has not gone beyond its peak  [6] . They used the Milstein method to simulate for analysis. Other authors have made contributions to mathematical epidemiology by performing simulations that explain the process of disease spread. In their works they build a disease spread prediction model based on the SIR model and applied parameter values to a stochastic model based on Gillespie's algorithm. This is applied to data and the conclusion was that the model well explains the process of the spread of the disease in the population  [10] . Infection-transmission deterministic models are based on the characteristics of population growth, disease occurrence, and spread within a population. This study seeks to incorporate a stochastic aspect in the deterministic SIR epidemiological model. In spite having a lot of work done on mathematical modeling, there isn't adequate literature on the modeling the evolution of disease in the population through simulation. The SIR model explained how the epidemic manifests in all the compartments. The reliability of the simulated values would set the precedent for the valued to be predicted based on the model is also explored. The stochastic SIR model. The interaction between states is made possible by events outlined in this model as birth, infection, non-AIDS death an AIDS death. Assuming that the resulting state is . Continuous-time Markov chains are the basic tool for building discrete population epidemic models. Discrete evolution is modelled in discrete time. Continuoustime Markov chains are the basic tool for building discrete population epidemic models. The wait times between events can either assume an exponentially distributed wait time or the rate of flow between compartments can assume any of the following distributions depending on the results. The data was obtained from NACC for HIV/AIDS cases. A stochastic SIR model was simulated with a mean step size of 0.006336446. 537 tau steps were made in the model. where C represents the variance of D The calculated value found was 64.958. The critical value was 47.4. Mathematical modeling of disease trajectory using Gillespie based algorithms is yet to be explored extensively in literature. In this study, a simulation was carried out on the SIR model to explain the trajectory of the disease by employing a stochastic element using Gillespie's simulation algorithm. After simulating, values were produced by the algorithm for each time step. The simulated curves were compared to HIV/AIDS data. Recommendation for research would be to explore other variations of the SIR model such as SI, SEIR under Gillespie's algorithm.
paper_214	This kind of model allow to the decision maker to create complex reports and graphs based on the columns from the dimension tables and measures from fact tables, that can be the base for creating alternatives and scenarious acording to the economical indicators. Building alternatives and scenarious is an elaborate task and must have a background in existing data structured in databases that have a special structure of dimensions and fact tables. Optimizing the use of a certain class of models for finding the best solutions. Decision trees, in addition to decision tables, chart highlights the relationship between the variables of the problem, making it possible representation of complex situations. In addition to the measurable value of which is seeking optimum value, the model states and restrictions on them. It is the only method that can be applied to unstructured problems. Such testing may be actions that can be made explicit in the framework model; enable better decision-making structure problem, allowing exploration of information flows and operational procedures without interfering with the actual operation of the system; using cybernetic control system, which underlies decision making in practice; There are a large number of parcel simulation program. simulation models have a procedural nature, their solution involving the processing of experiments created within the system. always taking into account the unique factors of a problem specific results can not be transferred to other issues; results are difficult to interpret, being dependent on random factors; no matter how powerful your computer is, the optimal solution is difficult to obtain a model that has many equations and a significant number of parameters. These limitations have led to the use of simulation only when the interactions between components of the system are complex, when factors random have a significant and requires a large number of observations on the behavior data, the problem can not be solved by an algorithm or experiments direct. In the first case, the user is provided customized views of data stored by performing a diverse set of operations on transactional data. After analyzing the results achieved and the objectives of their reporting, signaled differences and after identify problems it was reveal the need to take action. The result of the information stage is a formal description of the problem identified the category to which it belongs and responsibilities involved. For choosing the solution which takes the results of the previous stages, the action is chosen according to the criterion of selection and decision-making model. From model design and solution choice there is a strict demarcation, certain activities may be conducted during both phases, and return of election phase in phase. Structural problems use mathematical formulas and analytical method to achieve an optimum solution. In order to improve efficiency the best solution search algorithms are used. Implementation is the phase that involves the integration model chosen solution in context and simulating the real system. Personal data is data that relates to the behavioral aspects of decisionmakers in making decisions. The management of the database depends on the organization of data. In most cases there is SGBS transactional relational data system and a management database for multidimensional data warehouses created. The data dictionary is a catalog of all data from the database. It is the component that differentiates interactive decision support systems to other systems. The models are domain-specific and models can be classified into strategic, tactical and operational models models. Database management system allows creating new models models using programming languages, update and modify existing models, establish interdependencies between models. Manage in a logical manner a variety of models to consistency of the data model and provides integration of application systems components maker. In building a data warehouse is based on the analysis of data. Extract information in order to obtain information for decision making. Basically there are two steps: designing and populating data. In such a model is highlighted: quantitative data centralized called measures of activity quantitative criteria for centralized aggregation  ,  referred sizes  relational table that stores the measures identified by   the facts dimensions is called table  Tables where aggregation criteria has explicit codes,  called type tables list. The databases permits flexibility in creating queries by choosing columns from dimension tables and measures from the fact table and so the decision maker may choose what economical indicators is useful to analyse and also permits to create different types of graphs such as columns, pies, scatter points or webs.
paper_216	This model is found as promising alternative as far as pricing of European options is concerned, due to its varied volatility of the underlying security and estimation of the risk neutral MGF. This study made an attempt to improve the accuracy of option price estimation using Wavelet method and it improves the accuracy due to its ability to estimate the risk neutral MGF. The study was carried out using simulated stock prices of 1024 observations. Derivatives includes; Forwards, futures, options and swaps. An option is a type of derivative that gives the option holder the right but not obligation to buy or sell an underlying security at a specified price, at any time on or before a given date in the future as agreed on. The holder of a put option has the right but not the obligation to sell underlying security under some predefined terms of an agreement which includes; strike or exercise price, maturity date and option volume. European-style options can be exercised at the maturity date only while American options can be exercised at any time prior to or on the maturity date. American options need more complex pricing methodology due to the extra feature of early exercising. The Monte Carlo method, which is based on repeated computation and random sampling can be used for pricing American options  [1] . The Black-Scholes option pricing formula which has been used as the benchmark to price European options in most of the previous researches due to its simplicity and low computational demand  [2] . This Black-Scholes formula remains the most widely used model in pricing options though it has some known biases which include volatility smiles and skewness. This model derives the closed form solution for pricing of a European options that is why it is used as a benchmark. The Wavelet based pricing model is another nonparametric method alternative used to price derivatives  [4] . The remainder of the paper is structured as follows: Section 2 discusses the existing literature; Section 3 presents an overview of the Black-Scholes model and Wavelet based pricing model. Lastly, section 5 concludes the study. The simple closed form solution of European options was derived during the financial crisis  [2] . The derivation of the pricing formula is independent on the parametric form of the underlying security's price dynamics and hence the success of this model depends on the accurate capture of dynamics of the underlying security's price process. According to the researches as far as pricing derivative securities is concerned, Wavelet based option pricing model is the latest option pricing model in the literature  [6] . The approximation of the implied risk-neutral MGF of the underlying security (asset) returns was focused by developing a non -parametric option pricing model called Wavelet model  [4] . The following are some examples of these advantages; when there is presence of jumps in the underlying process MGF is more tractable, the option's obtained MGF is a continuous function, statistical moments of the asset distributions can be obtained using MGF and the risk neutral MGF can be used to estimate the out of sample options that has maturity dates which are different. European options can also be priced using the Shannon wavelet  [14] . Another example is the application of the wavelets in nonparametrically estimation of a diffusion function  [15] . is the cumulative distribution function. ( is the bilateral inverse Laplace transform. Interest rate r is assumed to be constant. When T -t = 1, Θ (v) is the risk-neutral MGF for the rate of return per unit of time. This needs to be approximated by wavelets. On this criterion, the Franklin hat function performs very well because it is symmetric, smooth and piecewise continuous. This function also emulates the probability density function of asset returns. In this study Monte Carlo simulation was used to generate 1024 stock prices. These data is divided into three, In-The-Money options, At-The-Money options and Out-of -The money options. Therefore, one of the most significant input of this study is that the wavelet -based pricing model is an alternative model for pricing options and other derivatives on the same underlying asset with varying times to maturity and different strike values. Other complex options include; Bermuda options and exotic options. Moreover, hedging performance may be evaluated with respect to each of the two models.
paper_219	This paper introduces a framework to determine data quality on enterprise networks for net-centric and net-ready initiatives as introduced by the US Department of Defense (DoD). Traditionally quality of data delivered to an enterprise user focuses on network performance, i.e. quality of service (QoS). The QDS attribute brings distinction to the resultant data quality of the network's quality of service. There is also a need to assess the quality of data sharing across the enterprise network. The overall value of data quality on enterprise networks is decided using a minimax decision model consisting of the three attributes. The resultant minimax value correlates to the lowest performing attributes of the framework. The presented framework offers decision support tools to enable agencies to allocate limited resources towards improving the performance of their net-centric service offerings to the enterprise network. That same data abundance challenges the network capacity and overloads the capacity of the human operator. A movement to shift from "platform-centric warfare" to "network-centric warfare" was initiated in the final years of the 20 th century  [1] . All of this is in addition to the historical problems of network management and quality of service. When discussing the theoretical, it is easy to dismiss the challenges of limited resources to implement new policies such as schedule, budget, manpower, etc. Traditional QoS expects prioritization has occurred prior to data entering the network. Thus the term data relevancy (DR) is introduced into the model for valuing data quality in the context of net-centric / net-ready. Section 2 of this paper provides a brief overview of the origins of net-centric and net-ready policies. Section 3 defines a new model for valuing the quality of data by measuring user satisfaction. Section 4 presents a method for applying the new model attributes to evaluate data quality across the enterprise. Vital to the value of NCW is "the content, quality, and timeliness of information moving between nodes on the [enterprise] network." This directive and the subsequent series of 8320 series documents identify "the cornerstones of Net-Centric Data sharing"; data shall be visible, accessible, understandable, and trusted  [6] . Making data visible is achieved via deployment of discovery capabilities that access and search data asset catalogs and registries in the enterprise  [6] . These enterprise catalogs and registries contain discovery metadata entries for an individual data asset or group of data assets  [6] . Untrusted data can introduce error, uncertainty, and delay into the military decision process  [8] . Net-centric operational tasks are those that "produce information, products, or services for or consume information, products, or services from external IT"  [3] . Evaluation of the data sharing enterprise requires a holistic view that considers the net-centric attributes of the data simultaneously with the quality of service for the data network. Additionally, the interdependence between cyber security and net-centric principles are indicated in the most recent update to the DoD's instruction for enterprise data sharing  [5] . 1  illustrates the mapping relationship for each enterprise attribute to various DoD objectives of net-centric, net-ready and cyber-security. Sampling users is the preferred and direct method for measuring QoE. The question arises how to get the users' opinions. Usually the MOS is formed from arithmetic mean of a population of user opinions. develop or enhance the prediction model of QoE for each attribute. For a sample of size n consider v strata with sample s k in each stratum, i.e. ∑ i=1 to v n i = n and s = U k=1 to v s k . where n ≤ N and s = {1, 2,…, n}. QDS is a subjective rating from the perspective of the end user  [11] . Traditionally these QoE MOS ratings were undertaken by panels of experts. To produce a subjective rating MOS, or QoE, the ITU-T P.862.1  [13]  is used to map raw PESQ to the final rating. A standard to address the rating of motion imagery (i.e. Over the last several decades there have been many papers on the topic of QoS. But just for completeness a brief explanation of QoS is given here. QoS is in essence an engineering optimization problem where the objective is to maximize users' satisfaction while minimizing cost of delivery of the supporting network services. The most widely deployed QoS architecture used to deliver a SLA on an enterprise IP/MPLS network is referred to as a differentiated service (Diffserv)  [11] . However there are a number of challenges to QoE discussed in  [19]  and  [20] . To understand cause and effect it is ideal to have the full reference i.e. Models have been developed to correlate the QoS with QoE for multimedia applications  [12] . For both models  [21]  showed QoE had an exponential model in terms of their QoS objective metric(s). A number of authors  [12, 15] The measure of data relevance expresses the utility provided by the data towards the consumer's objective(s). A scalar measure is required to support evaluating the degree of relevance on the enterprise performance. The three principal elements of data relevance and their respective effects on the QoE for data relevance are now discussed, as depicted in  Fig. Intrinsic data relevance represents the relative value (i.e. Enterprise data systems can offer multiple forms of data to the consumer (e.g. overhead, side-view, rear-view, distant, near). target while in port, target while in open ocean, target when first detected, target after engagement). The goal of the tagging and discovery process is to connect highly relevant data with an authorized consumer. metadata) with those of the producer. (2) A high user satisfaction for the enterprise data quality can only occur when all three attributes have high QoEs. Consider a simple example in  Table 1  to illustrate the minimax in a game theory context. Player one has three action options U, D, N to choose from. f d (s) = u HT . The x-axis of the chart in  Fig. 3  is organized to reflect the connection between QDS and QoS. The final enterprise data quality, as shown in  Fig. 3 , as formula_6 In  Fig. This paper introduced a framework to determine data quality on enterprise networks for DoD net-centric and net-ready initiatives. The framework's data quality model consists of three attributes: quality of data at the source, data relevance, and network QoS. The final value of data quality for the enterprise network was demonstrated as the attribute with the lowest score.
paper_241	C transmission, power transformer is one of the most important equipment. differential current) and can be prevented using differential protection and microcontroller based relay protection. Different methods have being suggested by the researchers and are adopted in implementing a differential protection scheme for power transformer which is microcontroller based,  [3]  designed a differential current protection of singlephase transformer using Arduino with voice alert. The protection techniques employed differential relay mechanism with Arduino. At all times, the Arduino senses the condition of the transformer. This paper is concern with the implementation of digital differential current protection of a single phase transformer using Arduino Uno microcontroller as a mechanism of differential relay. The fundamental idea of the differential current protection scheme is to provide protection to the transformer if any fault occurs in the protected zone that will cause an imbalance in the differential currents. The output power in a transformer is equal to that of the input power hence, for differential current protection of the current transformers reduces the currents at the primary and secondary sides to a measureable value and in such a way that they are equal  [5] . When there is an occurrence of internal fault an imbalanced or non-zero differential current will flow through the operating coil of the differential relay which would be substituted with a microcontroller that sends a trip signal to the relays needed to open the circuit. The differential protection scheme is concerned with the faults that arise from associated difficulties such as the magnetizing inrush current and saturation. It is transient in nature so it lasts for just a few seconds and does not cause any permanent damage to the transformer. The power transformer to be protected is connected to the main AC supply and delivered power (P=IV) to the output terminal. The power transformer primary and secondary currents are measured by the current transformers, which are to be protected by the Arduino and the circuit breaker. The step-down transformer is used to step down the primary or source voltage from 220 to 12V. The current transformers are each connected to the primary and secondary side of the step-up transformer and are used to reduce the currents to a measureable value that can be sensed by the current sensors. The Arduino microcontroller acts as differential current comparator connected to both current sensors ACS712 and as control unit in this design. The flow chart above is a brief description of how the differential protection scheme works using arduino microcontroller as the differential relay. For the load with both the 200W and 60W bulbs the current values and difference were larger than with each connected separately.
paper_251	Secondly, Now computing power or connected computing power (with network) has more demanding and significant role in almost all areas of epoch including market analysis, searching, map, accounting, medical, trading, shopping, rescue operations and many more, the list is endless. A cloud service provider has deployed and manages to sufficient number of resource that has been shared to the entire subscriber as per the load requirement of the individuals. Proposed agent based approach has provides the efficient and accurate solutions for efficient scheduling and monitoring in cloud computing. In the cloud computing. They are able to communicate i.e. social in nature, mobile i.e. autonomous and distributive in nature. SaaS (Software as a service). Platform as a Service (PaaS). monitoring and scheduling using software agent New Relic service has been subscribed. In this the java agent has been customized to meet the monitoring and scheduling of the SaaS services. Propose system has surveyed and identified the problem domain that must be addressed in context of the cloud computing and consequently present the idea of agent integration. These are following 1. Detail Objective of the proposed agent based SaaS Service. Following goals has been achieved or solved with integrating of the Mobile Agent to Cloud Computing service realization 1. For SaaS development Codenvy has been subscribed. Cloud bees service integrating a SaaS deployment on it and delivering through agent. Resources: Algorithm for Scheduling (influenced from Aneka) developed onto the Cloudbees Proposed Algorithm Step1: Register user login Step2: Sign in the user login Step3: Select multi monitoring agent in cloud environment (for each subc_request with QoS constraints) Step 4: Mapping of resources between cloud client, cloud agent and cloud service providers. Time to execute the task 3. Table 2  shows the response time of the deployed SaaS obtained results and has been compared with existing agent based method (in seconds) . Table 3  shows the user satisfaction of the deployed SaaS obtained results and has been compared with existing malty agent based method  Rather than performing analytical calculation, proposed system has been compared with the author's MABOCCF and NMABOCCF method. For evaluation of the propose agent based method data set (jsp) has been developed using jsp web pages and deployed onto the cloud evaluating the elasticity and its monitoring. Proposed agent based methods obtained result has been found satisfactory and performs better than existing available solution. Security enhancement using Agent for following attack internal attacks and DoS (Denial of Service) attack. Develop a security perimeter based on anomaly detection using Application Process Management by integrating the mobile Agent on them for the cloud computing paradigm.
paper_272	Parameters of a vacuum interrupter are essential. Moreover the switching process associated with power electronics offers many modern benefits as a static switching system when joints together by using a snubber circuits for damping chopping current and transient over voltages or other facilities  [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] . In addition the inside electroplates some characteristics can be described as followings: Non-sustained disruptive discharge voltages In general, the above characteristics are not involving associated with only vacuum switchgear itself a lone, but when connects the switchgear with power distribution transforms or synchronous motors, there are also many negative interactive processes for instance hysteresis currents, eddy currents and high voltage capacitive inside each transformer (Parasites electrostatics charged) feedback to the interrupter process. K is Boltzmann's constant (1.37x10-23 Joules/K) The constant A is approximately 6x10 5 for most metals If the high frequency accompanying re-ignitions and voltage escalation in one phase couple into other two phases, the process of virtual current chopping can occur  [6] . Virtual current chopping involves the load current in the other two phases being forced to zero by superimposed highfrequency re ignition coupled current. It is important to appreciate that while current chopping and voltage escalation can occur in a single -phase circuit, virtual current chopping is specifically a 3-phase characteristic: the effects of normal current chopping, multiple re ignition and over voltage escalation in one phase can generate surge over voltages in the second and third phases. Restrike overvoltage due to the multiple re-ignition of circuit breaker when a switching interrupter process is initiated before current zero. The process of current chopping is the premature suppression of 50Hz or 60Hz circuit current before normal current zero due to instability of the arcs in a vacuum interrupter  [6] [7] . Although the current in the vacuum interrupter can chop to zero almost instantaneously (fraction of a microsecond), the current in the load inductance-3ph coils in the power transformer cannot attain zero value instantaneously. The second transient ends up in a negative loop of current that changes the polarity to positive at just about 480 µs time scale indicates in the figure. formula_0 From equation  (4)  express in dimensionless from the current in the inductor of any parallel RLC circuit, with any degree of damping. Suppose Vo =13.8√2 KV So. Zo = √L/C = 5X104 ohms η. Transient over-voltages (450us -480us-500us). The theory of the snubber electronic circuit and other power electronic applications offer more reliable static switching process in the field of medium voltage switching system  [16] [17] [18] .
paper_294	This study therefore, looks at the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. The study sampled 377 out of the population of 20,000. The snowball sampling technique was used in selecting the subjects for the study. Mean and standard deviation statistics was used in answering the research questions. The study found that the Tiv people had various strategies they adopted in managing postharvest losses of cassava. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. This is the very core of storage and preservation. Public libraries are supposed to be conscious of ensuring that these postharvest losses management strategies are captured, acquired stored in audiovisual format. Essentially, all these were part of their management strategies for postharvest losses of cassava. The study intends to investigate the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. Specifically, the study sought to: 1. Identify Tiv management strategies of postharvest losses of cassava 2. Determined whether public library has impacted on Tiv management strategies of postharvest losses of cassava. storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. What are the Tiv management strategies for postharvest losses of cassava? In what ways does the public library impact on Tiv management strategies for postharvest losses of cassava? What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? Public library is a library that is established and managed with public funds. Public libraries are established wholly or partly from public funds. They are not restricted to any class of persons in the community but freely available to all. Public libraries anywhere they are established, is for the purpose of development. They are a reservoir of society's intellectual history, the custodians of people's knowledge and information. This is their most crucial function of all. A direct loss is disappearance of food by spillage, or consumption by insects, rodents, and birds. Peeling the cassava, grating, squeezing (demoisturizing) it and frying known as Gari Akom 4. Incidentally, these methods of cassava processing among the Tiv people practiced as strategies for the management of postharvest losses of cassava were passed down from one generation to another orally. The public library owes the people a duty to go into the hinterlands of the Tiv nation in Benue State to collect audiovisual materials on these postharvest losses management strategies. The snowball sampling was adopted in selecting the subjects. The researchers did this until they arrive at the sample size of 680. The instrument used for data collection was Questionnaire constructed by the researchers. Section B consisted of 7 variables of forms or Tiv management strategies for postharvest losses of cassava. Data were analyzed using mean and standard deviations. Importantly, on each research question, data were collected on related items in the instrument. The collected data were analyzed using mean and standard deviation. What are the Tiv management strategies for postharvest losses of cassava? To answer the research question, data were collected on Tiv management strategies for postharvest losses of cassava. The collected data were analyzed and presented in  Table 1 . From  Table 1 , it can be seen that the mean rating scores of all the items are above 2.50. In what ways does the public library impact on Tiv management strategies for postharvest losses of cassava? Table 2  shows the mean rating scores of all the items are below 2.50, which implies that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). What are the challenges faced by public libraries on storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava? To answer the research question, data were collected relating to challenges faced by public libraries on acquisition and storage of audiovisual materials on Tiv management strategies for postharvest losses of cassava. The collected data were analyzed and presented in  Table 3 . Table 3 , it can be seen that the mean rating scores of all the items are above 2.50. Based on the results in  Table 1 , the findings from the study revealed that processing cassava into Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom, Mtuhem (Base), Akpu, Kweesi or Akweesa Akom and Kamu Akom are the Tiv management strategies for postharvest losses of cassava. Table 2 , findings of the study revealed that the public library does not impact on Tiv management strategies for postharvest losses of cassava through acquisition and storage of audiovisual materials on Kpor Akom (Cassava Chips), Mwem ma Kpor (Cassava Flour), Gari Akom (peeled, grated, squeezed and fried cassava), Mtuhem ma Akom (Base), Akpu, Kweesi or Akweesa Akom (Cassava Cake) and Kamu Akom (edible cassava starch). The research discovered that the Tiv people had various strategies they adopted in managing postharvest losses of cassava. However, the public library failed to acquire and store audiovisual materials on these management strategies for postharvest losses of cassava; information that would have been used for improving and reducing postharvest losses of cassava by farmers. The study was carried out to investigate the impact of the public library through the storage of audiovisual materials on Tiv Management Strategies of Postharvest losses of Cassava for the Development of Agriculture in Benue State of Nigeria. The study found out that public library does not impact on Tiv management strategies of postharvest losses of cassava through storage of audiovisual materials on the subject matter.
paper_298	The study investigated the awareness and use of library catalogue for accessing information resources by users of Federal University of Kashere (FUK) Library. Four research questions guided the study. The instrument used for data collection was questionnaire. Poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. Guidelines on how to use catalogue and library in general should be prepared and issued to registered users. Students' skills in search catalogue can be created through library tours, orientations, seminars, lectures, library staff, handbooks and use of library studies. Kumar revealed that 26.99% respondents use the OPAC fortnightly and 22.84% weekly. Okorafor discovered that catalogue use in Latunde Odeku Medical Library was poor. In essence, students use the catalogue to enable them conduct research in the library. It is on this note that this study investigates the use of library catalogue by undergraduate student in the university library. Therefore, the study investigates the access and use of library catalogue by students of Federal University of Kashere, Gombe State. The study is designed to achieve the following objectives: a. c. To ascertain the extent of use of the library catalogue by students to access information resources. d. To identify the constraints associated with the use of library catalogue. The study is designed to answer the following research questions: a The gender breakdown is presented on  Table 1 . The question on the demographic information of the respondents was on gender presented in  Table 1 . It shows that majority of the respondents 188 (69%) were male, followed by females 84 (31%). Table 2  revealed that majority of the respondents 262 (96.3%) were aware of the existence of the library catalogue in the university library. This findings correspond with that of Oghenekaro on the "use of library catalogue in Nigerian University Libraries: A focus on Redeemer's University Library", that majority of respondents 225 (89.6%) were aware of the existence of catalogue in the library  [1] . This indicates that majority of the respondents were not aware of online public access catalogue in the library and they only use manual catalogue for information retrieval. Their responses were presented in the  Table 4  below. Table 5  indicated that majority of the respondents 158 (60%) used the library catalogue regularly when searching for materials in the library. The study found that majority of the respondents were aware of library catalogue in the library including OPAC  [12] . Table 7  revealed that 118 (45%) of the respondents use catalogue to access information for assignment, followed by those that use it to retrieve information for research 96 (37%). The results are presented in  Table 8 . Table 8  focused on the challenges associated with catalogue use for information retrieval. Higher number of the respondents indicated poor computer skills hindering navigation when searching OPAC 96 (37%) and inability to locate materials on the shelves, indicated in the catalogue as being available 58 (22%) were the challenges affecting the use of catalogue as an information retrieval tools. This finding corroborates with that of Ogbole & Morayo which revealed that (41.1%) of the respondents rarely use OPAC. The study revealed that majority of the university library users were male. Another source of frustration was reasonable number of the respondents indicated poor computer skills hindering navigation when searching OPAC and inability to locate materials on the shelves, indicated in the catalogue as being available were the challenges affecting the use of catalogue as an information retrieval tools. This will enable them gain the needed skills to use the library catalogue maximally when searching for information resources. c. Guidelines on how to use catalogue and library in general should be prepared and issued to registered users.
paper_305	This is primarily due to difficulties in uncovering uncertainties in information provided by credit applicants and also due to lack of reliable automated techniques that would improve the efficiency of manual underwriting procedures. In this paper, we report on the results of a MSc. The implementation was based on the java netbeans development platform to create an interface that was used to train a model and its subsequent use in predicting credit decisions. The results obtained proved that such a mechanism can be applied to augment manual credit appraising processes, especially where large volumes of applications are to be processed within limited timeframes. Loans constitute the cornerstone of the banking industry's financial portfolios. Therefore the screening of the customer's financial history as well as the ability to remain faithful to new financial obligations is a very significant factor before any credit decision is taken and it is a major step in reducing credit risk. Despite the increase in consumer loans defaults and competition in the banking market, most of the Kenyan commercial banks are reluctant to use artificial intelligence technologies in their decision-making routines. Generally, bank loan officers rely on traditional methods to guide them in evaluating the worthiness of loan applications. A checklist of bank rules, conventional statistical methods and personal judgment are used to evaluate loan applications. Given the absence of objectivity, such judgment is biased, ambiguous and nonlinear and humans have limited capabilities to discover useful relationships or patterns from a large volume of historical data. Further, the complexity of loan decision tools and variation between applications is an opportunity for the use of a machine learning tool to provide learning capability that does not exist in other technologies. The purpose of this study was to develop a loan decision system using the logistic regression Meta modeling algorithm -Logitboost around Java based open source software for the Kenya commercial banks. This is the first empirical research of its kind in our country that addresses in a systematic way the issue of using Meta classifiers in loan ap-plications. Further, the study champions the use of open source software tools in business intelligence applications. The general objectives of this study were to: 1) Implement the meta learning algorithm -LogitBoost to develop as system for evaluating credit applications to support loan decisions in Kenyan financial institutions 2) Outline some of the challenges of using the learning algorithm in the decision-making process for the banking industry in Kenya 3) Champion the applicability of Java as an open source software in business intelligence applications From time immemorial in the banking sector, banks have relied on the personal assessment of loan risks or on the traditional statistical methods to predict the default of loans instead of using a standardized evaluation tool. The implementation of such models would considerably improve the quality of decision making and the efficiency of credit analysis processes. The traditional credit appraising techniques based on a hybrid mixture of manual and statistical techniques such as indices and reporting, credit bureau references, post screening, fact act, multiple credit accounts and initial credit line, the manual input are definitely inadequate in modern times. Automated techniques have progressively become popular in contemporary loan appraisal processes. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced  [4] [5] [6] . One such algorithm that effectively addresses these issues is the LogitBoost Meta classifier -based on the log of the odds ratio for the dependent variable  [7, 8] . Although the model was reported to perform relatively better than models developed using other approaches; as part of the limitations and recommendation, they suggested that such a model is usually a black box and more insight the model parameters was required to make it more effective. There have been various other attempts to deal with the loan appraisal problem using various techniques  [10]  to varied degrees of success. The reported technique derives from the intuitive understanding that instead of putting all the effort on finding highly accurate base classifiers, it becomes sufficient or even desirable to use a set of weaker hypotheses. A decision stump is a decision tree with only a single root node. It works as follows: 1. Figure 1  illustrates the combination criterion. The implementation detailed lay in the use of a logistic regression that models the posterior class probabilities Pr (G = k|X = x) for the K classes. Combine it with the other previously selected v. classifiers vi. Reweight the data vii. Learn all K classifiers again, select the best, combine, viii. Repeat until you have T classifiers selected We report on cross validation as under. Select the next partition as testing and use the rest as training data. v. Classify until each partition has been used as the test set. Calculate an average performance. This strategy is similar to the use of two files as discussed earlier but relies on the learner to automatically partition a given data set into two given a split percentage The accuracy returned by the training set is 19 correctly classified instances out of 20 instances. This gives an accuracy of 19/20=95% This gives a recall value of 7/8=0.88  Figure 2 . Three options were investigated for training the algorithm namely: a) The use of single file both for training and testing the model through stratified cross validation. Finally, as stated earlier in the introduction, it is not prudent to completely rely on an automated credit appraising as some cases might require subjective interpretation and personal judgment. As a conclusion, the reported work indeed confirmed that: 1) Machine learning procedures can be applied in financial modeling applications to augment manual underwriting techniques 2) These procedures can greatly improve the efficiency of such techniques because of their ability to handle large items of data generating very useful statistics 3) This work can be improved through the use of enhanced data set pre-processing procedures, the use of a cost matrix as well as parameter tuning to settle on the most effective set for various data mining requirements.
paper_310	We are now living in the 21 st century. Now, smart phone has become the most essential thing in our daily life. We are using Arduino software to interface the Bluetooth module with microcontroller. Here in the project the Android smart phone is used as a remote control for operating the Robot. Bluetooth module, DC motors are interfaced to the Microcontroller. f) Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. h) Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. The system consists of following parts: a) Arduino UNO (ATMEGA 328P) b) Bluetooth module (HC-05) c) Smart phone d) Motor driver (l293D) e) Arduino software (version 1.8.1) The basic building blocks of the project have been described below: Microcontroller will act as the brain of the robot. Cprogram is very easy to implement for programming the Arduino UNO. The smart phone is the transmitter of this circuit. It sends the data to microcontroller through Bluetooth module. Motor driver IC is used to control the dc motors. The program is burnt in the microcontroller using burner software. When signal data arrives the Arduino the pin which corresponds to the particular input is set to high. Motor driver switches accordingly the data bit, if the data bit is low then the corresponding pin of the motor driver doesn't work else highbit then thecorrespondingpinof the motor driver is on. There are two steps of the programming. As seen from the  Figure 6 . The DC power supple feeds the Microcontroller and the Bluetooth module. Hardware of this project consists of Arduino UNO, Bluetooth module and a motor driver IC. Through the Bluetooth module for monitoring and controlling the particular motor reaches the board and process accordingly and the output of the Arduino goes to the motor driver IC and it controls the particular motor. When user sends any data to the Arduino board then the corresponding pin of Arduino goes to high state and switches the motor driver ic in the on mode. Then through the data cable we insert the commands in the microcontroller ATMEGA 328P. The instructions are sent by the smart phone. The robot can be used for surveillance.
paper_333	The diagnosis of diseases on the plant is a very important to provide large quantity and good qualitative agricultural products. There are several issues and diseases which try to decline the yield with quality. Therefore, an efficient practice of IT based solution in this domain will increases productivity and quality of Enset products. Food security is a challenge in many developing countries like Ethiopia. It is also the sector that is given an overriding focus in the government's plan for growth of the economy as a whole. Locally the plant is called Enset and botanically, it is named as Ensete ventricosum (Welw.) Ethiopia is one of the grand producer of Enset in African continent countries. There are several issues and diseases which tries to decline the yield with quality. There is a potential need for technology supported alternative systems to support the manual identification of Enset crop diseases so as to optimize the accuracy for remedial action. If the Enset crop diseases can be minimized then definitely the production will significantly be increased to contribute to the country's economy towards a new boost. Therefore, the implementation of IT based solution in the sector will have a paramount importance to facilitate the activities such as economic, social and ecologic development in the country by increasing efficiency in both quality and quantity of Enset crops production, to sustain dependability of customer preferences and to preserve the ecology  [2] . The remaining part of this paper is organized as follows. In Section III, we present the architecture of proposed system. Experimental results are reported in Section IV. Section V presents the conclusion and discusses of future works. Figure 2  shows the architecture of the proposed system A. Dataset Partitioning The images of the dataset are collected from different parts of southern Ethiopia. This can help the farmers to produce a good quality and quantity Enset product.
paper_389	In this paper, a large number of experiments have been carried out using conditional random fields. The experimental corpus has been tested by Changjiang Daily for many years. Word probability, the paper explores the probability characteristic of word location. Peng F establishes a Chinese character segmentation model based on CRF. Enter the sentence as "This is Wuhan." 2, marking O cannot appear behind the mark I, can only be O or B. 3, Mark B can only be followed by the mark I. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. Combinations: A combination of 2 or 5 different word or string features at different positions. For example, "the previous word is a number, the current word is a quantifier", "the second word of the current word is the number, the first word is the number, the current word is the number, the latter number is the number, Two words are time words "and other characteristics. Using the CRF model generated by the training, the test corpus is tested and a prediction result is obtained. The corpus is from 1950 to 2005, and the scale is 2564168000 sentences. The corpus content mainly comes from newspaper news. In different applications, the performance requirements of the word segmentation system have different emphases. And then use these tools and the corpus carried out a number of experiments.
paper_391	Globally, tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually. Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The WEKA 3.6.1 tool was used for classification, clustering, association and attribute selection. But, in the class attribute, it is 0.72. Tetanus is caused by a toxin produced during the anaerobic growth of Clostridium tetani. Globally tetanus is responsible for 5% of maternal deaths and 14% of neonatal deaths annually, accounting for up to 25% neonate's death in some African countries  [3] [4] [5] . Data mining is the process of discovering interesting patterns and knowledge from large amounts of data. The information is rich and massive. Preprocessing solves issues about noise, incomplete and inconsistent data. The data used in this investigation are the TT immunization data. It has a dimension of 7033 rows and 12 columns. Finally, data have been saved in ". csv" file formats and stored as an ". How does this classification work? The approaches are; (a). And the topmost node in a tree is the root node. The training tuples are described by m attributes. Each of the tuples represents a point in an m-dimensional space. This assumption is called class conditional independence. This is for assessing how "accurate" your classifier is at predicting the class label of tuples. Training and testing are performed k times. "How does the k-means ( ) algorithm work?" The 5680 of mothers were from rural Ethiopia, and more of them (3484) were in the age range from 25-34. (Table 1)  As can be seen in  Figure 2 , the accuracy rate of the classifiers on training data is relatively high. Briefly described as follows: Cluster 0-This group have 1534 (27%) instances of which consist of mothers who live in the rural, no access to radio, no access to TV, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 1-This group consists of 1004 (18%) instances of which mothers who live in the rural, no access to radio, access to TV yes, orthodox in religion, Oromo in ethnic, literacy status unable to read mothers, mothers think the distance from a health facility is a big problem, husband's education no education, women age 15-24, mother marital status married, head of household male, tt-vaccinated yes. Cluster 2-This group consists of 2063 (37%) instances of which mothers who live in the rural, no access to radio, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated No. Cluster 3-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, Protestant in religion, others in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education primary, women age 25-34, mother marital status married, hh_head male, tt-vaccinated yes. Cluster 4-This group consists of 633 (11%) instances of which mothers who live in the rural, access to radio yes, no access to TV, orthodox in religion, Amhara in ethnic, mother's literacy status unable to read, mothers think the distance from a health facility is a big problem, husband's education no education, women age 35-49, mother marital status married, hh_head Female, tt-vaccinated yes.
paper_402	The BPNN and SFS were used interchangeably to identify the relevant features that contributed with the response variable. The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete. The utilization of ANN modelling made its way into the prediction of fresh and hardened properties of concrete based on given experimental input parameters, whereby several authors developed AI models to predict the compressive strength of normal weight, light weight and recycled concrete  [14] [15] [16] [17] . As a result, BPNN proved to be more accurate than SMD in the prediction of compressive strength and slump flow of UHPC. Meaning, the model does not produce any analytical model with a mathematical structure that can be studied. In addition prediction of compressive strength of high strength and high performance concrete was addressed by other researchers  [20, 21] . The reduction in the covariate domain improves the accuracy of the fitting model, decreases its computation time, and facilitates a better understanding of the data processing  [27] . Moreover, Rodriguez-Galiano et al. Four types of machine learning algorithms were used as wrappers for the SFS. There are two types of ANN models: (1) feed forward; and (2) feed backward. Equation  1 shows the linear combination of mapping weights from each input neuron, via wires, to the hidden neurons. There are two types of search algorithms: sequential forward selection and sequential backward selection. The selected features, using SFS, were analyzed by the previous BPNN model. Table 3  shows the statistical measurements calculated for both cases. Table 4  shows the coefficient values, with their corresponding symbols, for each UHPC constituent with the statistical measurements of the LSR model. The LSR model is a linear function and its form is shown in  (2) . fc = θ1C + θ2SI + θ3FA + θ4W (2)
paper_418	This paper examined the challenges in choosing between additive and mixed models in time series decomposition. Also, when there is no trend (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. They do not depend on the level of the trend  [3] . According to  [4] , the appropriate model is additive when the seasonal standard deviations show no appreciable increase or decrease relative to any increase or decrease in the seasonal means. On the other hand, the appropriate model is multiplicative when the seasonal standard deviations show appreciable increase/decrease relative to any increase /decrease in the seasonal means. The higher the trend, the more intensive these variations are. Iwueze and Akpanta  [6]  pointed out that an additive model is appropriate when the seasonal standard deviations show no appreciable increase or decrease relative to any increase or decrease in the seasonal means while a multiplicative model is usually appropriate when the seasonal standard deviations show appreciable increase/decrease relative to any increase or decrease in the seasonal means. This is an indication that the seasonal variation equals a certain percentage of the level of the time series. In such cases it is appropriate to use a multiplicative model. Oladugba et al  [9]  gave brief description of additive and multiplicative seasonality. They pointed out that any of the tests for constant variance can be used to identify a series that admits the additive model. This is an improvement over what is in existence. For additive and mixed models, 1 (a) The row means mimic the shape of the trending parameters and do not contain the seasonal effect for the additive model. 2 (a) The column means mimic the shape of the trending parameters and contain seasonal indices for additive model. (b) For mixed model, the column means mimic the shape of the trending curves of the original series and contain the seasonal indices. Estimation of Trend Parameters Row means and overall means are used to estimate parameters of the trend line. formula_10 When there is no trend ( 0 b = ) we obtain from  (Table 1)  . j j j X S a e = + (27)  From table 2, we observed that the estimates of the trend parameters and seasonal indices are not the same for both additive and mixed. Table 3  that when there is no trend i.e. (b=0), the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models. Estimates of trend parameters and seasonal indices are discussed. Also, when there is no trend ( ) 0 b = , the estimates of row, column and overall means are the same for the two models while the estimates of seasonal indices are not the same for both additive and mixed models.
paper_428	In Non-Newtonian fluids, the most frequently encountered fluids are pseudoplastic fluids, and Navier-stokes equations alone are insufficient to describe the rheological properties of these fluids, therefore, to overcome this defect, several rheological model like Ellis model, Power law model, Carreaus model and Cross model are presented, but little attention has been compensated to the Williamson fluid model and estimated to explain the rheological properties of pseudoplastic fluids. Williamson analyzed the flow of pseudoplastic materials and presented model to described the behavior of pseudoplastic material and explain convenient importance of plastic flows, and also recognized that viscous flows are very varied from plastic flows  [1] . Bau investigated the thermal convection in a saturated stratified medium bounded between two parallel eccentric cylinders with the help of a regular perturbation expansion along Daarcy-Rayleigh number; it was observed that the appropriate preference of eccentricity values can maximize the heat transfer inside annulus of various thermal insulators  [37] . Deka   is wedge angle parameter. E 0 = E 6 , E′ 0 = 0, F 0 = 1 (9) E′ ∞ = 1, F ∞ = 0 Where E 6 is injection/suction parameter. (11) (12) (13)  is solved numerically with the help of 5 th order Runge-Kutta-Fehlberg method. Further details about the obtained numerical solutions are presented in the next section. [78] , Yih  [79]  and Rashidi et al. The influence of thermal radiation is to enhance the amount of heat, while in other hand an increase in values of Prandtl number causes to decline the temperature distribution. The important conclusions of the analysis are 1.
paper_432	The mathematical analysis method used. And found some results; The theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds, possibility study of Hamiltonian tubes when the simplistic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models. Then the Cotangent Bundle has a dual space * . Generically, the corresponding Hamiltonian system `(%) =& H '%, % ). Given any , manifold , of dimensionn, with -. b) Let exp ∶ → 3 > be given by exp(% =J •" … ∈ 3 > . 8 ' : K V → R s> ( K ), satisfying R o 8 ' u 0 ' and the equivariance property. 8 ' ( K , n, ' ) = 8 ' ( K, n ). ' The twisted product × ° is the quotient of × 3 by the twist. Now consider a G on a manifold. šJ% ± ∈ , with isotropy subgroup = ² . In the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models. The symplectic reduction of the cotangent bundles * Q has more structure than a symplectic manifold. Then ∅isc , invariant surjective submersion and descends to a symplectic homeomorphic. The first result of the theory in cotangent bundles reduction, the theory developed for the problem with a single or bit type playing an important role in the solution to the general problem of a singular cotangent bundles reduction for base manifolds, Hamiltonian tubes when the symplectic manifolds is a cotangent bundles, in the concrete case of cotangent bundles there is a strong motivation coming from geometric mechanics and geometric quantization that makes it desirable to obtain explicit fiber local models and the first work studying symplectic normal forms in the specific case of cotangent bundles. Conclude that the theory of reduction of cotangent bundles developed playing an important role in solution of the general problem for reduction a single or bit type cotangent bundles for base manifolds and found that the phase space is the cotangent bundle T * Q of a configuration space .
paper_444	In this paper, for better described the uncertain resource constrained project scheduling problem, we firstly consider the uncertain resource availability project scheduling problem based on uncertainty theory. To meet the manger goals, it is assumed that the increased quantities of resource are uncertain variables and the finish time of each activity is a decision variable. The other constraint is the resource constraint in which the demand of resource shall not exceed the total supply of resource for each resource type at any time. Furthermore, the equivalent form of the above model is given and its equivalence is proved. The resource constrained project scheduling problem (RCPSP) takes into account the balance of makespan and total cost through resource allocation and reasonable activity schedules while the precedence constrains between the activities and resource constrains are all satisfied. In recent years, many scholars discussed different types of resource constrained project scheduling problems, such as multi-mode RCPSR  [1] [2] [3] , multi-project RCPSP  [4] [5] , robust RCPSP  [6] [7] , and so on. Xie  [9]  supposed that the increased quantities of resource were real-value variables, and built a multi-mode resource constrained project scheduling model for minimizing both the project makespan and cost. Its constrains were the resource and priority rules. In this paper, we consider uncertain resource availability project scheduling problem based on uncertainty theory, in which the increased quantities of resource are uncertain variables, and the finish time of each activity is a decision variable. Then, we build a multi-objective model which is under resource and precedence rule constrained to minimize the resource cost and the project completion time. For solving the above model, the equivalent form of the model is provided and the proof is given. The construction of this paper is organized as follows. (1) This paper only considers renewable resources. Then excepted value of G is formula_1 Theorem 2. If the function Q(R A , R P , ⋯ , R ) ) is strictly increase with respect to R A , R P , ⋯ , R S and strictly decreaseing with respect to R STA , R STP , ⋯ , R ) , then formula_2 holds if and only if formula_3 To solve the model (1), according to the operational law of uncertain variables, we transformed it into its equivalent form, as shown below. is an uncertain variable, and the inverse uncertainty distribution of ? is formula_7 Since ℳ6∑ ∈8 9 ≤ + ; ≥ < = , then, ℳ6∑ ∈8 9 − − ≤ 0; ≥ < = . By Theorem 2, we have ∑ ∈8 9 − − Φ MA (1 − <) ≤ 0，  (12)  i.e. For every activity , the increased quantity of resource is assumed to be a linear uncertain  variable Y(3, 7) . The duration time, cost and resource requirement of activities are presented in  Table 1 . The cost per time unit of additional resource ! The constrains are recourse constraint and precedence constraint. The information of the activities. By describing the increased quantities of resource as uncertain variables, an uncertain resource constrained project scheduling problem is discussed in this paper. To solve this model, we transformed the uncertain model into equivalent form and proved it.
paper_462	The goal of postgraduate education for master's degree in clinical medicine is to cultivate high-level medical and health professionals. Since 1998, the postgraduate education for master's degree in clinical medicine has been piloted in China. However, there are still difficulties in linking up with professional qualification certification, slow internationalization process and postgraduate students. There are many problems such as insufficient practical ability training. "5 + 3" new training mode of training in combination. Since the implementation of the new training mode, the quality of postgraduate training has been significantly improved, the employment rate of graduates has been steadily improved, the influence of the school has been expanded, and the experience for relevant units to carry out the reform of postgraduate training mode of clinical medicine master's degree has also been provided for reference. Chongqing Medical University was founded in 1956. Since 1979, it has enrolled doctoral and master's degree students. In 1998, it became the first batch of doctoral and master's degree in clinical medicine in China. Pilot units for professional degrees. Since 1998, the school has vigorously developed professional degree postgraduate education  [1] . Since 2009, the school has been focusing on reforming the training mode of clinical master degree postgraduates (hereinafter referred to as clinical master). For a long time, the medical postgraduates trained in our country are seriously lacking in practical operation ability, resulting in the embarrassing situation that "medical doctor will not see a doctor". To solve these problems, the Academic Degree Committee of the State Council officially launched the pilot work of clinical medicine professional degree in 1998. The development of postgraduate education of clinical medicine degree in China is faced with the characteristics of short time, complex training objects, multiple administrative departments involved in medical degree education, and diversified training channels  [2] . In the long-term practice of training clinical master, Chongqing Medical University has gradually explored a new "5+3" training mode which combines clinical master education with regular training, and completed the educational practice of "Innovation and Practice of the Training Mode of Clinical Master Professional Degree in China"  [3] . Educational development and other key problems of clinical master training. The system reform of training mode for clinical master of Medical University has effectively solved the problem of insufficient practical ability of clinical master, and has realized the degree education and professional qualification recognition of clinical medicine specialty. The organic cohesion of syndromes has effectively improved the quality of clinical master training. The school accurately grasps the law of professional degree postgraduate education, changes the concept of professional degree postgraduate education, and orientates the training purpose of clinical master as "training doctors who can really see a doctor", aiming to improve the clinical practice ability of postgraduates as the main objective, and is in line with the training goal, thus laying a solid foundation for the organic connection between the two. "Four syndromes in one" has greatly saved the resources of education and training, and provided qualifications guarantee for the connection of clinical medical professional degree education and vocational qualification certification. The Graduate Management Department of the Department organically integrates the training of clinical master's degree, the training of seven-year students' master's degree, and the application of resident doctors for master's degree related to the work of degree award, and arranges the clinical training and clinical competence assessment of clinical master's degree as a whole. The tutorial responsibility system is applied in postgraduate training, but after clinical master's enters the clinical rotation, many times are not in the Department where the tutor is located, and there is a problem that no one manages the professional degree students when they rotate in the clinical departments outside the department  [7] . Effective management during the transition period. In terms of setting up, a complete system of management rules for clinical master's degree has been established, and the experience of reform has been standardized and institutionalized, thus forming a long-term mechanism for linking professional degree education with professional qualification certification. It is closely related to clinical practice. System The secondary schools awarded professional degrees in clinical medicine in our university are all the standardized training bases for residents in Chongqing. Students are brought into the "two levels, two stages" training after they enter school. Requirements for the first stage of training. How to objectively and effectively assess the clinical competence of postgraduates is the key to ensure the quality of clinical medicine degree award. The main reason for this phenomenon is that since the trial implementation of clinical medical professional degree, the state has not evaluated the quality of the pilot units and lacked an effective monitoring mechanism. School research and explore more rigorous clinical training and assessment methods. The Interim Measures for the Implementation of the Regulations on Academic Degrees of the People's Republic of China promulgated in 1981 stipulates that before the postgraduate's reply, the degree-granting unit shall employ one or two experts from the disciplines concerned with the thesis to review the thesis. The clinical master emphasizes on examining the clinical competence of postgraduates. At the same time, the cost of training clinical masters has increased substantially. It stipulates that clinical master can apply for a degree only by publishing a review or case analysis. Postgraduates can devote all their energy and time to clinical ability training. Thirdly, we should formulate an enrollment system conducive to the development of professional degrees. The number of professional degree postgraduates enrolled by tutors in clinical departments is clearly required. The school has thoroughly reformed the training mode of clinical master, organically docked school education with post-graduation education and lifelong education, promoted the seamless docking of professional degree education with industry access standards, accelerated the internationalization process of higher medical personnel training mode, and formed a new systematic training mode for clinical doctors, which has great application value in the whole country. The school has set up a clinical medical degree supervision group, which is specially responsible for the supervision of the training process of clinical master's degree. It has overcome many obstacles and steadfastly promoted the reform. It has basically realized the seamless connection between the degree education of clinical medicine specialty and the certification of professional qualification. The main results are as follows: Dozens of brothers such as Fudan University learn from our experience. Many excellent students have become an indispensable new force in clinical colleges. They have achieved the ultimate goal of training "doctors who can really see a doctor". The employment rate of graduates has been guaranteed to be 100% for a long time. In December 2013, the school sponsored the National Symposium on the Reform of the Training Model of Clinical Master's Degree Postgraduates. At present, the school has completed more than 10 research reports and published more than 30 academic papers on the sub-project "Construction and Practice of Quality Assurance System for Medical Degree Postgraduates" of the Ministry of Education Innovation Project. The article "Construction and Practice of Quality Assurance System for Master of Clinical Medicine Degree Postgraduates" published in "Degree and Graduate Education" won the second prize for Excellent Thesis of "Degree and Graduate Education", and it is also the only prize-winning thesis in the field of medical science. Since the implementation of the reform of the training mode of clinical master, the combination of degree education and vocational education in our school has become closer. In the past five years, a total of 2063 clinical masters have been enrolled in the standardized resident training system. Professor Xie Peng, Professor Huang Ailong and Professor Wang Zhibiao were appointed experts of the Discipline Review Group of the Academic Degree Committee of the State Council. The "5 + 3" reform of clinical medicine master's degree in Chongqing Medical University has established a "one goal", that is, to train "doctors who really can see a doctor", and established the basic principles of the reform of training mode of clinical medicine master's degree and the training of clinicians. Chongqing Medical University combines many years of clinical master's education practice, highlights its own characteristics, gives full play to the advantages of running a school, and strives to solve the problem of linking professional degree education with vocational qualification certification. It hopes that through continuous exploration and reform, it can play a role in attracting more brothers. The unit pays attention to the degree education of clinical medicine specialty, strengthens the exchange study, improves the quality of medical higher education in our country, realizes the seamless connection between degree education of clinical medicine specialty and professional qualification certification, and explores the new mode of integrating medical education of our country with international practice.
paper_476	The inverse relationship of frequency with cost and operational efficiency becomes the key to the decision of delivery frequency. However, the increasing in delivery frequency will lead to changes in the workload of different resources (distributors, facilities, equipment, etc.) at various stages in the delivery system, which will more likely to result in uneconomical performance. At present, most express companies are operating different delivery frequencies in different regions. There are various of applications of system dynamics in the area of transportation  [2, 3] . Fan Xuemei et al. [5]  pointed out that joint delivery can effectively improve the efficiency of urban delivery. It proposed an assessment framework for joint delivery. First, it analyzes the boundary and causality of its delivery system. Then, it establishes a simulation model for the operation of the delivery system on the base of system dynamics. As the first step, this paper defines the research boundary of JDL delivery system. Secondly, it constructs subsystems of cost and resource operation efficiency for the delivery activities in the boundary. Each delivery operation mainly includes: storage, ferry, sorting, transportation and terminal delivery. The relationship of the workflows in the delivery system is shown in  Figure 1 . Based on the details of research object, this paper constructs two subsystems: delivery cost subsystem and delivery resource subsystem. According to investigation results, the cost of storage and ferry in JDL only accounted for about 8% of the average cost in daily delivery. The number of employees is affected by factors such as order quantity, transportation efficiency, delivery frequency, and the number of transport vehicles. Performance wage is determined by the actual amount of work performed during the delivery process. Sorting staffs consist of sorters and on-site logistics personnel. The sorter is the person who operates on the sorting equipment. The delivery frequency refers to the number of times of terminal delivery by the company in unit time (in days). These factors present a complex, nonlinear, and inverse relationship between each other. As is shown in  Figure 2 , 58 causal loops are formed. In this paper, four different schemes of delivery frequency were set up for simulation analysis. Different delivery frequencies have different splitting ways of total order quantity per day. On the other hand, the cost index was calculated based on the summed number of shipments as JDL adopts single-batch delivery. Setting reason for each scheme. The delivery frequency in scenarios 2 and 3 was increased by 9.5% over that in scenario 1. When the volume of orders increased from 1496 to 5300, the increased percentage of total delivery cost showed a downward trend followed by an upward trend. The total delivery cost in scenario 4 was increased by 25% in average than that in scenario 1. As the order volume increased, the increased percentage of total cost tended to decrease in fluctuation. In this situation, the delivery frequency should be increased accordingly. Within the orders volume ruled in this paper, the delivery frequency and the cumulative delivery costs are positively correlated. Compared to scenario 1, scenario 4 had an average increase of 315 yuan per day in total sorting costs. This is because that under the current total delivery volume, the order quantity after splitting in the scenario 4 was higher. Figures 12 and 13  show that the transportation cost in scenario 2 was increased by an average of 14% than that in scenario 1. The transportation cost in scenario 4 was increased by an average of 45% over scenario 1. The transportation cost in scenario 1 showed a rising trend with the increase of orders. When the order volume was 2406, 2530 and 4700, the transportation cost rose rapidly. The main reason is that in this case the number of transportation vehicles at the sorting center was fixed. As a result, the overall efficiency of the delivery staff was reduced. At the same time, the increase in the delivery frequency also improved the rental cost of the delivery station. Figures 16, 17 and 18  demonstrate that the delivery frequency hade different effects on different transportation vehicles. Figures 19, 20, 21  and 22show that the delivery frequency had different effects on terminal delivery operations. In some cases, the area utilization efficiencies in Scenario 1 and Scenario 3 were the same. This is because the difference of orders volume in the largest batch between two scenarios was small. As a result, the company had to increase the number of delivery personnel and the area of shipments. As shown in  Figures 21 and 22 , the overall utilization efficiency of delivery station A was approximately 5% higher than that of delivery station B. The results of this study indicate that under the impact of order volume there is no fixed relationship between delivery frequency with delivery cost and resource operational efficiency. With the same delivery frequency, the orders splitting ratio influences delivery cost and resource utilization efficiency significantly. With the same delivery frequency, different order splitting ratios affect delivery cost and resource operational efficiency. Recommendation II: one should consider constraints such as delivery resources and consumer satisfaction to achieve the appropriate decision of delivery frequency. With different delivery frequency, JDL's delivery resources and consumer service quality are different. Therefore, JDL needs to consider different constraints when making delivery frequency decisions. When transportation vehicles are in tight supply, the delivery frequency should be increased when the order volume is 2530. The delivery frequency has different effects on the resource operational efficiency in different delivery stages.
paper_479	Inability to model an individual's handwriting over time has made estimating a full likelihood ratio for comparative handwriting analysis impossible thereby employing nuisance parameters and subjectivity in computation of L R that is not full. With the help of handwriting model for individual writers, little or no assumptions and no nuisance parameters were employed in achieving full likelihood ratio for comparative handwriting analysis in forensic science. From the research carried out, it can be concluded that modeling an individual's handwriting is a crucial factor in achieving a full likelihood ratio, little/or no inconclusiveness in result reporting and a less degree of disagreements for handwriting identification in a forensic environment. Handwriting has remained one of the most frequently occurring patterns that we come across in everyday life. The likelihood ratio paradigm has been studied as a means for quantifying the strength of evidence for a variety of forensic evidence types in handwriting and other types of forensic evidence such as earmark, speech, footprint, fingerprint, glass fragments and DNA  [1] [2] [3] [4] [5] [6] [7] [8] . For instance, someone might have fabricated the writing with some special skills. It is straightforward for a DNA as compared to other areas of forensic evidence because estimation is done for the relative frequency of different DNA profile in relevant populations. The purpose of this paper is to illustrate how neural network approach to comparative handwriting analysis can greatly affect the outcome of a forensic investigation and make full likelihood ratio achievable. The novelty of our proposed implementation relies on natural handwriting samples over a period of six months from known individuals to form the database to model the writing profile for each writer. The writer's profile is a very important factor that is considered to accurately estimate a full likelihood ratio. Computational approaches to the handwriting facet of questioned document (QD) examination were developed with a view towards providing a scientific basis for handwriting evidence, formalizing human expert-based approaches and validating existing methodology. A statistical model for writer verification was developed; it allows computing the likelihood ratio based on a variety of different feature types. [19]  described a statistical model for the writer verification task to determine if two documents were written by the same writer, their model followed the scenario proposed by  [17]  thus suffers from the same weakness  [20]  presented one conceivable way to deal with gauge a probability proportion in near handwriting analysis was delineated. Assessment of signature handwriting evidence through score-based likelihood ratio based on comparative measurement of relevant dynamic features was carried out by  [22, 9]  presented score-based approaches to calculating forensic probability ratios that have been established as becoming more progressively common in forensic literature. Figure 1  shows the graphical representation for the handwriting modeling. Back Propagation Neural Network (BPNN) served the sake of supervised learning of the system. We target is set for each character in the handwriting. Transfer function can add non-linearity to the network. Weight from second hidden unit i and output unit j is ! ) Due to the complexity of modeling the handwriting of a writer and the absence of industrial size databases from which different handwriting can be described  [17, 9]  estimated a marginal L R when the full L R was not possible also in the presence of some parameters considered to be nuisance. Base on decision law i.e. If L R value greater than 1 H p is true If L R value less than 1 H p is false. An inconclusive state is declared if and only if L R value = 1 With the BPNN algorithm to model handwriting pattern for each writer and L R estimation described in  [10]  this paper was able to deal with the problem of inconclusiveness as there was no inconclusive results reported in the investigation, determine who the writer of a questioned document is as well as eliminate the inclusion of parameters considered nuisance in investigation because each writer's handwriting profile could be ascertained due to the BPNN training of the Handwriting categories. A more elaborate and collated result table is presented in  Table  2 . Full L R void of nuisance parameters is needed for most forensic investigators. Several factors have to be put in place to estimate a full L R for forensic handwriting investigations.
paper_492	In this paper, the model of the transfer of data between different nervous systems is shown using the concept of category theory. The use of wheels and gears, the creation of new energy from steam engines to nuclear power, and the changing information transmission methods from flags or smoke to the internet have been the catalyst for the beginning of each stage. In parallel with advances in technology, metals, chemicals, and even semiconductors have emerged as a new material for new products. But the structure of our brain has not changed since tens of thousands of years ago. Although for learning process Hebb rules is used on the circuits, operations such as back propagation and Markov process are not used. In this paper, the model of the transfer of data between different nervous systems is shown using the concept of category theory. The same is true for the recognition process. The same applies to general figures. Many East Asian recognize kanji as a combination of parts. In this chapter, it is presented that arbitrary time series data can be divide into basic sequences as the logical basis of neural networks. Finally, it shows the hierarchically connected neural network that can process for general time series data. Any time series data consisted finite type of element can be divided into multiple subsequences where the same element does not appear more than once. For simplicity, assume time series data consisted of 10 type elements from a 0 to a 9 . The dividing is done by the following procedure. In given example, the leading element is a1, followed by a7, a4, a6 and a6. When the following conditions are true, the element allocate to the top of new subsequence. In this example a6 is the concerned element. (3) If the maximum length of subsequence is defined, a new subsequence is allocated after the subsequence that reaches the maximum length, add new element to the concerned new subsequence. Figure 2  shows the affinity with the neural circuit. It is a neural network having an input consisting of a plurality of bits are shown. When the first data c 0 is received activate the bottom. For the next data c 1 additionally activates the elements which has been activated by the first data c 0 . In the neural network shown in  Figure 2 , the elements are activated one after another by the time series data (in this case, the basic subsequence) from below, and the result is output upward. Therefore, the couplings between the elements are enhanced (Hebb rule) by repeating this conversion, as a result the elements involved in the conversion will be activated by only receiving the first element of the time series data. After the first data reception, the connected elements are activated as described above. That is, the operation with no reaction within waiting time is invalid and aborted. Both state transition diagram is shown in the existing  Figure 3 . This neural network is called a basic unit. Since general time series data is consisted of hierarchy of basic subsequences, basic units can process general time series data by identifying the outputs of lower layer basic units as the new time series data. Received data comes from the environment that encourages some judgment. In the  Figure 4  activity changes of each unit on processing the time series data of eating behavior is shown. The ability to imitate fellow's action is indispensable. For example, a family might have conversation like this toward you who is about to have a birthday the next day. Like talking about your childhood while eating cake. Figure 5  is an illustration for showing the state change of each part in the neural network. Stimulus from the sensory organs from the bottom becomes time series data and is transmitted to the upper part. As a result, the shape of the chocolate in the episodic memory is identified as the thing seen in front. The following is a description of  Figure 6  associated with the definition of the category shown in Tom Leinster "Basic Category Theory"  [4] . Next, we introduce a function called functor between different categories. The process of migration can be described mathematically using free functor or free construction functor. The process of taking the consistency between the two memories was explained using the idea of a free functor in category theory. From an engineering point of view, the basic unit can be realized by a small microprocessor, and the neural network consists of multiple randomly connected units. Each unit has the same function, but the connections between the units are not the same. This process is close to the rehabilitation process of the brain that has had a stroke. There may be cases of errors in the accuracy of the operation compared to the circuit using the existing logic IC because there is a probabilistic part, but the bud of a new strategy might be hidden in the vicinity of the malfunction. From the viewpoint of neuroscience, even if there are no "parts" equivalent to the basic unit in the process of stimulation from the sensory organs propagating to the cerebral cortex, axons parallel to the propagation direction connect between the layers, and it can be said that it is a passage of serial parallel mutually converted information.
paper_507	The main aim of present study is to integrate the result of our study with spatial data, soil parameters, land inventory and used the output as a user friendly application using GIS which could predict the future susceptibility of region to landslide and% contribution of each factor for the same. Efficiency of the application will be calculated by the help of previously acquired data of the study area at different places and then the reliability of the application will be judged. Uttarakhand is an appropriate choice for the study since the newly developed state has been fighting with the catastrophe and is making front paper headlines for landslide at Vishnuprayag, Baldora, Lambagharchatti, Jharkula, phatabyung, and Amiya landslides  [4] . These map divides the land into homogeneous areas or domain and their ranking according to degree of actual hazard potential caused by mass movement  (Guzzetti et. al, 1999 and varnes 1984)    [7, 8, 9] . In the present study with the help of satellite imageries like DEM from cartosat-1 and topographic maps from GSI (geographical survey of India) a susceptibility map is prepared. With the help of ANN model, we generated weightage for each factor and using this the hazard zonation map is produced  [10, 11] . The result obtained i.e. Preparation of a landslide hazard zonation map that would divide Uttarkashi into different zones depending upon the factors. In the present paper landslide hazard zonation map has been prepared for the Rishikesh-Uttarkashi-Gaumukh-Gangotri. Therefore, the relation between landslide occurrence and the conditioning parameters used is crucially important for landslide susceptibility mapping. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area (Mohammad Onagh, 2012). Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. Digitization for the various shape files has then being done by retrieving the concerned shape file and the map form which digitization has to be done. In the present study we selected 107 points and all the six factors namely soil depth, soil type, rock type, land cover, slope and elevation and a excel database is created. The dataset is categorized into 60% training and 40% validation. The data is categorized into training, testing and validation, and all the six factors namely soil depth, soil type, rock and a excel database is created. It may be possible that any parameter is important with respect to landslide occurrence for the given area but it is also possible that the importance of same parameter is negligible for another area  [13] . Thus a number of thematic maps (referred to as data layer in GIS) based on the specific parameters which are related to occurrence of landslide viz. The dataset is categorized into 60% training and 40% validation. The study has to led the determination of factors on the basis of past studies and determination of weightage for the chosen six factors namely soil depth, soil texture, rock type, height, slope and land cover. The result of present study with spatial data, soil parameters, land inventory and presented by a landslide hazard zonation map and a user friendly application using GIS that could predict the future susceptibility of region to landslide and percentage contribution of each factor for the same.
