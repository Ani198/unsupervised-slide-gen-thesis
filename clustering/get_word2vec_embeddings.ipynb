{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import sent_tokenize\n",
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_vocab(paper_tokenized_text):\n",
    "    vocab = set()\n",
    "    for sent in paper_tokenized_text:\n",
    "        vocab = vocab.union(set(sent))\n",
    "    return vocab\n",
    "\n",
    "def common_words_num(vocab, new_model, base_model):\n",
    "    words_num = 0\n",
    "    for i in vocab:\n",
    "        if (i in new_model) or (i in base_model):\n",
    "            words_num += 1\n",
    "    return words_num\n",
    "\n",
    "def get_sentence_embeddings(sentence, new_model, base_model):\n",
    "    word_embeddings = []\n",
    "    for word in set(sentence):\n",
    "        if word in new_model:\n",
    "            word_emb = list(new_model[word])\n",
    "            word_embeddings.append(word_emb)\n",
    "        elif word in base_model:\n",
    "            word_emb = list(base_model[word]) \n",
    "            word_embeddings.append(word_emb)\n",
    "    return word_embeddings\n",
    "\n",
    "def get_embeddings_for_paper(paper_text, new_model, base_model):\n",
    "    sentence_embeddings = []\n",
    "    for i, sentence in enumerate(paper_text):\n",
    "        sentence_emb_matrix = np.array(get_sentence_embeddings(sentence, new_model, base_model))\n",
    "        \n",
    "        if sentence_emb_matrix.shape[0] == 0:\n",
    "            sentence_embeddings.append(list(np.random.uniform(-1, 1, 300)))\n",
    "        else:\n",
    "            sentence_embeddings.append(list(np.mean(sentence_emb_matrix, axis = 0)))\n",
    "    \n",
    "    print((len(sentence_embeddings), len(sentence_embeddings[0])))\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepairing Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/papers_dataset.csv')\n",
    "dataset.keywords = dataset.keywords.apply(ast.literal_eval)\n",
    "dataset.sections = dataset.sections.apply(ast.literal_eval)\n",
    "\n",
    "test_df = dataset[dataset.partition == 'test']\n",
    "print(test_df.shape)\n",
    "\n",
    "texts = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    text = row['abstract']\n",
    "    for i in row['sections'].values():\n",
    "        text += ' ' + i\n",
    "    texts.append(text)\n",
    "    \n",
    "test_df['text'] = texts\n",
    "test_df['sentences'] = test_df.text.apply(sent_tokenize)\n",
    "test_df['sentences'] = test_df['sentences'].apply(lambda x: [sent for sent in x if len(sent.split()) > 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7004\n"
     ]
    }
   ],
   "source": [
    "sentences = test_df.sentences.values\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "sentences_tokenized = [[re.sub(r'\\[.*?\\]', '', re.sub(r'\\d+', '', w)).lower() for w in sent] for sent in sentences]\n",
    "sentences_tokenized = [[tokenizer.tokenize(i) for i in sent] for sent in sentences_tokenized]\n",
    "sentences_tokenized = [[[w for w in sentence if not w in stop_words] for sentence in paper] for paper in sentences_tokenized]\n",
    "\n",
    "tok_test_vocab = set()\n",
    "for sent in sentences_tokenized:\n",
    "    for i in sent:\n",
    "        tok_test_vocab = tok_test_vocab.union(set(i))\n",
    "print(len(tok_test_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_df['sentences_tokenized'] = sentences_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>category</th>\n",
       "      <th>name</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_formulas</th>\n",
       "      <th>num_figures</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>sections</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>sent_by_page</th>\n",
       "      <th>partition</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentences_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paper_1</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>A Model for Clustering Social Media Data for E...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>A Model for Clustering Social Media Data for E...</td>\n",
       "      <td>[Social Media, Twitter Application Programming...</td>\n",
       "      <td>Through Social media, people are able to write...</td>\n",
       "      <td>{'Introduction': 'Clustering is a descriptive ...</td>\n",
       "      <td>76</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>test</td>\n",
       "      <td>Through Social media, people are able to write...</td>\n",
       "      <td>[Through Social media, people are able to writ...</td>\n",
       "      <td>[[social, media, people, able, write, short, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paper_2</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>An Intelligent System for Traffic Control in S...</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>An Intelligent System for Traffic Control in S...</td>\n",
       "      <td>[Smart Cities, Traffic Congestion, Intelligent...</td>\n",
       "      <td>Current traffic light systems use a fixed time...</td>\n",
       "      <td>{'Introduction': '', 'Background': 'Traffic co...</td>\n",
       "      <td>99</td>\n",
       "      <td>14.142857</td>\n",
       "      <td>test</td>\n",
       "      <td>Current traffic light systems use a fixed time...</td>\n",
       "      <td>[Current traffic light systems use a fixed tim...</td>\n",
       "      <td>[[current, traffic, light, systems, use, fixed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paper_3</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Architecture Trends of Adaptive Educational Hy...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>Architecture Trends of Adaptive Educational Hy...</td>\n",
       "      <td>[Adaptive Educational Hypermedia Systems, Arch...</td>\n",
       "      <td>The aim of this article is to present the gene...</td>\n",
       "      <td>{'Introduction': 'Adaptive Hypermedia Educatio...</td>\n",
       "      <td>203</td>\n",
       "      <td>15.615385</td>\n",
       "      <td>test</td>\n",
       "      <td>The aim of this article is to present the gene...</td>\n",
       "      <td>[The aim of this article is to present the gen...</td>\n",
       "      <td>[[aim, article, present, general, architecture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>paper_21</td>\n",
       "      <td>Computer Science and Technology</td>\n",
       "      <td>A New Powerful Scheme Based on Self Invertible...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A New Powerful Scheme Based on Self Invertible...</td>\n",
       "      <td>[Minimum Distance, Minimum Weight, BCH Codes, ...</td>\n",
       "      <td>In this paper, we present the powerful scheme ...</td>\n",
       "      <td>{'Introduction': 'In telecommunication and sto...</td>\n",
       "      <td>60</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>test</td>\n",
       "      <td>In this paper, we present the powerful scheme ...</td>\n",
       "      <td>[In this paper, we present the powerful scheme...</td>\n",
       "      <td>[[paper, present, powerful, scheme, zsismp, zi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>paper_31</td>\n",
       "      <td>Computer Science and Technology</td>\n",
       "      <td>Design and Implementation of Intelligent Medic...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>Design and Implementation of Intelligent Medic...</td>\n",
       "      <td>[Smart Medical Care, ZigBee, Semantic Matching]</td>\n",
       "      <td>With the continuous improvement of human livin...</td>\n",
       "      <td>{'Introduction': 'From 1990 to 2017, the morbi...</td>\n",
       "      <td>124</td>\n",
       "      <td>24.800000</td>\n",
       "      <td>test</td>\n",
       "      <td>With the continuous improvement of human livin...</td>\n",
       "      <td>[With the continuous improvement of human livi...</td>\n",
       "      <td>[[continuous, improvement, human, living, cond...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    paper_id                         category  \\\n",
       "1    paper_1          Artificial Intelligence   \n",
       "2    paper_2          Artificial Intelligence   \n",
       "3    paper_3          Artificial Intelligence   \n",
       "21  paper_21  Computer Science and Technology   \n",
       "31  paper_31  Computer Science and Technology   \n",
       "\n",
       "                                                 name  num_pages  \\\n",
       "1   A Model for Clustering Social Media Data for E...          4   \n",
       "2   An Intelligent System for Traffic Control in S...          8   \n",
       "3   Architecture Trends of Adaptive Educational Hy...         14   \n",
       "21  A New Powerful Scheme Based on Self Invertible...          5   \n",
       "31  Design and Implementation of Intelligent Medic...          6   \n",
       "\n",
       "    num_formulas  num_figures  \\\n",
       "1              0            4   \n",
       "2              5            9   \n",
       "3              0           12   \n",
       "21             1            2   \n",
       "31             2           10   \n",
       "\n",
       "                                                title  \\\n",
       "1   A Model for Clustering Social Media Data for E...   \n",
       "2   An Intelligent System for Traffic Control in S...   \n",
       "3   Architecture Trends of Adaptive Educational Hy...   \n",
       "21  A New Powerful Scheme Based on Self Invertible...   \n",
       "31  Design and Implementation of Intelligent Medic...   \n",
       "\n",
       "                                             keywords  \\\n",
       "1   [Social Media, Twitter Application Programming...   \n",
       "2   [Smart Cities, Traffic Congestion, Intelligent...   \n",
       "3   [Adaptive Educational Hypermedia Systems, Arch...   \n",
       "21  [Minimum Distance, Minimum Weight, BCH Codes, ...   \n",
       "31    [Smart Medical Care, ZigBee, Semantic Matching]   \n",
       "\n",
       "                                             abstract  \\\n",
       "1   Through Social media, people are able to write...   \n",
       "2   Current traffic light systems use a fixed time...   \n",
       "3   The aim of this article is to present the gene...   \n",
       "21  In this paper, we present the powerful scheme ...   \n",
       "31  With the continuous improvement of human livin...   \n",
       "\n",
       "                                             sections  num_sentences  \\\n",
       "1   {'Introduction': 'Clustering is a descriptive ...             76   \n",
       "2   {'Introduction': '', 'Background': 'Traffic co...             99   \n",
       "3   {'Introduction': 'Adaptive Hypermedia Educatio...            203   \n",
       "21  {'Introduction': 'In telecommunication and sto...             60   \n",
       "31  {'Introduction': 'From 1990 to 2017, the morbi...            124   \n",
       "\n",
       "    sent_by_page partition                                               text  \\\n",
       "1      25.333333      test  Through Social media, people are able to write...   \n",
       "2      14.142857      test  Current traffic light systems use a fixed time...   \n",
       "3      15.615385      test  The aim of this article is to present the gene...   \n",
       "21     15.000000      test  In this paper, we present the powerful scheme ...   \n",
       "31     24.800000      test  With the continuous improvement of human livin...   \n",
       "\n",
       "                                            sentences  \\\n",
       "1   [Through Social media, people are able to writ...   \n",
       "2   [Current traffic light systems use a fixed tim...   \n",
       "3   [The aim of this article is to present the gen...   \n",
       "21  [In this paper, we present the powerful scheme...   \n",
       "31  [With the continuous improvement of human livi...   \n",
       "\n",
       "                                  sentences_tokenized  \n",
       "1   [[social, media, people, able, write, short, m...  \n",
       "2   [[current, traffic, light, systems, use, fixed...  \n",
       "3   [[aim, article, present, general, architecture...  \n",
       "21  [[paper, present, powerful, scheme, zsismp, zi...  \n",
       "31  [[continuous, improvement, human, living, cond...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Word2vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 03:31:37: loading projection weights from ../Fine-Tuning-Word2Vec-Embeddings/GoogleNews-vectors-negative300.bin\n",
      "INFO - 03:33:12: KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from ../Fine-Tuning-Word2Vec-Embeddings/GoogleNews-vectors-negative300.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2021-05-10T03:33:12.358454', 'gensim': '4.0.1', 'python': '3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'load_word2vec_format'}\n",
      "INFO - 03:33:13: loading projection weights from ../Fine-Tuning-Word2Vec-Embeddings/new_word2vec.model\n",
      "INFO - 03:33:49: KeyedVectors lifecycle event {'msg': 'loaded (29222, 300) matrix of type float32 from ../Fine-Tuning-Word2Vec-Embeddings/new_word2vec.model', 'binary': False, 'encoding': 'utf8', 'datetime': '2021-05-10T03:33:49.386476', 'gensim': '4.0.1', 'python': '3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "base_model = KeyedVectors.load_word2vec_format('../Fine-Tuning-Word2Vec-Embeddings/GoogleNews-vectors-negative300.bin', \n",
    "                                               binary=True)\n",
    "new_model = KeyedVectors.load_word2vec_format(\"../Fine-Tuning-Word2Vec-Embeddings/new_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "(76, 300)\n",
      "100.0\n",
      "------------------------------\n",
      "96\n",
      "(96, 300)\n",
      "98.00332778702163\n",
      "------------------------------\n",
      "202\n",
      "(202, 300)\n",
      "97.84172661870504\n",
      "------------------------------\n",
      "58\n",
      "(58, 300)\n",
      "95.94594594594594\n",
      "------------------------------\n",
      "118\n",
      "(118, 300)\n",
      "98.34710743801654\n",
      "------------------------------\n",
      "182\n",
      "(182, 300)\n",
      "97.279792746114\n",
      "------------------------------\n",
      "122\n",
      "(122, 300)\n",
      "99.79550102249489\n",
      "------------------------------\n",
      "108\n",
      "(108, 300)\n",
      "79.82062780269058\n",
      "------------------------------\n",
      "66\n",
      "(66, 300)\n",
      "98.36734693877551\n",
      "------------------------------\n",
      "99\n",
      "(99, 300)\n",
      "99.00332225913621\n",
      "------------------------------\n",
      "45\n",
      "(45, 300)\n",
      "96.06299212598425\n",
      "------------------------------\n",
      "159\n",
      "(159, 300)\n",
      "99.77876106194691\n",
      "------------------------------\n",
      "133\n",
      "(133, 300)\n",
      "97.31012658227847\n",
      "------------------------------\n",
      "118\n",
      "(118, 300)\n",
      "98.75346260387812\n",
      "------------------------------\n",
      "96\n",
      "(96, 300)\n",
      "99.11111111111111\n",
      "------------------------------\n",
      "194\n",
      "(194, 300)\n",
      "98.49699398797596\n",
      "------------------------------\n",
      "48\n",
      "(48, 300)\n",
      "98.48484848484848\n",
      "------------------------------\n",
      "98\n",
      "(98, 300)\n",
      "98.18181818181819\n",
      "------------------------------\n",
      "57\n",
      "(57, 300)\n",
      "97.64243614931237\n",
      "------------------------------\n",
      "145\n",
      "(145, 300)\n",
      "92.81129653401797\n",
      "------------------------------\n",
      "133\n",
      "(133, 300)\n",
      "96.0691823899371\n",
      "------------------------------\n",
      "118\n",
      "(118, 300)\n",
      "99.62073324905182\n",
      "------------------------------\n",
      "110\n",
      "(110, 300)\n",
      "98.59943977591037\n",
      "------------------------------\n",
      "59\n",
      "(59, 300)\n",
      "97.6923076923077\n",
      "------------------------------\n",
      "76\n",
      "(76, 300)\n",
      "97.85714285714285\n",
      "------------------------------\n",
      "143\n",
      "(143, 300)\n",
      "98.7603305785124\n",
      "------------------------------\n",
      "87\n",
      "(87, 300)\n",
      "98.08429118773945\n",
      "------------------------------\n",
      "68\n",
      "(68, 300)\n",
      "97.75280898876404\n",
      "------------------------------\n",
      "71\n",
      "(71, 300)\n",
      "96.3302752293578\n",
      "------------------------------\n",
      "76\n",
      "(76, 300)\n",
      "92.56756756756756\n",
      "------------------------------\n",
      "86\n",
      "(86, 300)\n",
      "98.28571428571429\n",
      "------------------------------\n",
      "168\n",
      "(168, 300)\n",
      "99.23469387755102\n",
      "------------------------------\n",
      "163\n",
      "(163, 300)\n",
      "98.73217115689383\n",
      "------------------------------\n",
      "103\n",
      "(103, 300)\n",
      "99.71949509116409\n",
      "------------------------------\n",
      "175\n",
      "(175, 300)\n",
      "99.48387096774194\n",
      "------------------------------\n",
      "116\n",
      "(116, 300)\n",
      "94.7098976109215\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for paper_text in test_df.sentences_tokenized.values:\n",
    "    print(len(paper_text))\n",
    "    get_embeddings_for_paper(paper_text, new_model, base_model)\n",
    "\n",
    "    paper_vocab = get_paper_vocab(paper_text)\n",
    "    print(common_words_num(paper_vocab, new_model, base_model)/len(paper_vocab)*100)\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76, 300)\n",
      "(96, 300)\n",
      "(202, 300)\n",
      "(58, 300)\n",
      "(118, 300)\n",
      "(182, 300)\n",
      "(122, 300)\n",
      "(108, 300)\n",
      "(66, 300)\n",
      "(99, 300)\n",
      "(45, 300)\n",
      "(159, 300)\n",
      "(133, 300)\n",
      "(118, 300)\n",
      "(96, 300)\n",
      "(194, 300)\n",
      "(48, 300)\n",
      "(98, 300)\n",
      "(57, 300)\n",
      "(145, 300)\n",
      "(133, 300)\n",
      "(118, 300)\n",
      "(110, 300)\n",
      "(59, 300)\n",
      "(76, 300)\n",
      "(143, 300)\n",
      "(87, 300)\n",
      "(68, 300)\n",
      "(71, 300)\n",
      "(76, 300)\n",
      "(86, 300)\n",
      "(168, 300)\n",
      "(163, 300)\n",
      "(103, 300)\n",
      "(175, 300)\n",
      "(116, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_df['sentence_embeddings'] = test_df.sentences_tokenized.apply(lambda x: \n",
    "                                                                   get_embeddings_for_paper(x, new_model, base_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>category</th>\n",
       "      <th>name</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_formulas</th>\n",
       "      <th>num_figures</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>sections</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>sent_by_page</th>\n",
       "      <th>partition</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentences_tokenized</th>\n",
       "      <th>sentence_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paper_1</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>A Model for Clustering Social Media Data for E...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>A Model for Clustering Social Media Data for E...</td>\n",
       "      <td>[Social Media, Twitter Application Programming...</td>\n",
       "      <td>Through Social media, people are able to write...</td>\n",
       "      <td>{'Introduction': 'Clustering is a descriptive ...</td>\n",
       "      <td>76</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>test</td>\n",
       "      <td>Through Social media, people are able to write...</td>\n",
       "      <td>[Through Social media, people are able to writ...</td>\n",
       "      <td>[[social, media, people, able, write, short, m...</td>\n",
       "      <td>[[0.25343436, 0.012410111, -0.34438372, 0.3019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paper_2</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>An Intelligent System for Traffic Control in S...</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>An Intelligent System for Traffic Control in S...</td>\n",
       "      <td>[Smart Cities, Traffic Congestion, Intelligent...</td>\n",
       "      <td>Current traffic light systems use a fixed time...</td>\n",
       "      <td>{'Introduction': '', 'Background': 'Traffic co...</td>\n",
       "      <td>99</td>\n",
       "      <td>14.142857</td>\n",
       "      <td>test</td>\n",
       "      <td>Current traffic light systems use a fixed time...</td>\n",
       "      <td>[Current traffic light systems use a fixed tim...</td>\n",
       "      <td>[[current, traffic, light, systems, use, fixed...</td>\n",
       "      <td>[[0.08013701, -0.043167926, 0.019797344, 0.273...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paper_3</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Architecture Trends of Adaptive Educational Hy...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>Architecture Trends of Adaptive Educational Hy...</td>\n",
       "      <td>[Adaptive Educational Hypermedia Systems, Arch...</td>\n",
       "      <td>The aim of this article is to present the gene...</td>\n",
       "      <td>{'Introduction': 'Adaptive Hypermedia Educatio...</td>\n",
       "      <td>203</td>\n",
       "      <td>15.615385</td>\n",
       "      <td>test</td>\n",
       "      <td>The aim of this article is to present the gene...</td>\n",
       "      <td>[The aim of this article is to present the gen...</td>\n",
       "      <td>[[aim, article, present, general, architecture...</td>\n",
       "      <td>[[0.13080898, -0.11806064, -0.047516536, 0.097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>paper_21</td>\n",
       "      <td>Computer Science and Technology</td>\n",
       "      <td>A New Powerful Scheme Based on Self Invertible...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A New Powerful Scheme Based on Self Invertible...</td>\n",
       "      <td>[Minimum Distance, Minimum Weight, BCH Codes, ...</td>\n",
       "      <td>In this paper, we present the powerful scheme ...</td>\n",
       "      <td>{'Introduction': 'In telecommunication and sto...</td>\n",
       "      <td>60</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>test</td>\n",
       "      <td>In this paper, we present the powerful scheme ...</td>\n",
       "      <td>[In this paper, we present the powerful scheme...</td>\n",
       "      <td>[[paper, present, powerful, scheme, zsismp, zi...</td>\n",
       "      <td>[[0.22547568, 0.05606357, -0.1207298, 0.160863...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>paper_31</td>\n",
       "      <td>Computer Science and Technology</td>\n",
       "      <td>Design and Implementation of Intelligent Medic...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>Design and Implementation of Intelligent Medic...</td>\n",
       "      <td>[Smart Medical Care, ZigBee, Semantic Matching]</td>\n",
       "      <td>With the continuous improvement of human livin...</td>\n",
       "      <td>{'Introduction': 'From 1990 to 2017, the morbi...</td>\n",
       "      <td>124</td>\n",
       "      <td>24.800000</td>\n",
       "      <td>test</td>\n",
       "      <td>With the continuous improvement of human livin...</td>\n",
       "      <td>[With the continuous improvement of human livi...</td>\n",
       "      <td>[[continuous, improvement, human, living, cond...</td>\n",
       "      <td>[[-0.21172313, 0.35368863, -0.16834831, 0.2698...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    paper_id                         category  \\\n",
       "1    paper_1          Artificial Intelligence   \n",
       "2    paper_2          Artificial Intelligence   \n",
       "3    paper_3          Artificial Intelligence   \n",
       "21  paper_21  Computer Science and Technology   \n",
       "31  paper_31  Computer Science and Technology   \n",
       "\n",
       "                                                 name  num_pages  \\\n",
       "1   A Model for Clustering Social Media Data for E...          4   \n",
       "2   An Intelligent System for Traffic Control in S...          8   \n",
       "3   Architecture Trends of Adaptive Educational Hy...         14   \n",
       "21  A New Powerful Scheme Based on Self Invertible...          5   \n",
       "31  Design and Implementation of Intelligent Medic...          6   \n",
       "\n",
       "    num_formulas  num_figures  \\\n",
       "1              0            4   \n",
       "2              5            9   \n",
       "3              0           12   \n",
       "21             1            2   \n",
       "31             2           10   \n",
       "\n",
       "                                                title  \\\n",
       "1   A Model for Clustering Social Media Data for E...   \n",
       "2   An Intelligent System for Traffic Control in S...   \n",
       "3   Architecture Trends of Adaptive Educational Hy...   \n",
       "21  A New Powerful Scheme Based on Self Invertible...   \n",
       "31  Design and Implementation of Intelligent Medic...   \n",
       "\n",
       "                                             keywords  \\\n",
       "1   [Social Media, Twitter Application Programming...   \n",
       "2   [Smart Cities, Traffic Congestion, Intelligent...   \n",
       "3   [Adaptive Educational Hypermedia Systems, Arch...   \n",
       "21  [Minimum Distance, Minimum Weight, BCH Codes, ...   \n",
       "31    [Smart Medical Care, ZigBee, Semantic Matching]   \n",
       "\n",
       "                                             abstract  \\\n",
       "1   Through Social media, people are able to write...   \n",
       "2   Current traffic light systems use a fixed time...   \n",
       "3   The aim of this article is to present the gene...   \n",
       "21  In this paper, we present the powerful scheme ...   \n",
       "31  With the continuous improvement of human livin...   \n",
       "\n",
       "                                             sections  num_sentences  \\\n",
       "1   {'Introduction': 'Clustering is a descriptive ...             76   \n",
       "2   {'Introduction': '', 'Background': 'Traffic co...             99   \n",
       "3   {'Introduction': 'Adaptive Hypermedia Educatio...            203   \n",
       "21  {'Introduction': 'In telecommunication and sto...             60   \n",
       "31  {'Introduction': 'From 1990 to 2017, the morbi...            124   \n",
       "\n",
       "    sent_by_page partition                                               text  \\\n",
       "1      25.333333      test  Through Social media, people are able to write...   \n",
       "2      14.142857      test  Current traffic light systems use a fixed time...   \n",
       "3      15.615385      test  The aim of this article is to present the gene...   \n",
       "21     15.000000      test  In this paper, we present the powerful scheme ...   \n",
       "31     24.800000      test  With the continuous improvement of human livin...   \n",
       "\n",
       "                                            sentences  \\\n",
       "1   [Through Social media, people are able to writ...   \n",
       "2   [Current traffic light systems use a fixed tim...   \n",
       "3   [The aim of this article is to present the gen...   \n",
       "21  [In this paper, we present the powerful scheme...   \n",
       "31  [With the continuous improvement of human livi...   \n",
       "\n",
       "                                  sentences_tokenized  \\\n",
       "1   [[social, media, people, able, write, short, m...   \n",
       "2   [[current, traffic, light, systems, use, fixed...   \n",
       "3   [[aim, article, present, general, architecture...   \n",
       "21  [[paper, present, powerful, scheme, zsismp, zi...   \n",
       "31  [[continuous, improvement, human, living, cond...   \n",
       "\n",
       "                                  sentence_embeddings  \n",
       "1   [[0.25343436, 0.012410111, -0.34438372, 0.3019...  \n",
       "2   [[0.08013701, -0.043167926, 0.019797344, 0.273...  \n",
       "3   [[0.13080898, -0.11806064, -0.047516536, 0.097...  \n",
       "21  [[0.22547568, 0.05606357, -0.1207298, 0.160863...  \n",
       "31  [[-0.21172313, 0.35368863, -0.16834831, 0.2698...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('./test_papers_word2vec.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
